<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>aml on jemoka.com</title><link>https://www.jemoka.com/tags/aml/</link><description>Recent content in aml on jemoka.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://www.jemoka.com/tags/aml/index.xml" rel="self" type="application/rss+xml"/><item><title>AML: Dipping into PyTorch</title><link>https://www.jemoka.com/posts/kbhaml_dipping_into_pytorch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.jemoka.com/posts/kbhaml_dipping_into_pytorch/</guid><description>Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.
Getting Started To get started, let&amp;rsquo;s open a colab and import Torch!</description></item><item><title>AML: It Takes Two</title><link>https://www.jemoka.com/posts/kbhaml_it_takes_two/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.jemoka.com/posts/kbhaml_it_takes_two/</guid><description>Hello everyone! It&amp;rsquo;s April, which means we are ready again for a new unit. Let&amp;rsquo;s dive in.
You know what&amp;rsquo;s better than one neural network? TWO!!! Multi-modal approaches&amp;mdash;making two neural networks interact for a certain result&amp;mdash;dominate many of the current edge of neural network research. In this unit, we are going to introduce one such approach, Generative Adversarial Networks (GAN), but leave you with some food for thought for other possibilities for what training multiple networks together can do.</description></item><item><title>AML: REINFORCE(ment learning)</title><link>https://www.jemoka.com/posts/kbhaml_reinforce/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.jemoka.com/posts/kbhaml_reinforce/</guid><description>Woof. As I begin to write this I should add that this unit is going to be conceptually dense. Though we are teaching one particular algorithm (incidentally, named, REINFORCE), the world of reinforcement learning is build by one, if not many, very advanced treatments in maths.
So if anything, I would focus on getting the conceptual flavor of how these problems are formulated and discuses. If you can be along for the mathematical and algorithmic journey, then even better &amp;mdash; but by no means required or expected&amp;hellip; There&amp;rsquo;s still lots for all of us to learn together.</description></item><item><title>AML: Time to Convolve</title><link>https://www.jemoka.com/posts/kbhaml_time_to_convolve/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.jemoka.com/posts/kbhaml_time_to_convolve/</guid><description>Welcome back! I think, over the last few days, we have been hyping up convolutional neural networks enough such that you are probably ready to dive right in. So&amp;hellip; Let&amp;rsquo;s, uh, motivate it first!
Why do we use a CNN? Let&amp;rsquo;s think of a toy problem to play with. Given a pattern made using two colours (let&amp;rsquo;s name them a and b, or perhaps black and white), let&amp;rsquo;s classify whether it is the &amp;ldquo;zebra&amp;rdquo; pattern&amp;quot; or the &amp;ldquo;checkerboard&amp;rdquo; pattern.</description></item></channel></rss>