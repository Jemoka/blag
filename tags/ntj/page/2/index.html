<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>ntj</title><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css><link rel=alternate type=application/rss+xml href=/tags/ntj/index.xml title=jemoka.com></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><main><div id=title><h1><span class=listhash>#</span>ntj</h1></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhluz_2021/"'><span class=top><h1><a class=noannot>Luz 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.1101/2021.03.24.21254263
One-Liner Review paper presenting the \(ADReSS_o\) challenge and current baselines for three tasks
Notes Three tasks + state of the art:
Classification of AD: accuracy \(78.87\%\) Prediction of MMSE score: RMSE \(5.28\) Prediction of cognitive decline: accuracy \(68.75\%\) Task 1 AD classification baseline established by decision tree with late fusion
(LOOCV and test)
Task 2 MMSE score prediction baseline established by grid search on parameters.
SVR did best on both counts; results from either model are averaged for prediction.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmahajan_2021/"'><span class=top><h1><a class=noannot>Mahajan 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.3389/fnagi.2021.623607
One-Liner Trained a bimodal model on speech/text with GRU on speech and CNN-LSTM on text.
Novelty A post-2019 NLP paper that doesn&rsquo;t use transformers! (so faster (they used CNN-LSTM) lighter easier) &ldquo;Our work sheds light on why the accuracy of these models drops to 72.92% on the ADReSS dataset, whereas, they gave state of the art results on the DementiaBank dataset.&rdquo; Notable Methods Bi-Modal audio and transcript processing vis a vi Shah 2021, but with a CNN-LSTM and GRU on the other side.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhlindsay_2021/"'><span class=top><h1><a class=noannot>Lindsay 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.3389/fnagi.2021.642033
One-Liner Proposed cross-linguistic markers shared for AD patients between English and French; evaluated features found with standard ML.
Novelty Multi-lingual, cross-linguistic analysis.
Notable Methods Looked at common patters between the two languages Linguistic results scored by IUs on CTP task Key Figs Figure 1 This figure tells us the various approaches measured.
Table 2 Here&rsquo;s a list of semantic features extracted
Table 3 Here&rsquo;s a list of NLP features extracted.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhlaguarta_2021/"'><span class=top><h1><a class=noannot>Laguarta 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.3389/fcomp.2021.624694
One-Liner Proposed a large multimodal approach to embed auditory info + biomarkers for baseline classification.
Novelty Developed a massively multimodal audio-to-embedding correlation system that maps audio to biomarker information collected (mood, memory, respiratory) and demonstrated its ability to discriminate cough results for COVID. (they were looking for AD; whoopsies)
Notable Methods Developed a feature extraction model for AD detection named Open Voice Brain Model Collected a dataset on people coughing and correlated it with biomarkers Key Figs Figure 2 This is MULTI-MODAL as heck</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmartinc_2021/"'><span class=top><h1><a class=noannot>Martinc 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.3389/fnagi.2021.642647
One-Liner Combined bag-of-words on transcript + ADR on audio to various classifiers for AD; ablated BERT&rsquo;s decesion space for attention to make more easy models in the future.
Novelty Pre-processed each of the two modalities before fusing it (late fusion) Archieved \(93.75\%\) accuracy on AD detection The data being forced-aligned and fed with late fusion allows one to see what sounds/words the BERT model was focusing on by just focusing on the attention on the words Notable Methods Used classic cookie theft data bag of words to do ADR but for words multimodality but late fusion with one (hot-swappable) classifier Key Figs How they did it This is how the combined the forced aligned (:tada:) audio and transcript together.</span></div><ul class="pagination pagination-default"><li class=page-item><a href=/tags/ntj/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/tags/ntj/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/tags/ntj/ aria-label="Page 1" class=page-link role=button>1</a></li><li class="page-item active"><a aria-current=page aria-label="Page 2" class=page-link role=button>2</a></li><li class=page-item><a href=/tags/ntj/page/3/ aria-label="Page 3" class=page-link role=button>3</a></li><li class=page-item><a href=/tags/ntj/page/4/ aria-label="Page 4" class=page-link role=button>4</a></li><li class=page-item><a href=/tags/ntj/page/3/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/tags/ntj/page/4/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer><p id=footer>&copy; 2019-2022 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>