[{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhistudio_meeting_nodes/","tags":null,"title":""},{"categories":null,"contents":"Separated qubits don\u0026rsquo;t really like to interact. Instead, then, we just make them bigger and control them at the same time. We can implement gates via a sequence of pulses. If you work with interacting qubits a lot, you will end up with the APR Paradox.\nIf you take two qubits, and move them though two gates, you essentially will get entangled results.\nTo make this works, you will need to take some probability. Know correlation, expectation, etc.\n","permalink":"https://www.jemoka.com/posts/kbhmaking_qubits_interact/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhpoint_estimate/","tags":null,"title":""},{"categories":null,"contents":" \\(A\\) does all the asking, \\(B\\) has all the decision making power Population \\(A\\)\u0026rsquo;s match never goes up at best, they stay the same Population \\(B\\)\u0026rsquo;s match can never go down. At worse, they stay the same. Population \\(A\\) always ends up with the highest-preferred person in their realm of possibility Population \\(B\\) always ends up with the lowest-preferred person in their realm of possibility ","permalink":"https://www.jemoka.com/posts/kbhproperties_of_the_stable_matching_algorithm/","tags":null,"title":""},{"categories":null,"contents":" \u0026ldquo;Are the nodes system independent of the class system?\u0026rdquo; Does the model require a set of L2 class? Can we build the model to take advantage of as many 10* things as possible? A preso Demo of a kid moving through MVP vis a vis advantage over just taking all classes Naming skills that would go on the graph Figuring: comparability with flattening like in a L1 system ","permalink":"https://www.jemoka.com/posts/kbhrnn_notes/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrural_hospitals_problem/","tags":null,"title":""},{"categories":null,"contents":"The Stable Matching Problem is Wes Chao\u0026rsquo;s favourite algorithm.\nConsider two populations, \\(A\\) and \\(B\\), who want to form paired relationships between a person \\(A\\) and \\(B\\). \\(A_i\\) has a list of their ranked order matches (I want to be paired with \\(B_1\\) most, \\(B_4\\) second, etc.), and so does \\(B_i\\) (I want to be paired with \\(A_4\\) most \\(A_9\\) second, etc.)\nWe want to discover a stable matching, where pairs are most unwilling to move. We can solve it using the stable matching algorithm.\nNueva Invention Studio speed-dating noises?\napplications of the stable matching problem Dating Applying to college Both of these are high-stress situations, especially if you are doing asking You can mathematically prove that person doing the asking gets the best result Hence, it shows us that the best possible outcomes go to the people who are willing to ask and get rejected.\nextensions to the stable matching problem the stable matching problem can be extended to the rural hospitals problem, which is slightly better.\n","permalink":"https://www.jemoka.com/posts/kbhstable_matching_problem/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhz_score/","tags":null,"title":""},{"categories":null,"contents":"\\begin{align} v+(-1)v \u0026amp;= (1+(-1))v \\\\ \u0026amp;= 0v \\\\ \u0026amp;= 0 \\end{align}\nAs \\((-1)v=0\\), \\((-1)v\\) is the additive identity of \\(v\\) which we defined as \\(-v\\) \\(\\blacksquare\\).\n","permalink":"https://www.jemoka.com/posts/kbh1v_1/","tags":null,"title":"-1v=-v"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhq/","tags":null,"title":":q"},{"categories":null,"contents":"\\begin{align} 0v \u0026amp;= (0+0)v \\\\ \u0026amp;= 0v+0v \\end{align}\nGiven scalar multiplication is closed, \\(0v \\in V\\), which means \\(\\exists -0v:0v+(-0v)=0\\). Applying that to both sides:\n\\begin{equation} 0 = 0v\\ \\blacksquare \\end{equation}\nThe opposite proof of \\(\\lambda 0=0\\) but vectors work the same exact way.\n","permalink":"https://www.jemoka.com/posts/kbhzero_times_vector/","tags":null,"title":"0v=0"},{"categories":null,"contents":"eigenvalue is the scalar needed to scale the basis element of a one dimensional invariant subspace of a Linear Map to represent the behavior of the map:\n\\begin{equation} Tv = \\lambda v \\end{equation}\nNote we require \\(v \\neq 0\\) because otherwise all scalars count.\neigenvector is a vector that forms the basis list of length 1 of that 1-D invariant subspace under \\(T\\).\n\u0026ldquo;operators own eigenvalues, eigenvalues own eigenvectors\u0026rdquo;\nWhy is eigenvalue consistent per eigenvector? Because a linear map has to act on the same way to something\u0026rsquo;s basis as it does to the whole space.\nMotivation Take some subspace \\(U \\subset V\\):\n\\begin{equation} U = \\{\\lambda v\\ |\\ \\lambda \\in \\mathbb{F}, v \\in V\\} = span(v) \\end{equation}\nNow, if \\(T|_{U}\\) is an operator on \\(U\\), \\(U\\) would be an invariant subspace of \\(T\\) of dimension 1 (its basis being the list \\(\\{v\\}\\)).\nTherefore, for some vector \\(v \\in U\\) (basically like various scalings of \\(v\\)), \\(T\\) will always send back to \\(U\\) so we can represent it yet again with another scalar on \\(v\\), like \\(\\lambda v\\).\nIn this case, then, we can write that:\n\\begin{equation} Tv = \\lambda v \\end{equation}\nAnd then the usual definition of eigenvalues persist.\nconstituents linear map \\(T \\in \\mathcal{L}(V)\\) vector \\(v \\in V\\), such that \\(v \\neq 0\\) scalar \\(\\lambda \\in \\mathbb{F}\\) requirements If there exists \\(v \\in V\\) such that \\(v\\neq 0\\) and:\n\\begin{equation} Tv = \\lambda v \\end{equation}\nthen, \\(\\lambda\\) is called an eigenvalue, and \\(v\\) the eigenvector.\nadditional information properties of eigenvalues Suppose \\(V\\) in finite-dimensional, \\(T \\in \\mathcal{L}(V)\\) and \\(\\lambda \\in \\mathbb{F}\\), then:\n\\(\\lambda\\) is an eigenvalue of \\(T\\) \\(T - \\lambda I\\) is not injective \\(T - \\lambda I\\) is not surjective \\(T - \\lambda I\\) is not invertable Showing one shows all.\nProof:\n\\(1 \\implies 2\\) Suppose \\(\\lambda\\) is an eigenvalue of \\(T\\). Then, we have some \\(v \\in V\\) such that:\n\\begin{equation} Tv = \\lambda v \\end{equation}\nNow:\n\\begin{align} \u0026amp;Tv = \\lambda v \\\\ \\Rightarrow\\ \u0026amp; Tv - \\lambda v = 0 \\\\ \\Rightarrow\\ \u0026amp; Tv - \\lambda Iv = 0 \\\\ \\Rightarrow\\ \u0026amp; (T-\\lambda I)v = 0 \\end{align}\nthe last step by \\((T+S)v = Tv+Sv\\), the property of the vector space of \\(\\mathcal{L}(V)\\) (or any \\(\\mathcal{L}\\)).\nAnd therefore, \\(v \\in null\\ (T-\\lambda I)\\), and \\(v\\neq 0\\). And so \\(null\\ (T-\\lambda I) \\neq \\{0\\}\\) and so \\(T-\\lambda I\\) is not injective, as desired.\nThe reverse of this result shows the opposite direction that \\(1 \\implies 2\\).\nThe others \\(I \\in \\mathcal{L}(V)\\), \\(T \\in \\mathcal{L}(V)\\), \\(\\mathcal{L}(V)\\) is closed, so \\((T - \\lambda I) \\in \\mathcal{L}(V)\\), and so it is an operator. Having 2) implies all other conditions of non-injectivity, non-surjectivity, non-invertiblility by injectivity is surjectivity in finite-dimensional operators\nlist of eigenvectors are linearly independent Let \\(T \\in \\mathcal{L}(V)\\), suppose \\(\\lambda_{j}\\) are distinct eigenvalues of \\(T\\), and \\(v_1, \\ldots, v_{m}\\) the corresponding eigenvectors, then \\(v_1, \\ldots, v_{m}\\) is linearly independent.\nproof:\nWe will show this by contradiction. Suppose \\(v_1, \\ldots, v_{m}\\) are linearly dependent; then, by the Linear Dependence Lemma, \\(\\exists v_{j}\\) such that:\n\\begin{equation} v_{j} \\in span(v_1, \\dots, v_{j-1}) \\end{equation}\nMeaning:\n\\begin{equation} v_{j} = a_1v_1 + \\dots + a_{j-1}v_{j-1} \\end{equation}\nGiven the list is a list of eigenvalues, we can apply \\(T\\) to both sides to get:\n\\begin{equation} \\lambda_{j}v_{j} = a_1\\lambda_{1}v_1 + \\dots + a_{j-1}\\lambda_{j-1}v_{j-1} \\end{equation}\nWe can also get another definition for \\(\\lambda_{j} v_{j}\\) by simply multiplying the definition for \\(v_{j}\\) above by \\(\\lambda_{j}\\):\n\\begin{align} \u0026amp;v_{j} = a_1v_1 + \\dots + a_{j-1}v_{j-1}\\ \\text{from above} \\\\ \\Rightarrow\\ \u0026amp; \\lambda_{j} v_{j} = a_1\\lambda_{j}v_1 + \\dots + a_{j-1}\\lambda_{j}v_{j-1} \\end{align}\nNow, subtracting our two definitions of \\(\\lambda_{j} v_{j}\\), we get:\n\\begin{equation} 0 = a_1 (\\lambda_{j} - \\lambda_{1})v_{1} + \\dots +a_{j-1} (\\lambda_{j} - \\lambda_{j-1})v_{j-1} \\end{equation}\nRecall now that the eigenvalue list \\(\\lambda_{j}\\) are distinct. This means all \\(\\lambda_{j} - \\lambda_{k \\neq j} \\neq 0\\). No \\(v_{j} =0\\); so if we choose the smallest positive integer for \\(j\\), the list before it \\(v_1, \\dots, v_{j-1}\\) is linearly independent (as no value in that list would satisfy the Linear Dependence Lemma). This makes \\(a_{j} =\\dots =a_{j-1} = 0\\).\nAnd yet, substituting this back into the expression for \\(v_{j}\\), we have \\(v_{j} = 0\\), reaching contradiction. So therefore, the list of eigenvectors are linearly independent. \\(\\blacksquare\\)\noperators on finite dimensional V has at most dim V eigenvalues As a corollary of the above result, suppose \\(V\\) is finite dimensional; then, each operator on \\(V\\) has at most \\(dim\\ V\\) distinct eigenvalues because their eigenvectors form an linearly independent list and length of linearly-independent list \\(\\leq\\) length of spanning list.\neigenspaces are disjoint the eigenspaces of a Linear Map form a direct sum:\nproof:\nCorollary of result above. Because eigenvectors (i.e. bases) from distinct eigenspaces are linearly independent. So the only way to write \\(0\\) is by taking each to \\(0\\). So by taking the bases all to \\(0\\), you take the \\(0\\) vector from each space, which shows that the eigenspaces are a direct sum. \\(\\blacksquare\\)\nfinding eigenvalues with actual numbers \\begin{equation} \\lambda_{j} \\in Spec(T) \\Rightarrow det(\\lambda_{j}I-T) = 0 \\end{equation}\nThe right polynomial \\(det(\\lambda_{j} I-T) = 0\\) is named the \u0026ldquo;characteristic polynomial.\u0026rdquo;\nnatural choordinates of a map Given the eigenvectors \\((x+,y+), (x-,y-)\\), we can change coordinates of your matrix into the natural choordinates.\n\\begin{equation} A = \\begin{pmatrix} x+ \u0026amp; x- \\\\y+ \u0026amp; y- \\end{pmatrix} \\begin{pmatrix} \\lambda+ \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda- \\end{pmatrix} \\begin{pmatrix} x+ \u0026amp; x- \\\\y+ \u0026amp; y- \\end{pmatrix}^{-1} \\end{equation}\nThis makes scaling matricides much much easier. If you think about multiplying the above matrix \\(n\\) times, the inverse and non-inverse cancells out.\n","permalink":"https://www.jemoka.com/posts/kbheigenvalue/","tags":null,"title":"1-d invariant subspace"},{"categories":null,"contents":" New Deal ","permalink":"https://www.jemoka.com/posts/kbh1980s_political_alignment/","tags":null,"title":"1980s Political Alignment"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbh1a/","tags":null,"title":"1a"},{"categories":null,"contents":"Suppose \\(T\\) is a function from \\(V\\) to \\(W\\). Let the \u0026ldquo;graph\u0026rdquo; of \\(T\\) be the subset of \\(V \\times W\\) such that:\n\\begin{equation} graph\\ T = \\{(v,Tv) \\in V \\times W \\mid v \\in V\\} \\end{equation}\nShow that \\(T\\) is a linear map IFF the graph of \\(T\\) is a subspace of \\(V \\times W\\).\nReview: A Linear Map Recall that a function \\(T: V \\to W\\) is called a linear map if it is a map that\u0026hellip;\nis additive: so \\(Tv + Tu = T(v+u): v,u \\in V\\) is homogeneous, so \\(\\lambda Tv = T\\lambda v: \\lambda \\in \\mathbb{F}, v \\in V\\) Given Graph is Subspace Given the graph of \\(T\\) is a subspace of \\(V \\times W\\), we desire that the function \\(T\\) is a linear map and therefore additive and homogeneous.\nBy declaration before, \\(graph\\ T\\) is a subspace, meaning it would be closed under adddition and scalar multiplication. We will use this fact to show that \\(T\\) follows the properties of a linear map.\nAdditivity We first desire that \\(T\\) is additive, that is, for \\(v,u \\in V\\), we desire \\(Tv + Tu = T(v+u)\\).\nLet \\(v,u \\in V\\), and let \\(a,b \\in graph\\ T\\) declared as follows:\n\\begin{equation} \\begin{cases} a = (v,Tv) \\in V \\times W \\\\ b = (u,Tu) \\in V \\times W \\end{cases} \\end{equation}\nWe are given that \\(graph\\ T\\) is a subspace of \\(T\\). As such, it is closed under addition; meaning, the sum of two elements from the space must remain in the space. Therefore:\n\\begin{equation} (v, Tv) + (u,Tu) = (v+u, Tv+Tu) \\in graph\\ T \\end{equation}\nAnd now, the latter being in \\(graph\\ T\\) implies that \\(\\exists\\) some \\(c \\in graph\\ T\\), \\(n \\in V\\) such that:\n\\begin{equation} c := (n, Tn) = (v+u, Tv+Tu) \\end{equation}\nTaking the latter equivalence and solving for \\(n\\), we have that \\(n = v+u\\). And so, we have that:\n\\begin{equation} (v+u, T(v+u)) = (v+u, Tv+Tu) \\end{equation}\nTherefore, \\(T(v+u) = Tv+Tu\\), as desired.\nHomogeneity We now desire that \\(T\\) is homogeneous. That is, for \\(v \\in V, \\lambda \\in \\mathbb{F}\\), we desire \\(\\lambda Tv = T\\lambda v\\).\nLet \\(v \\in V\\), \\(\\lambda \\in \\mathbb{F}\\), and \\(a \\in graph\\ T\\) declared as follows:\n\\begin{equation} a = (v, Tv) \\in V \\times W \\end{equation}\nBy the same logic before, \\(graph\\ T\\) is closed under scalar multiplication; meaning, the product of en element from the space to a scalar remain in the space. Therefore:\n\\begin{equation} \\lambda (v,Tv) = (\\lambda v, \\lambda Tv) \\in graph\\ T \\end{equation}\nThe latter being in \\(graph\\ T\\) implies that \\(\\exists\\) some \\(c \\in graph\\ T\\), \\(n \\in V\\) such that:\n\\begin{equation} c :=(n,Tn) = (\\lambda v, \\lambda Tv) \\end{equation}\nTaking the latter equivalence and solving for \\(n\\), we have \\(n = \\lambda v\\). And so, we have:\n\\begin{equation} (\\lambda v, T \\lambda v) = (\\lambda v, \\lambda Tv) \\end{equation}\nAnd therefore, \\(T\\lambda v = \\lambda Tv\\), as desired.\nHaving shown that \\(T\\) is now both additive and homogeneous, we have that \\(T\\) is a linear map, as desired.\nGiven \\(T\\) is a Linear Map We will essentially prove the previous condition backwards.\nWe are given that the graph of \\(T\\) is a subset of \\(V \\times W\\), and that \\(T: V \\to W\\) is a linear map. We desire that the graph of \\(T\\) is a subspace of \\(V \\times W\\).\nRecall that to show that a subset is a subspace, on simply has to show that it has closed operations and that it contains the additive identity.\nAdditive Identity Recall that the additive identity in \\(V \\times W\\) is the tuple that\u0026rsquo;s identically \\((0,0) \\in V \\times W\\).\nAs \\(V\\) is a vector space, \\(0 \\in V\\). Any linear map will send \\(0\\) to \\(0\\). Therefore, \\(T 0 = 0\\).\nTherefore, construct \\(a \\in graph\\ T\\):\n\\begin{equation} a = (0, T 0) \\in V \\times W = (0, 0) \\end{equation}\nBy construction, we have shown that the additive identity of \\(V \\times W\\) is in \\(graph\\ T\\).\nClosure of Addition Given WLOG \\(a,b \\in graph\\ T\\), we desire that \\(a+b \\in graph\\ T\\).\nLet \\(v,u \\in V\\), and let \\(a,b \\in graph\\ T\\) declared as follows:\n\\begin{equation} \\begin{cases} a = (v,Tv) \\in V \\times W \\\\ b = (u,Tu) \\in V \\times W \\end{cases} \\end{equation}\nNow:\n\\begin{equation} a+b = (v,Tv) + (u+Tu) = (v+u, Tv+Tu) \\end{equation}\nGiven \\(T\\) is a linear map, we have WLOG \\(Tv+Tu = T(v+u)\\). And therefore:\n\\begin{equation} a+b = (v,Tv) + (u+Tu) = (v+u, Tv+Tu) = (v+u, T(v+u)) \\in \\{(v,Tv) \\mid v \\in V\\} \\end{equation}\nHence, \\(graph\\ T\\) is closed under addition.\nClosure of Scalar Multiplication Given WLOG \\(a \\in graph\\ T, \\lambda \\in \\mathbb{F}\\), we desire that \\(\\lambda a \\in graph\\ T\\).\nLet \\(v \\in V, \\lambda \\in \\mathbb{F}\\), and let \\(a \\in graph\\ T\\) declared as follows:\n\\begin{equation} a = (v,Tv) \\in V \\times W \\end{equation}\nNow:\n\\begin{equation} \\lambda a = \\lambda (v,Tv) = (\\lambda v, \\lambda Tv) \\end{equation}\nGiven \\(T\\) is a linear map, we have WLOG \\(\\lambda Tv = T\\lambda v\\). And therefore:\n\\begin{equation} \\lambda a = \\lambda (v,Tv) = (\\lambda v, \\lambda Tv) = (\\lambda v, T \\lambda v)\\in \\{(v,Tv) \\mid v \\in V\\} \\end{equation}\nHence, \\(graph\\ T\\) is closed under scalar multiplication.\nHaving shown \\(graph\\ T\\) to be closed under addition and scalar multiplication, as well as containing the additive identity, we see that it is a subspace of \\(V \\times W\\) of which it is a subset.\nHaving shown both directions of the proof, \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_3_e_problem_1/","tags":null,"title":"3.E Problem 1"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhaaa/","tags":null,"title":"AAA"},{"categories":null,"contents":"Welcome to the personal site of Houjun \u0026ldquo;Jack\u0026rdquo; Liu.\nI\u0026rsquo;m on the blaggosphere as @jemokajack and, far more frequently, u/jemoka and @jemoka.\nWho\u0026rsquo;s this guy? I am a human interested in linguistic analysis, L2 learning, and user interfaces. AGI \u0026amp; Emacs are cool. I run Condution, Shabang, and MODAP, do research in NLP and education pedagogy, direct Science Friday streams, captain an entrepreneurship studio, and recently began working for Brian MacWhinney on projects in linguistics. Oh, right, I also go to school at Nueva, a small high school in San Mateo. Apparently I\u0026rsquo;m being sent away to a farm next year.\nNeed to catch me? Email me at houjun@jemoka.com. Please do email me, I actually check.\nRecent Projects Take a look at my GitHub profile for programming projects. For larger scale things, take a look at the Projects Index on this site.\nNotes This site also contains the vast majority of my course notes. It is a organized in a zettlekasten format. To begin exploring, why don\u0026rsquo;t you check out Nueva Courses Index. Accompanying this index is the collection of assignments found here which a few friends has collected together.\njklsnt Some friends and I started a small collection of fun internets that we made. Check it out!.\nHow do I know you are you? Good question! gpg --locate-keys houjun@jemoka.com. Note that GPG don\u0026rsquo;t actually check fingerprints you received so do that yourself. The same key should also be found at gpg --keyserver pgp.mit.edu --recv-keys 1807A0C6 if my WKD CNAME thing go down.\nBugga Bugga Bontehu? Sometimes I use this domain as a downlink to fastcalculator to friends and coworkers. To achieve this, here are two links you could click on that I don\u0026rsquo;t always promise do anything: oliver and socks.\n","permalink":"https://www.jemoka.com/posts/kbhindex/","tags":null,"title":"About"},{"categories":null,"contents":"To determine\n","permalink":"https://www.jemoka.com/posts/kbhaccounting_price/","tags":null,"title":"accounting price"},{"categories":null,"contents":"Capecitabmine =\u0026gt; 5-Fluoropyrimidine =\u0026gt; Cancer cell death.\n","permalink":"https://www.jemoka.com/posts/kbhaction_of_capecitabmine/","tags":null,"title":"action of Capecitabmine"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhaction_research/","tags":null,"title":"action research"},{"categories":null,"contents":"Comes from doi.org/10.3389/fcomp.2020.00001\nADR is a vectorization/encoding technique whereby time-series data is segmented, clustered via solf-organizing maps, and the centroids of the clusters are used as the encoding\n","permalink":"https://www.jemoka.com/posts/kbhactive_data_representation/","tags":null,"title":"Active Data Representation"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhactive_listening/","tags":null,"title":"active listening"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhactive_recall/","tags":null,"title":"active recall"},{"categories":null,"contents":"Operation that adds elements in a set\nconstituents A set \\(V\\) Each non-necessarily-distinct elements \\(u,v \\in V\\) requirements addition on a set \\(V\\) is defined by a function that assigned an element named \\(u+v \\in V\\) (its closed), \\(\\forall u,v\\in V\\)\nadditional information See also addition in \\(\\mathbb{F}^n\\)\n","permalink":"https://www.jemoka.com/posts/kbhadding/","tags":null,"title":"adding"},{"categories":null,"contents":"The additive identity allows another number to retain its identity after adding. That is: there exists an element \\(0\\) such that \\(v+0=v\\) for whatever structure \\(v\\) and addition \\(+\\) you are working with.\n","permalink":"https://www.jemoka.com/posts/kbhadditive_identity/","tags":null,"title":"additive identity"},{"categories":null,"contents":"Assume for the sake of contradiction \\(\\exists\\ 0, 0\u0026rsquo;\\) both being additive identities in vector space \\(V\\).\nTherefore:\n\\begin{equation} 0+0\u0026rsquo; = 0\u0026rsquo; +0 \\end{equation}\ncommutativity.\nTherefore:\n\\begin{equation} 0+0\u0026rsquo; = 0 = 0\u0026rsquo;+0 = 0' \\end{equation}\ndefn. of identity.\nHence: \\(0=0\u0026rsquo;\\), \\(\\blacksquare\\).\n","permalink":"https://www.jemoka.com/posts/kbhadditive_identity_is_unique_in_a_vector_space/","tags":null,"title":"additive identity is unique in a vector space"},{"categories":null,"contents":"Take a vector \\(v \\in V\\) and additive inverses \\(a,b \\in V\\).\n\\begin{equation} a+0 = a \\end{equation}\ndefn. of additive identity\n\\begin{equation} a+(v+b) = a \\end{equation}\ndefn. of additive inverse\n\\begin{equation} (a+v)+b = a \\end{equation}\nassociativity\n\\begin{equation} 0+b = a \\end{equation}\ndefn. of additive inverse\n\\begin{equation} b=a\\ \\blacksquare \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhadditive_inverse_is_unique_in_a_vector_space/","tags":null,"title":"additive inverse is unique in a vector space"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhadhd/","tags":null,"title":"ADHD"},{"categories":null,"contents":"adMe: absorbtion, distribution, metabolism, excretion.\nPharmacology treatment of diseases. The microbiome regulates metabolism.\n","permalink":"https://www.jemoka.com/posts/kbhadme/","tags":null,"title":"adMe"},{"categories":null,"contents":"ADReSS Challenge is a Alzheimer\u0026rsquo;s Dementia Recognition challenge from the data available on DementiaBank.\n","permalink":"https://www.jemoka.com/posts/kbhadress_challenge/","tags":null,"title":"ADReSS Challenge"},{"categories":null,"contents":"The ADReSS Literature Survey is a literature survey for the results published during the ADReSS Challenge.\nAntonsson 2021: disfluency + SVF features trained on SVM: lexical \u0026gt; narrative qual. Chlasta 2021: features extracted from VGGish on SVM; also trained new CNN from .wav. Sadeghian 2021: Used GA for feature sel., achieved 94% w/ MMSE alone; dev\u0026rsquo;d ASR tool. Martinc 2021: CBOW (text) + ADR (sound) late fusion\u0026rsquo;d to a BERT, ablated for features. Meghanani 2021: spontaneous speech transcripts with fastText and CNN; 83.33% acc. Yuan 2021: ERNIE on transcripts with pause encoding; 89.6% acc. Jonell 2021: Developed a kitchen sink of diag. tools and correlated it with biomarkers. Laguarta 2021: multimodel (OVBM) to embed auditory info + biomarkers for clsf. Shah 2021: late fusion of n-gram and OpenSMILE on std. classifiers. Lindsay 2021: Cross-linguistic markers shared for AD patients between English and French. Zhu 2021: late fusion of CTP task for AD clsf. w/ transf., mobilenet, yamnet, mockingjay. Guo 2021: WLS data to augment CTP from ADReSS Challenge and trained it on a BERT. Balagopalan 2021: lexo. and synt. features trained on a BERT and other models. Mahajan 2021: a bimodal model on speech/text with GRU on speech and CNN-LSTM on text. Parvin 2020: excercize scheme effects on theta/alpha ratio and Brain wave frequency. Luz 2021: review paper presenting the ADReSSo challenge and current baselines. From Meghanani 2021, a review:\n","permalink":"https://www.jemoka.com/posts/kbhadress_literature_survey/","tags":["index"],"title":"ADReSS Literature Survey Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhadvertising/","tags":null,"title":"advertising"},{"categories":null,"contents":"an affine subset of \\(V\\) is a subset of \\(V\\) that is the sum of a vector and one of its subspace; that is, an affine subset of \\(V\\) is a subset of \\(V\\) of the form \\(v+U\\) for \\(v \\in V\\) and subspace \\(U \\subset V\\).\nfor \\(v \\in V\\) and \\(U \\subset V\\), an affine subset \\(v+U\\) is said to be parallel to \\(U\\).\nthat is, an affine subset for \\(U \\subset V\\) and \\(v \\in V\\):\n\\begin{equation} v + U = \\{v+u : u \\in U\\} \\end{equation}\nadditional information two affine subsets parallel to \\(U\\) are either equal or disjoint Suppose \\(U\\) is a subspace of \\(V\\); and \\(v,w \\in V\\), then, if one of the following is true all of them are true:\n\\(v-w \\in U\\) \\(v+U = w+U\\) \\((v+U) \\cap (w+U) \\neq \\emptyset\\) \\(1 \\implies 2\\) Given \\(v-w \\in U\\)\u0026hellip;.\nFor an element in \\(v+U\\), we have that \\(v+u = (w-w)+v+u = w+((v-w)+u) \\in w + U\\). This is because \\(U\\) is closed so adding \\(v-w \\in U\\) and \\(u\\) will remain being in \\(U\\). \\(w-w=0\\) just by everything being in \\(V\\).\nWe now have \\(v+u \\in w+U\\ \\forall u \\in U\\); we now can reverse the argument to argue in a similar fashion that \\(w+u \\in v+U\\ \\forall u \\in U\\). So, we have that \\(v+U \\subset w+U\\) and \\(w+U \\subset v+U\\). So \\(v+U = w+U\\), as desired.\n\\(2 \\implies 3\\) By definition of \\(v+U=w+U\\) as long as \\(v+U\\) and \\(w+U\\) is not empty sets, which they can\u0026rsquo;t be because \\(U\\) is a vector space so guaranteed nonempty.\n\\(3\\implies 1\\) Given \\((v+U) \\cap (w+U) \\neq \\emptyset\\), we have that there exists some \\(u_1, u_2 \\in U\\) such that \\(v+u_1 = w+u_2\\). Because everything here is in \\(V\\), we can add their respective inverses (\u0026ldquo;move them around\u0026rdquo;) such that: \\(v-w = u_2-u_1\\). Therefore \\(u_2-u_1 \\in U \\implies v-w \\in U\\).\n","permalink":"https://www.jemoka.com/posts/kbhparallel_linear_algebra/","tags":null,"title":"affine subset"},{"categories":null,"contents":"Agricultural Adjustment Administration is a part of the New Deal programs to support the agricultural sector and maintain supply. They regulated production of seven different crops to group increase farming income. It is very far-reaching of other parts of the economy.\nIt was ruled unconstitutional in 1936.\n","permalink":"https://www.jemoka.com/posts/kbhagricultural_adjustment_administration/","tags":null,"title":"Agricultural Adjustment Administration"},{"categories":null,"contents":"AgRP is a type of neurons that stimulates food intake.\nInhibit metacortin Activate NPY Release GABA Diet-induced obesity blunts AgRP response, and so, because AgRP plays a part in thermoregulation, diet-inducsed obesity responds less to temperature changes.\n","permalink":"https://www.jemoka.com/posts/kbhagrp/","tags":null,"title":"AgRP"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhai/","tags":null,"title":"AI"},{"categories":null,"contents":"AI Ethics is the Ethics of training AI models.\n","permalink":"https://www.jemoka.com/posts/kbhai_ethics/","tags":null,"title":"AI Ethics"},{"categories":null,"contents":"AIBridge is an introductory AI bootcamp developed and taught by Prof. Xin Liu, yours truly, and Samuel Ren in collaboration with AIFS.\nCourse website: AIBridge Course Website\nAIBridge Lecture Codealongs AIBridgeLab D1Aft AIBridgeLab D2Aft AIBridgeLab D3/D4 AIBridgeLab D4Aft oeansut\\n \\n aosntegu\\n \\n\n","permalink":"https://www.jemoka.com/posts/kbhaibridge/","tags":null,"title":"AIBridge"},{"categories":null,"contents":" Welcome to the AIBridge Course homepage.\nThe purpose of AIBridge is to bridge the gap between computer science and other disciplines. To many, working with AI might seem like an unreachable objective. However, in reality, one week is enough to get started. AIBridge will provide basic programming capability in Python and knowledge of object-oriented programming as well as the concepts behind machine learning and how to implement it using a popular toolbox, Scikit-Learn. Students work to complete a personally-defined project using techniques in AI, with data from their own research or with problems supplied by the Course. This one week course will be hosted in-person at UC Davis and will target mainly undergraduate and non-technical graduate students.\nThe course is taught by Prof. Xin Liu in collaboration with Houjun \u0026ldquo;Jack\u0026rdquo; Liu, Samuel Ren, and Albara Ah Ramli.\nEvergreen Resources Python Tutorial: W3 Schools Python Documentation: Python.org SciKit Documentation: scikit-learn.org Iris Dataset: UCI DB, or, for better user experience, scikit Wine Dataset: UCI DB Class Discord: Invite Data-Loading Cheat-Sheet: Colab When in doubt\u0026hellip;\nGoogle it! Try it! Andrew Ng\u0026rsquo;s Machine Learning Suite of Courses DONE Day 1: Python Basics On Monday, 06/27/2022, we covered the basics of Python so that we are all up to speed to perform basic ML with the Scikit Learn toolkit.\nIntroductory Remarks: Slides Lecture on Python Basics: Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Morning Lecture Notebook, Morning Lab Notebook, Afternoon Lecture Notebook, Afternoon Lab Notebook Day 1 feedback survey: Link\nDONE Day 2: OOP + Linear Models Today, we are going to cover the basic intuition and terminology behind Object Oriented Programming, as well as introduce two simple, linear approaches to Machine Learning tasks: linear regression and logistic regression.\nLecture on OOP and more on functions (morning): Slides Lecture on Linear and Logistic Regression (afternoon): Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Morning Lecture Notebook, Morning Lab Notebook, Afternoon Lab Notebook Day 2 feedback survey: Link\nDONE Day 3: Data + Classifier Today, we are going to cover data cleaning, and three more classifiers!\nLecture on data cleaning and pandas (morning): Slides Lecture on three classification algorithms (afternoon): Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Morning Lab Notebook, Afternoon Lab Notebook Day 3 feedback survey: Link\nDONE Day 4: Operations and Clustering Today, we are going to work on the validation operations tools, and talk about clustering\nLecture on training and data operations (morning): Slides Lecture on clustering and feature operations (afternoon): Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Afternoon Notebook Day 4 feedback survey: Link\nDay 5: Closing Thoughts Today, we are going to tie some loose ends with missing data, error analysis, semi supervised learning, cross validation, and ethics.\nClosing thoughts lecture (morning): Slides Final Project: AIBridge Final Project\nDay 5/Bootcamp feedback survey: Link\nOther Links and Resources Tools we use: AIBridge Packages and Tools Cleaned Wine Dataset (try cleaning it yourself before using!): Google Drive Iris Data with Temperature (don\u0026rsquo;t use without instructions, though!): Google Drive ","permalink":"https://www.jemoka.com/posts/kbhaibridge_course_website/","tags":null,"title":"AIBridge Course Website"},{"categories":null,"contents":"Part 1: ML Training Practice One of the things that makes a very good Sommelier is their ability to figure out as much details about a wine as possible with very little information.\nYou are tasked with making a Sommelier program that is able to figure both the type and quality of wine from available chemical information. Also, you have a \u0026ldquo;flavor-ater\u0026rdquo; machine that makes a linear combination of multiple chemical features together (similar to PCA), which is counted as one chemical feature after combination.\nA good Sommelier uses as little information as possible to deduce the quality and type. So, what is the best model(s) you can build for predicting quality and type of wine based on the least amount of features? What features should you choose?\nGood luck!\nPart 2: ML Project Walk-through Create your own machine learning experiement! Begin with a problem in your field; go through the available/your own data, determine what type of problem it is, and discuss why machine learning could be a good solution for the problem. Research/quantify the baselines in the field for the task (remembering our discussion on ML validation methods), and determine a list of possible features of your data.\nIf we were to help collect data together, how can we best collect a representative sample? How expensive (resources, monetary, or temporal) would it be? What are some ethical issues?\nSelect the features in the data available to you that would be most relavent (this time you are not trying to minimize the features, but select the most appropriate ones), and the model/training mechanism you think would be most appropriate.\nFinally, present your thinking! Share with us a few (1-3) slides on Friday afternoon. If you have additional time, possibly train the model on baseline data!\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_final_project/","tags":null,"title":"AIBridge Final Project"},{"categories":null,"contents":"SPOILER ALERT for future labs!! Don\u0026rsquo;t scroll down!\nWe are going to create a copy of the iris dataset with a random variance.\nimport sklearn from sklearn.datasets import load_iris Let\u0026rsquo;s load the iris dataset:\nx,y = load_iris(return_X_y=True) Because we need to generate a lot of random data, let\u0026rsquo;s import random\nimport random Put this in a df\nimport pandas as pd df = pd.DataFrame(x) df 0 1 2 3 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 .. ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 [150 rows x 4 columns] Let\u0026rsquo;s make 150 random numbers with pretty low variance:\nrandom_ns = [random.uniform(65,65.2) for _ in range(0, 150)] random_series = pd.Series(random_ns) random_series 0 65.127515 1 65.034572 2 65.123271 3 65.043985 4 65.145743 ... 145 65.036410 146 65.157172 147 65.034925 148 65.037373 149 65.042466 Length: 150, dtype: float64 Excellent. Now let\u0026rsquo;s put the two things together!\ndf[\u0026#34;temp\u0026#34;] = random_series df 0 1 2 3 temp 0 5.1 3.5 1.4 0.2 65.127515 1 4.9 3.0 1.4 0.2 65.034572 2 4.7 3.2 1.3 0.2 65.123271 3 4.6 3.1 1.5 0.2 65.043985 4 5.0 3.6 1.4 0.2 65.145743 .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 146 6.3 2.5 5.0 1.9 65.157172 147 6.5 3.0 5.2 2.0 65.034925 148 6.2 3.4 5.4 2.3 65.037373 149 5.9 3.0 5.1 1.8 65.042466 [150 rows x 5 columns] And, while we are at it, let\u0026rsquo;s make new labels\nnames = pd.Series([\u0026#34;sepal length\u0026#34;, \u0026#34;sepal width\u0026#34;, \u0026#34;pedal length\u0026#34;, \u0026#34;pedal width\u0026#34;, \u0026#34;temp\u0026#34;]) df.columns = names df sepal length sepal width pedal length pedal width temp 0 5.1 3.5 1.4 0.2 65.127515 1 4.9 3.0 1.4 0.2 65.034572 2 4.7 3.2 1.3 0.2 65.123271 3 4.6 3.1 1.5 0.2 65.043985 4 5.0 3.6 1.4 0.2 65.145743 .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 146 6.3 2.5 5.0 1.9 65.157172 147 6.5 3.0 5.2 2.0 65.034925 148 6.2 3.4 5.4 2.3 65.037373 149 5.9 3.0 5.1 1.8 65.042466 [150 rows x 5 columns] Excellent. Let\u0026rsquo;s finally get the flower results.\ndf[\u0026#34;species\u0026#34;] = y df sepal length sepal width pedal length pedal width temp species 0 5.1 3.5 1.4 0.2 65.127515 0 1 4.9 3.0 1.4 0.2 65.034572 0 2 4.7 3.2 1.3 0.2 65.123271 0 3 4.6 3.1 1.5 0.2 65.043985 0 4 5.0 3.6 1.4 0.2 65.145743 0 .. ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 2 146 6.3 2.5 5.0 1.9 65.157172 2 147 6.5 3.0 5.2 2.0 65.034925 2 148 6.2 3.4 5.4 2.3 65.037373 2 149 5.9 3.0 5.1 1.8 65.042466 2 [150 rows x 6 columns] And dump it to a CSV.\ndf.to_csv(\u0026#34;./iris_variance.csv\u0026#34;, index=False) Let\u0026rsquo;s select for the input data again:\nX = df.iloc[:,0:5] y = df.iloc[:,5] X sepal length sepal width pedal length pedal width temp 0 5.1 3.5 1.4 0.2 65.127515 1 4.9 3.0 1.4 0.2 65.034572 2 4.7 3.2 1.3 0.2 65.123271 3 4.6 3.1 1.5 0.2 65.043985 4 5.0 3.6 1.4 0.2 65.145743 .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 146 6.3 2.5 5.0 1.9 65.157172 147 6.5 3.0 5.2 2.0 65.034925 148 6.2 3.4 5.4 2.3 65.037373 149 5.9 3.0 5.1 1.8 65.042466 [150 rows x 5 columns] And use the variance threshold tool:\nfrom sklearn.feature_selection import VarianceThreshold sel = VarianceThreshold(0.1) sel.fit_transform(X) 5.1 3.5 1.4 0.2 4.9 3 1.4 0.2 4.7 3.2 1.3 0.2 4.6 3.1 1.5 0.2 5 3.6 1.4 0.2 5.4 3.9 1.7 0.4 4.6 3.4 1.4 0.3 \u0026hellip;\nAs we expected.\nAnd let\u0026rsquo;s use the select k best tool:\nfrom sklearn.feature_selection import SelectKBest, chi2 sel = SelectKBest(chi2, k=4) res = sel.fit_transform(X, y) res 5.1 3.5 1.4 0.2 4.9 3 1.4 0.2 4.7 3.2 1.3 0.2 4.6 3.1 1.5 0.2 5 3.6 1.4 0.2 5.4 3.9 1.7 0.4 4.6 3.4 1.4 0.3 5 3.4 1.5 0.2 \u0026hellip;\nAlso, as we expected. Got rid of temp.\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_iris_variance_worksheet/","tags":null,"title":"AIBridge Iris Variance Worksheet"},{"categories":null,"contents":"This is usually not needed if you are using Google Colab. If you are following the instructions provided during our lecture series, please disregard this page.\nHowever, students have expressed interest in working with their own system\u0026rsquo;s copy of Jupyter or local installation. We therefore provide a set of very tenuous instructions for installing the tools used in our session using vanilla C-Python (i.e. not anaconda/conda/miniconda.)\nPython Our tools target Python 3.8+. Use your system\u0026rsquo;s package manager to install Python at least version 3.8, or use Python Foundation\u0026rsquo;s universal installers.\nPackages Python sometimes ships pip, its packaging utility separately. Refer to your own distribution\u0026rsquo;s installation instructions if none of pip or pip3 or python -m pip or python -m pip.\nOnce your copy of pip has been identified, let\u0026rsquo;s move on to\u0026hellip;\nInstalling Packages Here are the packages we will need for our sessions:\nscikit-learn pandas numpy Along with its respective dependencies. Here\u0026rsquo;s a one-liner:\npython3 -m pip install scikit-learn pandas numpy Good luck!\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_packages/","tags":null,"title":"AIBridge Packages and Tools"},{"categories":null,"contents":"Rewa Rai Nitin Lab, Dept. of Food Sci + Tech - Davis\nWine Classification Task Whole data:\nDecision Tree: 98.46% Random Forest: 99.84% Gaussian NB: 97.08% Regression Task Feature selection with 2 best features actually improved.\nTalkthrough Detecting berry infection by leaf classification. Use FTIR spectroscopy as a means of infection classification.\nTana Hernandez PHD Student, Nitin Lab, Dept. of Food Sci + Tech - Davis\nTalkthrough Given input for reaction, predict resulting gell strength from protein+carbo+lactic acid.\nGoal to figure out what features are o predict gell formation. Use feature extraction to reduce the need of doing.\nWet lab task: use high-throughput 96 hole plates to measuring kinetics of absorborance and kinetics. In a single hour, 96 data points can be acquired.\nThen, droplet elements are added to the plates.\nModel: take feature inputs which was selected, classification on gell formation and regression for time for gell.\nJimmy Nguyen PHD Student, Nitin Lab, Dept. of Food Sci + Tech - Davis\nTalk through Need: creating plant-based products which just feels and tastes like actual meet based food.\nTask: given molecular information, classify taste based on like-product and unlike\nLuyao Ma Postdoc Researcher, Nitin Lab, Dept. of Food Sci + Tech - Davis\nTalk thought Problem: lots of antimicrobian resistance in food: on track for 10 million deaths due to antimicrobial resistance. This is caused by antibiotics given to animals, which then is given indirectly to humans. Humans gut bactorials became more more resistant to antibiotics due to antimicrobial bacterial deveolping in animal guts.\nCurrent surveilance systems for antibiotic bacteria: require centralized lab for analysis, data collection is slow, and data integration is very slow (2ish years to publish final results), protocol also changes.\nGoal: rapid in field automatic detection scheme\nExpose wells of bacterial to detect color intensity\n? PHD Student, USDA\nWine Naive bayes (6 RFE features); XE Boost Random Forest + Search with 9 features\nTalkthrough Dietary data Random calls Interested in gut miocrobiome influences. Goal: which factors to predict CAZyme dyvirsetiy?\nRandom forest regression Need for prediction for which features: use Shapley Addadtive for result intepretation.\nYue Wine OH WOWO\nReg:\n99.98 train, 59.788 test.\nBalanced dataset Sequential feature selection PCA -\u0026gt; 3 features Random Forest Something else: ExhaustiveFeatureSelector\nClsf:\nstill 4 features.\nTalkthrough Deep learning, CV applications.\nNutrition product validation so far is entirely manual; current work in bias are mostly political, so finding a ground truth is difficult.\nSupervised is probability difficult; getting the data and cluster.\nSriya Sunil PhD Food Science, Cornell\nWine Decision tree classifier; resulted in 7 features.\n99.97% train, 97.08% test.\nSupport Vector Regression; resulted in 7 features as well.\n39.25% train, 32.79% test.\nTalkthrough Microbial growth on baby spinach. Features: initial counts, prevalence of bacteria, growth of bacteria.\nOutput regression to time to spoilage\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_student_presentations/","tags":null,"title":"AIBridge Student Presentations"},{"categories":null,"contents":"Welcome to the Day-2 Afternoon Lab! We are super excited to work through tasks in linear regression and logistic regression, as well as familiarize you with the Iris dataset.\nIris Dataset Let\u0026rsquo;s load the Iris dataset! Begin by importing the load_iris tool from sklearn. This is an easy loader scheme for the iris dataset.\nfrom sklearn.datasets import load_iris Then, we simply execute the following to load the data.\nx,y = load_iris(return_X_y=True) We use the return_X_y argument here so that, instead of dumping a large CSV, we get the neat-cleaned input and output values.\nLet\u0026rsquo;s inspect this data a little.\nx[0] 5.1 3.5 1.4 0.2 We can see that each sample of the data is a vector in \\(\\mathbb{R}^4\\). They correspond to four attributes:\nseptal length septal width pedal length pedal width What\u0026rsquo;s the output?\ny[0] 0 We can actually see all the possible values of the output by putting it into a set.\nset(y) 0 1 2 There are three different classes of outputs.\nIris Setosa Iris Versicolour Iris Virginica Excellent. So we can see that we have a dataset of four possible inputs and one possible output. Let\u0026rsquo;s see what we can do with it.\nLogistic Regression The simplest thing we can do is a logistic regression. We have a there categories for output and a lot of data for input. Let\u0026rsquo;s figure out if we can predict the output from the input!\nLet\u0026rsquo;s import logistic regression tool first, and instantiate it.\nfrom sklearn.linear_model import LogisticRegression reg = LogisticRegression() We will \u0026ldquo;fit\u0026rdquo; the data to the model: adjusting the model to best represent the data. Our data has 150 samples, so let\u0026rsquo;s fit the data on 140 of them.\ntesting_samples_x = x[-5:] testing_samples_y = y[-5:] x = x[:-5] y = y[:-5] Wonderful. Let\u0026rsquo;s fit the data onto the model.\nreg = reg.fit(x,y) Let\u0026rsquo;s go ahead and run the model on our 10 testing samples!\npredicted_y = reg.predict(testing_samples_x) predicted_y 2 2 2 2 2 And, let\u0026rsquo;s figure out what our actual results say:\ntesting_samples_y 2 2 2 2 2 Woah! That\u0026rsquo;s excellent.\nLinear Regression Instead of predicting the output classes, we can predict some values from the output. How about if we used septal length, width, and pedal length to predict petal width? The output now is a number, not some classes, which calls for linear regression!\nLet\u0026rsquo;s import linear regression tool first, and instantiate it.\nfrom sklearn.linear_model import LinearRegression reg = LinearRegression() We will \u0026ldquo;fit\u0026rdquo; the data to the model again. As we have cleaned out the testing_samples, we simply need to split out the fourth column for the new x and y:\nnew_x = x[:,:3] new_y = x[:,3] new_testing_samples_y = testing_samples_x[:,3] new_testing_samples_x = testing_samples_x[:,:3] Taking now our newly parsed data, let\u0026rsquo;s fit it to a linear model.\nreg = reg.fit(new_x,new_y) Let\u0026rsquo;s go ahead and run the model on our 10 testing samples!\nnew_predicted_y = reg.predict(new_testing_samples_x) new_predicted_y 1.7500734 1.61927061 1.79218767 2.04824364 1.86638164 And, let\u0026rsquo;s figure out what our actual results say:\nnew_testing_samples_y 2.3 1.9 2 2.3 1.8 Close on some samples, not quite there on others. How good does our model actually do? We can use .score() to figure out the \\(r^2\\) value of our line on some data.\nreg.score(new_x, new_y) 0.9405617534915884 Evidently, it seems like about \\(94\\%\\) of the variation in our output data can be explained by the input features. This means that the relationship between septals are not exactly a linear pattern!\nNow you try Download the wine quality dataset Predict the quality of wine given its chemical metrics Predict if its red or white wine given its chemical metrics Vary the amount of data used to .fit the model, how does that influence the results? Vary the amount in each \u0026ldquo;class\u0026rdquo; (red wine, white wine) to fit the model, how much does that influence the results. ","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d1aft/","tags":null,"title":"AIBridgeLab D2Aft"},{"categories":null,"contents":"Woah! We talked about a lot of different ways of doing classification today! Let\u0026rsquo;s see what we can do about this for the Iris dataset!\nIris Dataset Let\u0026rsquo;s load the Iris dataset! Begin by importing the load_iris tool from sklearn. This is an easy loader scheme for the iris dataset.\nfrom sklearn.datasets import load_iris Then, we simply execute the following to load the data.\nx,y = load_iris(return_X_y=True) We use the return_X_y argument here so that, instead of dumping a large CSV, we get the neat-cleaned input and output values.\nA reminder that there is three possible flowers that we can sort by.\nDecision Trees Scikit learn has great facilities for using decision trees for classification! Let\u0026rsquo;s use some of them by fitting to the Iris dataset.\nLet us begin by importing the SciKit learn tree system:\nfrom sklearn.tree import DecisionTreeClassifier We will fit and instantiate this classifier and fit it to the data exactly!\nclf = DecisionTreeClassifier() clf = clf.fit(x,y) One cool thing about decision trees is that we can actually see what its doing! by looking at the series of splits and decisions. This is a function provided by tree too.\n# We first import the plotting utility from matplotlib import matplotlib.pyplot as plt # as well as the tree plotting tool from sklearn.tree import plot_tree # We call the tree plot tool, which puts it on teh matplotlib graph for side effects plot_tree(clf) # And we save the figure plt.savefig(\u0026#34;tree.png\u0026#34;) Cool! As you can see, by the end of the entire graph, the gini impurity of each node has been sorted to 0.\nApparently, if the third feature (pedal length) is smaller that 2.45, it is definitely the first type of flower!\nCan you explain the rest of the divisions?\nThere are some arguments available in .fit of a DecisionTreeClassifier which controls for when splitting ends; for instance, max_depth controls the maximum depth by which the tree can go.\nExtra Addition! Random Forests. If you recall, we make the initial splitting decisions fairly randomly, and simply select the one with the lowest Ginni impurity. Of course, this makes the selection of the initial sets of splits very important.\nWhat if, instead of needing to make a decision about that now, we can just deal with it later? Well, that\u0026rsquo;s where the addition of Random Forests come in.\nAs the name suggests, instead of having one great tree that does a \u0026ldquo;pretty good\u0026rdquo; job, we can have a lot of trees acting in ensemble! We can randomly start a bunch of random trees, and pick the selection that most would correspond with.\nRandom forests come from the ensemble package from sklearn; we can use it fairly simply:\nfrom sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier() Wonderful! I bet you can guess what the syntax is. Instead of fitting on the whole dataset, though, we will fit on the first 145 items.\nclf = clf.fit(x[:-5],y[:-5]) We can go ahead and run predict on some samples, just to see how it does on data it has not already seen before!\nclf.score(x[-5:], y[-5:]) 1.0 As you can see, it still does pretty well!\nSVM Let\u0026rsquo;s put another classification technique we learned today to use! Support Vector Machines. The entire syntax to manipulate support vector machines is very simple; at this point, you can probably guess it in yours sleep :)\nLet\u0026rsquo;s import a SVM:\nfrom sklearn import svm Great. Now, we will instantiate it and fit it onto the data. SVC is the support-vector machine classifier.\nclf = svm.SVC() clf.fit(x,y) Excellent, now, let\u0026rsquo;s score our predictions:\nclf.score(x,y) 0.9733333333333334 As you can see, our data is not entirely linear! Fitting our entire dataset onto a linear SVM didn\u0026rsquo;t score perfectly, which means that the model is not complex enough to support our problem.\nScikit\u0026rsquo;s support vector machine supports lots of nonlinearity function; this is set by the argument kernel. For instance, if we wanted a nonlinear, exponential function kernel (where nonlinear function \\(f(x,x\u0026rsquo;)= e^{-\\gamma||\\big\u0026lt;x,x\u0026rsquo;\\big\u0026gt;||^2}\\)), we can say:\nclf = svm.SVC(kernel=\u0026#34;rbf\u0026#34;) clf.fit(x,y) clf.score(x,y) 0.9733333333333334 Looks like our results are fairly similar, though.\nNaive Bayes One last one! Its Bayes time. Let\u0026rsquo;s first take a look at how an Naive Bayes implementation can be done via Scikit learn.\nOne of the things that the Scikit Learn Naive Bayes estimator does differently than the one that we learned via probabilities is that it assumes that\u0026mdash;instead of a uniform distribution (and therefore \u0026ldquo;chance of occurrence\u0026rdquo; is just occurrence divided by count), our samples are normally distributed. Therefore, we have that\n\\begin{equation} P(x_i | y) = \\frac{1}{\\sqrt{2\\pi{\\sigma^2}_y}}e^{\\left(-\\frac{(x_i-\\mu_y)^2}{2{\\sigma^2}_y}\\right)} \\end{equation}\nWe can instantiate such a model with the same exact syntax.\nfrom sklearn.naive_bayes import GaussianNB clf = GaussianNB() clf = clf.fit(x,y) Let\u0026rsquo;s see how it does!\nclf.score(x,y) 0.96 Same thing as before, it seems simple probabilities can\u0026rsquo;t model our relationship super well. However, this is still a fairly accurate and powerful classifier.\nNow you try! Try all three classifiers on the Wine dataset for red-white divide! Which one does better on generalizing to data you haven\u0026rsquo;t seen before? Explain the results of the decision trees trained on the Wine data by plotting it. Is there anything interesting that the tree used as a heuristic that came up? The probabilistic, uniform Naive-Bayes is fairly simple to implement write if we are using the traditional version of the Bayes theorem. Can you use Pandas to implement one yourself? ","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d3_d4/","tags":null,"title":"AIBridgeLab D3/D4"},{"categories":null,"contents":"Welcome to the Day-3 Morning Lab! We are glad for you to join us. Today, we are learning about how Pandas, a data manipulation tool, works, and working on cleaning some data of your own!\nIris Dataset We are going to lead the Iris dataset from sklearn again. This time, however, we will load the full dataset and parse it ourselves (instead of using return_X_y.)\nLet\u0026rsquo;s begin by importing the Iris dataset, as we expect.\nfrom sklearn.datasets import load_iris And, load the dataset to see what it looks like.\niris = load_iris() iris.keys() dict_keys([\u0026#39;data\u0026#39;, \u0026#39;target\u0026#39;, \u0026#39;frame\u0026#39;, \u0026#39;target_names\u0026#39;, \u0026#39;DESCR\u0026#39;, \u0026#39;feature_names\u0026#39;, \u0026#39;filename\u0026#39;, \u0026#39;data_module\u0026#39;]) We have a pretty large dictionary full of information! Let\u0026rsquo;s pull out data (our input data), target (our output data), and feature_names, the names of our feature.\niris_in = iris[\u0026#34;data\u0026#34;] iris_out = iris[\u0026#34;target\u0026#34;] iris_names = iris[\u0026#34;feature_names\u0026#34;] Data Manipulation pandas is a very helpful utility that allow us to see into data more conveniently. The object that we are usually working with, when using pandas, is called a DataFrame. We can actually create a DataFrame pretty easily. Let\u0026rsquo;s first import pandas\nimport pandas as pd Loading Data We have aliased it as pd so that its easier to type. Awesome! Let\u0026rsquo;s make a DataFrame.\ndf = pd.DataFrame(iris_in) df 0 1 2 3 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 .. ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 [150 rows x 4 columns] Nice! We have our input data contained in a data frame and nicely printed in a table; cool! However, the column names 1, 2, 3, 4 aren\u0026rsquo;t exactly the most useful labels for us. Instead, then, let\u0026rsquo;s change the column headers to:\niris_names sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) How? We can both get and set the columns via df.columns:\ndf.columns = iris_names Let\u0026rsquo;s look at the DataFrame again!\ndf sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 .. ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 [150 rows x 4 columns] Excellent! Now our data frame looks much more reasonable.\nWranging Data How do we manipulate the data around? Well, we can index this data by both columns and rows.\nIndexing by columns first is very easy. Pandas tables are, by default, \u0026ldquo;column-major\u0026rdquo;. This means that we can just index the columns just like a list!\ndf[\u0026#34;petal width (cm)\u0026#34;] 0 0.2 1 0.2 2 0.2 3 0.2 4 0.2 ... 145 2.3 146 1.9 147 2.0 148 2.3 149 1.8 Name: petal width (cm), Length: 150, dtype: float64 Nice! I want to know introduce the idea of a \u0026ldquo;cursor\u0026rdquo;. A \u0026ldquo;cursor\u0026rdquo; is used to index this high-dimensional data; think about it as the way to turn this table into something like an indexable 1-D list.\nThe simplest cursor is .loc (\u0026ldquo;locator.\u0026rdquo;)\nUnlike list indexing directly, .loc is \u0026ldquo;row-major:\u0026rdquo; the first index selects rows instead of columns.\ndf.loc[0] sepal length (cm) 5.1 sepal width (cm) 3.5 petal length (cm) 1.4 petal width (cm) 0.2 Name: 0, dtype: float64 Nice! You can see that .loc turned our table into a list, with each \u0026ldquo;sample\u0026rdquo; of the data more clearly represented by indexing it like a list.\nWhat if, then, we want to select the \u0026ldquo;pedal width\u0026rdquo; value inside this sample? We just select the first index, a comma, then select the second index.\ndf.loc[0, \u0026#34;petal width (cm)\u0026#34;] 0.2 Excellent! We can see, because we changed the header columns to be strings, we have to index them like strings.\nWhat if, instead of the first row, we want to get\u0026hellip; say, the first, fifth, and sixth rows? Unlike traditional lists, Pandas\u0026rsquo; cursors can be indexed by a list.\nSo this:\ndf.loc[0] sepal length (cm) 5.1 sepal width (cm) 3.5 petal length (cm) 1.4 petal width (cm) 0.2 Name: 0, dtype: float64 turns into\ndf.loc[[0,2,8,9]] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 2 4.7 3.2 1.3 0.2 8 4.4 2.9 1.4 0.2 9 4.9 3.1 1.5 0.1 This would give us the 0th, 2nd, 8th, and 9th row!\nThis is all good, but, it\u0026rsquo;s kind of annoying to type the column names (like \u0026ldquo;petal width (cm)\u0026rdquo;) every time! No worries, we can address this.\niloc is a variant of loc which uses integer indexes. For row indexing, the syntax remains exactly the same; iloc, however, converts all column indexes to integers sequentially. Therefore:\ndf.loc[0, \u0026#34;petal width (cm)\u0026#34;] becomes\ndf.iloc[0, 3] 0.2 Nice! Isn\u0026rsquo;t that convenient.\nSome statistics The main gist of the lab here is to manipulate the input data a little. Pandas provides many helpful utilities to help us with that. For instance, let\u0026rsquo;s take a single feature in the data, say, the pedal with:\npwidth = df[\u0026#34;petal width (cm)\u0026#34;] # same pwidth = df.iloc[:,3], where : returns everything in the row dimention pwidth 0 0.2 1 0.2 2 0.2 3 0.2 4 0.2 ... 145 2.3 146 1.9 147 2.0 148 2.3 149 1.8 Name: petal width (cm), Length: 150, dtype: float64 We can now find out how distributed this data is, to glean some info about normalization! The most basic is for us to find the mean width of the petals:\npwidth.mean() 1.1993333333333336 Awesome! We can calculate the standard by applying this constant to that entire row. The syntax works just like how you expect\u0026mdash;subtracting a scalar from the whole column just subtracts that constant from every element\u0026mdash;without any fuss:\n(((pwidth-pwidth.mean())**2).sum()/len(pwidth))**0.5 0.7596926279021594 Cool! In the scheme of things, that\u0026rsquo;s actually a pretty good. However, if it was not, we could normalize the data!\nLet\u0026rsquo;s first get the norm of the vector\npwidth_norm = sum(pwidth**2)**0.5 pwidth_norm 17.38763928772391 And, let\u0026rsquo;s normalize our vector by this norm!\npwidth_normd = pwidth/pwidth_norm pwidth_normd 0 0.011502 1 0.011502 2 0.011502 3 0.011502 4 0.011502 ... 145 0.132278 146 0.109273 147 0.115024 148 0.132278 149 0.103522 Name: petal width (cm), Length: 150, dtype: float64 Excellent. Let\u0026rsquo;s find out its standard deviation again! This time we will use .std() instead.\npwidth_normd.std() 0.04383790440709825 Much better.\nNow you try Load the wine dataset into a DataFrame and manipulate it. Feed slices back into our functions yesterday! Can you make the subsets of the data you made yesterday via the .iloc notation to make slicing easier? Can you quantify the accuracy, precision, and recall on a shuffled version of the wine dataset and logistic regression? seed=0 Is there any columns that need normalisation? Any outliers (2 std. dev away)? Why/why not? Create a balanced version of the wine dataset between red and white classes. Does fitting this normalized version into our model makes training results better? ","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d2aft/","tags":null,"title":"AIBridgeLab D3Morning"},{"categories":null,"contents":"Let\u0026rsquo;s run some clustering algorithms! We are still going to use the Iris data, because we are super familiar with it already. Loading it works the exactly in the same way; I will not repeat the notes but just copy the code and description from before here for your reference\nIris Dataset Let\u0026rsquo;s load the Iris dataset! Begin by importing the load_iris tool from sklearn. This is an easy loader scheme for the iris dataset.\nfrom sklearn.datasets import load_iris Then, we simply execute the following to load the data.\nx,y = load_iris(return_X_y=True) We use the return_X_y argument here so that, instead of dumping a large CSV, we get the neat-cleaned input and output values.\nk-means clustering The basics of k-means clustering works exactly the same as before, except this time we have to specify and get a few more parameters. Let\u0026rsquo;s begin by importing k-means and getting some clusters together!\nfrom sklearn.cluster import KMeans Let\u0026rsquo;s instantiate the KMeans cluster with 3 clusters, which is the number of classes there is.\nkmeans = KMeans(n_clusters=3) kmeans = kmeans.fit(x) Great! Let\u0026rsquo;s take a look at how it sorted all of our samples\nkmeans.labels_ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2 2 0 Let\u0026rsquo;s plot our results.\nimport matplotlib.pyplot as plt We then need to define some colours.\ncolors=[\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;blue\u0026#34;] Recall from yesterday that we realized that inner Septal/Pedal differences are not as variable as intra Septal/Pedal differences. So, we will plot the first and third columns next to each other, and use labels_ for coloring.\n# for each element for indx, element in enumerate(x): # add a scatter point plt.scatter(element[0], element[1], color=colors[kmeans.labels_[indx]]) # save our figure plt.savefig(\u0026#34;scatter.png\u0026#34;) Nice. These look like the main groups are captured!\nLet\u0026rsquo;s compare that to intended classes\ny 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 There are obviously some clustering mistakes. Woah! Without prompting with answers, our model was able to figure out much of the general clusters at which our data exists. Nice.\nWe can also see the \u0026ldquo;average\u0026rdquo;/\u0026ldquo;center\u0026rdquo; for each of the clusters:\nkmeans.cluster_centers_ 5.9016129 2.7483871 4.39354839 1.43387097 5.006 3.428 1.462 0.246 6.85 3.07368421 5.74210526 2.07105263 Nice! These are what our model thinks are the centers of each group.\nPrinciple Component Analysis Let\u0026rsquo;s try reducing the dimentionality of our data by one, so that we only have three dimensions. We do this, by, again, begin importing PCA.\nfrom sklearn.decomposition import PCA When we are instantiating, we need to create a PCA instance with a keyword n_components, which is the number of dimensions (\u0026ldquo;component vectors\u0026rdquo;) we want to keep.\npca = PCA(n_components=3) Great, let\u0026rsquo;s fit our data to this PCA.\npca.fit(x) Wonderful. singular_values_ is how we can get out of the PCA\u0026rsquo;d change of basis results:\ncob = pca.components_ cob 0.36138659 -0.08452251 0.85667061 0.3582892 0.65658877 0.73016143 -0.17337266 -0.07548102 -0.58202985 0.59791083 0.07623608 0.54583143 So, we can then take a change of basis matrix and apply it to some samples!\ncob@(x[0]) 2.81823951 5.64634982 -0.65976754 What\u0026rsquo;s @? Well\u0026hellip; Unfortunately, Python has different operator for matrix-operations (\u0026ldquo;dot\u0026rdquo;); otherwise, it will perform element-wise operations.\nWe can actually also see the \\(R^2\\) values on each of the axis: the variance explained by each of the dimensions.\npca.explained_variance_ 4.22824171 0.24267075 0.0782095 Nice! As you can see, much of the variance is contained in our first dimension here.\n","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d4aft/","tags":null,"title":"AIBridgeLab D4Aft"},{"categories":null,"contents":"AIFS is a food systems institute at UC Davis.\n","permalink":"https://www.jemoka.com/posts/kbhaifs/","tags":null,"title":"AIFS"},{"categories":null,"contents":"I am honestly not entirely sure why or what state of mind I was in circa 2017 to write, edit, and act! in this video, but I did.\nThis is an adaption of a Greek-Style story which someone else wrote, I don\u0026rsquo;t know who.\nVideo produced mostly by myself in front of a green screen, with help from my lovely mother as well as a very nice teacher named Joseph O\u0026rsquo;Brian.\nhttps://youtu.be/b1YxOkcwtgw\nBe prepared. 前方高能\n","permalink":"https://www.jemoka.com/posts/kbhair_a_greek_style_myth/","tags":null,"title":"Air: A Greek Style Myth"},{"categories":null,"contents":"algebra is the study of\u0026hellip;\nsymbols/variables transformations/operations: \u0026ldquo;add\u0026rdquo;, \u0026ldquo;multiply\u0026rdquo; simple functions abstraction substitution ","permalink":"https://www.jemoka.com/posts/kbhalgebra/","tags":null,"title":"algebra"},{"categories":null,"contents":"Begin with a new installation of MFA, and head to the directory. First run validate with the original dictionary.\nmfa validate ~/Downloads/tb/my_corpus english_us_arpa english_us_arpa We see that there is in deed an section of corpus that is out-of-vocab.\nINFO - 11 OOV word types INFO - 18 total OOV tokens Therefore, we will generate a new dictionary based on the existing dictionary of english_us_arpa.\nFirst download the english_us_arpa model\nmfa model download g2p english_us_arpa Then, perform the actual dictionary generation:\nmfa g2p english_us_arpa ~/Downloads/tb/my_corpus ~/Downloads/tb/my_corpus/new_dict.txt There is a chance this command fails with\nThere was an issue importing Pynini, please ensure that it is installed. If you are on Windows, please use the Windows Subsystem for Linux to use g2p functionality. If so, install pynini\nconda add pynini Finally, run the mfa g2p command above to generate pronunciations.\nYou should end up with a file named new_dict.txt, which should include missing words.\nFinally, perform alignment with this new dictionary.\nmfa align ~/Downloads/tb/my_corpus ~/Downloads/tb/my_corpus/new_dict.txt english_us_arpa ~/Downloads/tb/my_corpus_output Notice here the second argument of mfa align is no longer english_us_arpa, our base dictionary. Instead, it is our custom dictionary.\n","permalink":"https://www.jemoka.com/posts/kbhalign_with_new_vocab/","tags":null,"title":"Align with New Vocab"},{"categories":null,"contents":" Want to interview more severe ashma Want to find someone younger Difference between marketing and purchaser.\nTaking to people Spoke with Matt. Talked with more details with prototyping and how they can build a unique product.\nHave not gotten back to him yet.\n","permalink":"https://www.jemoka.com/posts/kbhalivio_april_checkin/","tags":null,"title":"Alivio April Checkin"},{"categories":null,"contents":"Here\u0026rsquo;s a list of the interactive lecture notes I have put together for AML:\nAML: Dipping into PyTorch AML: Iris Strikes Bath AML: Your First Article ","permalink":"https://www.jemoka.com/posts/kbhaml_lecture_index/","tags":["index"],"title":"AML Lecture Index"},{"categories":null,"contents":"Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.\nGetting Started To get started, let\u0026rsquo;s open a colab and import Torch!\nimport torch import torch.nn as nn The top line here import PyTorch generally, and the bottom line imports the Neural Network libraries. We will need both for today and into the future!\nTensors and AutoGrad The most basic element we will be working with in Torch is something called a tensor. A tensor is a variable, which holds either a single number (scalar, or a single neuron) or a list of numbers (vector, or a layer of neurons), that can change. We will see what that means in a sec.\nYour First Tensors Everything that you are going to put through to PyTorch needs to be in a tensor. Therefore, we will need to get good at making them! As we discussed, a tensor can hold an number (scalar), a list (vector) or a (matrix).\nHere are a bunch of them!\nscalar_tensor = torch.tensor(2.2) vector_tensor = torch.tensor([1,3,4]) matrix_tensor = torch.tensor([[3,1,4],[1,7,4]]) You can perform operations on these tensors, like adding them together:\ntorch.tensor(2.2) + torch.tensor(5.1) tensor(7.3000) Vector and Matrix tensors work like NumPy arrays. You can add them pairwise:\ntorch.tensor([[3,1,4],[1,7,4]]) + torch.tensor([[0,2,1],[3,3,4]]) tensor([[ 3, 3, 5], [ 4, 10, 8]]) Connecting Tensors A single number can\u0026rsquo;t be a neural network! ([citation needed]) So, to be able to actually build networks, we have to connect tensors together.\nSo, let\u0026rsquo;s create two tensors, each holding a neuron, and connect them together!\nHere are two lovely scalar tensors:\nvar_1 = torch.tensor(3.0, requires_grad=True) var_2 = torch.tensor(4.0, requires_grad=True) var_1, var_2 (tensor(3., requires_grad=True), tensor(4., requires_grad=True)) We initialized two numbers, 3, which we named var_1, and 4, which we named var_2.\nThe value requires_grad here tells PyTorch that these values can change, which we need it to do\u0026hellip; very shortly!\nFirst, though, let\u0026rsquo;s create a latent variable. A \u0026ldquo;latent\u0026rdquo; value is a value that is the result of operations on other non-latent tensors\u0026mdash;connecting the activation of some neurons together with a new one. For instance, if I multiplied our two tensors together, we can create our very own latent tensor.\nmy_latent_value = var_1*var_2 my_latent_value tensor(12., grad_fn=\u0026lt;MulBackward0\u0026gt;) Evidently, \\(3 \\cdot 4 = 12\\).\nAutograd Now! The beauty of PyTorch is that we can tell it to set any particular latent variable to \\(0\\) (Why only \\(0\\), and \\(0\\) specifically? Calculus; turns out this limitation doesn\u0026rsquo;t matter at all, as we will see), and it can update all of its constituent tensors with required_grad \u0026ldquo;True\u0026rdquo; such that the latent variable we told PyTorch to set to \\(0\\) indeed becomes \\(0\\)!\nThis process is called \u0026ldquo;automatic gradient calculation\u0026rdquo; and \u0026ldquo;backpropagation.\u0026rdquo; (Big asterisks throughout, but bear with us. Find Matt/Jack if you want more.)\nTo do this, we will leverage the help of a special optimization algorithm called stochastic gradient descent.\nLet\u0026rsquo;s get a box of this stuff first:\nfrom torch.optim import SGD SGD \u0026lt;class \u0026#39;torch.optim.sgd.SGD\u0026#39;\u0026gt; Excellent. By the way, from the torch.optim package, there\u0026rsquo;s tonnes (like at least 20) different \u0026ldquo;optimizer\u0026rdquo; algorithms that all do the same thing (\u0026ldquo;take this latent variable to \\(0\\) by updating its constituents\u0026rdquo;) but do them in important different ways. We will explore some of them through this semester, and others you can Google for yourself by looking up \u0026ldquo;PyTorch optimizers\u0026rdquo;.\nOk, to get this SGD thing up and spinning, we have to tell it every tensor it gets to play with in a list. For us, let\u0026rsquo;s ask PyTorch SGD to update var_1 and var_2 such that my_latent_value (which, remember, is var1 times var2) becomes a new value.\nAside: learning rate\nNow, if you recall the neural network simulation, our model does not reach the desired outcome immediately. It does so in steps. The size of these steps are called the learning rate; the LARGER these steps are, the quicker you will get close to your desired solution, but where you end up getting maybe farther away from the actual solution; and vise versa.\nThink about the learning rate as a hoppy frog: a frog that can hop a yard at a time (\u0026ldquo;high learning rate\u0026rdquo;) can probably hit a target a mile away much quicker, but will have a hard time actually hitting the foot-wide target precisely; a frog that can hop an inch at a time (\u0026ldquo;low learning rate\u0026rdquo;) can probably hit a target a mile away\u0026hellip;. years from now, but will definitely be precisely hitting the foot-wide target when it finally gets there.\nSo what does \u0026ldquo;high\u0026rdquo; and \u0026ldquo;low\u0026rdquo; mean? Usually, we adjust learning rate by considering the number of decimal places it has. \\(1\\) is considered a high learning rate, \\(1 \\times 10^{-3} = 0.001\\) as medium-ish learning rate, and \\(1 \\times 10^{-5}=0.00001\\) as a small one. There are, however, no hard and fast rules about this and it is subjcet to experimentation.\nSo, choose also an appropriate learning rate for our optimizer. I would usually start with \\(3 \\times 10^{-3}\\) and go from there. In Python, we write that as 3e-3.\nSo, let\u0026rsquo;s make a SGD, and give it var_1 and var_2 to play with, and set the learning rate to 3e-3:\nmy_sgd = SGD([var_1, var_2], lr=3e-3) my_sgd SGD ( Parameter Group 0 dampening: 0 differentiable: False foreach: None lr: 0.003 maximize: False momentum: 0 nesterov: False weight_decay: 0 ) Wonderful. Don\u0026rsquo;t worry much about how many of these means for now; however, we will see it in action shortly.\nNow! Recall that we allowed my_sgd to mess with var_1 and var_2 to change the value of my_latent_value (the product of var_1 and var_2).\nCurrent, var_1 and var_2 carries the values of:\nvar_1, var_2 (tensor(3., requires_grad=True), tensor(4., requires_grad=True)) And, of course, their product my_latent_value carries the value of:\nmy_latent_value tensor(12., grad_fn=\u0026lt;MulBackward0\u0026gt;) What if we want my_latent_value to be\u0026hellip; \\(15\\)? That sounds like a good number. Let\u0026rsquo;s ask our SGD algorithm to update var_1 and var_2 such that my_latent_value will be \\(15\\)!\nWaaait. I mentioned that the optimizers can only take things to \\(0\\). How could it take my_latent_value to \\(15\\) then? Recall! I said SGD takes a latent variable to \\(0\\). So, we can just build another latent variable such that, when my_latent_value is \\(15\\), our new latent variable will be \\(0\\), and then ask SGD optimize on that!\nWhat could that be\u0026hellip; Well, the squared difference between \\(15\\) and my_latent_value is a good one. If my_latent_value is \\(15\\), the squared difference between it and \\(15\\) will be \\(0\\), as desired!\nSo, similar to what we explored last semester, we use sum of squared difference as our loss because it will be able to account for errors of fit in both directions: a \\(-4\\) difference in predicted and actual output is just as bad as a \\(+4\\) difference.\nTurns out, the \u0026ldquo;objective\u0026rdquo; for SGD optimization, the thing that we ask SGD to take to \\(0\\) on our behalf by updating the parameters we allowed it to update (again, they are var_1 and var_2 in our case here), is indeed the loss value of our model. Sum of squared errors is, therefore, called our loss function for this toy problem.\nSo let\u0026rsquo;s do it! Let\u0026rsquo;s create a tensor our loss:\nloss = (15-my_latent_value)**2 loss tensor(9., grad_fn=\u0026lt;PowBackward0\u0026gt;) Nice. So our loss is at \\(3\\) right now; when my_latent_value is correctly at \\(15\\), our loss will be at \\(0\\)! So, to get my_latent_value to \\(15\\), we will ask SGD to take loss to \\(0\\).\nTo do this, there are three steps. COMMIT THIS TO MEMORY, as it will be basis of literally everything else in the future.\nBackpropagate: \u0026ldquo;please tell SGD to take this variable to \\(0\\), and mark the correct tensors to change\u0026rdquo; Optimize: \u0026ldquo;SGD, please update the marked tensors such that the variable I asked you to take to \\(0\\) is closer to \\(0\\)\u0026rdquo; Reset: \u0026ldquo;SGD, please get ready for step 1 again by unmarking everything that you have changed\u0026rdquo; Again! Is it commited to memory yet?\nBackprop Optimize Reset I am stressing this here because a lot of people 1) miss one of these steps 2) do them out of order. Doing these in any other order will cause your desired result to not work. Why? Think about what each step does, and think about doing them out of order.\nOne more time for good luck:\nBackprop! Optimize! Reset! Let\u0026rsquo;s do it.\nBackprop! Backpropergation marks the correct loss value to minimize (optimze towards being \\(0\\)), and marks all tensors with requires_grad set to True which make up the value of that loss value for update.\nSecretly, this steps takes the partial derivative of our loss against each of the tensors we marked requires_grad, allowing SGD to \u0026ldquo;slide down the gradient\u0026rdquo; based on those partial derivatives. Don\u0026rsquo;t worry if you didn\u0026rsquo;t get that sentence.\nTo do this, we call .backward() on the loss we want to take to \\(0\\):\nloss.backward() None This call will produce nothing. And that\u0026rsquo;s OK, because here comes\u0026hellip;\nOptimize! The next step is tell SGD to update all of the tensors marked for update in the previous step to get loss closer to \\(0\\). To do this, we simply:\nmy_sgd.step() None This call will produce nothing. But, if you check now, the tensors should updated.\nAlthough\u0026hellip; You should\u0026rsquo;t check! Because we have one more step left:\nReset! my_sgd.zero_grad() None I cannot stress this enough. People often stop at the previous step because \u0026ldquo;ooo look my tensors updated!!!\u0026rdquo; and forget to do this step. THIS IS BAD. We won\u0026rsquo;t go into why for now, but basically not resetting the update mark results in a tensor being updated twice, then thrice, etc. each time you call .step(), which will cause double-updates, which will cause you to overshoot (handwavy, but roughly), which is bad.\nooo look my tensors updated!!! var_1, var_2 (tensor(3.0720, requires_grad=True), tensor(4.0540, requires_grad=True)) WOAH! Look at that! Without us telling SGD, it figured out that var_1 and var_2 both need to be BIGGER for my_latent_value, the product of var_1 and var_2 to change from \\(12\\) to \\(15\\). Yet, the product of \\(3.0720\\) and \\(4.0540\\) is hardly close to \\(15\\).\nWhy? Because our step size. It was tiny! To get my_latent_value to be properly \\(15\\), we have to do the cycle of 1) calculating new latent value 2) calculating new loss 3) backprop, optimize, reset, a LOT of times.\nNow do that a lot of times. for _ in range(100): my_latent_value = var_1*var_2 loss = (15-my_latent_value)**2 loss.backward() # BACKPROP! my_sgd.step() # OPTIMIZE! my_sgd.zero_grad() # RESET! var_1, var_2 (tensor(3.4505, requires_grad=True), tensor(4.3472, requires_grad=True)) Weird solution, but we got there! The product of these two values is indeed very close to \\(15\\)! Give yourself a pat on the back.\nSo why the heck are we doing all this So why did we go through all the effort of like 25 lines of code to get two numbers to multiply to \\(15\\)? If you think about Neural Networks as a process of function fitting, we are essentially asking our very basic \u0026ldquo;network\u0026rdquo; (as indeed, the chain of tensors to build up to our latent value, then to compute our loss, is a network!) to achieve a measurable task (\u0026ldquo;take the product of these numbers to \\(15\\)\u0026rdquo;). Though the relationships we will be modeling in this class will be more complex than literal multiplication, it will be just using more fancy mechanics of doing the same thing\u0026mdash;taking tensors values which was undesirable, and moving them to more desirable values to model our relationship.\ny=mx+b and your first neural network \u0026ldquo;module\u0026rdquo; nn.Linear The power of neural networks actually comes when a BUNCH of numbers gets multiplied together, all at once! using\u0026hellip; VECTORS and MATRICIES! Don\u0026rsquo;t remember what they are? Ask your friendly neighborhood Matt/Jack.\nRecall, a matrix is how you can transform a vector from one space to another. Turns out, the brunt of everything you will be doing involves asking SGD to move a bunch of matricies around (like we did before!) such that our input vector(s) gets mapped to the right place.\nA matrix, in neural network world, is referred to as a linear layer. It holds a whole series of neurons, taking every single value of the input into account to producing a whole set of output. Because of this property, it is considered a fully connected layer.\nLet\u0026rsquo;s create such a fully-connected layer (matrix) in PyTorch! When you ask PyTorch to make a matrix for you, you use the nn sublibrary which we imported before. Furthermore, and this is confusing for many people who have worked with matricies before, you specify the input dimension first.\nmy_matrix_var_1 = nn.Linear(3, 2) my_matrix_var_1 Linear(in_features=3, out_features=2, bias=True) my_matrix_var_1 is a linear map from three dimensions to two dimensions; it will take a vector of three things as input and spit out a vector of two.\nNote! Although my_matrix_var_1 is a tensor under the hood just like var_1, we 1) didn\u0026rsquo;t have to set default values for it 2) didn\u0026rsquo;t have to mark it as requires_grad. This is because, unlike a raw Tensor which often does not require to be changed (such as, for instance, the input value, which you can\u0026rsquo;t change), a matrix is basically ALWAYS a tensor that encodes the weights of a model we are working with\u0026mdash;so it is always going to be something that we will ask SGD to change on our behalf.\nSo, since you are asking SGD to change it anyways, PyTorch just filled a bunch of random numbers in for you and set requires_grad on for you to my_matrix_var_1. If you want to see the actual underlying tensor, you can:\nmy_matrix_var_1.weight Parameter containing: tensor([[-0.2634, 0.3729, 0.5019], [ 0.2796, 0.5425, -0.4337]], requires_grad=True) As you can see, we have indeed what we expect: a tensor containing a \\(2\\times 3\\) matrix with requires_grad on filled with random values.\nHow do we actually optimize over this tensor? You can do all the shenanigans we did before and pass my_matrix_var_1 to SGD, but this will quickly get unwieldy as you have more parameters. Remember how we had to give SVG a list of EVERYTHING it had to keep track of? var_1 and var_2 was simple enough, but what if we had to do var_1.weight, var_2.weight, var_3.weight\u0026hellip; \u0026hellip; \u0026hellip; ad nausium for every parameter we use on our large graph? GPT3 has 1.5 billion parameters. Do you really want to type that?\nNo.\nThere is, of course, a better way.\nnn.Module This, by the way, is the standard of how a Neural Network is properly built from now on until the industry moves on from PyTorch. You will want to remember this.\nLet\u0026rsquo;s replicate the example of our previous 3=\u0026gt;2 dimensional linear map, but with a whole lot more code.\nclass MyNetwork(nn.Module): def __init__(self): # important: runs early calls to make sure that # the module is correct super().__init__() # we declare our layers. we will use them below self.m1 = nn.Linear(3,2) # this is a special function that you don\u0026#39;t actually call # manually, but as you use this module Torch will call # on your behalf. It passes the input through to the layers # of your network. def forward(self, x): # we want to pass whatever input we get, named x # through to every layer. right now there is only # one fully-connected layer x = self.m1(x) return x What this does, behind the scenes, is to wrap our matrix and all of its parameters into one giant module. (NOTE! This is PyTorch-specific language. Unlike all other vocab before, this term is specific to PyTorch.) A module is an operation on tensors which can retain gradients (i.e. it can change, i.e. requires_grad=True).\nLet\u0026rsquo;s see it in action. Recall that our matrix takes a vector of 3 things as input, and spits out a vector of 2 things. So let\u0026rsquo;s make a vector of three things:\nthree_vector = torch.tensor([1.,2.,3.]) three_vector tensor([1., 2., 3.]) By the way, notice the period I\u0026rsquo;m putting after numbers here? That\u0026rsquo;s a shorthand for .0. So 3.0 = 3.. I want to take this opportunity to remind you that the tensor operations all take FLOATING POINT tensors as input, because the matrices themselves as initialized with random floating points.\nLet\u0026rsquo;s get an instance of the new MyNetwork module.\nmy_network = MyNetwork() my_network MyNetwork( (m1): Linear(in_features=3, out_features=2, bias=True) ) And apply this operation we designed to our three-vector!\nmy_network(three_vector) tensor([0.3850, 1.4120], grad_fn=\u0026lt;AddBackward0\u0026gt;) Woah! It mapped our vector tensor in three dimensions to a vector tensor in two!\nThe above code, by the way, is how we actually use our model to run predictions: my_network is transforming the input vector to the desired output vector.\nCool. This may not seem all that amazing to you\u0026hellip; yet. But, remember, we can encode any number of matrix operations in our forward() function above. Let\u0026rsquo;s design another module that uses two matricies\u0026mdash;or two fully-connected layers, or layers for short (when we don\u0026rsquo;t specify what kind of layer it is, it is fully connected)\u0026mdash;to perform a transformation.\nWe will transform a vector from 3 dimensions to 2 dimensions, then from 2 dimensions to 5 dimensions:\nclass MyNetwork(nn.Module): def __init__(self): # important: runs early calls to make sure that # the module is correct super().__init__() # we declare our layers. we will use them below self.m1 = nn.Linear(3,2) self.m2 = nn.Linear(2,5) # this is a special function that you don\u0026#39;t actually call # manually, but as you use this module Torch will call # on your behalf. It passes the input through to the layers # of your network. def forward(self, x): # we want to pass whatever input we get, named x # through to every layer. right now there is only # one fully-connected layer x = self.m1(x) x = self.m2(x) return x Of course, this network topology is kind of randomly tossed into the network.\nDoing everything else we did before again, we should end up a vector in 5 dimensions, having been transformed twice behind the scenes!\nmy_network = MyNetwork() my_network MyNetwork( (m1): Linear(in_features=3, out_features=2, bias=True) (m2): Linear(in_features=2, out_features=5, bias=True) ) And apply this operation we designed to our three-vector!\nmy_network(three_vector) tensor([ 0.8241, -0.1014, 0.2940, -0.2019, 0.6749], grad_fn=\u0026lt;AddBackward0\u0026gt;) Nice.\nAnd here\u0026rsquo;s the magical thing: when we are asking SGD to optimize this network, instead of needing to pass every darn parameter used in this network into SVG, we can just pass in:\nmy_network.parameters() \u0026lt;generator object Module.parameters at 0x115214270\u0026gt; This is actually a list of every single tensor that has requires_grad=True that we secretly created. No more typing out a list of every parameter to SGD like we did with var_1 and var_2! We will see this in action shortly.\nHow to Train Your Dragon Neural Network Note, the MyNetwork transformation is currently kind of useless. We know it maps the vector [1,2,3] to some arbitrary numbers above (i.e. 0.8241 an such). That\u0026rsquo;s quite lame.\nWe want our network to model some relationship between numbers, that\u0026rsquo;s why we are here. Let\u0026rsquo;s, arbitrarily and for fun, ask SGD to update my_network such that it will return [1,2,3,4,5] given [1,2,3].\nBy the way, from here on, I will use MyNetwork to refer to the model 3=\u0026gt;2=\u0026gt;5 network we made above generally, and my_network the specific instantiation of MyNetwork whose parameters we will ask SGD to update.\nLet\u0026rsquo;s get a clean copy of MyNetwork first:\nmy_network = MyNetwork() my_network MyNetwork( (m1): Linear(in_features=3, out_features=2, bias=True) (m2): Linear(in_features=2, out_features=5, bias=True) ) And, let\u0026rsquo;s create a static (i.e. SGD cannot change it) input and output vector pair which we will pass into our operation:\nmy_input = torch.tensor([1.,2.,3.]) my_desired_output = torch.tensor([1.,2.,3.,4.,5.]) my_input,my_desired_output (tensor([1., 2., 3.]), tensor([1., 2., 3., 4., 5.])) We will pass our input through the my_network operation, and figure out what our inputs currently map to:\nmy_network_output = my_network(my_input) my_network_output tensor([-1.4672, -0.7089, -0.2645, -0.0598, 0.1239], grad_fn=\u0026lt;AddBackward0\u0026gt;) Ah, clearly not [1,2,3,4,5]. Recall we want these values to be the same as my_output, which they isn\u0026rsquo;t doing right now. Let\u0026rsquo;s fix that.\nCan you guess what loss function we will use? \u0026hellip; That\u0026rsquo;s right, the same exact thing as before! Squaring the difference.\nloss = (my_network_output-my_desired_output)**2 loss tensor([ 6.0869, 7.3380, 10.6571, 16.4821, 23.7766], grad_fn=\u0026lt;PowBackward0\u0026gt;) Waiiiit. There\u0026rsquo;s a problem. Remember, SGD can take a single latent value to \\(0\\). That\u0026rsquo;s a whole lotta latent values in a vector! Which one will it take to \\(0\\)? Stop to think about this for a bit: we want to take all of these values to \\(0\\), but we can take only a single value to \\(0\\) with SGD. How can we do it?\nTo do this, we just\u0026hellip; add the values up using the torch.sum function!\nloss = torch.sum((my_network_output-my_desired_output)**2) loss tensor(64.3406, grad_fn=\u0026lt;SumBackward0\u0026gt;) Nice. We now have something to optimize against, let\u0026rsquo;s actually create our optimizer! Remember that, instead of passing in every single parameter we want PyTorch to change manually, we just pass in my_network.parameters() and PyTorch will scan for every single parameter that lives in MyNetwork and give it all to SGD:\nmy_sgd = SGD(my_network.parameters(), lr=1e-6) my_sgd SGD ( Parameter Group 0 dampening: 0 differentiable: False foreach: None lr: 1e-06 maximize: False momentum: 0 nesterov: False weight_decay: 0 ) Just for running this model, we are going to run our network with more steps (\\(50,000\\)), but with smaller step sizes (\\(1 \\times 10^{-6}\\)). We will not worry about it too much for now, and dive into discussing it further for network parameter tuning.\nSo, let\u0026rsquo;s make the actual training loop now that will take the latent variable named my_network_output, created by applying my_network on my_input, to take on the value of my_desired_output! Can you do it without looking? This will be almost the same as our first training loop, except we are asking our network to calculate the current latent output (instead of computing it from scratch each time.)\nfor _ in range(50000): # calculate new latent variable my_network_output = my_network(my_input) # calculate loss loss = torch.sum((my_network_output-my_desired_output)**2) # Backprop! loss.backward() # Optimize! my_sgd.step() # Reset! my_sgd.zero_grad() my_network(my_input) tensor([-0.9814, 0.4252, 1.8085, 2.7022, 3.5517], grad_fn=\u0026lt;AddBackward0\u0026gt;) Not great! But\u0026mdash;we are both ordered correctly and \u0026mdash; if you just kept running this loop, we will eventually converge (arrive at) the right answer! For kicks, let\u0026rsquo;s run it \\(50000\\) more times:\nfor _ in range(50000): # calculate new latent variable my_network_output = my_network(my_input) # calculate loss loss = torch.sum((my_network_output-my_desired_output)**2) # Backprop! loss.backward() # Optimize! my_sgd.step() # Reset! my_sgd.zero_grad() my_network(my_input) tensor([0.9975, 1.9986, 3.0006, 4.0026, 5.0052], grad_fn=\u0026lt;AddBackward0\u0026gt;) Would you look at that! What did I promise you :)\nYour network learned something! Specifically, the skill of mapping \\([1,2,3]\\) to \\([1,2,3,4,5]\\)! Congrats!\nChallenge Now that you know how to get the network to map a specific vector in three dimensions to a specific place in five dimensions, can you do that more generally? Can you generate and give your own network enough examples such that it will learn to do that for ALL vectors in three dimensions?\nSpecifically, generate a training set of in python and train your neural network now to perform the following operation:\nGiven a vector \\([a,b,c]\\), return \\([a,b,c,c+1,c+2]\\), for every integer \\([a,b,c]\\).\nHint: pass in many examples for correct behavior sequentially during each of your training loops, calculating loss and running the optimization step (i.e. back! optimize! reset!) after each example you give.\n","permalink":"https://www.jemoka.com/posts/kbhaml_dipping_into_pytorch/","tags":null,"title":"AML: Dipping into PyTorch"},{"categories":null,"contents":"You are no doubt familiar with the Iris dataset: a dataset containing flower pedal shapes and their corresponding sub-type of Iris flower: Setosa, Versicolour, and Virginica.\nWe are going to take those pedal measurements, and predict the type of Iris we are looking at!\nLet\u0026rsquo;s get the Iris dataset first. Turns out, Scikit Learn (your old friend from last semester) ships a copy of the Iris dataset with itself. So, we will load the dataset from it.\nLet\u0026rsquo;s first import what we need:\nimport torch import torch.nn as nn import sklearn from sklearn import datasets import pandas as pd Excellent. To load the built-in Iris dataset from sklearn, we can:\n# load iris iris = datasets.load_iris() # put input features into a dataframe df = pd.DataFrame(data=iris.data, columns=iris.feature_names) # add targets column from iris data df[\u0026#34;target\u0026#34;] = iris.target df sepal length (cm) sepal width (cm) ... petal width (cm) target 0 5.1 3.5 ... 0.2 0 1 4.9 3.0 ... 0.2 0 2 4.7 3.2 ... 0.2 0 3 4.6 3.1 ... 0.2 0 4 5.0 3.6 ... 0.2 0 .. ... ... ... ... ... 145 6.7 3.0 ... 2.3 2 146 6.3 2.5 ... 1.9 2 147 6.5 3.0 ... 2.0 2 148 6.2 3.4 ... 2.3 2 149 5.9 3.0 ... 1.8 2 [150 rows x 5 columns] You can imagine that this dataset could have been loaded from a CSV, etc.\nJust to recap, here are the columns of this dataset:\ndf.columns Index([\u0026#39;sepal length (cm)\u0026#39;, \u0026#39;sepal width (cm)\u0026#39;, \u0026#39;petal length (cm)\u0026#39;, \u0026#39;petal width (cm)\u0026#39;, \u0026#39;target\u0026#39;], dtype=\u0026#39;object\u0026#39;) Now, pause. Let\u0026rsquo;s think about two questions from last semester:\nWhat type of ML problem is this? (Classification? Regression? Clustering?) Before any engineering: How many input features are there? How many output features? \u0026hellip;\n\u0026hellip;\nWhat type of ML problem is this? Classification Before any engineering: 4 input features, 1 output feature Awesome. Let\u0026rsquo;s inspect this dataset again:\ndf sepal length (cm) sepal width (cm) ... petal width (cm) target 0 5.1 3.5 ... 0.2 0 1 4.9 3.0 ... 0.2 0 2 4.7 3.2 ... 0.2 0 3 4.6 3.1 ... 0.2 0 4 5.0 3.6 ... 0.2 0 .. ... ... ... ... ... 145 6.7 3.0 ... 2.3 2 146 6.3 2.5 ... 1.9 2 147 6.5 3.0 ... 2.0 2 148 6.2 3.4 ... 2.3 2 149 5.9 3.0 ... 1.8 2 [150 rows x 5 columns] You will notice that the targets are not shuffled. If we fit this into our neural network, it will overfit\u0026mdash;memorize output without generalization\u0026mdash;to one target, then to another, etc.\nSo first, let\u0026rsquo;s shuffle this table. To do so, we will simply ask Pandas to resample \\(100\\%\\) of the dataset; it will do this sampling randomly:\ndf = df.sample(frac=1) df sepal length (cm) sepal width (cm) ... petal width (cm) target 49 5.0 3.3 ... 0.2 0 93 5.0 2.3 ... 1.0 1 50 7.0 3.2 ... 1.4 1 145 6.7 3.0 ... 2.3 2 14 5.8 4.0 ... 0.2 0 .. ... ... ... ... ... 48 5.3 3.7 ... 0.2 0 91 6.1 3.0 ... 1.4 1 45 4.8 3.0 ... 0.3 0 131 7.9 3.8 ... 2.0 2 5 5.4 3.9 ... 0.4 0 [150 rows x 5 columns] You will note, however, that the indicies are reshuffled as well! This is actually Pandas being helpful\u0026mdash;allowing us to unshuffle the dataset if needed. But, we actually have no need to do this.\n","permalink":"https://www.jemoka.com/posts/kbhaml_iris_strikes_bath/","tags":null,"title":"AML: Iris Strikes Back"},{"categories":null,"contents":"Hello y\u0026rsquo;all! This quick post about\u0026hellip; writing your first \u0026ldquo;article\u0026rdquo; (ahem, MA) for this class. To me, the most rewarding part of our journey together is to be able to support everyone through writing very presentable reports\u0026mdash;even if it is on old or simple problems\u0026mdash;but in the format from which you can easily jump off and write a fully-blown scientific article in the future.\nexemplar we discussed (Kameswari, Sravani, and Mamidi 2020)\nI should add that, despite the word being used, this is by no means the only way that you can write a wonderful scientific article. It just had the right structure for us to go over in class :)\noverall goals We want to make reports that are clear (easily understandable for audience), concise (only uses words when needed, prioritizing figures and intuition), and precise (when making claims, they are supported clearly with data). And so, the following sections focus mostly on improving those three criteria.\ndiscussion per section Let\u0026rsquo;s now switch to bullet-point format, going over some ideas of how to make strong sections:\nabstract Guiding Questions: What is your study about? What did you do? How well did it work?\nmore jargon here is OK, but should be immediately summative of your whole study only use enough jargon to make it concise: this section should be crystal clear for everyone of your academic peers (i.e. anyone in our class/familiar with ML) it should be easily skimmable intro/motivation/background Guiding Questions: Why are you making big picture choices you made? Did anyone do it before you? How did they do it? What changes are you making (for this class, \u0026ldquo;none\u0026rdquo; is acceptable)?\nimmediately, you should state what you are trying to do and why its worthwhile of your reader\u0026rsquo;s time keep the goal scope small: not \u0026ldquo;cancer is an sickness that affects a lot of people\u0026rdquo;, but \u0026ldquo;this specific gene is correlated with cancer prognosis\u0026rdquo; justify why you are using AML tools! if the relationship you are modeling is a line, deep learning is way overkill summarize previous work: if anyone did it before you, what approach did they do; why? Are you/why are you doing anything difference (i.e. why do you believe methods to be not as good?) methods Guiding Questions: How did you do your study?\ngive as much information for a peer (i.e. anyone in our class/familiar with ML) to be able to reproduce your entire study good to think about\u0026hellip; data sourcing detailed notes on data preparation and feature engineering (bonus points for code) model selection + motivation (should be broadly given in the prev. section already) model implementation (layer counts, activations, seeds) (hyper)parameters (LR, batch size, epoch, optimizer momentum/beta, layer seeds) \u0026mdash; saying \u0026ldquo;default\u0026rdquo; here is fine but be specific about what you are leaving to default training environment (hardware, library versions, time it took, etc.) Also, from a class standpoint, we want to see your hard work in actually practicing the skills we are learning!\nresults/data Guiding Questions: Why should your peer believe you did what you said you did?\nmotivate validation metrics used (i.e. why is success in this validation metric a measurement by proxy of success in the stated problem in the intro?) report the clear details the withholding scheme used\u0026mdash;simple train/val split? k-fold? leave-one-out? present in graphical or tabular form! your clear key takeaways; in general, keep words in this section to a minimum \u0026ldquo;these distributions look visually different!\u0026rdquo; \u0026ldquo;these lines are definitely parallel!\u0026rdquo; \u0026ldquo;this number is definitely larger than this other number!\u0026rdquo; During the process of \u0026ldquo;NTJ\u0026rdquo;, a paper reading methodology taught by the XRT lab, the skill of jumping abstract =\u0026gt; data (\u0026ldquo;figures\u0026rdquo;) =\u0026gt; takeaways (\u0026ldquo;novelty\u0026rdquo;) is greatly emphasized. Usually, the best papers will represent their key takeaways clearly and graphically in this section, so that the reader only need to go into the methods section strictly when needed to reproduce or clarify questions.\nconclusion/discussion Guiding Questions: Summarize.\nIt is often good to include future work here as well as well as fascinating extensions of your choosing. This section differs from the abstract in both the inclusion of future work, as well as its audience: while the abstract need only to be crystal clear for your peers, the conclusion should be clear to everyone in the field \u0026mdash; so redefinition of paper-specific jargon, etc.\nethics Guiding Questions: Where did your data come from; why is its collection and processing (they are independent permissions!) legal and ethical? Why are you not breaking the world?\nSee: this Medium article for more!\nfrom the experts NIPS (leading ML conference) rubric for a paper (jump to \u0026ldquo;Review Content\u0026rdquo; section)\nKameswari, Lalitha, Dama Sravani, and Radhika Mamidi. 2020. “Enhancing Bias Detection in Political News Using Pragmatic Presupposition.” In Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, nil. doi:10.18653/v1/2020.socialnlp-1.1. ","permalink":"https://www.jemoka.com/posts/kbhaml_your_first_article/","tags":null,"title":"AML: Your First Article"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhanatomy/","tags":null,"title":"anatomy"},{"categories":null,"contents":"anatomy learning is the learning of anatomy.\nAnatomy information acquired prior to medical school has a positive correlation in medical school outcomes. Also leveraging anatomy information.\n","permalink":"https://www.jemoka.com/posts/kbhanatomy_learning/","tags":null,"title":"anatomy learning"},{"categories":null,"contents":"Angelman Syndrome is a syndrome is ~1 in 15000, clinically recognizable, developmental delay syndrome.\ncause of Angelman Syndrome Angelman Syndrome is primarily caused by the UBE3A and the ubiquitin proteasome system. Poly-ubiquitin chain asks to discard cells.\n","permalink":"https://www.jemoka.com/posts/kbhangelman_syndrome/","tags":null,"title":"Angelman Syndrome"},{"categories":null,"contents":"Need-finding conversation Main idea: testing?\u0026mdash;pregnancy testing and COVID testing\ntalking to longer-scope challenges in visually impaired community Navigation; transportation Cannot see markers on smaller steps; trying to find an uber drive and cannot reorient ","permalink":"https://www.jemoka.com/posts/kbhanna_s_team_checkin/","tags":null,"title":"Anna's Team Checkin"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhanotehuaoeu/","tags":null,"title":"anotehuaoeu"},{"categories":null,"contents":"Anoushka is a student at Nueva, also the host of Project80, among other things.\n","permalink":"https://www.jemoka.com/posts/kbhanoushka_krishnan/","tags":null,"title":"Anoushka Krishnan"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhanthony_badger/","tags":null,"title":"Anthony Badger"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2020.607449\nOne-Liner oral lexical retrieval works better than qualitative narrative analysis to classify dementia; and semantic fluency + Disfluency features chucked on an SVM returns pretty good results.\nNovelty Tried two different assays of measuring linguistic ability: oral lexical retrieval metrics, and qualitative discourse features analysis of speech.\nNotable Methods Subjects divided into three groups\nGreat cog. decline Impaired but stable Healthy controls Administered BNT and SVF tests as baseline\nKey Figs Table 3 This figure tells us that the percentages of unrelated utterances was a statistically significant metric to figure differences between the three experimental groups.\n(CD, CS, HC: cognitive decline, cognitively stable (but declining normally), healthy control)\n(no other items are bolded)\nTable 4 This figure tells us the disfluency features analyzed. None of them were independently statistically significant.\nTable 5 This figure tells us that analyzing Semantic Verbal Fluency, plus the information of disfluency, trained on an SVM, actually shows \u0026gt;90% recall value?\nNew Concepts Discourse-Completion Task oral lexical retrieval discourse features modalization Semantic Verbal Fluency Boston Naming Test ","permalink":"https://www.jemoka.com/posts/kbhantonsson_2021/","tags":["ntj"],"title":"Antonsson 2021"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhany_name_here/","tags":null,"title":"any name here"},{"categories":null,"contents":"eansoetuhaosneu\n","permalink":"https://www.jemoka.com/posts/kbhaosneuhasoneuh/","tags":null,"title":"aosneuhasoneuh"},{"categories":null,"contents":"Electrostatics Other Factoids charged Chapters coulomb\u0026rsquo;s law superposition electric field ","permalink":"https://www.jemoka.com/posts/kbhap_phys_c_em_index/","tags":["index"],"title":"AP Phys C EM Index"},{"categories":null,"contents":"AP Phys C Mech is an examination held by the CollegeBoard in mechanics.\nThings to Study Permittivity of free space Impulse Springs! In general. Perhaps review old notes. How to be faster? Kepler\u0026rsquo;s Laws of Planetary Motion\n","permalink":"https://www.jemoka.com/posts/kbhap_phys_c_mech_index/","tags":["index"],"title":"AP Phys C Mech Index"},{"categories":null,"contents":"AP Statistics is an examination by the CollegeBoard.\nSee also crap to remember for AP Stats\nNon-Focus Mistakes file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf Interpretation of regression outputs Backlog Chi-square file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf Notes confidence interval hypothesis testing t-statistics chi-square data inference binomial distribution ","permalink":"https://www.jemoka.com/posts/kbhapstats/","tags":["index"],"title":"AP Statistics Index"},{"categories":null,"contents":"Show that:\n\\begin{equation} \\dv t e^{tA} = e^{tA}A \\end{equation}\nWe can apply the result we shown in eigenvalue:\n\\begin{equation} \\dv t \\qty(e^{tA}) = \\dv t \\qty(I + \\sum_{k=1}^{\\infty} \\frac{t^{k}}{k!}A^{k}) = \\qty(\\sum_{k=1}^{\\infty }\\frac{1}{k!}kt^{k-1}A^{k-1})A \\end{equation}\nWe do this separation because \\(k=0\\) would\u0026rsquo;t make sense to raise \\(A\\) (\\(k-1=-1\\)) to as we are unsure about the invertability of \\(A\\). Obviously \\(\\frac{1}{k!}k = \\frac{1}{(k-1)!}\\). Therefore, we can shift our index back yet again:\n\\begin{equation} \\qty(\\sum_{k=1}^{\\infty }\\frac{1}{k!}kt^{k-1}A^{k-1})A = \\qty(\\sum_{j=0}^{\\infty }\\frac{1}{j!}t^{j}A^{j})A \\end{equation}\nAwesome. So now we have the taylor series in \\(e^{tA}\\) back, times \\(A\\).\nSo therefore:\n\\begin{equation} \\qty(\\sum_{j=0}^{\\infty }\\frac{1}{j!}t^{j}A^{j})A = e^{tA}A \\end{equation}\nBe forewarned:\n\\begin{equation} e^{A}e^{B} \\neq e^{A+B} \\end{equation}\nmostly because matrix multiplication is not commutative..\n","permalink":"https://www.jemoka.com/posts/kbhapplying_eigenspace/","tags":null,"title":"applying eigenspace"},{"categories":null,"contents":"If we take entangled qubits, and separate them real far away, their behavior would be the same even despite it will take longer for light to travel.\n","permalink":"https://www.jemoka.com/posts/kbhapr_paradox/","tags":null,"title":"APR Paradox"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhaps/","tags":null,"title":"APS"},{"categories":null,"contents":"Background In the 60s, economists that the pricing of options were independent of pricing of underlying assets. Nowadays, we can see that, if the underlying assets were obeying of a Brownian Motion, there is no additional degree of freedom that options can bring: that knowing the stocks will tell you exactly through a DiffEQ how the option will evolve.\nThe idea, then, is that you can replicate options: by dynamically buying and selling pairs of securities in the same way as the option, your new portfolio can track the option exactly.\nOf course, there is a certain amount of volatility associated with Brownian Motion markets.\nUnfortunately, there is no one fixed volatility which can be used to model all options; you can fit a volatility given all strike prices\u0026mdash;creating an implied volatility surface.\nOtherwise, you can also model volatility as a random variable, a stochastic process modeled by stochastic volatility.\nReading pg 350-352: diffusion are described by stochastic differential equations Option Pricing A Vanilla Call Given some current price \\(S\\), option price \\(K\\), time to maturity \\(T\\); the payoff increases linearly after the option matures. How much should the option be changed for the right to buy the option after \\(T\\) days?\nWe can use the option info to calculate the implied volatility.\n","permalink":"https://www.jemoka.com/posts/kbharbitrage_pricing/","tags":null,"title":"Arbitrage Pricing"},{"categories":null,"contents":"Require: analyze movie + quote [story + bellows]\nStuff from Chaing \u0026ldquo;you will be twenty-five then\u0026rdquo; \u0026ldquo;Over time, the sentences I wrote grew shapelier, more cohesive. I had reached the point where it worked better when I didn\u0026rsquo;t think about it too much. Instead of carefully trying to design a sentence before writing, I could simply begin putting down strokes immediately; my initial strokes almost always turned out to be compatible with an elegant rendition of what I was trying to say. I was developing a faculty like that of the heptapods.\u0026rdquo; \u0026ldquo;I could understand that: the physical attributes that humans found intuitive, like kinetic energy or acceleration, were all properties of an objectat a given moment in time.\u0026rdquo; \u0026ldquo;And these were conducive to a teleological interpretation of events: by viewing events over a period of time, one recognized that there was a requirement that had to be satisfied, a goal of minimizing or maximizing. And one had to know the initial and final states to meet that goal; one needed knowledge of the effects before the causes could be initiated. I was growing to understand that, too.\u0026rdquo; \u0026ldquo;Gary once told me that the fundamental laws of physics were time-symmetric, that there was no physical difference between past and future.\u0026rdquo; \u0026ldquo;Explain it by saying that light minimized the time needed to travel to its destination, and one saw the world as the heptapods saw it.\u0026rdquo; Stuff from Bellows \u0026ldquo;A reader who says that poetry is what has been lost in translation is also claiming to be simultaneously in full possession of the original (which is poetry) and of the translation (which is not). Otherwise there would be no knowing if anything has been lost, let alone knowing that it was poetry.\u0026rdquo; \u0026ldquo;Because if the inhabitants of the distant planet did have a language, and if the space crew had learned it, then it must be possible for them to say what the aliens had said. Must, not should: radically untranslatable sounds do not make a language simply because we could not know it was a language unless we could translate it, even if only roughly\u0026rdquo; \u0026ldquo;The tonal and rhythmic patterns of whale song are of such complexity as to make it quite impossible to believe that what we can hear (and pick up on instruments more sensitive than human ears) is just random noise.\u0026rdquo; nobody knows how to translate “animal signals” into human speech or vice versa. When and if we ever can translate nonhuman noises into human speech, species-related ineffabilities will evaporate like the morning haze. Translation is the enemy of the ineffable. One causes the other to cease to exist. Movie Script LOUISE \u0026ldquo;We are so bound by time; by its order.\u0026rdquo; DR. KETTLER \u0026ldquo;Not everyone is wired for what you\u0026rsquo;re about to do. Our brains aren\u0026rsquo;t always able to process experiences like this.\u0026rdquo; RUSSIAN SCIENTIST: \u0026ldquo;Their final words translate to, \u0026ldquo;There is no time, many become one.\u0026rdquo; I fear we have all been given weapons because we answered the timeline wrong, please, if you - -\u0026rdquo; Childhood Flashback \u0026ldquo;memory is a strange thing\u0026rdquo; \u0026ldquo;we are so bound by time, by its order\u0026rdquo;\u0026mdash;progression of time backed into the inherentness of language the story opens with an emotional connection to the audience for imprinting Lecture Scene\u0026mdash;Alien Invasion \u0026ldquo;Authority assess the object\u0026rdquo; emphasis a weird place Shadow framing of the events question: why is she so worried Second time with plan down from blackness Army Scene + Fly Away Scene \u0026ldquo;Fluttering\u0026rdquo; reproduced Army attempts to replicate the results Sanskrit word for \u0026ldquo;war\u0026rdquo; and its translation\u0026mdash;\u0026ldquo;Louis: desire for more cows; Cal: argument\u0026rdquo; Film environment from the back, filming forward Helicopter Scene Louis: communication as first priority vs. Army: understanding as first priority Approach Scene Music pitched the \u0026ldquo;alien\u0026rdquo; flutters tonally Kind of emotional communication with the audience, the Dalian sound + high highs EQs is a foreignization technique taller than it was in the movie Entry Scene Driving: perspective questions \u0026ldquo;every 18 hours\u0026rdquo; continuous panning from black downwards Sargent\u0026rsquo;s differing eye length Difference in gravity and perspectives? Camera angle trickery, panning down is no longer the same direction \u0026ldquo;light at the end of the tunnel\u0026rdquo; again, music + dissonant sounds create foreignization in the audience \u0026ldquo;they arrive\u0026rdquo;\nCommunication scene They are very animal-like as portrayed in the film \u0026ldquo;Visual Aid\u0026rdquo; scenes Pairwise matching \u0026ldquo;A more advanced race\u0026rdquo; Rosetta stone behavior Taking of headgear: contrast between small vs. large (size differences Increasing breathing during a moment of transition Introductory scene not sure if they have names making an assumption what is happening to Louise\u0026rsquo;s thoughts \u0026ldquo;As Banks studies the language, she starts to have flashback-like visions of her daughter. \u0026quot; Panic worried Scene Fear of the unknown Thoughts flashing back: symbols muting sounds Flashing back being real: Thoughts having panicked sensation of time Repeating single syllable\u0026mdash;foreignization \u0026ldquo;unline speech, a logogram is free of time\u0026rdquo; Voiceover Scene No directionality: complex understanding The repeated vocalizations help highlight the distantness Dialogue Between the two \u0026ldquo;Sapire-Wolf Hypothesis\u0026rdquo;: being used incorrectly; hallucinating Louis' China mobilizing forces Majiong is a form of mobiling forces Final Message Group \u0026ldquo;offer\u0026rdquo; vs \u0026ldquo;use\u0026rdquo; \u0026mdash; the US understands it as \u0026ldquo;offer\u0026rdquo; and China understands it as \u0026ldquo;use\u0026rdquo; contextual intepretation varies how the use of linguistics language as something contextually dependent large amounts of communication can be packed very densely Non-Zero Sum Game the ask to work together need to be interpreted differently different parts of time fold together to become hole: \u0026ldquo;non zero sum game\u0026rdquo; The alien speech is being subtitled! Final communication Result becomes \u0026ldquo;objective\u0026rdquo;: i.e. there is a direct understanding of the aliens, suddenly Also, palendromic names: \u0026ldquo;Hannah\u0026rdquo; for daughter, not translatable Seeing into the future Seeing into Time Being able to understand heptopod + time properly means that they are able to understand time Gave private number in the future allow you to see the past: General Shang can see into the future Why is the banquet colored yellow Finale Repeat of the opener scene: pan down as a trope that cycles from the beginning Ian is in Louise\u0026rsquo;s house! The house+baby scenes (which is different from baby nature scenes) is lit orange in the same way as the banquet scene whereas the hospital scene and the house scene were lit blue ","permalink":"https://www.jemoka.com/posts/kbharrival_movie/","tags":null,"title":"Arrival Movie"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbharthur_m_schlesinger/","tags":null,"title":"Arthur M. Schlesinger"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhasbmb/","tags":null,"title":"ASBMB"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhasee_prism/","tags":null,"title":"ASEE Prism"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhasip/","tags":null,"title":"ASIP"},{"categories":null,"contents":"ASR are tech that helps make transcripts from speech\n","permalink":"https://www.jemoka.com/posts/kbhasr/","tags":null,"title":"ASR"},{"categories":null,"contents":"associative means that operations can be grouped in any way as long as order is preserved.\nThat is:\n\\begin{equation} (AB)C = A(BC) \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhassociative/","tags":null,"title":"associative"},{"categories":null,"contents":"~ Given functions \\(f(n)\\) and \\(g(n)\\), if:\n\\begin{equation} \\lim_{n\\to \\infty} \\left(\\frac{f(n)}{g(n)}\\right) = 1 \\end{equation}\nwe say that \\(f \\sim g\\).\nThat \u0026ndash; the relationship between \\(f\\) and \\(g\\) grows in a similar fashion as \\(n\\) increases. For instance:\n\\(f(n) = n+1\\) \\(g(n) = n+2\\) Therefore:\n\\begin{equation} f\\sim g = \\lim_{n\\to \\infty} \\frac{f(n)}{g(n)} = \\lim_{n\\to \\infty} \\frac{n+1}{n+2} = 1 \\end{equation}\nThe \\(\\sim\\) operator is commutative (\\(f \\sim g \\Rightarrow g\\sim f\\)) and transitive (\\(f\\sim g, g\\sim h \\Rightarrow f \\sim h\\)).\no(n) Given two functions \\(f(n)\\), \\(g(n)\\), if their relationship shows:\n\\begin{equation} \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\end{equation}\nwe can write it as\n\\begin{equation} f = o(g) \\end{equation}\nThis tells us that if \\(n\\) becomes very large, \\(g\\) becomes much larger than \\(f\\). \\(f\\) does not grow nearly as fast as \\(g\\).\nThe operation is not commutative, but is transitive (\\(f = o(g), g = o(h) \\Rightarrow f = o(h)\\))\nO(n) Given two functions \\(f(n)\\), \\(g(n)\\).\n\\begin{equation} \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} \u0026lt; \\infty \\end{equation}\nthat the relationship between \\(f(n)\\) and \\(g(n)\\) is countable as \\(n\\) trends to infinity.\nWe can also say that, given \\(n\\), \\(n_0\\), and some \\(c\\) which \\(\\forall n, n \u0026gt; n_0\\), there is:\n\\begin{equation} |f(n)| \u0026lt; |cg(n)| \\end{equation}\nThis tells us that \\(f(n)\\) does not grow much much faster than \\(g(n)\\).\nTherefore:\nIf \\(f \\sim g\\), \\(f = O(g)\\) (as they grow together, \\(f\\) is not much faster) If \\(f = o(g)\\), \\(f=O(g)\\) (as \\(f\\) does not grow at all, \\(f\\) is not faster) \\(\\theta\\)(n) \\(f=\\theta(g)\\) IFF \\(f=O(g)\\) and \\(g=O(f)\\), its essentially \\(\\sim\\) but without the strict requirement of a 1:1 ratio.\n\\(\\omega\\)(n) and \\(\\Omega\\)(n) The inverses of \\(O\\) and \\(o\\):\n\\(f(n) = O(g(n)) \\Rightarrow g(n) = \\omega(f(n))\\) \\(f(n) = o(g(n)) \\Rightarrow g(n) = \\Omega(f(n))\\) ","permalink":"https://www.jemoka.com/posts/kbhasymtotic_analysis/","tags":null,"title":"asymtotic analysis"},{"categories":null,"contents":"You can use atoms as many different types of qubits.\nmanipulating physical qubits To make physical qubits go to different states, we will again use something in the ancillary states. Rotating it to \\(z\\) \u0026mdash; leverage one lazer to make it fall; \\(rx\\), \\(ry\\), we leverage combinations of two light.\nvarious qubit implementations Implementations of physical qubits\nType Superconductor Ions Atoms Company Google, IBM, Rigetti IonQ, Honeywell Atom Computing, QuEra Nature Artifical Natural Natural Calibration Individual calibration Naturally calibrated Naturally calibrated Coherence Time Short Long Long Connectivity Adjacent connectivity All-to-all More than adjacent Scalability Compatible with existing tech Not easily scalable Potentially scalable Speed Fast gates Kinda fast Untested possible uses for qubits Here are some possible uses for physical qubits\nTraveling salesman Research + simulations Cryptography ","permalink":"https://www.jemoka.com/posts/kbhatoms_as_qubits/","tags":null,"title":"atoms as qubits"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhauthoritarianism/","tags":null,"title":"Authoritarianism"},{"categories":null,"contents":"autism is a spectrum disorder that are caused by both environmental and genetic factors.\nKey Question: how can different chromatin regulators lead to the same set of symptoms named \u0026ldquo;autism\u0026rdquo;.\nautism gene signature The gene signature of autism can be measured in clean and quantitative assays.\n","permalink":"https://www.jemoka.com/posts/kbhautism/","tags":null,"title":"autism"},{"categories":null,"contents":"Key sequence In this chapter, we defined complex numbers, their definition, their closeness under addition and multiplication, and their properties These properties make them a field: namely, they have, associativity, commutativity, identities, inverses, and distribution. notably, they are different from a group by having 1) two operations 2) additionally, commutativity and distributivity. We then defined \\(\\mathbb{F}^n\\), defined addition, additive inverse, and zero. These combined (with some algebra) shows that \\(\\mathbb{F}^n\\) under addition is a commutative group. Lastly, we show that there is this magical thing called scalar multiplication in \\(\\mathbb{F}^n\\) and that its associative, distributive, and has an identity. Technically scalar multiplication in \\(\\mathbb{F}^n\\) commutes too but extremely wonkily so we don\u0026rsquo;t really think about it. New Definitions complex number addition and multiplication of complex numbers subtraction and division of complex numbers field: \\(\\mathbb{F}\\) is \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) power list \\(\\mathbb{F}^n\\): F^n coordinate addition in \\(\\mathbb{F}^n\\) additive inverse of \\(\\mathbb{F}^n\\) \\(0\\): zero scalar multiplication in \\(\\mathbb{F}^n\\) Results and Their Proofs properties of complex arithmetic commutativity associativity identities additive inverse multiplicative inverse distributive property properties of \\(\\mathbb{F}^n\\) addition in \\(\\mathbb{F}^n\\) is associative addition in \\(\\mathbb{F}^n\\) is commutative addition in \\(\\mathbb{F}^n\\) has an identity (zero) addition in \\(\\mathbb{F}^n\\) has an inverse scalar multiplication in \\(\\mathbb{F}^n\\) is associative scalar multiplication in \\(\\mathbb{F}^n\\) has an identity (one) scalar multiplication in \\(\\mathbb{F}^n\\) is distributive Question for Jana No demonstration in exercises or book that scalar multiplication is commutative, why? Interesting Factoids You can take a field, look at an operation, and take that (minus the other op\u0026rsquo;s identity), and call it a group (groups (vector spaces (fields ))) ","permalink":"https://www.jemoka.com/posts/kbhaxler_a/","tags":null,"title":"Axler 1.A"},{"categories":null,"contents":"Key Sequence \\(\\mathbb{F}^{n}\\) not being a field kinda sucks, so we made an object called a \u0026ldquo;vector space\u0026rdquo; which essentially does everything a field does except without necessitating a multiplicative inverse Formally, a vector space is closed over addition and have a scalar multiplication. Its addition is commutative, both addition and scalar multiplication is associative, and distributivity holds. There is an additive identity, additive inverse, and multiplicative identity. We defined something called \\(\\mathbb{F}^{S}\\), which is the set of functions from a set \\(S\\) to \\(\\mathbb{F}\\). Turns out, \\(\\mathbb{F}^{S}\\) is a Vector Space Over \\(\\mathbb{F}\\) and we can secretly treat \\(\\mathbb{F}^{n}\\) and \\(\\mathbb{F}^{\\infty}\\) as special cases of \\(\\mathbb{F}^{s}\\). We established that identity and inverse are unique additively in vector spaces. Lastly, we proved some expressions we already know: \\(0v=0\\), \\(-1v=-v\\). New Definitions addition and scalar multiplication vector space and vectors vector space \u0026ldquo;over\u0026rdquo; fields \\(V\\) denotes a vector space over \\(\\mathbb{F}\\) \\(-v\\) is defined as the additive inverse of \\(v \\in V\\) Results and Their Proofs \\(\\mathbb{F}^{\\infty}\\) is a Vector Space over \\(\\mathbb{F}\\) \\(\\mathbb{F}^{S}\\) is a Vector Space Over \\(\\mathbb{F}\\) All vector spaces \\(\\mathbb{F}^{n}\\) and \\(\\mathbb{F}^{\\infty}\\) are just special cases \\(\\mathbb{F}^{S}\\): you can think about those as a mapping from coordinates \\((1,2,3, \\dots )\\) to their actual values in the \u0026ldquo;vector\u0026rdquo; additive identity is unique in a vector space additive inverse is unique in a vector space \\(0v=0\\), both ways (for zero scalars and vectors) \\(-1v=-v\\) Questions for Jana The way Axler presented the idea of \u0026ldquo;over\u0026rdquo; is a tad weird; is it really only scalar multiplication which hinders vector spaces without \\(\\mathbb{F}\\)? In other words, do the sets that form vector spaces, apart from the \\(\\lambda\\) used for scalar multiplication, need anything to do with the \\(\\mathbb{F}\\) they are \u0026ldquo;over\u0026rdquo;? The name of the field and what its over do not have to be the same\u0026mdash;\u0026ldquo;vector space \\(\\mathbb{C}^2\\) over \\(\\{0,1\\}\\)\u0026rdquo; is a perfectly valid statement If lists have finite length \\(n\\), then what are the elements of \\(\\mathbb{F}^{\\infty}\\) called? \u0026ldquo;we could think about \\(\\mathbb{F}^{\\infty}\\), but we aren\u0026rsquo;t gonna.\u0026rdquo; Why is \\(1v=v\\) an axiom, whereas we say that some \\(0\\) exists? because we know 1 already, and you can follow the behavor of scalar multiplication what\u0026rsquo;s that thing called again in proofs where you just steal the property of a constituent element?: inherits Interesting Factoids The simplest vector space is \\(\\{0\\}\\) ","permalink":"https://www.jemoka.com/posts/kbhaxler_1_b/","tags":null,"title":"Axler 1.B"},{"categories":null,"contents":"Key Sequence we defined subspace and how to check for them we want to operate on subsets, so we defined the sum of subsets we saw that the sum of subspaces are the smallest containing subspace and finally, we defined direct sums and how to prove them New Definitions subspace sum of subsets direct sum Results and Their Proofs checking for subspaces simplified check for subspace sum of subspaces is the smallest subspace with both subspaces creating direct sums a sum of subsets is a direct sum IFF there is only one way to write \\(0\\) a sum of subsets is only a direct sum IFF their intersection is the set containing \\(0\\) Questions for Jana Does the additive identity have be the same between different subspaces of the same vector space? yes, otherwise the larger vector space has two additive identities. Does the addition and multiplication operations in a subspace have to be the same as its constituent vector space? by definition Why are direct sums defined on sub-spaces and not sum of subsets? because the union is usually not a subspace so we use sums and keep it in subspaces ","permalink":"https://www.jemoka.com/posts/kbhaxler_1_c/","tags":null,"title":"Axler 1.C"},{"categories":null,"contents":"3: Show that the set of differential real-valued functions \\(f\\) on the interval \\((-4,4)\\) such that \\(f\u0026rsquo;(-1)=3f(2)\\) is a subspace of \\(\\mathbb{R}^{(-4,4)}\\)\n4: Suppose \\(b \\in R\\). Show that the set of continuous real-valued functions \\(f\\) on the interval \\([0,1]\\) such that \\(\\int_{0}^{1}f=b\\) is a subspace of \\(\\mathbb{R}^{[0,1]}\\) IFF \\(b=0\\)\nAdditive Identity:\nassume \\(\\int_{0}^{1}f=b\\) is a subspace\n","permalink":"https://www.jemoka.com/posts/kbhaxler_1_c_excercises/","tags":null,"title":"Axler 1.C Exercises"},{"categories":null,"contents":"Key Sequence we defined the combination of a list of vectors as a linear combination and defined set of all linear combination of vectors to be called a span we defined the idea of a finite-dimensional vector space vis a vi spanning we took a god-forsaken divergence into polynomials that will surely not come back and bite us in chapter 4 we defined linear independence + linear dependence and, from those definition, proved the actual usecase of these concepts which is the Linear Dependence Lemma we apply the Linear Dependence Lemma to show that length of linearly-independent list \\(\\leq\\) length of spanning list as well as that finite-dimensional vector spaces make finite subspaces. Both of these proofs work by making linearly independent lists\u0026mdash;the former by taking a spanning list and making it smaller and smaller, and the latter by taking a linearly independent list and making it bigger and bigger New Definitions linear combination span + \u0026ldquo;spans\u0026rdquo; finite-dimensional vector space infinite-demensional vector space finite-dimensional subspaces polynomial \\(\\mathcal{P}(\\mathbb{F})\\) \\(\\mathcal{P}_{m}(\\mathbb{F})\\) degree of a polynomial \\(\\deg p\\) linear independence and linear dependence Linear Dependence Lemma Results and Their Proofs span is the smallest subspace containing all vectors in the list \\(\\mathcal{P}(\\mathbb{F})\\) is a vector space over \\(\\mathbb{F}\\) the world famous Linear Dependence Lemma and its fun issue length of linearly-independent list \\(\\leq\\) length of spanning list subspaces of inite-dimensional vector spaces is finite dimensional Questions for Jana obviously polynomials are non-linear structures; under what conditions make them nice to work with in linear algebra? what is the \u0026ldquo;obvious way\u0026rdquo; to change Linear Dependence Lemma\u0026rsquo;s part \\(b\\) to make \\(v_1=0\\) work? for the finite-dimensional subspaces proof, though we know that the process terminates, how do we know that it terminates at a spanning list of \\(U\\) and not just a linearly independent list in \\(U\\)? direct sum and linear independence related; how exactly? Interesting Factoids I just ate an entire Chinese new-year worth of food while typing this up. That\u0026rsquo;s worth something right\n","permalink":"https://www.jemoka.com/posts/kbhaxler_2_a/","tags":null,"title":"Axler 2.A"},{"categories":null,"contents":"Key Sequence we defined basis of a vector space\u0026mdash;a linearly independent spanning list of that vector space\u0026mdash;and shown that to be a basis one has to be able to write a write an unique spanning list we show that you can chop a spanning list of a space down to a basis or build a linearly independent list up to a basis because of this, you can make a spanning list of finite-dimensional vector spaces and chop it down to a basis: so every finite-dimensional vector space has a basis lastly, we can use the fact that you can grow list to basis to show that every subspace of \\(V\\) is a part of a direct sum equaling to \\(V\\) New Definitions basis and criteria for basis\nI mean its a chapter on bases not sure what you are expecting.\nResults and Their Proofs a list is a basis if you can write every memeber of their span uniquely every finite-dimensional vector space has a basis dualing basis constructions all spanning lists contains a basis of which you are spanning a linearly independent list expends to a basis every subspace of \\(V\\) is a part of a direct sum equaling to \\(V\\) Questions for Jana Is the subspace direct sum proof a unique relationship? That is, is every complement \\(W\\) for each \\(U \\subset V\\) unique? ","permalink":"https://www.jemoka.com/posts/kbhaxler_2_b/","tags":null,"title":"Axler 2.B"},{"categories":null,"contents":"Key Sequence Because Length of Basis Doesn\u0026rsquo;t Depend on Basis, we defined dimension as the same, shared length of basis in a vector space We shown that lists of the right length (i.e. dim that space) that is either spanning or linearly independent must be a basis\u0026mdash;\u0026ldquo;half is good enough\u0026rdquo; theorems we also shown that \\(dim(U_1+U_2) = dim(U_1)+dim(U_2) - dim(U_1 \\cap U_2)\\): dimension of sums New Definitions dimension Results and Their Proofs Length of Basis Doesn\u0026rsquo;t Depend on Basis lists of right length are basis linearly independent list of length dim V are a basis of V spanning list of length of dim V are a basis of V dimension of sums Questions for Jana Example 2.41: why is it that \\(\\dim U \\neq 4\\)? We only know that \\(\\dim \\mathcal{P}_{3}(\\mathbb{R}) = 4\\), and \\(\\dim U \\leq 4\\). Is it because \\(U\\) (i.e. basis of \\(U\\) doesn\u0026rsquo;t span the polynomial) is strictly a subset of \\(\\mathcal{P}_{3}(\\mathbb{R})\\), so there must be some extension needed? because we know that \\(U\\) isn\u0026rsquo;t all of \\(\\mathcal{P}_{3}\\). Interesting Factoids ","permalink":"https://www.jemoka.com/posts/kbhaxler_2_c/","tags":null,"title":"Axler 2.C"},{"categories":null,"contents":"OMGOMGOMG its Linear Maps time! \u0026ldquo;One of the key definitions in linear algebra.\u0026rdquo;\nKey Sequence We define these new-fangled functions called Linear Maps, which obey \\(T(u+v) = Tu+Tv\\) and \\(T(\\lambda v) = \\lambda Tv\\) We show that the set of all linear maps between two vector spaces \\(V,W\\) is denoted \\(\\mathcal{L}(V,W)\\); and, in fact, by defining addition and scalar multiplication of Linear Maps in the way you\u0026rsquo;d expect, \\(\\mathcal{L}(V,W)\\) is a vector space! this also means that we can use effectively the \\(0v=0\\) proof to show that linear maps take \\(0\\) to \\(0\\) we show that Linear Maps can be defined uniquely by where it takes the basis of a vector space; in fact, there exists a Linear Map to take the basis anywhere you want to go! though this doesn\u0026rsquo;t usually make sense, we call the \u0026ldquo;composition\u0026rdquo; operation on Linear Maps their \u0026ldquo;product\u0026rdquo; and show that this product is associative, distributive, and has an identity New Definitions Linear Map \u0026mdash; additivity (adding \u0026ldquo;distributes\u0026rdquo;) and homogeneity (scalar multiplication \u0026ldquo;factors\u0026rdquo;) \\(\\mathcal{L}(V,W)\\) any polynomial map from Fn to Fm is a linear map addition and scalar multiplication on \\(\\mathcal{L}(V,W)\\); and, as a bonus, \\(\\mathcal{L}(V,W)\\) a vector space! naturally (almost by the same \\(0v=0\\) proof), linear maps take \\(0\\) to \\(0\\) Product of Linear Maps is just composition. These operations are: associative distributive has an identity Results and Their Proofs technically a result: any polynomial map from Fn to Fm is a linear map basis of domain of linear maps uniquely determines them Questions for Jana why does the second part of the basis of domain proof make it unique? ","permalink":"https://www.jemoka.com/posts/kbhaxler_3_a/","tags":null,"title":"Axler 3.A"},{"categories":null,"contents":"Key Sequence we defined the null space and injectivity from that, we showed that injectivity IFF implies that null space is \\(\\{0\\}\\), essentially because if \\(T0=0\\) already, there cannot be another one that also is taken to \\(0\\) in an injective function we defined range and surjectivity we showed that these concepts are strongly related by the fundamental theorem of linear maps: if \\(T \\in \\mathcal{L}(V,W)\\), then \\(\\dim V = \\dim null\\ T + \\dim range\\ T\\) from the fundamental theorem, we showed the somewhat intuitive pair about the sizes of maps: map to smaller space is not injective, map to bigger space is not surjective we then applied that result to show results about homogeneous systems homogenous system with more variables than equations has nonzero solutions inhomogenous system with more equations than variables has no solutions for an arbitrary set of constants New Definitions null space injectivity range surjectivity homogeneous system Results and Their Proofs the null space is a subspace of the domain injectivity IFF implies that null space is \\(\\{0\\}\\) the fundamental theorem of linear maps \u0026ldquo;sizes\u0026rdquo; of maps map to smaller space is not injective map to bigger space is not surjective solving systems of equations: homogenous system with more variables than equations has nonzero solutions inhomogenous system with more equations than variables has no solutions for an arbitrary set of constants Questions for Jana \u0026ldquo;To prove the inclusion in the other direction, suppose v 2 null T.\u0026rdquo; for 3.16; what is the first direction? maybe nothing maps to \\(0\\) ","permalink":"https://www.jemoka.com/posts/kbhaxler_3_b/","tags":null,"title":"Axler 3.B"},{"categories":null,"contents":"matricies!!!!\nKey Sequence matricies exist, you can add them, scalarly multiply them, and actually multiply them they can represent Linear Maps by showing where they take basis unsurprisingly, the set of matricies of a shape is a vector space New Definitions matricies matrix of Linear Map matrix addition and scalar multiplications matrix multiplication \\(\\mathbb{F}^{m,n}\\) Results and Their Proofs sums and scalar multiplication of matricies, and why they work to represent Linear Maps \\(\\mathbb{F}^{m,n}\\) is a vector space Interesting Factoids its literally matricies\n","permalink":"https://www.jemoka.com/posts/kbhaxler_3_c/","tags":null,"title":"Axler 3.C"},{"categories":null,"contents":"isomorphisms. Somebody\u0026rsquo;s new favourite word since last year.\nKey Sequence we showed that a linear map\u0026rsquo;s inverse is unique, and so named the inverse \\(T^{-1}\\) we then showed an important result, that injectivity and surjectivity implies invertability this property allowed us to use invertable maps to define isomorphic spaces, naming the invertable map between them as the isomorphism we see that having the same dimension is enough to show invertability (IFF), because we can use basis of domain to map the basis of one space to another we then use that property to establish that matricies and linear maps have an isomorphism between them: namely, the matrixify operator \\(\\mathcal{M}\\). this isomorphism allow us to show that the dimension of a set of Linear Maps is the product of the dimensions of their domain and codomain (that \\(\\dim \\mathcal{L}(V,W) = (\\dim V)(\\dim W)\\)) We then, for some unknown reason, decided that right this second we gotta define matrix of a vector, and that linear map applications are like matrix multiplication because of it. Not sure how this relates finally, we defined a Linear Map from a space to itself as an operator we finally show an important result that, despite not being true for infinite-demensional vector space, injectivity is surjectivity in finite-dimensional operators New Definitions invertability isomorphism + isomorphic vector spaces matrix of a vector operator Results and Their Proofs linear map inverse is unique injectivity and surjectivity implies invertability two vector spaces are isomorphic IFF they have the same dimension matricies and Linear Maps from the right dimensions are isomorphic \\(\\dim \\mathcal{L}(V,W) = (\\dim V)(\\dim W)\\) \\(\\mathcal{M}(T)_{.,k} = \\mathcal{M}(Tv_{k})\\), a result of how everything is defined (see matrix of a vector) \u0026ldquo;each column of a matrix represents where each of the basis of the input gets taken to\u0026rdquo; So applying a vector to a matrix shows the linear combination of what where the basis sent linear maps are like matrix multiplication injectivity is surjectivity in finite-dimensional operators Questions for Jana why doesn\u0026rsquo;t axler just say the \u0026ldquo;basis of domain\u0026rdquo; directly (i.e. he did a lin comb instead) for the second direction for the two vector spaces are isomorphic IFF they have the same dimension proof? because the next steps for spanning (surjectivity) and linear independence (injectivity) is made more obvious clarify the matricies and Linear Maps from the right dimensions are isomorphic proof what is the \u0026ldquo;multiplication by \\(x^{2}\\)\u0026rdquo; operator? literally multiplying by \\(x^{2}\\) how does the matrix of a vector detour relate to the content before and after? I suppose an isomorphism exists but it isn\u0026rsquo;t explicitly used in the linear maps are like matrix multiplication proof, which is the whole point because we needed to close the loop of being able to linear algebra with matricies completely, which we didn\u0026rsquo;t know without the isomorphism between matricies and maps Interesting Factoids ","permalink":"https://www.jemoka.com/posts/kbhaxler_3_d/","tags":null,"title":"Axler 3.D"},{"categories":null,"contents":"No idea why this is so long!!!\nKey Sequence Firehose of a chapter.\nWe first began an unrelated exploration in Product of Vector Spaces (\u0026ldquo;tuples\u0026rdquo;): we show that the Product of Vector Spaces is a vector space because you can build a list out of zeroing every element except each one on each basis of each element of the tuple sequentially, we learned that the dimension of the Product of Vector Spaces is the sum of the spaces\u0026rsquo; dimension. we defined the product-to-sum map \\(\\Gamma\\) \\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\Gamma\\) is injective and, as a result, \\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\dim (U_1 + \\dots + U_{m}) = \\dim U_1 + \\dots + \\dim U_{m}\\) We then tackled the fun part of this chapter, which is affine subsets, parallel structures, quotient spaces, quotient map (affine subsetification maps) we learned an important and useful result that two affine subsets parallel to \\(U\\) are either equal or disjoint (\\(v-w \\in U\\) means \\(v+U = w+U\\) means \\(v+U \\cap w+U \\neq \\emptyset\\), means the first thing) we defined the operations on quotient space, and showed that quotient space operations behave uniformly on equivalent affine subsets. This, and the usual closer proof, demonstrates that quotient spaces is a vector space with the help of the affine subsetification map (the quotient map \\(\\pi\\)), we show that the dimension of a quotient space is the difference between dimensions of its constituents essentially by invoking rank-nullity theorem after knowing the fact that \\(null\\ \\pi = U\\) (because \\(u+U\\) is an affine subset that has not been shifted (think about a line moving along itself\u0026hellip; it doesn\u0026rsquo;t move)) Then, and I\u0026rsquo;m not quite sure why, we defined \\(\\widetilde{T}: V / null\\ T \\to W\\), for some \\(T: V\\to W\\), defined as \\(\\widetilde{T}(v+null\\ T) = Tv\\). We show that the map is Linear, injective, its range is \\(range\\ T\\), and so it forms an isomorphism between \\(V / null\\ T\\) and \\(range\\ T\\). Here\u0026rsquo;s something: products and quotients, the intuition\nNew Definitions Product of Vector Spaces operations on Product of Vector Spaces product summation map \\(\\Gamma\\) sum of vector and subspace parallel + affine subset quotient space operations on the quotient space quotient map \\(\\widetilde{T}\\) Results and Their Proofs Product of Vector Spaces is a vector space dimension of the Product of Vector Spaces is the sum of the spaces\u0026rsquo; dimension Results relating to \\(\\Gamma\\) \\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\Gamma\\) is injective \\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\dim (U_1 + \\dots + U_{m}) = \\dim U_1 + \\dots + \\dim U_{m}\\) results relating to affine subsets and quotient spaces two affine subsets parallel to \\(U\\) are either equal or disjoint quotient space operations behave uniformly on equivalent affine subsets quotient space is a vector space: bleh just prove it yourself. additive identity is \\(0+U\\) and additive inverse is \\(-v + U\\). dimension of a quotient space is the difference between dimensions of its constituents results relating to \\(\\widetilde{T}\\) \\(\\widetilde{T}\\) is well defined properties of \\(\\widetilde{T}\\) it is linear it is injective its range is the range of \\(range\\ T\\) it is an isomorphism between \\(V / null\\ T\\) and \\(range\\ T\\) Questions for Jana what\u0026rsquo;s the point of learning about \\(\\widetilde{T}\\)? how are Product of Vector Spaces and quotient space opposites of each other?: products and quotients, the intuition Interesting Factoids Happy Lunar New Year! Also, let\u0026rsquo;s hope this is not a trend:\n","permalink":"https://www.jemoka.com/posts/kbhaxler_3_e/","tags":null,"title":"Axler 3.E"},{"categories":null,"contents":"EIGENSTUFF and OPERATORS! Invariant subspaces are nice.\nSometimes, if we can break the domain of a linear map down to its eigenvalues, we can understand what its doing on a component-wise level.\nKey Sequence we defined an invariant subspace, and gave a name to 1-D invariant subspaces: the span of eigenvectors we showed some properties of eigenvalues and showed that a list of eigenvectors are linearly independent a correlate of this is that operators on finite dimensional V has at most dim V eigenvalues finally, we defined map restriction operator and quotient operator, and showed that they were well-defined New Definitions invariant subspace conditions for nontrivial invariant subspace eigenvalues + eigenvectors + eigenspace two new operators: map restriction operator and quotient operator Results and Their Proofs properties of eigenvalues list of eigenvectors are linearly independent eigenspaces are disjoint operators on finite dimensional V has at most dim V eigenvalues quotient operator is well-defined Questions for Jana Interesting Factoids \u0026ldquo;eigenvalue\u0026rdquo; is sometimes called the \u0026ldquo;characterizing value\u0026rdquo; of a map\nfinding eigenvalues with actual numbers natural choordinates of a map ","permalink":"https://www.jemoka.com/posts/kbhaxler_5_a/","tags":null,"title":"Axler 5.A"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.635945\nOne-Liner extracted lexicographic and syntactical features from ADReSS Challenge data and trained it on various models, with BERT performing the best.\nNovelty ???????\nSeems like results here are a strict subset of Zhu 2021. Same sets of dataprep of Antonsson 2021 but trained on a BERT now. Seem to do worse than Antonsson 2021 too.\nNotable Methods Essentially Antonsson 2021\nAlso performed MMSE score regression. Key Figs Table 7 training result This figure shows us that the results attained by training on extracted feature is past the state-of-the-art at the time.\nTable 4 These tables tells us the feature extracted\n","permalink":"https://www.jemoka.com/posts/kbhbalagopalan_2021/","tags":["ntj"],"title":"Balagopalan 2021"},{"categories":null,"contents":"A basis is a list of vectors in \\(V\\) that spans \\(V\\) and is linearly independent\nconstituents a LIST! of vectors in vector space \\(V\\) requirements the list is\u0026hellip; linear independent spans \\(V\\) additional information criteria for basis A list \\(v_1, \\dots v_{n}\\) of vectors in \\(V\\) is a basis of \\(V\\) IFF every \\(v \\in V\\) can be written uniquely as:\n\\begin{equation} v = a_1v_1+ \\dots + a_{n}v_{n} \\end{equation}\nwhere \\(a_1, \\dots, a_{n} \\in \\mathbb{F}\\).\nforward direction Suppose we have \\(v_1, \\dots, v_{n}\\) as the basis in \\(V\\). We desire that \\(v_1, \\dots v_{n}\\) uniquely constructs each \\(v \\in V\\).\nBy definition, they span \\(V\\) and are linear independent in \\(V\\).\nBecause of the spanning quality, there exists at least one set of \\(a_1, \\dots, a_{n} \\in \\mathbb{F}\\) such that we can write:\n\\begin{equation} v \\in V = a_1v_1+ \\dots + a_{n}v_{n} \\end{equation}\nSuppose now that we have another representation of \\(v\\) via scalars \\(c_1, \\dots, c_{n}\\) and our same list of vectors:\n\\begin{equation} v \\in V =^{?} c_1v_1+ \\dots + c_{n}v_{n} \\end{equation}\nSubtracting the two expressions, we have that:\n\\begin{equation} 0 = (a_1-c_1)v_1 + \\dots +(a_{n}-c_{n}) v_{n} \\end{equation}\nBy definition that \\(v_1 \\dots v_{n}\\) is linearly independent, we have that \\(a_j-c_j=0 \\implies a_{j}=c_{j}\\). Therefore, there is only one unique representation for \\(v\\) as a linear combination of vectors \\(v_1, \\dots v_{n}\\).\n(to be honest, we could have just applied that as the definition of linear independence that the scalars in a linear combo of linearly independent list is unique but this is the more careful definition.)\nbackward direction Suppose we have a list \\(v_1, \\dots v_{n}\\) which uniquely constructs each \\(v \\in V\\). We desire that \\(v_1, \\dots v_{n}\\) is a basis in \\(V\\). Given a linear combination thereof can construct all \\(v \\in V\\), we can say that \\(v_1, \\dots v_{n}\\) spans \\(V\\).\nAs \\(V\\) is a vector space, we have \\(0 \\in V\\). Therefore, there exists some scalars \\(a_1, \\dots a_{n}\\) for which:\n\\begin{equation} 0 = a_1v_1 + \\dots +a_{n}v_{n} \\end{equation}\n(as we already established \\(v_1, \\dots, v_{n}\\) spans \\(V\\) and \\(0 \\in V\\))\nOf course, we are given that \\(v_1, \\dots v_{n}\\) uniquely constructs each \\(v \\in V\\). As the trivial solution does exist: that \\(a_1 = \\dots = a_{n} = 0\\), it is the only solution.\nBy definition of linear independence, then, \\(v_1, \\dots v_{n}\\) is linearly independent. Having constructed that \\(v_1, \\dots v_{n}\\) is both a spanning set in \\(V\\) and are linearly independent, we have that they are a basis of \\(V\\). \\(\\blacksquare\\)\nDualing Basis Construction These are two results that says: \u0026ldquo;you can build up a linearly independent list to a basis or you can pluck away a spanning list to a basis\u0026rdquo;.\nall spanning lists contains a basis of which you are spanning Every spanning list in \\(V\\) contains the basis (and possibly some more) in \\(V\\).\nRead: \u0026ldquo;apply Linear Dependence Lemma your way to success\u0026rdquo;.\nBegin with a spanning list \\(v_1, \\dots v_{m}\\) of \\(V\\). We run a for loop for the list.\nStep 0:\nIf \\(v_1=0\\) (i.e. \\(v_1 \\in span(\\{\\})\\)), delete \\(v_1\\). Otherwise, do nothing.\nStep \\(j\\):\nIf \\(v_{j}\\) is in \\(span(v_1, \\dots v_{j-1})\\), \\(v_{j}\\) satisfies the Linear Dependence Lemma\u0026rsquo;s first condition, and therefore naturally satisfies the second condition (removal from list keeps the same span because \\(v_{j}\\) can just be rewritten from \\(v_1, \\dots v_{j-1}\\)).\nSo we remove \\(v_{j}\\) if it is indeed in the span of the previous vectors. By the Linear Dependence Lemma, the new list spans the same space the old list.\nConclusion\nBy the end of this process, no vectors left in the list will satisfy the Linear Dependence Lemma (read: we got rid of all of them.) Therefore, the list is linearly independent. However, every step of the way the Linear Dependence Lemma ensures that the new list spans the same space; therefore, the new list still spans \\(V\\). Having constructed a linearly independent list that spans \\(V\\), we declare the new list as a basis of \\(V\\).\nAs all we did was pluck vectors out of the old list, the new list is a sublist of the old list. This means that the spanning list (old list) contains the new list, which is a basis. \\(\\blacksquare\\)\na linearly independent list expends to a basis Every linearly independent list of vectors in finite-dimensional vector spaces can be extended to a basis.\nRecall first that every finite-dimensional vector space has a basis.\nLet\u0026rsquo;s begin with a linearly independent list in \\(V\\) \\(u_1, \\dots u_{m}\\). Let\u0026rsquo;s recruit also a basis of \\(V\\): \\(w_{1}, \\dots w_{m}\\).\nNaturally: \\(u_1, \\dots u_{m}, w_1, \\dots w_{m}\\) spans \\(V\\) (as the \\(w\\) vectors already span \\(V\\)). We will now apply the fact that all spanning lists contains a basis of which you are spanning (the order of \\(u\\) vectors first and \\(w\\) vectors second ensuring that you try to remove the \\(w\\), and, as \\(u\\) are linearly independent, none of them will be removed) to get back a basis in \\(V\\) consisting of all \\(u\\) and some \\(w\\). \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhbasis/","tags":null,"title":"basis"},{"categories":null,"contents":"Suppose \\(v_1, \\dots v_{n} \\in V\\) is a basis of some vector space \\(V\\); \\(w_1, \\dots w_{n} \\in W\\) is just a good\u0026rsquo;ol list of length \\(n= \\dim V\\) in \\(W\\).\nThere exists a unique linear map \\(T \\in \\mathcal{L}(V,W)\\) such that\u0026hellip;\n\\begin{equation} Tv_{j} = w_{j} \\end{equation}\nfor each \\(j = 1, \\dots n\\)\nIntuition The layperson\u0026rsquo;s explanation of this result: 1) that, for everywhere you want to take the basis of one space, there\u0026rsquo;s always a unique linear map to take you there. 2) that, a linear map is determined uniquely by what it does to the basis of its domain.\nProof We have two vector spaces, \\(V\\) and \\(W\\); \\(v_1, \\dots v_{n} \\in V\\) forms a basis of \\(V\\); \\(w_1, \\dots w_{n} \\in W\\) are just some vectors in \\(W\\).\nDefinition We define some \\(T: V \\to W\\) as follows:\n\\begin{equation} T(c_1v_1 + \\dots + c_{n}v_{n}) = c_1 w_1 + \\dots + c_{n} w_{n} \\end{equation}\nwhere, \\(c_1, \\dots c_{n} \\in \\mathbb{F}\\). Note that the actual values of \\(c\\) doesn\u0026rsquo;t actually matter here.\nExistence We now show that the \\(T\\) defined above has the property of mapping \\(Tv_{j} \\to w_{j}\\).\nAs the basis \\(v_1, \\dots v_{n}\\) is a spanning list of \\(V\\), some \\(T\\) that takes an arbitrary linear combination of \\(v\\) as input does indeed have domain \\(V\\). Due to addition\u0026rsquo;s closure, a linear combination of \\(w\\) is \\(\\in W\\). This makes \\(T\\) at least a function from \\(V \\to W\\).\nOf course, by taking all \\(c_{i}\\) to \\(0\\) except for the index \\(c_{j}\\) you are interested in to \\(1\\), you can show that this \\(T\\) takes \\(v_{j}\\) to \\(w_{j}\\).\nWe now show that \\(T\\) is a Linear Map. This part proof is just route algebra so I won\u0026rsquo;t type it again.\nUniqueness Suppose there is a Linear Map that has the desired property: that \\(T \\in \\mathcal{L}(V,W)\\) and that \\(Tv_{j}=w_{j}, \\forall j=1, \\dots n\\). For any scalar \\(c_{j}\\), the homogeneity of \\(T\\) indicates that this same \\(T\\) has to take \\(T(c_{j}v_{j}) = c_{j}Tv_{j} = c_{j}w_{j}\\).\nNow, the additivity of \\(T\\) also indicates that we can string these \\(c_{j} v_{j}\\) together in the same \\(T\\); that:\ngiven \\(T(c_{j}v_{j}) = c_{j}w_{j}\\), we can just string it all together to get \\(T(c_1v_1 + \\dots + c_{n}v_{n}) = c_1w_1+ \\dots + c_{n}w_{n}\\).\nThis means that there is only one \\(T\\) that behaves in the way that we desire, on the span of \\(v_1 \\dots v_{n}\\). Those vectors being the basis, their span is just the domain \\(V\\). This makes \\(T\\) uniquely determined on \\(V\\) as we were able to construct the original given map simply by following the rules of the Linear Map.\n","permalink":"https://www.jemoka.com/posts/kbhbasis_of_domain/","tags":null,"title":"basis of domain"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhbatchalign/","tags":null,"title":"batchalign"},{"categories":null,"contents":"Things to include Rev How to handle interspersed results Utterance segmentation Why --prealigned and the overall performance of MFA Beginning/End Bullet and why we throw away Rev\u0026rsquo;s output fixbullets and manual utterance segmentation \u0026amp;*INV= interspersed comments ","permalink":"https://www.jemoka.com/posts/kbhbatchalign_paper_outline/","tags":null,"title":"Batchalign Paper Outline"},{"categories":null,"contents":"Bending is what happens when you apply a transverse load to an object and it goes wooosh.\nThat\u0026rsquo;s cool. Now how does it work? see Euler-Bernoulli Theory\n","permalink":"https://www.jemoka.com/posts/kbhbending/","tags":null,"title":"bending"},{"categories":null,"contents":"A binary operation means that you are taking two things in and you are getting one thing out; for instance:\n\\begin{equation} f: (\\mathbb{F},\\mathbb{F}) \\to \\mathbb{F} \\end{equation}\nThis is also closed, but binary operations dons\u0026rsquo;t have to be.\n","permalink":"https://www.jemoka.com/posts/kbhbinary_operation/","tags":null,"title":"binary operation"},{"categories":null,"contents":"A binomial distribution is a typo of distribution whose contents are:\nBinary Independent Fixed number Same probability The expected value of \\(X\\) following a binomial distribution is \\(np\\), and the standard deviation of \\(X\\) would be \\(\\sqrt{np(1-p)}\\).\n","permalink":"https://www.jemoka.com/posts/kbhbinomial_distribution/","tags":null,"title":"binomial distribution"},{"categories":null,"contents":"bioinformatics is a field of biology that deals with biology information. Blending CS, Data, Strategies and of course biology into one thing.\nFirst, let\u0026rsquo;s review genetic information\npossible use for bioinformatics Find the start/stop codons of known gene, and determine the gene and protein length ","permalink":"https://www.jemoka.com/posts/kbhbioinformatics/","tags":null,"title":"bioinformatics"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhblack_thursday/","tags":null,"title":"Black Thursday"},{"categories":null,"contents":"People have been trading options for a very long time, but there wasn\u0026rsquo;t a good way of quantify the value of an option.\nThere are two main types of uses for Black-Scholes Formula\nyou can use all variables and determine the value of options you can get the price of options being traded, then compute the $σ$\u0026mdash;the market\u0026rsquo;s estimation of volatility (how much they want the insurance policy that is the options) constituents \\(S_0\\): stock price \\(X\\): exercise price \\(r\\): risk-free interest rate \\(T\\): maturity time \\(\\sigma\\): standard-deviation of log returns\u0026mdash;\u0026ldquo;volatility\u0026rdquo; Black-Scholes Formula for an European \u0026ldquo;Call\u0026rdquo; Option Here is the scary formula:\n\\begin{equation} C_0 = S_0 \\mathcal{N}(d_{1})-Xe^{-rT}\\mathcal{N}(d_{2}) \\end{equation}\nwhere, the variables are defined above, and:\n\\begin{equation} \\begin{cases} d_1 = \\frac{\\ln\\qty(\\frac{S_0}{X})+\\qty(r+\\frac{\\sigma^{2}}{2})T}{\\sigma \\sqrt{t}}\\\\ d_2 = \\frac{\\ln\\qty(\\frac{S_0}{X})+\\qty(r-\\frac{\\sigma^{2}}{2})T}{\\sigma \\sqrt{t}} \\end{cases} \\end{equation}\nand \\(\\mathcal{N}\\) is the area at point under the standard normal distribution.\noh god So let\u0026rsquo;s dissect this a little.\nThe first term:\n\\begin{equation} S_0\\mathcal{N}(d_{1}) \\end{equation}\nis the \u0026ldquo;current\u0026rdquo; stock price, weighted by the probability of you being willing to exercise it.\nand the second term:\n\\begin{equation} Xe^{-rT}\\mathcal{N}(d_{2}) \\end{equation}\nis the \u0026ldquo;price\u0026rdquo; of the exercise (what you need to pay, if exercising the option, to get the stock.)\nThis strike price \\(X\\) is discounted by \\(e^{-rT}\\), which is like a time machine that rolls that strike price back to what it would be today (so that it\u0026rsquo;s comparable to \\(S_0\\).) As \\(r\\) is the risk free interest rate, we are essentially saying: \u0026ldquo;in a perfectly functional market, over the next \\(T\\) days, how will our asset grow?\u0026rdquo;\nThis is again weighted by the probability of you being willing to exercise it\u0026mdash;through modified slightly differently.\nTherefore, subtracting the two terms, we get the actual value of the option\u0026mdash;the money you would gain by exercising it, then immediately selling the stock, weighted by how willing you are actually to excercise it.\nLet\u0026rsquo;s now take a look at those \u0026ldquo;probabilities\u0026rdquo; \\(d_{\\{1,2\\}}\\). These factors essentially provide quantification of the statement that: \u0026ldquo;the higher our current price is ABOVE the excrecise price\u0026mdash;accounting for volatility\u0026mdash;the more willing we are to excercise the option.\u0026rdquo;\nNote then, \\(\\ln\\qty(\\frac{S_{0}}{X})\\) form the top of both expressions. That essentially measures how high the current price \\(S_0\\) deviates from the strike price \\(X\\).\nNow, as volatility \\(\\sigma\\) increases, \\(d_1\\) increases and \\(d_2\\) decreases (as \\(\\frac{\\sigma^{2}}{2}\\) is being added in \\(d_1\\) and subtracted in \\(d_2\\)). This is because, as volatility increase, you are less certain about what the actual \u0026ldquo;pay\u0026rdquo; (price) is, but your option\u0026mdash;given its constant strike price\u0026mdash;provides the certainty in gain.\n","permalink":"https://www.jemoka.com/posts/kbhblack_scholes_formula/","tags":null,"title":"Black-Scholes Formula"},{"categories":null,"contents":"The bloch sphere is a sphere encoding all possible probabilities of a qubit shared between two axis, \\(|u\\big\u0026gt;\\) and \\(|d\\big\u0026gt;\\).\nYou will notice that its a unit sphere, in which any magnitude has size \\(1\\). Hence, probabilities would result as projected onto each of the directions.\n","permalink":"https://www.jemoka.com/posts/kbhbloch_sphere/","tags":null,"title":"bloch sphere"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhbluest_eye/","tags":null,"title":"Bluest Eye"},{"categories":null,"contents":"General Information Due Date Topic Important Documents \u0026lt;2022-05-06 Fri\u0026gt; Bluest Eye Essay Bluest Eye Prompt Beauty: discuss Morrison’s treatment of the idea of beauty. From what, where, or whom does this notion come? What effect does it have on the way one perceives the world? On the way others perceive an individual?\nHow does beauty (the acquisition of it, the lack of it, or the presence of it) determine one’s fate in America? Is beauty a necessarily fixed entity or does it fluctuate at the whim of society? How much or to what extent does one’s perception of beauty contribute to one’s sense of self-worth?\nQuotes Bin Beauty Claudia: I had only one desire: to dismember it. To see of what it was made, to discover the dearness, to find the beauty, the desirability that had escaped me, but apparently only me. Pecola: Thrown, in this way, into the binding conviction that only a miracle could relieve her, she would never know her beauty. She would see only what there was to see: the eyes of other people. Maureen: Maureen agreed. \u0026ldquo;Ooooo yes. My mother told me that a girl named Audrey, she went to the beauty parlor where we lived before, and asked the lady to fix her hair like Hedy Lamarr’s, and the lady said, \u0026lsquo;Yeah, when you grow some hair like Hedy Lamarr’s.\u0026rsquo;\u0026rdquo; She laughed long and sweet. (post pecola beat-up) Pauline (Polly): Along with the idea of romantic love, she was introduced to another—physical beauty. In equating physical beauty with virtue, she stripped her mind, bound it, and collected self-contempt by the heap. Pauline (Polly) cont\u0026rsquo;d: She was never able, after her education in the movies, to look at a face and not assign it some category in the scale of absolute beauty, and the scale was one she absorbed in full from the silver screen. Pauline (Polly): More and more she neglected her house, her children, her man\u0026mdash;\u0026hellip;the dark edges that made the daily life with the Fishers lighter, more delicate, more lovely \u0026hellip; Here she found beauty, order, cleanliness, and praise. Pauline (Polly): Pauline kept this order, this beauty, for herself, a private world, and never introduced it into her storefront, or to her children. Cholly after Aunt Death: The funeral banquet was a peal of joy after the thunderous beauty of the funeral. It was like a street tragedy with spontaneity tucked softly into the corners of a highly formal structure. Soaphead Church: He thought it was at once the most fantastic and the most logical petition he had ever received. Here was an ugly little girl asking for beauty. A surge of love and understanding swept through him, but was quickly replaced by anger. Claudia (reflecting on Pecola): All of our waste which we dumped on her and which she absorbed. And all of our beauty, which was hers first and which she gave to us. Eyes a: Her eyes are full of sorrow. She sings to me: \u0026ldquo;When the deep purple falls over sleepy garden walls, someone thinks of me\u0026hellip;.\u0026rdquo; ** Sub-Claim Synthesis There\u0026rsquo;s always the UCLA Writing Lab.\n","permalink":"https://www.jemoka.com/posts/kbhenglish_bluest_eye/","tags":null,"title":"Bluest Eye Essay Planning"},{"categories":null,"contents":"A secondary source comparison activity for the Bluest Eye\nTony Morrison\u0026rsquo;s Rootedness That, if an action were to be done as in a community, its regarded as safer It is a very personal grief and a personal statement done among people you trust. Done within the context of the community, therefore safe.\nPublic (white-washed) and private image, by necessesity, is separated it\u0026rsquo;s just important that it be private. And then, whatever I do that is public can be done seriously.\nthat people are only defined by the uniqueness they have out of the tribe My single solitary and individual Jifejs like the lives of the tribe; it differs in these specific ways, but it is a balanced life because it is both solitary and representative\nPurpose of the novel is enlightening as well as an art form It should have something in it that enlightens; something in it that opens the door arid points the way. Something in it that suggests what the conflicts are, what the problems are.\nThe Novel is a middle class art form The history of the novel as a form began when there was a new class, a middle class, to read it; it was an art form that they needed.\nThat there is already a form of artistry for the lower class, but not middle class The lower classes didn\u0026rsquo;t need novels at that time because they had an art form already they had songs and dances, and ceremony, and gossip, and celebrations.\nnovels of manners tell people of a different world we call 1t the novel of manners, an art form designed to tell peole something they didn\u0026rsquo;t know.\nPortrays quintessential forms of connection How to get married. What a good living was.\nThe African Americans became unexclusive For a long time, the art form that was healing for Black people was music. That music is no longer exclusively ours; we don\u0026rsquo;t have exclusive rights to it.\nThat the story of the novel is told where the reader constructs the story together To construct the dialogue so that it is heard. So that there are no adverbs attached to them: \u0026ldquo;loudly,\u0026rdquo; \u0026ldquo;softly,\u0026rdquo; \u0026ldquo;he said menacingly.'\nThat the artistry is not described as Black but inherently black Black, because it uses the characteristics of Black art\n","permalink":"https://www.jemoka.com/posts/kbhsecondary_source_comparison_activity/","tags":null,"title":"Bluest Eye: secondary source comparison activity"},{"categories":null,"contents":"BNT is a discourse task where subjects are shown 60 pictures decreasing frequency and asked to recall the word.\n","permalink":"https://www.jemoka.com/posts/kbhboston_naming_test/","tags":null,"title":"Boston Naming Test"},{"categories":null,"contents":"Way of performing action research developed by Victoria Clarke and Virginia Braun in 2006\n","permalink":"https://www.jemoka.com/posts/kbhbraun_and_clarke_thematic_analysis/","tags":null,"title":"Braun and Clarke thematic analysis"},{"categories":null,"contents":"Professor Brian MacWhinney is a professor of psychology, modern languages, and language technology at CMU.\n","permalink":"https://www.jemoka.com/posts/kbhbrian_macwinney/","tags":null,"title":"Brian MacWhinney"},{"categories":null,"contents":"Brown v. Board of Education is a landmark case in the US. This lead for schools to be integrated, and many children were taken out of school out of protest due to the subsequent integration movement between schools.\n","permalink":"https://www.jemoka.com/posts/kbhbrown_v_board_of_education/","tags":null,"title":"Brown v. Board of Education"},{"categories":null,"contents":"Brownian Motion is the pattern for measuring the convergence of random walk through continuous timing.\ndiscrete random walk discrete random walk is a tool used to construct Brownian Motion. It is a random walk which only takes on two discrete values at any given time: \\(\\Delta\\) and its additive inverse \\(-\\Delta\\). These two cases take place at probabilities \\(\\pi\\) and \\(1-\\pi\\).\nTherefore, the expected return over each time \\(k\\) is:\n\\begin{equation} \\epsilon_{k} = \\begin{cases} \\Delta, p(\\pi) \\\\ -\\Delta, p(1-\\pi) \\end{cases} \\end{equation}\n(that, at any given time, the expectation of return is either\u0026mdash;with probability $π$\u0026mdash;\\(\\Delta\\), or\u0026ndash;with probability $1-π$\u0026mdash;\\(-\\Delta\\).\nThis makes \\(\\epsilon_{k}\\) independently and identically distributed. The price, then, is formed by:\n\\begin{equation} p_{k} = p_{k-1}+\\epsilon_{k} \\end{equation}\nand therefore the price follows a random walk.\nSuch a discrete random walk can look like this:\nWe can split this time from \\([0,T]\\) into \\(n\\) pieces; making each segment with length \\(h=\\frac{T}{n}\\). Then, we can parcel out:\n\\begin{equation} p_{n}(t) = p_{[\\frac{t}{h}]} = p_{[\\frac{nt}{T}]} \\end{equation}\nDescretized at integer intervals.\nAt this current, discrete moments have expected value \\(E[p_{n}(T)] = n(\\pi -(1-\\pi))\\Delta\\) and variance \\(Var[p_{n}(T)]=4n\\pi (1-\\pi)\\Delta^{2}\\). #why\nNow, if we want to have a continuous version of the descretized interval above, we will maintain the finiteness of \\(p_{n}(T)\\) but take \\(n\\) to \\(\\infty\\). To get a continuous random walk needed for Brownian Motion, we adjust \\(\\Delta\\), \\(\\pi\\), and \\(1-\\pi\\) such that the expected value and variance tends towards the normal (as we expect for a random walk); that is, we hope to see that:\n\\begin{equation} \\begin{cases} n(\\pi -(1-\\pi))\\Delta \\to \\mu T \\\\ 4n\\pi (1-\\pi )\\Delta ^{2} \\to \\sigma^{2} T \\end{cases} \\end{equation}\nTo solve for these desired convergences into the normal, we have probabilities \\(\\pi, (1-\\pi), \\Delta\\) such that:\n\\begin{equation} \\begin{cases} \\pi = \\frac{1}{2}\\qty(1+\\frac{\\mu \\sqrt{h}}{\\sigma})\\\\ (1-\\pi) = \\frac{1}{2}\\qty(1-\\frac{\\mu \\sqrt{h}}{\\sigma})\\\\ \\Delta = \\sigma \\sqrt{h} \\end{cases} \\end{equation}\nwhere, \\(h = \\frac{1}{n}\\).\nSo looking at the expression for \\(\\Delta\\), we can see that as \\(n\\) in increases, \\(h =\\frac{1}{n}\\) decreases and therefore \\(\\Delta\\) decreases. In fact, we can see that the change in all three variables track the change in the rate of \\(\\sqrt{h}\\); namely, they vary with O(h).\n\\begin{equation} \\pi = (1-\\pi) = \\frac{1}{2}+\\frac{\\mu \\sqrt{h}}{2\\sigma} = \\frac{1}{2}+O\\qty(\\sqrt{h}) \\end{equation}\nOf course:\n\\begin{equation} \\Delta = O\\qty(\\sqrt{h}) \\end{equation}\nSo, finally, we have the conclusion that:\nas \\(n\\) (number of subdivision pieces of the time domain \\(T\\)) increases, \\(\\frac{1}{n}\\) decreases, \\(O\\qty(\\sqrt{h})\\) decreases with the same proportion. Therefore, as \\(\\lim_{n \\to \\infty}\\) in the continuous-time case, the probability of either positive or negative delta (\\(\\pi\\) and \\(-\\pi\\) trends towards each to \\(\\frac{1}{2}\\)) by the same vein, as \\(\\lim_{n \\to \\infty}\\), \\(\\Delta \\to 0\\) Therefore, this is a cool result: in a continuous-time case of a discrete random walk, the returns (NOT! just the expect value, but literal \\(\\Delta\\)) trend towards \\(+0\\) and \\(-0\\) each with \\(\\frac{1}{2}\\) probability.\nactual Brownian motion Given the final results above for the limits of discrete random walk, we can see that the price moment traced from the returns (i.e. \\(p_{k} = p_{k-1}+\\epsilon_{k}\\)) have the properties of normality (\\(p_{n}(T) \\to \\mathcal{N}(\\mu T, \\sigma^{2}T)\\))\nTrue Brownian Motion follows, therefore, three basic properties:\n\\(B_{t}\\) is normally distributed by a mean of \\(0\\), and variance of \\(t\\) For some \\(s\u0026lt;t\\), \\(B_{t}-B_{s}\\) is normally distributed by a mean of \\(0\\), and variance of \\(t-s\\) Distributions \\(B_{j}\\) and \\(B_{t}-B_{s}\\) is independent Standard Brownian Motion Brownian motion that starts at \\(B_0=0\\) is called Standard Brownian Motion\nquadratic variation The quadratic variation of a sequence of values is the expression that:\n\\begin{equation} \\sum_{i=0}^{N-1} (x_{i+1}-x_i)^{2} \\end{equation}\nOn any sequence of values \\(x_0=0,\\dots,x_{N}=1\\) (with defined bounds), the quadratic variation becomes bounded.\n","permalink":"https://www.jemoka.com/posts/kbhbrownian_motion/","tags":null,"title":"Brownian Motion"},{"categories":null,"contents":" ","permalink":"https://www.jemoka.com/posts/kbhcalculating_shear_s_modulus/","tags":null,"title":"calculating shear's modulus"},{"categories":null,"contents":"duplicate article creation. see Cantilever Beams\n","permalink":"https://www.jemoka.com/posts/kbhcantilever_beam/","tags":null,"title":"cantilever beam"},{"categories":null,"contents":"A Cantilever beam is a rigid structure which is extended horizontally and supported on one end.\nWorking with Cantilever Beams curvature Let\u0026rsquo;s first define a function:\n\\begin{equation} w(x) \\end{equation}\nthis represents the deflection of the beam at point \\(x\\). We will begin by taking its derivative by location:\n\\begin{equation} \\Delta w = \\pdv{w}{x} \\end{equation}\nis the change in deflection over location. \u0026ldquo;How much deviation of the beam from the resting axi is there as you run along it?\u0026rdquo;\nWe now take another derivative:\n\\begin{equation} k = \\pdv[2]{w}{x} \\end{equation}\n\\(k\\) is defined as the \u0026ldquo;Curvature\u0026rdquo; of the beam: the \u0026ldquo;change in the change of bentness.\u0026rdquo; The intuition is essentially this:\na straight, flat beam fixed an one end has \\(\\Delta w=0\\), \\(k=0\\). It does not change from its resting axis, and its rate of change from resting does not change a straight, slanted beam fixed at one end has \\(\\Delta w=C, k=0\\). It changes from its resting axis with a linear rate, and its rate of change from resting does not change. a curved, slanted beam fixed at one end has \\(\\Delta \\omega = f(x), k=C\\). It changes from its resting axis non-linearly (hence curving at a function of \\(x\\)), and its rate of change from resting is changing at a constant \\(c\\). flexural rigidity Flexural Rigidity is the \u0026ldquo;force couple\u0026rdquo; (\u0026ldquo;rate\u0026rdquo;) which relates the Curvature of an non-rigid body and how much torque it actually generates given the object\u0026rsquo;s properties.\nRecall first our Elastic Modulus \\(E\\): it is a fraction of \\(\\frac{stress}{strain}\\) measured in Pascals (force per unit area, i.e. \\(\\frac{N}{m^{2}} = \\frac{kg}{ms^{2}}\\)).\nFind also second moment of area \\(I\\): a value in units \\(m^{4}\\) which is the sum (by area) of the squared displacement of each infinitesimal area to the axis of origin.\nAnd we bam! we multiply the two things together, creating a value \\(EI\\) in units \\(Nm^{2}\\).\nbending moment bending moment is the torque from bending. It is expressed usually in \\(M\\). As mentioned in the section about Flexural Rigidity, we can use that value to relate \\(M\\) with the actual Curvature of your object.\nSpecifically, that:\n\\begin{equation} M = -(EI)k = -EI\\pdv[2]{w}{x} \\end{equation}\n\u0026ldquo;bending moment is flexural rigidity times curvature\u0026rdquo; =\u0026gt; \u0026ldquo;[how much force per distance you exert] is the result of [how bendy your thing is] times [how much you bent it].\u0026rdquo;\nThere is a negative in front because if you pull out your lovely little right hand, point your thumb forward (+y), start curling your nice fingers around your nice hand (-z), you will notice that you are wrapping them downwards (the - part of the z) which is rather not positive. If we want \\(\\pdv[2]{w}{x}\\) to be positive (bend up), we will need to chuck a negative in front of it to make both things positive.\nThis relation, while intuitive, is not from first-principles. In order to get such a derivation, you read Wikipedia.\nmagic We can take two derivatives by location\u0026mdash;\n\\begin{equation} \\pdv[2] x \\qty(EI \\pdv[2]{w}{x}) = -\\mu \\pdv{w}{t}+q(x) \\end{equation}\nwhere \\(\\mu\\) is the mass density, \\(q(x)\\) is the force applied (in Newtons) by area. this is magic. Will come back to it.\nSolving this? See Finite Difference Method\nActually attempting to solve it Numerical Cantileaver Simulations\nWorking on Deformation ","permalink":"https://www.jemoka.com/posts/kbhcantilever_beams/","tags":null,"title":"Cantilever Beams"},{"categories":null,"contents":"A cancer drug to synthesize Fluoropyrimidine.\n","permalink":"https://www.jemoka.com/posts/kbhcapecitabmine/","tags":null,"title":"Capecitabmine"},{"categories":null,"contents":"CAPM is a method of portfolio selection analysis which focuses on maximizing return given some fixed variance.\nIt deals with optimal Capital Market Line, given here:\n\\begin{equation} E[R_{p}] = r_{f}+\\frac{\\sigma_{p}}{\\sigma_{T}}\\qty(E[R_{T}]-r_{f}) \\end{equation}\nWhich describes \\(E[R_{p}]\\), the expected return of an optimal portfolio in a market, given, \\(R_{T}\\) is the market return, \\(r_{f}\\) is the risk-free rate, \\(\\sigma_{p}\\) is the portfolio returns, and \\(\\sigma_{t}\\) is standard-deviation of the market returns.\nSharpe Ratio The Sharpe Ratio is a measure of the risk-adjusted performance of an asset\u0026mdash;given the rate of return of some risk-free asset.\nIt is defined as:\n\\begin{equation} S_{a} = \\frac{E[R_{a}-R_{b}]}{\\sigma_{a}} \\end{equation}\nwhere, \\(R_{a}\\) is the raw return of the asset, \\(R_{b}\\) is the risk-free rate of return, and \\(\\sigma_{a}\\) is the standard deviation of the asset \u0026ldquo;excess\u0026rdquo; return (i.e. standard deviation actual return - expected return\u0026mdash;how much extra there is).\nMinimum-Variance Boundary For a given a weighted-average portfolio of stocks their waited averages, and correlations between the stocks you can draw this curvy curve. Let pink dots represent the two securities in your portfolio, and various curves highlighting possible linear combinations thereof\u0026mdash;\nLet\u0026rsquo;s observe the boundary conditions of this curve.\nIf the two stocks are exactly negatively correlated, then the more risk you take the more return you have for one while less return you have for the other (hence, two straight divergent lines.)\nIf you have an exactly correlated portfolio, the two assets will will form a line.\nThe Effecient Frontier is the top half of this curve (i.e. higher risk/higher return is not a fun place to be, so that\u0026rsquo;s an inefficient frontier.)\nCapital Market Line The Capital Market Line is a line that uses the Sharpe Ratio of a market as a whole (how the market is performing against the risk-free rate) to analyze the performance of portfolio. It plots the performance of an \u0026ldquo;optimal portfolio\u0026rdquo; in a given market.\nLet\u0026rsquo;s construct first the Sharpe Ratio of a hypothetical market:\n\\begin{equation} \\frac{R_{t}-r_{f}}{\\sigma_{t}} \\end{equation}\nwhere \\(R_{T}\\) is the market return, \\(r_{f}\\) is the risk-free rate, and \\(\\sigma_{t}\\) is standard-deviation of the market returns.\nWe will multiply this value by the standard-deviation of your portfolio to calculate what the market claims should be your expected return. Then, we shift the line by the risk-free rate (as you are expected also to get that rate back in your return.\nSo an \u0026ldquo;effecient\u0026rdquo; portfolio (getting the max expected return per unit risk as measured by the market Sharpe Ratio) should behave like:\n\\begin{equation} E[R_{p}] = r_{f}+\\frac{E[R_{T}]-r_{f}}{\\sigma_{T}}\\sigma_{p} \\end{equation}\nagain, \\(R_{T}\\) is the market return, \\(r_{f}\\) is the risk-free rate, and \\(\\sigma_{t}\\) is standard-deviation of the market returns.\nThe one liner is: \u0026ldquo;the return of your portfolio should be the base return by risk-free rate, plus how much excess risk you are taking on (and therefore return you should be getting back by the Sharpe Ratio)\u0026rdquo;\n(how much you are expected to get (i.e. market Sharpe Ratio times your portfolio volatility), shifted back up by the risk-free rate.\nSharpe-Lintner CAPM A linear formulation of CAPM base on market-excess return (i.e. if you want to beat the market, you will have to sustain proportionally the same amount of risk.)\nTangency Portfolio There is a portfolio, which is named the Tangency Portfolio. This portfolio is the tangent point between the Capital Market Line and the Effecient Frontier.\nIt represents the point where you can get the highest return given some risk, but also control the risk at the market\u0026rsquo;s Sharpe Ratio.\nBlack\u0026rsquo;s CAPM CAPM depends on a risk-free asset. Black of Black-Scholes Formula fame derived another formulation of CAPM which doesn\u0026rsquo;t dependent on a risk-free asset.\nZero-Beta Portfolio To work with Black\u0026rsquo;s CAPM, we first define \\(0m\\), the Zero-Beta Portfolio (used in the formula as \\(R_{0m}\\), the return of the Zero-Beta Portfolio).\nIt is defined to be the portfolio with the minimum variance of all portfolios not correlated with \\(m\\).\n","permalink":"https://www.jemoka.com/posts/kbhcapm/","tags":null,"title":"Capital-Asset Pricing Model"},{"categories":null,"contents":"A category is defined as:\ncollection of objects, where if \\(X\\) is an object of \\(C\\) we write \\(X \\in C\\) ","permalink":"https://www.jemoka.com/posts/kbhcategory/","tags":null,"title":"category"},{"categories":null,"contents":"An abstract study of mathematics based on categories, functors, and natural transformations.\n","permalink":"https://www.jemoka.com/posts/kbhcategory_theory/","tags":null,"title":"category theory"},{"categories":null,"contents":"stock market crash of 1929 At October 24th, 1929, Black Thursday took place, and the stock market crashed. During this time, a record of 13 million shares traded, over $3b of losses. This began a 4 year slide of the global economy.\nCrash theories:\ndemand-driven theory Monetarist theory bank failures of 1929 Banks became irrelevant. Lots of risky loans given out, farmers are taken out huge loans and the banks can\u0026rsquo;t deal.\nother factors economy of credit tariffs ","permalink":"https://www.jemoka.com/posts/kbhcauses_of_the_great_depression/","tags":null,"title":"causes of the Great Depression"},{"categories":null,"contents":"\u0026ldquo;If sample size is large, the sampling distribution is normal. The larger \\(N\\) is, the more normal the resulting shape is.\u0026rdquo;\nIt is technically written as:\ngiven some random variable \\(Y\\), the normalized collection of a random variable \\(X\\) with samples \\(x_j\\),\n\\begin{equation} Y = \\frac{1}{\\sigma \\sqrt{N}} \\sum_{i=1}^N (x_1 - \\mu) \\end{equation}\nWe have that:\n\\begin{equation} \\lim_{N\\to \\infty} Y_n \\sim N(0,1) \\end{equation}\nThat, as long as you normalize a random variable and have enough of it, you get the normal distribution.\nNotably, for the central limit theorem to hold, the variance has to be finite (that the results vary in a certain finite value \\(\\sigma\\). With that \\(\\sigma\\) value, we can see above that the central limit theorem will eventually converge to the normal. THis is useful for the Random Walk Hypothesis.\n","permalink":"https://www.jemoka.com/posts/kbhcentral_limit_theorem/","tags":null,"title":"central limit theorem"},{"categories":null,"contents":" 80% of the human genome is actually transcribed very little \u0026ldquo;junk DNA\u0026rdquo; 40% IncRNA are gene specific ","permalink":"https://www.jemoka.com/posts/kbhchanges_to_central_dogma/","tags":null,"title":"changes to central dogma"},{"categories":null,"contents":"an atom is said to be charged when there is an imbalance between its number of protons and electrons.\nadditional information units of charge charge is measured in SI unit \\(C\\), coulomb. However, we are often dealing with \\(e\\), the charge of an electron (as ultimate that\u0026rsquo;s the principle way by which charge moves around). \\(e \\approx 1.6 \\times 10^{-19} C\\).\nnet charge can be neither created nor destroyed Unsurprisingly, though you can move electrons around, they will be conserved across a system.\n","permalink":"https://www.jemoka.com/posts/kbhcharged/","tags":null,"title":"charged"},{"categories":null,"contents":"\\(\\chi^2\\) is a test statistic for hypothesis testing.\nmotivation for chi-square The motivation for chi-square is because t-test (means, \u0026ldquo;is the value significantly different\u0026rdquo;) and z-test (proportion, \u0026ldquo;is the incidence percentage significantly different\u0026rdquo;) all don\u0026rsquo;t really cover categorical data samples: \u0026ldquo;the categories are distributed in this way.\u0026rdquo;\nTake, for instance, if we want to test the following null hypothesis:\nCategory Expected Actual A 25 20 B 25 20 C 25 25 D 25 25 \\(\\alpha = 0.05\\). What do we use to test this??\n(hint: we can\u0026rsquo;t, unless\u0026hellip;)\nEnter chi-square.\nchi-square test chi-square test is a hypothesis test for categorical data. It is responsible to translate differences in distributions into p-values for significance.\nBegin by calculating chi-square after you confirmed that your experiment meets conditions for inference (chi-square test).\nOnce you have that, look it up at a chi-square table to figure the appropriate p-value. Then, proceed with normal hypothesis testing.\nBecause of this categorical nature, chi-square test can also be used as a homogeneity test.\nconditions for inference (chi-square test) random sampling expected value for data must be \\(\\geq 5\\) sampling should be \\(\u0026lt;10\\%\\) or independent chi-square test for homogeneity The chi-square test for homogeneity is a test for homogeneity via the chi-square statistic.\nTo do this, we take the probability of a certain outcome happening\u0026mdash;if distributed equally\u0026mdash;and apply it to the samples to compare.\nTake, for instance:\nSubject Right Hand Left Hand Total STEM 30 10 40 Humanities 15 25 40 Equal 15 5 20 Total 60 40 100 We will then figure the expected outcomes:\nRight Left 24 16 24 16 12 8 Awesome! Now, calculate chi-square with each cell of measured outcomes. Calculate degrees of freedom by (num_row-1)*(num_col-1).\nchi-square test for independence The chi-square test for independence is a test designed to accept-reject the null hypothesis of \u0026ldquo;no association between two variables.\u0026rdquo;\nEssentially, you leverage the fact that \u0026ldquo;AND\u0026rdquo; relationships are multiplicative probabilities. Therefore, the expected outcomes are simply the multiplied/fraction of sums:\ncalculating chi-square \\begin{equation} \\chi^2 = \\frac{(\\hat{x}_0-x_0)^2}{x_0} +\\frac{(\\hat{x}_1-x_1)^2}{x_1} + \\cdots + \\frac{(\\hat{x}_n-x_n)^2}{x_n} \\end{equation}\nWhere, \\(\\hat{x}_i\\) is the measured value and \\(x_i\\) is the expected value.\n","permalink":"https://www.jemoka.com/posts/kbhchi_square/","tags":null,"title":"chi-square"},{"categories":null,"contents":"Chiara Marletto is an physicist working on Quantum mechanics working in D. of Physics, Wolfson College, University of Oxford.\nSubfield: constructor theory. She studies quantum theory.\n","permalink":"https://www.jemoka.com/posts/kbhchiara_marletto/","tags":null,"title":"Chiara Marletto"},{"categories":null,"contents":"I was digging through my OneDrive recently for work, and found this piece of writing.\nThere is naught but a small, dirt-filled puddle in front of this lawn. Yet only here – by the puddle – can Gary find a small, much-needed respite from the neverending work. Of course, without the hours he has committed to the sweatshop, his mother would have died ages ago from colora.\nBut how does it matter now? Rarely now \u0026ndash; once every year \u0026ndash; does he even earn the privilege to exit the heavily-guarded area to visit his mother; and how little time he has during such visits: each visit seems to just be a long walk, a knock, a kiss on the cheek \u0026ndash; then back to the workhouse he goes.\nNo, he must push on. Focusing his tired mind back to the concrete structure in front of him, he sees the supervisor hollering the same old phrase. Back to work! Back to work! Move! Move! Break is over!\nWhat is this break, even? The notable lack of timepieces around the lawn means the important task of timekeeping falls to the supervisors \u0026ndash; who, notably, have an obvious interest in shortening the break. And ‘lo, the breaks are shortened: Gary has always remembered the session as until the bottom of the clock, yet doubtless he will find himself staring at a clock hand pointing to the horizontal upon walking into the building.\nHe can do nothing now: there is one \u0026ndash; and the ultimate \u0026ndash; sanction for not listening to the supervisor, and he wants nothing to do with it: beating. Beating that gets harder, faster, as time progresses is the one, the only, and the final answer to all cases of disobedience. Heck, if the supervisor demands time run backwards during breaks, Cronus will listen and obey \u0026ndash; for even he, a god, is probably as scared of these “correctional sessions” as anyone else.\nThere is, then, no time to be wasted. Up towards the factory he walks \u0026ndash; joined by hundreds of others suffering a similar fate, doing the same tedious, repetitive tasks as him. If he hadn\u0026rsquo;t been made dependent \u0026ndash; addicted! in fact \u0026ndash; to the meager wages he received, he could have achieved greatness the world has yet to see.\nBut, alas, towards the factory he walks, steps. Timidly, slowly, shuffling his feet quickly enough so as to not anger the increasingly stressed supervisor. Stressed understandably, perhaps, due to the increasing external talk of organizing such congregations as the “Child Labour Committee”, which Gary himself isn’t sure to what extent he should trust.\nThe quarter-bell strikes. Indeed, his suspicions were correct \u0026ndash; yet superfluous. When all was thought and done, he couldn\u0026rsquo;t possibly have even produced the thought of defying the wishes of the supervisor, let along execute it. But now, he has not even the physical capacity to escape \u0026ndash; the door was locked, and locked means work eternal \u0026ndash; at least until the next meager halt seemingly few decades later.\nSuddenly, a clicking occurs. A machine screeching to a halt, perhaps due to the same overwork and misuse. In walks the supervisor: nevermind that: the work must go on!\nIt is now down to the same routine \u0026ndash; picking the smallest, nimblist of the bunch \u0026ndash; Gary, of course \u0026ndash; to, through great persuasion and threatenings of beatings, climb under the mechanical beast and undo the mess. It’s a dance of oil and gear that Gary has rehearsed many times before, each time dreading the next. Yet he still brought himself to perform the task time and time again for it, although dreadful, seems to be heavenly compared to the alternate: getting the “correctional session.”\nDown the cover he goes: a little pulling there, a little dabbing there, and Hark! The machine jumped to a start with a splash of brilliant pink hue, announcing \u0026ndash; celebrating, it seems \u0026ndash; itself as Gary’s final quarters.\nNevermind that: the work must go on!\n","permalink":"https://www.jemoka.com/posts/kbhchild_labour/","tags":null,"title":"Child Labour: A Short Story"},{"categories":null,"contents":"DOI: 10.3389/fpsyg.2020.623237\nOne-Liner (thrice) Used features extracted by VGGish from raw acoustic audio against a SVM, Perceptron, 1NN; got \\(59.1\\%\\) classif. accuracy for dementia Then, trained a CNN on raw wave-forms and got \\(63.6\\%\\) accuracy Then, they fine-tuned a VGGish on the raw wave-forms and didn\u0026rsquo;t report their results and just said \u0026ldquo;we discovered that audio transfer learning with a pretrained VGGish feature extractor performs better\u0026rdquo; Gah! Novelty Threw the kitchen sink to process only raw acoustic input, most of it missed; wanted 0 human involvement. It seems like last method is promising.\nNotable Methods fine-tuning VGGish against raw acoustic waveforms to build a classifier via a CNN.\nKey Figs Their fancy network Its just a CNN afaik with much maxpooling; could have used some skipped connections. I wonder if it overfit?\nTheir actual training results Looks generally pretty bad, but a run of their DemCNN seem to have gotten state-of-the-art results. Not sure where transfer training data went.\nNew Concepts VGGish Notes Accuracy question According to this the state of the art at the time from pure audio was 56.6%? For a binary classifier isn\u0026rsquo;t that just doing nothing?\nSo somebody did get better before?\n","permalink":"https://www.jemoka.com/posts/kbhchlasta_2021/","tags":["ntj"],"title":"Chlasta 2021"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhchromatin/","tags":null,"title":"chromatin"},{"categories":null,"contents":"civil rights movement starting civil rights moment was kicked off by the Rosa Parks incident, which caused the Montomery Bus Boycott.\nMartin Luther King capitalized the incident to kick start civil rights movement. He employed the method of nonviolence movement.\neducational integration in the civil rights movement K-12 disintegration: Brown v. Board of Education University of Georgia was the first disintegrated university in the south service integration in the civil rights movement Lunch counter boycotts. Nashville became the first desegregated lunch counter.\nSNICK SNICK is a student organization founded by Ella Baker in the civil rights movement that sent students into the most dangerous areas of segregation and leading protests.\nMotown Records Motown Records is an African-American owned Detroit record business\nMalcom X A civil rights movement activist, calling for more violent forms of protest and prosecuting specific white actions. Malcom X and Martin Luther King contradicted each other in methods of active persecution vs. nonviolent integration.\nBloody Sunday Bloody Sunday was a voting rights march from Selma to Montgomery. Peaceful protesters were attacked with nightsticks and tear gas. The event was widely televised: transforming the movement as a televised morality play.\nNonviolence helps getting the clergy leaders as a form of leveraging religion in a show of unity.\nBlack Power Movement A new chapter in the civil rights movement which incorporated less of the elements of integration but instead in wanted more sense of self-determination. nonviolence movement, which the Black Power Movement overrided, had ran its course when Martin Luther King was assassinated.\n","permalink":"https://www.jemoka.com/posts/kbhcivil_rights/","tags":null,"title":"civil rights movement"},{"categories":null,"contents":"A part of the New Deal programs for unmarried men to go and build American infrastructure outdoors under reasonably harsh conditions. \u0026ldquo;Kind of like boy scouts for adults.\u0026rdquo; It is structured like the military; Black men were segregated and not given leadership roles.\n1933-1942.\n","permalink":"https://www.jemoka.com/posts/kbhcivillian_conservation_corps/","tags":null,"title":"Civillian Conservation Corps"},{"categories":null,"contents":"to be closed means that the operation of a group applied to an element of a group would produce another element of the group.\n","permalink":"https://www.jemoka.com/posts/kbhclosed/","tags":null,"title":"closed"},{"categories":null,"contents":"Exploring CLRS.\n2 insertion sort ","permalink":"https://www.jemoka.com/posts/kbhclrs_index/","tags":["index"],"title":"CLRS Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcmu/","tags":null,"title":"CMU"},{"categories":null,"contents":"A brain signal to help maintain glucose homeostatis\nBrain takes glucose product + glucose uptake to control energy balance in food intake and energy expenditure.\nThe brain takes:\nNeural Behavioral Hormonal responses to maintain glucode uptake.\n","permalink":"https://www.jemoka.com/posts/kbhcns_regulation/","tags":null,"title":"CNS regulation"},{"categories":null,"contents":"A Code Review is a systematic study code by others\u0026mdash;like proofreading an essay. There\u0026rsquo;s a few different ways of doing Code Review.\nWhy do we code review? catch bugs, style deviations, design + convention violations security trade-off: having someone who is well-versed in security is useful to know how other people\u0026rsquo;s code work to learn additional skills, languages, frameworks Code Review Methodology Don\u0026rsquo;t do it Very fast! None of the benefits of code review Over-the-Shoulder Code Review Over-the-Shoulder Code Review typically is done over someone\u0026rsquo;s shoulder\u0026mdash;author walking the reviewer through code.\nProps Typically catch major + obvious issues Lightweight and fast\u0026mdash;most likely to get done Cons Author explains code as they go; biasing the reviewer Author knows the code, so may gloss over the parts they are familiar with + move at a pace faster than the reviewer can process The author only has two solders: can\u0026rsquo;t involve more than 2 people Usually real-time: the author is waiting for reviewer, which is blocking both author and reviewer\u0026rsquo;s schedules Pair Programming Pair Programming is a two-brains, one keyboard paradigm. The less experienced person types (moves at the pace of the slower person)\nPros Real-time feedback/correction; good for learning new things Writing code + code review at the same time\u0026mdash;total time saved Cons Two people working together are susceptible to bind slots still! Remember that it will take four times as long to do something: this trade-off is worth it! Formal Code Review Tools: Phabricator, Gerrit\nThe process of Formal Code Review is a very formal process.\nAuthor writes and commits code The diff of the commit is sent to the reviewer Reviewer reads through the code at their own pace Reviewer can comment on the entire diff, or on specific lines of code This can involve multiple reviewers This is basically informal code review, but solving the original issues.\nBest Practices Every commit must be code reviewed before pushed to other people The larger the commit, the more reviewers should be on it Reviewer Approval Levels +2: code can be pushed +1: code looks good, someone else should make the call 0: why am I here? I dunno -1: this code smells funny, but I\u0026rsquo;m willing to be overruled -2: this code must be changed before being pushed -1/-2: comment should clearly indicate whether they are blocking push.\n","permalink":"https://www.jemoka.com/posts/kbhcode_review/","tags":null,"title":"Code Review"},{"categories":null,"contents":"The time it takes for a qubit to oscillate between two states between damping down.\n","permalink":"https://www.jemoka.com/posts/kbhcoherence_time/","tags":null,"title":"coherence time"},{"categories":null,"contents":"The cold war is a period of time in which there is blocks of conflict. This is after WWII.\nSee also:\ncold war in vietnam ","permalink":"https://www.jemoka.com/posts/kbhcold_war/","tags":null,"title":"cold war"},{"categories":null,"contents":"A fact sheet on the progress of the cold war in Vietnam.\nprogression of US escalation in the war, an overview Reading: encyclopedia Britannica\n1959-1960: VCs initiated a group of ambushes which the exiled government led by Ngô Đình Diệm can no longer fend off 1961: Kennedy takes office, institutes a plan to put American advisors at all levels of Vietnam leadership 1963: Buddest monks rebelled, Ngô family rule became bizarre and leveraged their Roman Catholic views to persecute Buddhists in the region 1963: Ngô Đình Diệm is assassinated after the support of the US (Kennedy) via Cable 243 seeking a regime change 1963: Kennedy assisinated 1964: Vietnam situation worsens 1965: American government fully went in, and Ky was eased out of power when Neuyen Van Thieu ad full control 1965: US fighting was effective though unpresistent; viet cong just went in after US leaves 1967: Protests in the US lead to a growing anti-war sentiment, which the VietCong picked up on 1968: the Tet Offensive, a VietCong operation, tried to pillage South Vietnam. Though it failed, strong anti-war sentiments were stirred. 1969: Anti-War protests pick up force 1970: Ohio National Guard opens fire on unarmed protesters 1973: Peace Pact Signed after the US giving up, essentially 1975: Saigon falls, US evacuates anti-war protest motivation in Vietnam Reading: Protest against the War in Vietnam\nThe first protests rose in the 1950 and picked up force by the late 1960s when LBJ decided not to seek re-election.\nForeign policy is usually hard to change, but the strength of domestic dissent in Vietnam represents an usual shift which drove foreign policy changes.\nRight-wing sentiment: seeing the war as a means of future-proofing the American government from Communistic influences. Left-wing protest More organized than the spontaneous of the right-wing protest Split between moralistic + legalistic interests vs. national interest domestic political influence of the Vietnam War Reading: The War that Killed Trust, Karl Marlantes, 2017\n\u0026ldquo;Of course presidents lie\u0026rdquo;\u0026mdash;that the Vietnam War represented the shift away from genuine truthfulness as a part of American politics Killed 58,000 service-members, and made Americans cynical and distrustful of governmental institutions Systemic Cynicism Johnson\u0026rsquo;s \u0026ldquo;credibility gap\u0026rdquo;: that the president maybe lying. Nowadays this is commonplace, but back then it was quite unusual.\nCLAIM: engendered Cynicism threatened inaction.\nRacial Integration The cold war promised higher degrees of racial integration because of collective service.\nRepeated Touring That, post-draft, the American working class became much more likely to serve \u0026ldquo;voluntarily\u0026rdquo; by being recruited. Unlike the draft, which is some ways is universal service, the volunteer system is much more reliant upon th emiddle class.\nsocial impacts of the Vietnam War Reading: The Social Impact of War, Modell and Haggerty, 1991\nWars\u0026rsquo; effects can be treated with a lens of social manifestation The Vietnam war had an impact on the last 20 years of primary war literature draft The draft is the principle mechanism by which people into the war. The system facilitating the draft in the United States, the Selective Service System, is a good case study for such a system in the Vietnam War.\nBy its design, the draft is supposed to be an equitable process (baring gender and age.) However, the Vietnam War reveals that the military services was not straightforwardly distributed: often drafting children of lower socioeconomic status.\nexperience of servicemen in Vietnam Soldiers in the Vietnam War have shown some negative psychological side effects. Solders are shown to be \u0026ldquo;working through\u0026rdquo; the ideas to process, creating a larger effects.\neffects on the economy War veterans generally had higher incomes than non-vets, mostly because they have more income per level of educational attanment.\nhistoriographical school of Vietnam War Reading: James McLeroy, Small Wars Journal, (Army Special Forces Officer in I Corps, Vietnam, in 1968)\nOrthodox treatment Vietnam War as an extension/afterthought of late-20th century cold war history\nVietnam War escalated only because of United States involvement \u0026ldquo;anti-war\u0026rdquo; is not opposition against communistic conquest but opposition against war in itself Revisionist treatment Vietnam War as a calculable implementation of escalator revolutionary strategy modeled after Mao.\nVietnam War is not an insurgency or a civil war, but instead a part of the three-step guerrilla warfare Provocation of the United States is a part of the strategy\u0026mdash;to force them to move out of Vietnam and to encourage the communist bloc to provide more support ","permalink":"https://www.jemoka.com/posts/kbhcold_war_in_vietnam/","tags":null,"title":"cold war in vietnam"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcollectivist_economy/","tags":null,"title":"Collectivist Economy"},{"categories":null,"contents":"College application is the process of applying to an American college.\n","permalink":"https://www.jemoka.com/posts/kbhcollege_application/","tags":null,"title":"college application"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcollegeboard/","tags":null,"title":"CollegeBoard"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcolumn_space/","tags":null,"title":"column space"},{"categories":null,"contents":"commutativity means that the same operation can be ran in any order.\nThat is:\n\\begin{equation} ABC = ACB \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhcommutivity/","tags":null,"title":"commutativity"},{"categories":null,"contents":"A complex number is a type of number. They are usually written as \\(a+bi\\).\nFormally\u0026mdash;\n\\begin{equation} \\mathbb{C} = \\left\\{a+bi\\ \\middle |\\ a,b \\in \\mathbb{R} \\right\\} \\end{equation}\nconstituents an order pair of two elements \\((a,b)\\) where \\(a,b\\in \\mathbb{R}\\).\nrequirements there are 6. For all statements below, we assume \\(\\alpha = a+bi\\) and \\(\\beta=c+di\\), \\(\\lambda = e+fi\\), where \\(a,b,c,d,e,f \\in \\mathbb{R}\\) and therefore \\(\\alpha, \\beta,\\lambda \\in \\mathbb{C}\\).\ncommutativity \\(\\alpha + \\beta = \\beta + \\alpha\\) and \\(\\alpha\\beta = \\beta\\alpha\\) for all \\(\\alpha,\\beta \\in \\mathbb{C}\\).\nProof of complex number commutativity We desire \\(\\alpha + \\beta = \\beta + \\alpha\\).\n\\begin{align} \\alpha + \\beta \u0026amp;= (a+bi)+(c+di) \\\\ \u0026amp;=(a+c)+(b+d)i \\\\ \u0026amp;=(c+a)+(d+b)i \\\\ \u0026amp;=(c+di) + (a+bi) \\\\ \u0026amp;=\\beta+\\alpha\\ \\blacksquare \\end{align}\nleveraging the commutativity inside real numbers.\nInsights: combining and splitting\nThis proof has the feature of combining, operating (commuting, here), the splitting.\nassociativity \\((\\alpha +\\beta) + \\lambda = \\alpha + (\\beta +\\lambda)\\) and \\((\\alpha\\beta) \\lambda = (\\alpha \\beta) \\lambda\\)\nProven via the same trick from last time\nidentities \\(\\lambda + 0 = \\lambda\\), \\(\\lambda 1 = \\lambda\\)\nProof of complex number additive identity We desire that \\(\\lambda + 0 = 0\\).\n\\begin{align} \\lambda + 0 \u0026amp;= (e+fi) + (0+0i) \\\\ \u0026amp;= (e+0) + (f+0)i \\\\ \u0026amp;= e+fi\\ \\blacksquare \\end{align}\nmultiplicative identity is proven in the same way\nadditive inverse \\(\\forall \\alpha \\in \\mathbb{C}, \\exists !\\ \\beta \\in \\mathbb{C}: \\alpha + \\beta = 0\\)\nProof of complex number additive inverse We desire to claim that \\(\\forall \\alpha \\in \\mathbb{C}, \\exists !\\ \\beta \\in \\mathbb{C}: \\alpha + \\beta = 0\\), specifically that there is a unique \\(\\beta\\) which is the additive inverse of every \\(\\alpha\\).\nTake a number \\(\\alpha \\in \\mathbb{C}\\). We have that \\(\\alpha\\) would then by definition be some \\((a+bi)\\) where \\(a,b \\in \\mathbb{R}\\).\nTake some \\(\\beta\\) for which \\(\\alpha + \\beta = 0\\); by definition we again have \\(\\beta\\) equals some \\((c+di)\\) where \\(c,d \\in \\mathbb{R}\\).\n\\(\\because \\alpha + \\beta =0\\), \\(\\therefore (a+bi) + (c+di) = 0\\). \\(\\therefore (a+c) + (b+d)i = 0\\) \\(\\therefore a+c = 0, b+d = 0\\) \\(\\therefore c = -a, d = -b\\) We have created a unique definition of \\(c,d\\) and therefore \\(\\beta\\) given any \\(\\alpha\\), implying both uniqueness and existence.\nInsights: construct then generalize\nIn this case, the cool insight is the construct and generalize pattern. We are taking a single case \\(\\alpha\\), manipulating it, and wrote the result we want in terms of the constituents of \\(\\alpha\\). This creates both an existence and uniqueness proof.\nmultiplicative inverse \\(\\forall \\alpha \\in \\mathbb{C}, \\alpha \\neq 0, \\exists!\\ \\beta \\in \\mathbb{C} : \\alpha\\beta =1\\)\nThis is proven exactly in the same way as before.\ndistributive property \\(\\lambda(\\alpha+\\beta) = \\lambda \\alpha + \\lambda \\beta\\ \\forall\\ \\lambda, \\alpha, \\beta \\in \\mathbb{C}\\)\nProof of complex number distributive property We desire to claim that \\(\\lambda(\\alpha+\\beta) = \\lambda \\alpha + \\lambda \\beta\\).\n\\begin{align} \\lambda(\\alpha+\\beta) \u0026amp;= (e+fi)((a+bi)+(c+di))\\\\ \u0026amp;=(e+fi)((a+c)+(b+d)i)\\\\ \u0026amp;=((ea+ec)-(fb+fd))+((eb+ed)+(fa+fc))i\\\\ \u0026amp;=ea+ec-fb-fd+(eb+ed+fa+fc)i\\\\ \u0026amp;=ea-fb+ec-fd+(eb+fa+ed+fc)i\\\\ \u0026amp;=(ea-fb)+(ec-fd)+((eb+fa)+(ed+fc))i\\\\ \u0026amp;=((ea-fb)+(eb+fa)i) + ((ec-fd)+(ed+fc)i)\\\\ \u0026amp;=(e+fi)(a+bi) + (e+fi)(c+di)\\\\ \u0026amp;=\\lambda \\alpha + \\lambda \\beta\\ \\blacksquare \\end{align}\nInsights: try to remember to go backwards\nAt some point in this proof I had to reverse complex addition then multiplication, which actually tripped me up for a bit (\u0026ldquo;how does i distribute!!!\u0026rdquo;, etc.) Turns out, there was already a definition for addition and multiplication of complex numbers so we just needed to use that.\nadditional information addition and multiplication of complex numbers \\begin{align} (a+bi) + (c+di) \u0026amp;= (a+c)+(b+d)i \\\\ (a+bi)(c+di) \u0026amp;= (ac-bd)+(ad+bc)i \\end{align}\nwhere, \\(a,b,c,d\\in\\mathbb{R}\\).\nsubtraction and division of complex numbers Let \\(\\alpha, \\beta \\in \\mathbb{C}\\), and \\(-a\\) be the additive inverse of \\(\\alpha\\) and \\(\\frac{1}{\\alpha}\\) be the multiplicative inverse of \\(\\alpha\\).\nsubtraction: \\(\\beta-\\alpha = \\beta + (-\\alpha)\\) division: \\(\\frac{\\beta}{\\alpha} = \\beta\\frac{1}{\\alpha}\\) Simple enough, subtraction and division of complex numbers is just defined by applying the inverses of a number to a different number.\ncomplex numbers form a field See properties of complex arithmetic, how we proved that it satisfies a field.\ncomplex conjugate The complex conjugate of a complex number is defined as\n\\begin{equation} \\bar{z} = \\text{Re}\\ z - (\\text{Im}\\ z)i \\end{equation}\ni.e. taking the complex part to be negative. Say, \\(z = 3+2i\\), then \\(\\bar{z}=3-2i\\).\nabsolute value (complex numbers) The absolute value (complex numbers) of a complex number is:\n\\begin{equation} |z| = \\sqrt{{(\\text{Re}\\ z)^{2} + (\\text{Im}\\ z)^{2}}} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhcomplex_number/","tags":null,"title":"complex number"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcomplex_system/","tags":null,"title":"Complex System"},{"categories":null,"contents":"complexity theory is a theory in algorithms to analyze time classes.\nWe know that \\(O(n\\ log\\ n)\\) is between \\(O(n)\\) and \\(O(n^2)\\) \u0026mdash; so we can roughly call it \u0026ldquo;polynomial time.\u0026rdquo;\nSince the optimal comparison cannot be faster than polynomial time, we say that comparison-based sorting is a polynomial-time algorithm.\nFrom this information, we can come up with two main time classes: \\(P\\) for solutions with known polynomial time, \\(NP\\) for non-deterministic polynomial time.\nThink of it as \\(P\\) is solvable with polynomial time and \\(NP\\) is verifiable with polynomial time.\nThe cool thing about \\(NP\\) problems is that solving a subset of them (\u0026quot;\\(NP\\) hard\u0026quot; problems) solves all \\(NP\\) problems.\nreduction (algorithms) reduction is how you can use \\(NP-hard\\) problems to solve all \\(NP\\) problems in complexity theory.\nSay, multiplication:\nsay you have a basic algorithm to add we can perform multiplication by asking our black box addition algorithm to add \\(n\\) times in complexity theory terms, this means addition is \u0026ldquo;at least as hard\u0026rdquo; as multiplication. Because, if we can solve any addition problem, we can solve any multiplication problem. \u0026ldquo;Given this, do that.\u0026rdquo;\nproblem classes (see above)\n\u0026ldquo;Polynomial time\u0026rdquo; \\(P\\) \u0026mdash; problems solvable with polynomial time \u0026ldquo;Non-deterministic polynomial time\u0026rdquo; \\(NP\\) \u0026mdash; problem verifiable with polynomial time \u0026ldquo;Exponential time\u0026rdquo; \\(EXPTIME\\) \u0026mdash; problems that can only be solved in exponential time \u0026ldquo;2 Exponential time\u0026rdquo; \\(2EXPTIME\\) \u0026mdash; class of problems that takes \\(2^{2^n}\\) time to solve Space complexity works in a similar way.\n\\(P\\) and \\(NP\\) are deterministic and non-deterministic in context to a Turing machine.\nFundamentally, \\(P\\) and \\(NP\\) only apply to decision problems\u0026mdash;given a problem, output \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no.\u0026rdquo; However, this definition can be stretched: sorting is a decision problem, because it can be stated as \u0026ldquo;given an unsorted array, can you verify whether or not an array is sorted\u0026rdquo;\n","permalink":"https://www.jemoka.com/posts/kbhcomplexity_theory/","tags":null,"title":"complexity theory"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcomposite_system/","tags":null,"title":"composite system"},{"categories":null,"contents":"Computational Biology is a the study of biology using computation.\nRather that starting from the properties, start with the end states or what properties it has; instead, we define the initial values based on the edges.\nConstructor theory: https://en.wikipedia.org/wiki/Constructor_theory?\nthe relationship between temperature and occurance of a uniform gas is actually right-skewed; the mean temperature in a uniform closed system will be higher than the median temperature molecules are not static: at best, molecules are static when frozen in place; yet, generally it is not in their nature to stay solidly in place; they just shuffle around but maintain the molecular sturucture If the energy level is higher, it will ignore various troughs\n","permalink":"https://www.jemoka.com/posts/kbhcomputational_biology_index/","tags":null,"title":"Computational Biology Index"},{"categories":null,"contents":"conceptual grammar is the proposed universal grammar which connects semantic primes. In theory, this grammar is universal across languages.\nThere are three main categories of conceptual grammars:\nCombinatorics (connecting one idea to another) Account of valancies? #what Propositional complementation (location \u0026ldquo;something that happen in this place\u0026rdquo; ","permalink":"https://www.jemoka.com/posts/kbhconceptual_grammar/","tags":null,"title":"conceptual grammar"},{"categories":null,"contents":"Current automated lexicography (term definition) techniques cannot include contextual or new term information as a part of its synthesis. We propose a novel data harvesting scheme leveraging lead paragraphs in Wikipedia to train automated context-aware lexicographical models. Furthermore, we present ConDef, a fine-tuned BART trained on the harvested data that defines vocabulary terms from a short context. ConDef is determined to be highly accurate in context-dependent lexicography as validated on ROUGE-1 and ROUGE-L measures in an 1000-item withheld test set, achieving scores of 46.40% and 43.26% respectively. Furthermore, we demonstrate that ConDef\u0026rsquo;s synthesis serve as good proxies for term definitions by achieving ROUGE-1 measure of 27.79% directly against gold-standard WordNet definitions.Accepted to the 2022 SAI Computing Conference, to be published on Springer Nature\u0026rsquo;s Lecture Notes on Networks and Systems Current automated lexicography (term definition) techniques cannot include contextual or new term information as a part of its synthesis. We propose a novel data harvesting scheme leveraging lead paragraphs in Wikipedia to train automated context-aware lexicographical models. Furthermore, we present ConDef, a fine-tuned BART trained on the harvested data that defines vocabulary terms from a short context. ConDef is determined to be highly accurate in context-dependent lexicography as validated on ROUGE-1 and ROUGE-L measures in an 1000-item withheld test set, achieving scores of 46.40% and 43.26% respectively. Furthermore, we demonstrate that ConDef\u0026rsquo;s synthesis serve as good proxies for term definitions by achieving ROUGE-1 measure of 27.79% directly against gold-standard WordNet definitions.\n","permalink":"https://www.jemoka.com/posts/kbhcondef_abstract/","tags":null,"title":"ConDef Abstract"},{"categories":null,"contents":"There are many condition in the Great Depression caused\nby 1932, 1/4 had no work emigration exceeded immigration decrease in American birth increase of mental illness and suicide people create Hooverviles movies and radio became much more popular ","permalink":"https://www.jemoka.com/posts/kbhconditions_in_the_great_depression/","tags":null,"title":"conditions in the Great Depression"},{"categories":null,"contents":"proportional confidence intervals We will measure a single stastistic from a large population, and call it the point estimate. This is usually denoted as \\(\\hat{p}\\).\nGiven a proportion \\(\\hat{p}\\) (\u0026ldquo;95% of sample), the range which would possibly contain it as part of its \\(2\\sigma\\) range is the \\(95\\%\\) confidence interval.\nTherefore, given a \\(\\hat{p}\\) the plausible interval for its confidence is:\n\\begin{equation} \\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{equation}\nwhere, \\(n\\) is the sample size, \\(\\hat{p}\\) is the point estimate, and \\(z*=1.96\\) is the critical value, the z-score denoting \\(95\\%\\) confidence (or any other desired confidence level).\nconditions for proportional confidence interval There are the conditions that make a proportional confidence interval work\ndistribution is normal \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\) are both \\(\u0026gt;10\\) we are sampling with replacement, or otherwise sampling \\(\u0026lt;10\\%\\) of population (otherwise, we need to apply a finite population correction value confidence intervals The expression is:\n\\begin{equation} \\bar{x} \\pm t^* \\frac{s}{\\sqrt{n}} \\end{equation}\nwhere \\(t*\\) is the \\(t\\) score of the desired power level with the correct degrees of freedom; \\(s\\) the sample standard deviation, \\(n\\) the sample size, and \\(\\har{x}\\) the mean.\n","permalink":"https://www.jemoka.com/posts/kbhconfidence_interval/","tags":null,"title":"confidence interval"},{"categories":null,"contents":"constructor theory deals with \u0026ldquo;constructors\u0026rdquo;, a general type of computer.\nconstructor theory can give us a theory of the universal quantum constructor by expanding upon quantum information theory. It allows us to unify quantum and classical information by simply defining operations in terms of counterfactuals exclusively: that a space is entirely defined by what\u0026rsquo;s possible and what\u0026rsquo;s not possible.\nAccording to constructor theory, fundamental laws are not dynamical laws instead are boundary conditions. We can take the boundary conditions to form the most general set of initial conditions.\nyou can conjecture a set of laws is fully complete at some point, you will find something that hits the bounds then you revise the theory ","permalink":"https://www.jemoka.com/posts/kbhconstructor_theory/","tags":null,"title":"constructor"},{"categories":null,"contents":"Cookie Theft is a Discourse-Completion Task that involves describing the following picture:\n","permalink":"https://www.jemoka.com/posts/kbhctp/","tags":null,"title":"Cookie Theft Picture Description Task"},{"categories":null,"contents":"coulomb\u0026rsquo;s law is a principle that deals with the force that two charged particles exhibit to each other.\nconstituents \\(k\\), Coulomb\u0026rsquo;s Constant, found roughly to be \\(9 \\times 10^{9} \\frac{N m^{2}}{C}\\) \\(q_{1,2}\\), the charge of the two particles you are analyzing \\(r\\), distance between particles requirements \\begin{equation} \\vec{F_{E}} = k \\frac{q_1q_2}{r^{2}} \\end{equation}\nadditional information interpreting signs on \\(F_{e}\\) negative: attraction force between changes (the points have opposite signed charges, and so attract) positive: repulsion force between changes (the point have the same signed change, so repel) alternative formulation of Coulomb\u0026rsquo;s Law The law is often redefined with the language of the premittivity of free space:\n\\begin{equation} \\vec{F_{E}} = \\frac{1}{4\\pi \\epsilon_{0}} \\frac{q_1q_2}{r^{2}} \\end{equation}\nsuperposition The net electric force on a test change is simply the sum of the electric forces which other particles exhibit on the test change. That is:\n\\begin{equation} F_{on\\ 2} = F_{1 \\to 2} + F_{3 \\to 2} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhcoulomb_s_law/","tags":null,"title":"Coulomb's Law"},{"categories":null,"contents":"quantum information theory requires manipulating counterfactual information\u0026mdash;not what the current known states are, but what are the next possible states.\nInside physics, there is already a few principles which are counterfactual.\nConservation of energy: a perpetual machine is *impossible Second law: its impossible to convert all heat into useful work Heisenberg\u0026rsquo;s uncertainty: its impossible to copy reliable all states of a qubit With the impossibles, we can make the possible.\n","permalink":"https://www.jemoka.com/posts/kbhcounterfactual/","tags":null,"title":"counterfactual"},{"categories":null,"contents":"\\begin{equation} cov(x,y) = E[XY]-E[X]E[Y] \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhcovariance/","tags":null,"title":"covariance"},{"categories":null,"contents":"coveather is a novel consensus algorithm based on the proof of work mechanism.\nSee also minimum user base requirements for coveather and Coveather Abstract.\n","permalink":"https://www.jemoka.com/posts/kbhcoveather/","tags":null,"title":"coveather"},{"categories":null,"contents":"Digital Health Passes (DHP), systems of digitally validating quarantine and vaccination status such as the New York IBM Excelsior Pass, demonstrate a lawful means to approach some benefits offered by \u0026ldquo;true elimination\u0026rdquo; treatment strategies-which focus on the complete elimination of cases instead of investing more in controlling the progression of the disease-of COVID-19. Current implementations of DHPs require region-based control and central storage of Protected Health Information (PHI)-creating a challenge to widespread use across different jurisdictions with incompatible data management systems and a lack of standardized patient privacy controls. In this work, a mechanism for decentralized PHI storage and validation is proposed through a novel two-stage handshaking mechanism update to blockchain proof-of-stake consensus. The proposed mechanism, when used to support a DHP, allows individuals to validate their quarantine and testing universally with any jurisdiction while allowing their right of independent movement and the protection of their PHI. Implementational details on the protocol are given, and the protocol is shown to withstand a 1% disturbance attack at only 923 participants via a Monte-Carlo simulation: further validating its stability.\n","permalink":"https://www.jemoka.com/posts/kbhcoveather_abstract/","tags":null,"title":"Coveather Abstract"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcram/","tags":null,"title":"cram"},{"categories":null,"contents":"crap to remember for AP Stats is a cram sheet for the AP Statistics exam.\n95% confidence: \\(z^*=1.96\\)\n\\(r=1\\): perfect positive correlation \\(r=-1\\): perfect negative correlation \\(r=0\\): no correlation S: standard deviation of residuals R-sq: how much of varience in dep. var can be explained by indp. var SE: estimate of standard deviation of the random var. that is slope.\nFor lines:\nNote that p value from regression outputs are two-tailed. So divide by 2 if you want a one-tail result.\nMultiplication changes mean as well as well as standard deviation. Adding changes mean but not standard deviation.\nExpected value of the sum and differences of random variables are just the sums and differences of their expected value. \\(S = X+Y, \\bar{S} = \\bar{X}+\\bar{Y}\\).\nVariance of random variables are just the sum and differences of their variance. \\(S=X+Y,{\\sigma^2}_S = {\\sigma^2}_X+{\\sigma^2}_Y\\).\n#WHAPS\nwhat test what hypothesis and what significance level assumptions and conditions; state! random independent: \\(\\le 10\\%\\) of population. t and z special: normal (z tests: \\(np, n(1-p) \\geq 10\\), t tests: \\(n\u0026gt;30\\) or given) chi-square special: \\(\\forall\\ EV \u0026gt; 5\\) p: z-statistic that would XD: Control (control for confounding and bias, placebo, etc.), Randomization (spread uncontrolled variability), Replication (need to have adequate units and ability to be repeated)\n=\u0026gt; Describing a distribution\nCenter: Mean, Median, or Mode? figure by skew Shape: Symmetric vs Skewed? Unimodal vs Bimodal Spread: Range and Inter-Quartile Range Outlier: anything more than 1.5*IQR away Context: what the distribution shows \u0026ldquo;Experimental Unit\u0026rdquo;: a physic entity that\u0026rsquo;s the primary unit of interest in a research objective.\nConditions for binomial distribution:\nBinary Independent Fixed number of trials All trials with same probability Conditions for geometric distrubiton\nBinary Independent Fixed number of successes All trials with same probability state the thing, state the conditions: \u0026ldquo;normal distribution with n= s=\u0026rdquo;, binomial distribution with n= p= etc.\n","permalink":"https://www.jemoka.com/posts/kbhcrap_to_remember_for_ap_stats/","tags":null,"title":"crap to remember for AP Stats"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcredit/","tags":null,"title":"credit"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcredit_suisse/","tags":null,"title":"Credit Suisse"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcritical_value/","tags":null,"title":"critical value"},{"categories":null,"contents":"criticized the New Deal from all sides. Senator Huy P. Long claimed to \u0026ldquo;show our wealth.\u0026rdquo; nullification from conservative supreme court, FDR threatened to restructure + hurts his coalition.\nFDR ordered cuts in spending 1938 midterms: Republicans can block programs \u0026mdash; gained control of congress + created ability to gain control ","permalink":"https://www.jemoka.com/posts/kbhcriticism_of_the_new_deal/","tags":null,"title":"criticism of the New Deal (See file KBhnew_deal.org)"},{"categories":null,"contents":"constituents additional information lack of inverse of cross product The cross product doesn\u0026rsquo;t have an inverse\ngeometric interpretation of cross product \\begin{equation} a \\times b = |\\vec{a}| |\\vec{b}| \\sin \\theta n \\end{equation}\nwhere, \\(n\\) is the unit vector in some direction.\nThe length of the resulting vector in the cross product is the area of the parallelogram formed by the two vectors.\n","permalink":"https://www.jemoka.com/posts/kbhcross_product/","tags":null,"title":"cross product"},{"categories":null,"contents":"CrossFinder is a darkpool owned by Credit Suisse.\nFeatures:\nNormal darkpooling Routing the transaction out to other exchanges and dark-pools if needed Measuring the latency of each other exchange, etc. ","permalink":"https://www.jemoka.com/posts/kbhcrossfinder/","tags":null,"title":"CrossFinder"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcrystels/","tags":null,"title":"Crystals"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcultural_revolution/","tags":null,"title":"Cultural Revolution"},{"categories":null,"contents":"darkpools are non-exchange, non-published exchange which doesn\u0026rsquo;t have the same reporting obligations of a stock market. The only thing they have to report is the actual filled transactions after 90 seconds.\ndarkpools are used because the order book/bid-ask spread is not leaked, which means large transactions will not be able to influence the market as much.\n","permalink":"https://www.jemoka.com/posts/kbhdarkpool/","tags":null,"title":"darkpool"},{"categories":null,"contents":"For data inference tasks, categorical data\n","permalink":"https://www.jemoka.com/posts/kbhdata_inference/","tags":null,"title":"data inference"},{"categories":null,"contents":"a student approach to learning where learning outcomes are driven by student\u0026rsquo;s own experience to deeply drive educational results independenlty\n","permalink":"https://www.jemoka.com/posts/kbhdeep_approach/","tags":null,"title":"deep approach"},{"categories":null,"contents":"Facts Everybody writes bugs Debugging sucks Defensive Programming Tools + Techniques Use language features Specs, documentations, Test-Driven Development, unit testing Fail fast and loudly Systematic debugging Investing in tools Use Language Features Descriptors: static, final, pub./priv. Type checking: prevent type errors Automatic array bounds checking Memory management Compiler optimization Key idea: know what language features are available, why/when to use them. don\u0026rsquo;t work against the language in circumventing them\nSpecs, Docs., TDD, Unit Tests How should it work: specs How does it work: docs How will I know it works: TDD How do I know it still works: unit tests These all force you to think about your code before!! you write it so then you can correct them as soon as possible.\nFailing Fast and Failing Loudly The earlier you recognize there is a problem, the easier it is to fix it Problems not fixed can be lost, covered up, or even relied upon Learn from every failure How do we put this into practice Use asserts, exceptions, logging Fix/diagnose/track every bug, even if you choose not to fix it Add regression tests for every bug + run them regularly Systematic Debugging Systematic Debugging is a framework for debugging software.\nReproduce the bug Reduce the bug to the smallest possible, repeatable test case Faster test cases mean faster iterations in debugging Smaller test cases help eliminate possible causes for error Find the root cause Study data (logs, behavior, etc.), hypothesis, experiment, repeat Change code and data to get more information FIXING SYMPTOM IS NOT ENOUGH Fix the bug Add a regression test, and run all tests Reducing Test Case Start with the data that uncovered the bug Remove pieces of data until the bug no longer occurs Bracketing: create both a test case that fails and similar test cases that pass Binary search: remove/add back half of the data at a time Can work from either end: start with everything and reduce until disappearance, or start with only one line and build until bug Finding the Cause Trace through the program View intermediate results Every iteration of a for loop Input and output of a given function Tools to use assert() printing/logging a debugger binary search Tooling! Linter Fuzzer Sanitizer Valgrind DTrace ","permalink":"https://www.jemoka.com/posts/kbhdefensive_programming/","tags":null,"title":"Defensive Programming"},{"categories":null,"contents":"demand-driven theory hypothesis that the reason why the Great Depression took place was because people were not buying stocks, etc, and there was no demand.\nSee also: Monetarist theory.\n","permalink":"https://www.jemoka.com/posts/kbhdemand_driven_theory/","tags":null,"title":"demand-driven theory"},{"categories":null,"contents":"DementiaBank is a shared database of multimedia interactions for the study of communication in dementia. There are a few projects being explored for DementiaBank.\nSee also: ADReSS Literature Survey\n","permalink":"https://www.jemoka.com/posts/kbhdementiabank/","tags":null,"title":"DementiaBank"},{"categories":null,"contents":"Ideas Can we correlate any longitudinal data with NACC?\nData dementia/English/Lanzi: Alyssa Lanzi\u0026rsquo;s new data\ndementia/English/Delaware\nWhat are the standard for acoustic features?\nMotor cortex/frontal control may also be impacted\nVocal tremer\nWhat are the predictors? How automatic can we make it?\n","permalink":"https://www.jemoka.com/posts/kbhdementiabank_acoustics_brainstoming/","tags":null,"title":"DementiaBank Acoustics Brainstoming"},{"categories":null,"contents":"The DementiaBank Acoustics Project is a working title for an acoustic-only challenge for AD detection. This document serves as the lab notebook for this project.\nThis project will attempt to replicate some of the results of Wang 2019 and Martinc 2021, but focusing on minimizing human involvement; we will first work on raw transcript classification with ERNIE (cutting all CHAT annotations), then introduce pause-encoding in a manner similar to Yuan 2021 which is automated by MFA. Goal is to replicate the results of Yuan 2021/or even Martinc 2021 in a completely automated manner.\nBackground Reading I first began by doing a literature survey on the ADReSS Challenge results published in the Frontiers AD special interest group issue.\nProposal And then, we wrote a proposal: DementiaBank Acoustics Project Proposal\nBrainstoming More notes from the meeting: DementiaBank Acoustics Brainstoming\nProtocol Notes July 1st Began by moving a subsample of Pitt\u0026rsquo;s Cookie Theft to pitt-7-1 in the raw data folder Ran flo on all collected samples. Arguments used are the same as that for batchalign, except we filter out the INV tier as we are detecting AD on patient and not investigator: so flo +d +ca +t* -tINV Moved all collected samples (and changed extension to .txt) to the same sub-folder, but in transcripts_nodisfluency July 2nd Created a dataprep script dataprep.py which dumps a pickled copy of cleaned data to transcripts_nodisfluency/pitt-7-1.dat. Created sliding windows of 5 pieces of dialogue concatenated, stored it in transcripts_nodisfluency/pitt-7-1-windowed.dat Used tencent/HuYong\u0026rsquo;s nghuyong/ernie-2.0-en Ernie 2.0 model, the continuous language model from Baidu (Layer:12, Hidden:768, Heads:12) July 4th Finalized training code. Selected base hyperparameters {bs: 8, epochs: 2, lr: 3e-3, length: 60}. Again, we are using Baidu\u0026rsquo;s nghuyong/ernie-2.0-en. Started training fastcalculator on 24bc812 train: faithful-frog-3 {bs: 8, epochs: 2, lr: 3e-3, length: 60, pitt-7-1-windowed.dat }\nCommentary: LR could be too high, looking at the divergent loss behavior. Decision: dropping bs to 4 and lr to 1e-5, similar to previous transformers. Also training for 3 epochs. train: revived-disco-5 {bs: 4, epochs: 3, lr: 1e-5, length: 60, pitt-7-1-windowed.dat }\nCommentary: quintessential overfitting Decision: Made the corpus bigger cleaned the entire Pitt corpus (pitt-7-4 in the raw folder) to become training data. Similar to pitt-7-1, ran flo on all collected samples; arguments used are the same as that for batchalign, except we filter out the INV tier as we are detecting AD on patient and not investigator: so flo +d +ca +t* -tINV; the flo\u0026rsquo;d results are in transcripts_nodisfluency. the notable difference between the previous dataset 7-1 and the current one 7-4 is that the 7-4 are prepended numbered by the task (cookie/100-01.cha \u0026gt; =cookie-100-01.txt) New (full) Pitt data as prepared above is ran though the dataprep script as of b325514cfad79da82d7a519ed29ea19ed87b2be4 (difference is that empty/dummy files are ignored), and pickled at transcripts_nodisfluency/pitt-7-4.dat and transcripts_nodisfluency/pitt-7-4-windowed.dat respectively. For new data, window size is still 5, splitting 10 cases out for testing now instead of 5. train: vocal-oath-6 {bs: 4, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed.dat}\nCommentary: high recall, low precision. Perhaps classes aren\u0026rsquo;t balanced? Spoiler alert: they are not. An inspection of data reveals that there is 3211 rows of dementia, 2397 rows of control Decision: Created pitt-7-4-bal and pitt-7-4-windowed-bal series of data based on dataprep.py on 703f79248a20fd7a13a5033ca2bf7f691f42c941. This version force-crops to make sure that the dementia and control indicies have the exact same length for each class. train: helpful-leaf-7 {bs: 4, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat}\nBeautiful. Question now is whether or not there is data leakage/external heuristics. It is a good time to do some LOOCV. Getting this result without any disfluency calculations seems unlikely.\nBut anyways, going to discuss these results as they seem to meet results we see in Yuan 2021, even without top-N ensemble; though this is one trial, LOOCV may still show that we actually need it.\nJuly 5th Began the day with creating the script k-fold validation; I originally hoped to exactly replicate the procedure of Yuan 2021 for comparability, but, not sure how they got the actual result of a min/max range with LOOCV on binary; therefore, we will instead create a 95% confidence interval analysis via a single-variable t test on standard k-fold validation. K=50 During one-off testing, another set of hyperparameters seems to work too: {bs: 72, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat}. As we have not begun tuning for hyperparameters, we are just going to use this set, K=50, for the first k-fold trial. k-fold: F4ZVbGfdBAQvtvXemWZCZD code: 55f77ff1dea03c3ed66967864dc52fd2c0062f23\n{bs: 72, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat} K = 50\nIt seems like the results we got is consistent and validates in a manner which we expect.\nJuly 7th Yesterday was a day filled with working on batchalign, but we are back now. Today, I aim to look into the heuristic that I identified yesterday by playing with the model, which is that it seems like the model prefers the use of long-focused sentences about cookies, so the heruistic its picking up is probably on-topicness.\nI am going to first leverage the lovely cdpierse/transformers-interpret tool to help build some explainability by adding it to validate.py. Upon some human validation with random sampling, the model seem to do less well than I\u0026rsquo;d hoped. Running a train cycle with the new results/params seen above to see if it does better.\ntrain: brisk-oath-10 {bs: 72, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat}\nCommentary: It seems like the model is doing overall worse from validation data, but it does fairly well during test data. Decision: I can fairly confidently claim that the model is just fitting on topic. As in, if the topic is about cookies (theft/taking/cookie/mother/etc.), it will be classified as control. One thing that we can do is to claim this task as directly task-controlled: that is, include no data except cookie and control for that difference Then, the model would\u0026rsquo;t be able to predict the result b/c the variation in topic won\u0026rsquo;t have influence. This is going to be prepared in the cookiepitt-7-7-bal* based on dataprep.py in commit 518dec82bb961c0a8ad02e3080289b56102aa1a2 train: super-durian-11 {bs: 72, epochs: 3, lr: 1e-5, length: 60, cookiepitt-7-7-windowed-bal.dat}\nCommentary: the model is no where near convergence Decision: multiplying the LR by 10 train: floral-sunset-12 {bs: 72, epochs: 3, lr: 1e-4, length: 60, cookiepitt-7-7-windowed-bal.dat}\nCommentary: There we go. This seem to be more in line with what we see in Yuan 2021 Decision: ok, let\u0026rsquo;s elongate the actual content. Perhaps we can try a 7-element search instead? This is written as cookiepitt-7-7-*-long. Code based on 9e31f4bc13c4bfe193dcc049059c3d9bda46c8d0 train: sweet-plasma-13 {bs: 72, epochs: 3, lr: 1e-4, length: 60, cookiepitt-7-7-windowed-long-bal.dat}\nCommentary: underfitting Dropping batch size down to 64 to add more steps train: smart-river-14 {bs: 64, epochs: 3, lr: 1e-4, length: 60, cookiepitt-7-7-windowed-long-bal.dat}\nCommentary: this finally fits to the specifications which Yuan 2021 have revealed Decision: running k-fold on this architecture k-fold: XgsP4FVS6ScFxCZKFJoVQ5. Code: 3870651ba71da8ddb3f481a7c3e046397a09d8b2\nJuly 8th Began the day with aligning the entirety of cookie for both control and dementia, named the dataset alignedpitt-7-8 in the RAW folder\nPer what we discussed, will add [pause] as a token to the model. Then, transcript the text such that it would contain normalized values to the pauses for pauses \u0026gt; 0.250 seconds. Therefore, the data would look like\n\u0026ldquo;hello my name is [pause] 262 [pause] bob\u0026rdquo;\nJuly 9th Created transcript.py, which coverts the data in raw to transcripts_pauses, which contains pause values \u0026gt; 250 msc and prepends them with [pause] tokens The code from above is taken from check.py in batchalign, used transcript.py from 7e19a4912cf0ad5d269c139da5ce018615495ebb to clean out the dataset; placed it in similar txt format to alignedpitt-7-8 Ran dataprep with window size of 5, created alignedpitt-7-8.bat and alignedpitt-7-8-windowed.bat as the dataprep file starting a new training run, with [pause] added as a new token, code 06846c6c95e6b1ccf17f0660c5da76aa50231567 train: golden-tree-16 {bs: 64, epochs: 3, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}\nSo realistically, we have the same F1 between the two, but pause encoding increased the accuracy of prediction yet dropped recall dramatically.\nAs a random check, let\u0026rsquo;s find out if simple fine-tuning (only training on classifier) would work, so:\ntrain: jumping-blaze-17 {bs: 64, epochs: 3, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}. This time with only training classifier.\nCommentary: we did not like. start coverging Bumping LR by a factor of 10 train: vital-water-18 {bs: 64, epochs: 3, lr: 1e-3, length: 60, alignedpitt-7-8-windowed.dat}. This time with only training classifier.\nCommentary: barely started converging, seem to be a local Training for 2 more epochs train: fiery-smoke-19 {bs: 64, epochs: 5, lr: 1e-3, length: 60, alignedpitt-7-8-windowed.dat}. This time with only training classifier.\nCommentary: classic overfitting At this point, unlocking the model would probably be a good bet\ntrain: leafy-deluge-20 {bs: 64, epochs: 5, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}.\nTraining once again with code without locking, and bump LR down\nCommentary: classic the recall is slowly creeping up Decision: let\u0026rsquo;s go for 8 epochs train: royal-pond-21 {bs: 64, epochs: 8, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}.\nCommentary: let\u0026rsquo;s run k-fold now, with these settings.\nk-fold: QskZWfEsML52ofcQgGujE2. {bs: 64, epochs: 8, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}.\nOk, the base hypothesis from Yuan 2021 is very much confirmed here. The same training, same content, but pause encoding is very beneficial to the quality of the results. The results that they reported contained an ensemble data, which is in the high 80s; we can now continue doing something new as Yuan 2021\u0026rsquo;s conclusion is fairly achieved.\nWe can probably call the replication stage done, with no dramatically better effect.\nJuly 10th FluCalc! Leonid\u0026rsquo;s lovely new program can be an uberuseful feature extraction tool Let\u0026rsquo;s try using to build a new dataset, and network. FluCalc + Pause Encoding + Textual Data late fusion This is becoming alignedpitt-7-8-flucalc. As the program is currently under heavy development to include results from batchalign, we will specify version V 09-Jul-2022 11:00 for now. Done, the new data has the same i/o shape, but then has a bunch of features filtered for nulls which contains outputs from flucalc. Again, alignedpitt-7-8-flucalc from 4346fc07c4707343c507e32786b6769b6bd6fb49 does not take into account results from the %wor tier! July 11th ab19abd6486884141c9ab4e4e185255a77ae833e is the final-ish version of the late fusion model We are going to use alignedpitt-7-8-flucalc to start training train: royal-pond-21 {bs: 64, epochs: 8, lr: 1e-4, length: 60, alignedpitt-7-8-flucalc-windowed.dat}.\nCommentary: overfitting Decision, droping lr by a factor of 10, also increasing length to 70 train: fallen-dust-25 {bs: 64, epochs: 8, lr: 1e-5, length: 70, alignedpitt-7-8-flucalc-windowed.dat}.\nCommentary: overfitting Decision, droping lr by a factor of 10, dropping batch size to 32, training more to 10 train: dainty-meadow-26 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}.\nah\nAt this point, I think it\u0026rsquo;d be good to do some feature selection Let\u0026rsquo;s do a chi^2 correlation, and select 3 best features import pandas as pd DATA = \u0026#34;/Users/houliu/Documents/Projects/DBC/data/transcripts_pauses/alignedpitt-7-8-flucalc-windowed.bat\u0026#34; # read pickle df = pd.read_pickle(DATA) # test test_data = df[df.split==\u0026#34;test\u0026#34;] # also, get only train data df = df[df.split==\u0026#34;train\u0026#34;] df target mor_Utts ... split utterance trial sample ... 120-2 1049 1 -0.179084 ... train well the boy is getting some cookies handing o... 336-1 2492 0 -0.481740 ... train +oh okay, the the little girl askin(g) for the... 076-4 786 1 -0.179084 ... train well the little boy was looking at that cookie... 279-0 2250 1 1.980274 ... train kid\u0026#39;s stool turnin(g) [pause]540[pause] over s... 014-2 151 1 0.746355 ... train he\u0026#39;s fallin(g) off the chair down here or try... ... ... ... ... ... ... 208-0 1655 0 -0.481740 ... train the boy [pause]920[pause] is going after [paus... 492-0 2696 1 -0.179084 ... train oh yes quite a_lot the kid\u0026#39;s tryin(g) to get t... 497-1 2727 1 0.129396 ... train what else ? \u0026amp;uh the see the [pause]2400[pause]... 175-2 1535 0 0.863668 ... train the window is open you can see out the curtain... 279-0 2261 1 1.980274 ... train the other kid with [pause]610[pause] the stool... [2848 rows x 44 columns] Let\u0026rsquo;s slice out the bits which is labels, etc.\nin_data = df.drop(columns=[\u0026#34;utterance\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;split\u0026#34;]) in_data.columns Index([\u0026#39;mor_Utts\u0026#39;, \u0026#39;mor_Words\u0026#39;, \u0026#39;mor_syllables\u0026#39;, \u0026#39;#_Prolongation\u0026#39;, \u0026#39;%_Prolongation\u0026#39;, \u0026#39;#_Broken_word\u0026#39;, \u0026#39;%_Broken_word\u0026#39;, \u0026#39;#_Block\u0026#39;, \u0026#39;%_Block\u0026#39;, \u0026#39;#_PWR\u0026#39;, \u0026#39;%_PWR\u0026#39;, \u0026#39;#_PWR-RU\u0026#39;, \u0026#39;%_PWR-RU\u0026#39;, \u0026#39;#_WWR\u0026#39;, \u0026#39;%_WWR\u0026#39;, \u0026#39;#_mono-WWR\u0026#39;, \u0026#39;%_mono-WWR\u0026#39;, \u0026#39;#_WWR-RU\u0026#39;, \u0026#39;%_WWR-RU\u0026#39;, \u0026#39;#_mono-WWR-RU\u0026#39;, \u0026#39;%_mono-WWR-RU\u0026#39;, \u0026#39;Mean_RU\u0026#39;, \u0026#39;#_Phonological_fragment\u0026#39;, \u0026#39;%_Phonological_fragment\u0026#39;, \u0026#39;#_Phrase_repetitions\u0026#39;, \u0026#39;%_Phrase_repetitions\u0026#39;, \u0026#39;#_Word_revisions\u0026#39;, \u0026#39;%_Word_revisions\u0026#39;, \u0026#39;#_Phrase_revisions\u0026#39;, \u0026#39;%_Phrase_revisions\u0026#39;, \u0026#39;#_Pauses\u0026#39;, \u0026#39;%_Pauses\u0026#39;, \u0026#39;#_Filled_pauses\u0026#39;, \u0026#39;%_Filled_pauses\u0026#39;, \u0026#39;#_TD\u0026#39;, \u0026#39;%_TD\u0026#39;, \u0026#39;#_SLD\u0026#39;, \u0026#39;%_SLD\u0026#39;, \u0026#39;#_Total_(SLD+TD)\u0026#39;, \u0026#39;%_Total_(SLD+TD)\u0026#39;, \u0026#39;Weighted_SLD\u0026#39;], dtype=\u0026#39;object\u0026#39;) And the labels:\nout_data = df[\u0026#34;target\u0026#34;] out_data trial sample 120-2 1049 1 336-1 2492 0 076-4 786 1 279-0 2250 1 014-2 151 1 .. 208-0 1655 0 492-0 2696 1 497-1 2727 1 175-2 1535 0 279-0 2261 1 Name: target, Length: 2848, dtype: int64 And now, let\u0026rsquo;s select 3 best features.\nfrom sklearn.feature_selection import SelectKBest, f_classif k_best_tool = SelectKBest(f_classif, k=3) k_best_tool.fit(in_data, out_data) best_features = k_best_tool.get_feature_names_out() best_features %_WWR %_mono-WWR %Total(SLD+TD) OD = other disfluencies; SLD = stuttering-like disfluencies; TD = total disfluencies; WWR = whole-word-repetition\nok, let\u0026rsquo;s select those features\ntrain: visionary-plasma-27 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}. Also with feature selection.\nhmmm.\nI am curious if we just ran something like a decision tree, what happens.\nin_features = df.drop(columns=[\u0026#34;utterance\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;split\u0026#34;]) test_features = test_data.drop(columns=[\u0026#34;utterance\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;split\u0026#34;]) in_targets = df[\u0026#34;target\u0026#34;] test_targets = test_data[\u0026#34;target\u0026#34;] seed the classifier, and fit.\nfrom sklearn.ensemble import RandomForestClassifier clsf = RandomForestClassifier() clsf.fit(in_features, in_targets) clsf.score(test_features, test_targets) 0.5932203389830508 OK nevermind. What about SVC?\nfrom sklearn.svm import SVC clsf = SVC() clsf.fit(in_features, in_targets) clsf.score(test_features, test_targets) 0.5932203389830508 Turns out, deep learning still does better. I\u0026rsquo;m thinking maybe the output is being faulty, say, for something like the loss function.\nDecision: switching activation to sigmoid.\ntrain: sunny-bush-31 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nOk let\u0026rsquo;s think about this. Decision: added batch normalization.\ntrain: autumn-jazz-32 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nThe model maybe overfitting on some simple heuristic; some basic statistics revealed that these variables are actually quite differently distributed.\nPerhaps we should increase the complexity of the model?\ntrain: fallen-microwave-33 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nJust to test, I am bumping the LR to 1e-5, just to see what happens. I am very confused.\ntrain: upbeat-flower-35 {bs: 32, epochs: 10, lr: 1e-5, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nThe more we work on this, the more overfit it gets. (I FORGOT A RELUCTIFIER)\na note {bs: 32, epochs: 10, lr: 1e-5, length: 70, alignedpitt-7-11-flucalc-windowed.dat}, selected features\nPauses, no meta:\nPauses, meta:\nso effectively cointoss\nConcerns and Questions July 2nd pitt7-1/dementia/493-0 PAR tier \u0026ldquo;tell me everything you see going on in that picture\u0026rdquo; doesn\u0026rsquo;t seem to be labeled correctly; I am guessing that\u0026rsquo;s supposed to be INV? Has anyone tried to include investigator/participant cross-dialogue? July 4th Is the model overfitting on antiquated language? Is the model overfitting on cooke-theft on-topic-ness? July 11th LSTM only on pauses? ","permalink":"https://www.jemoka.com/posts/kbhdementiabank_acoustics_project/","tags":null,"title":"DementiaBank Acoustics Project"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdepression/","tags":null,"title":"depression"},{"categories":null,"contents":"Derivat\n","permalink":"https://www.jemoka.com/posts/kbhderivational_words/","tags":null,"title":"derivational words"},{"categories":null,"contents":"a\n","permalink":"https://www.jemoka.com/posts/kbhderivatives/","tags":null,"title":"derivative (finance)"},{"categories":null,"contents":"We will take \\(G(P(t),t)\\) to figure the price of an option, with \\(t\\) being time, strike price \\(X\\) (not introduced yet), and expiration date \\(T \u0026gt; t\\) on a stock with price \\(P(t)\\) at time \\(t\\).\nThis representation does something really important: it expresses \\(G\\) as a function of only the current stock price \\(P(t)\\).\n","permalink":"https://www.jemoka.com/posts/kbhderivative_pricing/","tags":null,"title":"Derivative Pricing"},{"categories":null,"contents":"A derived variable is a mapping between states to a set, usually the natural numbers. Remember, if we can, given a state and match it to a number and show a relation which would iterate the state and decrease the states\u0026rsquo; number. We can show that the algorithm terminates.\n","permalink":"https://www.jemoka.com/posts/kbhderived_variable/","tags":null,"title":"derived variable"},{"categories":null,"contents":"For a matrix, for instance, like:\n\\begin{equation} \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{bmatrix} \\end{equation}\nWe wish to find the matrix\u0026rsquo;s determinant; we write it down as:\n\\begin{equation} \\begin{vmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{vmatrix} \\end{equation}\ngeometric interpretation of determinants Geometrically, determinants are how matrices send a unit object after its mapping; i.e. how does it transform the area of a unit square.\n","permalink":"https://www.jemoka.com/posts/kbhdeterminants/","tags":null,"title":"determinants"},{"categories":null,"contents":"A health concern relating to glucose and obesity.\n","permalink":"https://www.jemoka.com/posts/kbhdiabetes/","tags":null,"title":"diabetes"},{"categories":null,"contents":"We have a function:\n\\begin{equation} |x|+|y|\\frac{dy}{dx} = \\sin \\left(\\frac{x}{n}\\right) \\end{equation}\nWe are to attempt to express the solution analytically and also approximate them.\nTo develop a basic approximate solution, we will leverage a recursive simulation approach.\nWe first set a constant \\(N\\) which in the \\(N\\) value which we will eventually vary.\nN = 0.5 We can get some values by stepping through \\(x\\) and \\(y\\) through which we can then figure \\(\\frac{dy}{dx}\\), namely, how the function evolves.\n# cache res = [] # number of steps steps = 1000 # seed values x = -5 y = 5 # step size step = 1/100 # for number of setps for _ in range(steps): # get the current equation and slope solution dydx = (sin(x/N)-abs(x))/abs(y) # append result res.append((x,y,dydx)) # apply the slope solution to iterate next y # step size is defined by `step` x += step y += dydx*step We have now a set of analytic solutions \\((x,y,\\frac{dy}{dx})\\). Let\u0026rsquo;s plot them!\nscatter_plot([i[0:2] for i in res]) Great, now we have a fairly non-specific but \u0026ldquo;correct\u0026rdquo; solution. We are now going to try to derive an analytic solution.\nWait\u0026hellip; That\u0026rsquo;s not the solution we got! But\u0026hellip; its close: the blue line simply need to be reflected across the \\(x\\) axis.\nIts actually fairly apparent why we will need this negative. We just declared that \\(y\\) was negative for that portion of the solution; the output of a square root could never be negative, so of course to achieve \\(y\\) being negative we have to take into account that square roots have a possible negative output as well.\nNice; now our analytical results agree with out numerical results.\n\\begin{equation} \\begin{cases} y\u0026gt;0 \u0026amp; y=\\sqrt{-2n\\cos\\left(\\frac{x}{n}\\right)-x\\vert x\\vert} +C \\\\ y\u0026lt;0 \u0026amp; y=-\\sqrt{2n\\cos\\left(\\frac{x}{n}\\right)+x\\vert x\\vert}+C \\end{cases} \\end{equation}\nMoving on to the result of the questions.\nSolution behavior The solution are unbounded and mostly decreasing. As \\(n\\in [-1,1]\\), the solution becomes unstable; a solution does not exist at \\(n=0\\).\nAt \\(n=0.5\\), a solution passes through \\((0,-1)\\).\n","permalink":"https://www.jemoka.com/posts/kbhchallenge_1/","tags":null,"title":"DiffEq: Challenge #1"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdifference_equation/","tags":null,"title":"difference equation"},{"categories":null,"contents":"Textbooks The textbook that we will be using is (Taylor 2011)\nContent Simple Differential Equations solving differential equations Uniqueness and Existance eigenvalue applying eigenspace Second-Order Linear Differential Equation Non-Linear Systems SIR Model Assignments Challenge #1 The Unreasonable Effectiveness of Mathematics in the Natural Sciences ","permalink":"https://www.jemoka.com/posts/kbhdiffeq_intro/","tags":["Index"],"title":"Differential Equations"},{"categories":null,"contents":"The dimension of a vector space is the length of any basis in the vector space. It is denoted as \\(\\dim V\\).\nadditional information See also finite-dimensional vector space and infinite-demensional vector space\ndimension of subspace is smaller or equal to that of its parent If we have a finite-dimensional \\(V\\) and a subspace thereof \\(U\\), then \\(\\dim U \\leq \\dim V\\).\nFirstly, the every subspace of a finite-dimensional vector space is a finite-dimensional vector space is itself a finite-dimensional vector space. Therefore, it has a finite dimension.\nThen, we will simply think of the basis of \\(U\\) as an linearly independent list in \\(V\\); and of course, the basis of \\(V\\) spans \\(V\\). As length of linearly-independent list \\(\\leq\\) length of spanning list, we have that length of basis of \\(U \\leq\\) length of basis of \\(V\\).\nThis makes \\(\\dim U \\leq \\dim V\\), as desired. \\(\\blacksquare\\)\nlists of right length are a basis These are two results that tell us if you are given a list of list of right length, one condition (spanning or linear independence) can tell you that they are a basis. It\u0026rsquo;s also known (as a John McHugh special:tm:) as the Half Is Good Enough theorems.\nlinearly independent list of length dim V are a basis of V Begin with an linearly independent list in \\(V\\) of length \\(\\dim V\\). We aim to extend this list into a basis of \\(V\\).\nAs we know all basis in \\(V\\) must have length \\(\\dim V\\), and the list is already length \\(\\dim V\\), no extension is needed to form a basis.\nAs every linearly independent list expends to a basis, we conclude that the list is already a basis of \\(V\\), as desired \\(\\blacksquare\\).\nspanning list of length of dim V are a basis of V Begin with a spanning list in \\(V\\) of length \\(\\dim V\\). We aim to reduce this list into a basis of \\(V\\).\nAs we know all basis in \\(V\\) must have length \\(\\dim V\\), and the list is already length \\(\\dim V\\), no reduction is needed to form a basis.\nAs all spanning lists contains a basis of which you are spanning, we conclude that the list is a basis of \\(V\\), as desired \\(\\blacksquare\\).\ndimension of sums See dimension of sums\n","permalink":"https://www.jemoka.com/posts/kbhdimension/","tags":null,"title":"dimension"},{"categories":null,"contents":"A direct sum is a sum of subspaces (not just subsets!!) where there\u0026rsquo;s only one way to represent each element.\nconstituents subspaces of \\(V\\) named \\(U_1, \\dots, U_{m}\\)\nrequirements The sum of subsets of \\(U_1+\\dots+U_{m}\\) is called a direct sum IFF:\neach element in \\(U_1+\\dots +U_{m}\\) can only be written in one way as a sum \\(u_1 +\\dots +u_{m}\\) (as in, they are linearly independent?)\nWe use \\(\\oplus\\) to represent direct sum.\nadditional information why is it called a direct sum? Something is not a direct sum if any of its components can be described using the others. Its kind of line linear independence but! on entire spaces.\na sum of subsets is a direct sum IFF there is only one way to write \\(0\\) Given \\(U_1, \\dots, U_{m}\\) are subspaces of \\(V\\), then \\(U_1+\\dots +U_{m}\\) is a direct sum IFF the only way to write \\(0\\) as a sum \\(u_1 +\\dots +u_{m}\\) is by taking each element to \\(0\\).\nProof:\nif\u0026mdash; If some \\(U_1 + \\dots +U_{m}\\) is a direct sum, definitionally there is only one way to write \\(0\\). And you can always write \\(0\\) by taking all the constituents to \\(0\\) as they are subspaces, so the additive identity exists.\nonly if\u0026mdash; We are given that there is only one way to write \\(0\\), that:\n\\begin{equation} 0 = u_1+ u_2+ \\dots+ u_{m}: u_j \\in U_{j} \\end{equation}\nas \\(U_{j}\\) are all subspaces, and the additive identity exists, we can say that \\(u_1=u_2=\\dots =0\\).\nAssume for the sake of contradiction that \\(U_1 + \\dots +U_{m}\\) is not a direct sum. Therefore:\n\\begin{equation} \\exists\\ v_1 = u_1+u_2+\\dots + u_{m}: u_{j} \\in U_{j} \\end{equation}\nand\n\\begin{equation} \\exists\\ v_1 = w_1+w_2+\\dots + w_{m}: w_{j} \\in U_{j} \\end{equation}\n\u0026ldquo;there are two unique representations of a vector given the sum of subsets\u0026rdquo;\nSubtracting these representations, then:\n\\begin{equation} (v_1-v_1) = (u_1-w_1) + \\dots +(u_{m}-w_{m}): u_{j}, w_{j} \\in U_{j} \\end{equation}\nFinally, then:\n\\begin{equation} 0 = (u_1-w_1) + \\dots +(u_{m}-w_{m}): u_{j}, w_{j} \\in U_{j} \\end{equation}\nWe have established that each slot that makes up this particular sum \\(=0\\). Therefore, \\(u_{i}-w_{i} = 0\\). This means $ui=wi$\u0026mdash;there are no two unique representations of \\(v_{1}\\). Reaching contradiction. \\(\\blacksquare\\)\na sum of subsets is only a direct sum IFF their intersection is set containing \\(0\\) Take \\(U\\) and \\(W\\), two subspaces of \\(V\\). \\(U+V\\) is a direct sum IFF \\(U \\cap W = \\{0\\}\\).\nProof:\nif\u0026mdash; Suppose \\(U+V\\) is a direct sum. \\(\\forall v \\in U \\cap V\\), as \\(v\\) is equal to itself, we have that:\n\\begin{equation} 0 = v+(-v) \\end{equation}\nwhere, \\(v\\) is in \\(U\\) and \\(-v\\) is in \\(V\\) (as both \\(U\\) and \\(V\\) are vector spaces, both would contain \\(-1v=-v\\) as we are given \\(v \\in U \\cap V\\) and scalar multiplication is closed on both.)\nBy the unique representation in the definition of direct sums, you have only one way to construct this expression: namely, that \\(v=0\\) as both are vector spaces so the additive identity exists on both.\nHence:\n\\begin{equation} \\{0\\} = U \\cap V \\end{equation}\nonly if\u0026mdash; Suppose \\(U \\cap W = \\{0\\}\\). Take also \\(u \\in U\\) and \\(w \\in W\\); we can construct an expression:\n\\begin{equation} u + w = 0 \\end{equation}\nIf we can show that there is only one unique combination of \\(u\\) and \\(w\\) to write \\(0\\), we satisfy the previous proof and therefore \\(U+W\\) is a direct sum.\nThe expression above implies that \\(w\\) is the additive inverse of \\(u\\); therefore; \\(u = -w\\). As both \\(U\\) and \\(W\\) are vector spaces, their elements all have inverses. As \\(u\\) is the inverse of \\(w\\), and given the definition of sum of subsets that \\(u \\in U\\) and \\(w \\in W\\), \\(u\\) and \\(w\\) are both in both \\(U\\) and \\(W\\).\nAs the intersection of \\(U\\) and \\(V\\) is \\(0\\), \\(u=w=0\\). Therefore, there is only one unique representation of \\(0\\), namely with \\(u=0,w=0\\), making \\(U+W\\) a direct sum. \\(\\blacksquare\\)\ndirect sum proofs are not pairwise! Those two proofs above only deal with pairs of sum of subsets. If you have multiple subsets, they don\u0026rsquo;t apply!\nevery subspace of \\(V\\) is a part of a direct sum equaling to \\(V\\) For every subspace \\(U\\) of a finite-dimensional \\(V\\), there is a subspace \\(W\\) of \\(V\\) for which \\(V = U \\oplus W\\).\nBecause \\(V\\) is defined to be finite-dimensional, and the fact that a finite-dimensional subspace is finite-dimensional, \\(U\\) is finite-dimensional.\nTherefore, because every finite-dimensional vector space has a basis, \\(U\\) has a basis \\(u_1, \\dots u_{m}\\).\nBecause bases are linearly independent, and \\(U \\subset V\\), \\(u_1, \\dots u_{m}\\) is a linearly independent list in \\(V\\).\nBecause a linearly independent list expends to a basis, we can construct \\(u_1, \\dots u_{m}, w_{1}, \\dots w_{n}\\) as the basis of \\(V\\). We will construct a \\(W = span(w_1, \\dots w_{n})\\) \u0026mdash; the space formed as the span of the \u0026ldquo;extension\u0026rdquo; vectors to make the basis in \\(V\\).\nBecause the list \\(u_{j}\\dots w_{k}\\) we made is a basis in \\(V\\), \\(U+W=V\\).\nYou can see this because every element \\(v \\in V\\) can be constructed with a linear combination \\(u_1, \\dots u_{m}, w_{1}, \\dots w_{n}\\) (again, because this list shown to be a basis of \\(V\\) therefore it spans \\(V\\).) Then, to show that \\(U+W=V\\), we can collapse \\(a_{1}u_1\\dots + a_{m}u_{m}=u \\in U\\), and \\(c_{1}w_1 \\dots +c_{m}w_{m} = w \\in W\\). Hence, every element \\(v \\in V\\) can be constructed by some \\(u \\in U + w \\in W\\), making \\(U+W=V\\).\nNow, we have to show that the combination is a direct sum. There is a few ways of going about this, the one presented by Axler is leveraging the fact that a sum of subsets is only a direct sum IFF their intersection is set containing \\(0\\)\u0026mdash;that \\(U \\cap W = \\{0\\}\\).\nGiven some element \\(v\\) that lives in the intersection between \\(U\\) and \\(W\\), it must be formed as a linear combination of two linearly independent lists (as \\(u_j, \\dots w_{j}\\) is a basis, they are linearly independent.)\nIntuition: if an non-zero element lives in the intersection between two linearly independent lists which together is still linearly independent, it must be able to be written by a linear combination of other elements of that linearly independent list to live in the intersection of the two lists\u0026mdash;which is absurd (violates the definition of linearly dependent). The only element for which this is an exception is \\(0\\).\nActual proof:\nsuppose \\(v \\in U \\cap W\\), so \\(v = a_1u_1\\dots +a_{m}v_{m}\\) as well as \\(v=b_1w_{1} + \\dots b_{n}w_{n}\\). Subtracting the two lists results in:\n\\begin{equation} 0 = a_1u_1+ \\dots a_{m} u_{m} - b_1w_1+ \\dots +b_{n}w_{n} \\end{equation}\nhaving already declared this list linearly independent, we see that each scalar \\(a_1, \\dots -b_{n}\\) must equal to \\(0\\) for this expression. Therefore, the intersection \\(v\\) must be \\(\\{0\\}\\) as \\(0u_1 + \\dots +0u_{m}=0\\).\n","permalink":"https://www.jemoka.com/posts/kbhdirect_sum/","tags":null,"title":"direct sum"},{"categories":null,"contents":"discourse features are marks of fluency/etc. which mark one\u0026rsquo;s speech.\n","permalink":"https://www.jemoka.com/posts/kbhdiscourse_features/","tags":null,"title":"discourse features"},{"categories":null,"contents":"A Discourse-Completion Task is a tool used to elicit speech acts, such as showing an image, etc. For instance,\ntypes of Discourse-Completion Tasks oral lexical retrival Cookie Theft ","permalink":"https://www.jemoka.com/posts/kbhdiscourse_completion_task/","tags":null,"title":"Discourse-Completion Task"},{"categories":null,"contents":"distributed algorithm is a type of algorithm that can be distributed across many modules.\nThere are a few core areas of research:\nfailure-proofing nodes is a distributed algorithm What if one processor fails? communication in a distributed algorithm What if communication between processors fails? What if timing fails? atomicity atomicity is a property of distributed algorithm where, for a set of steps, a processor can only do one or all of the steps. i.e.: if you are asking a node to do something, it can either do all of the thing or be able to roll back as if the entire thing didn\u0026rsquo;t happen.\nleader election (algorithms) leader election is the process by which a distributed algorithm elects the driving node among similar nodes.\nconsensus (algorithms) consensus is a mechanism in a distributed algorithm where the solution requires multiple processes to do the same calculation to confirm.\nalgorithms designed to be distributed MapReduce ","permalink":"https://www.jemoka.com/posts/kbhdistributed_algorithum/","tags":null,"title":"distributed algorithm"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdistributed_morphology/","tags":null,"title":"distributed morphology"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdistributivity/","tags":null,"title":"distributivity"},{"categories":null,"contents":"Divide by \\(2\\pi\\), or, how I learned to start worrying and hate Fourier Transforms.\nHello all. Good news first: our frequency theory is now correctly validated by data.\nIf you want a band-aid for the answer, here it is: divide everything we get out of the cantilever equations by \\(2\\pi\\); then, use the correct linear mass density: our Google sheets was off by a factor of almost \\(4\\) because of later-corrected apparent measurement error.\nThe bad news? You get pages of algebra to justify how, while getting a whole run-down of our entire theory so far for kicks.\nOur story begins at what popped out of the other end of the Euler-Lagrange equations (if you want the start of the Lagrangian analysis, read this from Mark, and plug the resulting Lagrangian into the Euler-Lagrange equation of the right shape.) But, either way, out will pop this fourth-order partial differential equation:\n\\begin{equation} EI\\pdv[4]{w}{x} = -\\mu \\pdv[2]{w}{t} \\end{equation}\nwhere, \\(w(x,t)\\) is a function of displacement by location by time, \\(E\\) the elastic modulus, and \\(I\\) the second moment of bending area.\nNow, fourth-order diffequs are already pain. PARTIAL forth order diffequs sounds darn near impossible. Wikipedia, to their credit, helpfully suggests the following to help tackle this problem:\nYou see, as we are trying to isolate possible individual frequencies, it make sense to essentially run a Fourier Transform on our algebra, to get the possible amplitude at each frequency \\(\\hat{w}(x)\\), given some frequency \\(\\omega\\) (no idea why they use \\(\\omega\\), I will use \\(f\\) for the rest of this article.)\nTo perform this analysis, Wikipedia suggests that we substitute our \\(w(x,t)\\) with its Fourier Definition, which is written as a function of the Fourier-decomposed version of the function \\(\\hat{w}(x)\\) (real component only, as imaginary pairs serve only as the conjugate), and then re-isolate those decomposed \\(\\hat{w}(x)\\). In this way, we get rid of the time dimension as sine waves oscillate ad infinium. Makes total sense.\nEXCEPT WHAT WIKIPEDIA GAVE ABOVE TO SUBSTITUTE IN ISN\u0026rsquo;T THE CORRECT FOURIER DECOMPOSITION\nHere\u0026rsquo;s the actual Fourier Transform intergral:\nwhere, \\(\\zeta=\\omega=f\\) , \\(f(x) = \\hat{w}(x)\\).\nWHAT DO YOU NOTICE? AN EXTRA \\(2\\pi\\).\nTHIS ALSO MEANS THAT THE FREQUENCY ANALYSTS IN THE REST OF THAT WIKI ARTICLE IS WRONG\nOk. I collect myself.\nSo, we now have that:\n\\begin{equation} w(x,t) = Re\\qty[\\hat{w}(x)e^{-i 2\\pi ft}] \\end{equation}\nRecall that we are trying to substitute this into\n\\begin{equation} EI\\pdv[4]{w}{x} = -\\mu \\pdv[2]{w}{t} \\end{equation}\nTaking two derivatives of the above Fourier decomposition equation by time (which is the dimension we are trying to get rid of to make the diffequ not partial), we have:\n\\begin{align} \\pdv[2]{w(x,t)}{t} \u0026amp;= \\pdv[2] t Re\\qty[\\hat{w}(x)e^{-i2\\pi ft}] \\\\ \u0026amp;= Re\\qty[\\hat{w}(x)\\pdv[2] t e^{-i2\\pi ft}] \\\\ \u0026amp;= Re\\qty[\\hat{w}(x)(2\\pi f)^{2} \\cdot e^{-i2\\pi ft}\\dots] \\end{align}\nNow, given we are only dealing with the real components of these things, everything on the \\(e^{-i\\dots }\\) part of the function wooshes away, cleanly leaving us with:\n\\begin{equation} \\pdv[2]{w(x,t)}{t} \u0026amp;= \\hat{w}(x)(2\\pi f)^{2} \\end{equation}\nYay! No longer differential. Substituting that into our original expression, and making the partials not partial anymore:\n\\begin{equation} EI\\pdv[4]{w}{x} + \\mu \\hat{w}(x)(2\\pi f)^{2}= 0 \\end{equation}\nExcellent. Now, onto solving this. The basic way to solve this is essentially to split the fourth-order differential into a 4x4 matrix, each one taking another derivative of the past. Then, to get a characteristic solution, you take its eigenvalues.\nBut instead of going about doing that, I\u0026rsquo;m going to give up and ask a computer. In the code, I am going to substitute \\(p\\) for \\(2\\pi\\) temporarily because FriCAS gets a little to eager to convert things into their sinusoidal forms if we leave it as \\(2\\pi\\).\nE,I,u,f = var(\u0026#34;E I u f\u0026#34;) x, L = var(\u0026#34;x L\u0026#34;) w = function(\u0026#39;w\u0026#39;)(x) p = var(\u0026#34;p\u0026#34;) _c0, _c1, _c2, _c3 = var(\u0026#34;_C0 _C1 _C2 _C3\u0026#34;) fourier_cantileaver = (E*I*diff(w, x, 4) - u*(p*f)^2*w == 0) fourier_cantileaver -f^2*p^2*u*w(x) + E*I*diff(w(x), x, x, x, x) == 0 solution = desolve(fourier_cantileaver, w, ivar=x, algorithm=\u0026#34;fricas\u0026#34;).expand() latex(solution) \\begin{equation} \\hat{w}(x) = _{C_{1}} e^{\\left(\\sqrt{f} \\sqrt{2\\pi} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{0}} e^{\\left(i \\, \\sqrt{f} \\sqrt{2\\pi} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{2}} e^{\\left(-i \\, \\sqrt{f} \\sqrt{2\\pi} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{3}} e^{\\left(-\\sqrt{f} \\sqrt{2 \\pi} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} \\end{equation}\nOk, so we have that each component solution is a combination of a bunch of stuff, times \\(\\pm i\\) or \\(\\pm 1\\). We are going to declare everything that\u0026rsquo;s invariant in the exponent to be named \\(\\beta\\):\n\\begin{equation} \\beta := \\sqrt{2\\pi f} \\qty(\\frac{u}{EI})^{\\frac{1}{4}} \\end{equation}\nAnd given this, we can then write the general solution for displacement by location (\\(w\\)) determined above more cleanly as:\n\\begin{equation} \\hat{w}(x) = _{C_{1}} e^{\\beta x} + _{C_{0}} e^{ \\beta ix} + _{C_{2}} e^{-\\beta ix} + _{C_{3}} e^{-\\beta x} \\end{equation}\nWe will make one more substitution\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\nRecall that:\n\\begin{equation} \\begin{cases} \\cosh x = \\frac{e^{x}+e^{-x}}{2} \\\\ \\cos x = \\frac{e^{ix}+e^{-ix}}{2}\\\\ \\sinh x = \\frac{e^{x}-e^{-x}}{2} \\\\ \\sin x = \\frac{e^{ix}-e^{-ix}}{2i}\\\\ \\end{cases} \\end{equation}\nWith a new set of scaling constants \\(d_0\\dots d_3\\) (because these substitutions essentially ignore any factors its being multiplied, but we don\u0026rsquo;t actually care about modeling amplitude with these expressions anyways, so we can just change the arbitrary initial-conditions scalars on the fly), and some rearranging, we can rewrite the above expressions into just a linear combination of those elements. That is, the same expression for \\(\\hat{w}(x)\\) at a specific frequency \\(f\\) can be written as:\n\\begin{equation} d_0\\cosh \\beta x +d_1\\sinh \\beta x +d_2\\cos \\beta x +d_3\\sin \\beta x = \\hat{w}(x) \\end{equation}\nfor some arbitrary initial conditions \\(d_0\\dots d_3\\). Significantly cleaner.\nSo, what frequencies will our fork oscillate at? Well, a mode for our fork is any set of \\(d_0 \\dots d_3\\) for which a solution for \\(\\hat{w}(x)\\) exists given our constants.\nAs it stands right now, it seems like we have four unknowns (\\(d_0 \\dots d_3\\)) but only one equation to solve with. That\u0026rsquo;s no bueno.\nEnter our initial conditions:\nThe top line states that: at \\(x=0\\), the bottom of the fork, our beam does not travel away from its natural axis (yes, because its a solid hunk of metal connected to the base), and it does not deflect (slope).\nThe bottom line stats that: at \\(x=L\\), the top of the fork is straight (which is true, the tip-top of the fork does indeed not bend, only the middleish parts bend.)\nSo, to get at the hidden system of four elements, we will take some derivatives of our original \\(\\hat{w}(x)\\) equation by \\(x\\), as prescribed by our initial conditions.\nwp = diff(w,x,1) wpp = diff(w,x,2) wppp = diff(w,x,3) (wp, wpp, wppp) (b*d3*cos(b*x) + b*d1*cosh(b*x) - b*d2*sin(b*x) + b*d0*sinh(b*x), -b^2*d2*cos(b*x) + b^2*d0*cosh(b*x) - b^2*d3*sin(b*x) + b^2*d1*sinh(b*x), -b^3*d3*cos(b*x) + b^3*d1*cosh(b*x) + b^3*d2*sin(b*x) + b^3*d0*sinh(b*x)) And then, we have a system:\ncond_1 = w.subs(x=0) == 0 cond_2 = wp.subs(x=0) == 0 cond_3 = wpp.subs(x=L) == 0 cond_4 = wppp.subs(x=L) == 0 conds = (cond_1, cond_2, cond_3, cond_4) conds (d0 + d2 == 0, b*d1 + b*d3 == 0, -b^2*d2*cos(L*b) + b^2*d0*cosh(L*b) - b^2*d3*sin(L*b) + b^2*d1*sinh(L*b) == 0, -b^3*d3*cos(L*b) + b^3*d1*cosh(L*b) + b^3*d2*sin(L*b) + b^3*d0*sinh(L*b) == 0) solve(conds, d0, d1, d2, d3).full_simplify() Ok so, we notice that out of all of these boundary expressions the \\(b^{n}\\) term drop out. Therefore, we have the system:\n\\begin{equation} \\begin{cases} d_0 + d_2 = 0 \\\\ d_1 + d_3 = 0 \\\\ -d_2 \\cos L\\beta + d_0 \\cosh L\\beta - d_3 \\sin L\\beta + d_1 \\sinh L\\beta = 0 \\\\ -d_3 \\cos L\\beta + d_1 \\cosh L\\beta + d_2 \\sin L\\beta + d_0 \\sinh L\\beta = 0 \\\\ \\end{cases} \\end{equation}\nGreat. Four unknowns, four equations. We can now figure out when a solution for \\(d_0, \\dots d_3\\) exists (or go about solving it, but turns out that\u0026rsquo;s significantly harder and wildly useless.)\nI will spare you the pages of route algebra needed to figure out when a solution exists. Suffice to say its lots of trig identities.\nBut, the satisfying conclusion is that, given the equations above, a solution exists for \\(d_0 \\dots d_3\\) (read: a mode for the beam exists), when:\n\\begin{equation} \\cos L\\beta \\cdot \\cosh L\\beta +1 = 0 \\end{equation}\nSo, any valid solutions for the expression \\(\\cos x \\cdot \\cosh x + 1 = 0\\) will be a valid product between \\(L\\beta\\). We can use this information to figure out the right frequencies by then solving for \\(f\\) embedded in \\(\\beta\\).\nSo, onto solving for \\(\\cos L\\beta \\cdot \\cosh L\\beta +1 = 0\\).\nWe again give up and ask a computer to do it.We will try to locate a root for \\(Lb\\) for every \\(\\pi\\) for two rounds around the circle (until \\(4 \\pi\\))\u0026mdash;there is a solution for every \\(\\pi\\), if you don\u0026rsquo;t believe me, plot it or change the bottom to try to find it for every \\(\\frac{\\pi}{2}\\), sage will crash:\nintervals = [jj*pi for jj in range(0, 5)] intervals [0, pi, 2*pi, 3*pi, 4*pi] We will now declare \\(x=L\\beta\\), and create a nonlinear expression in it:\nx = var(\u0026#34;x\u0026#34;) characteristic_eqn = 1 + cos(x)*cosh(x) == 0 characteristic_eqn cos(x)*cosh(x) + 1 == 0 Root finding time!\ncharacteristic_solutions = [characteristic_eqn.find_root(i,j) for (i,j) in zip(intervals,intervals[1:])] characteristic_solutions [1.8751040687120917, 4.6940911329739246, 7.854757438237603, 10.995540734875457] These are possible candidate values for \\(L\\beta\\). We will declare these values \\(s\\).\nSo, we now have that:\n\\begin{equation} L \\beta = s \\end{equation}\nSubstituting back our original definition for \\(\\beta\\), we have that:\n\\begin{equation} L \\sqrt{2\\pi f} \\qty(\\frac{u}{EI})^{\\frac{1}{4}} = s \\end{equation}\nNow, we will try to get \\(f\\) by itself:\n\\begin{align} \u0026amp;L \\sqrt{2\\pi f} \\qty(\\frac{u}{EI})^{\\frac{1}{4}} = s \\\\ \\Rightarrow\\ \u0026amp; \\sqrt{2\\pi f} = \\frac{s}{L} \\qty(\\frac{EI}{u})^{\\frac{1}{4}} \\\\ \\Rightarrow\\ \u0026amp; 2\\pi f = \\frac{s^{2}}{L^{2}} \\qty(\\frac{EI}{u})^{\\frac{1}{2}} \\\\ \\Rightarrow\\ \u0026amp; f = \\frac{s^{2}}{2\\pi L^{2}} \\qty(\\frac{EI}{u})^{\\frac{1}{2}} \\end{align}\nFinally, we have that \\(I = \\frac{1}{12} bh^{3}\\) for a rectangular prism; and that linear density is cross-sectional area times volumetric density \\(\\mu = \\rho \\cdot bh\\). Making these substitutions:\n\\begin{align} f \u0026amp;= \\frac{s^{2}}{2\\pi L^{2}} \\qty(\\frac{EI}{u})^{\\frac{1}{2}} \\\\ \u0026amp;= \\frac{s^{2}}{2\\pi L^{2}} \\qty(\\frac{Ebh^{3}}{12 \\rho bh})^{\\frac{1}{2}} \\\\ \u0026amp;= \\frac{s^{2}}{2\\pi L^{2}} \\qty(\\frac{Eh^{2}}{12 \\rho})^{\\frac{1}{2}} \\\\ \\end{align}\nWithout even getting to the frequency-based payoff, we immediately notice two takeaways.\nThe frequency of our fork is inversely proportional to length (i.e. \\(f = \\frac{1}{L^{2}}\\dots\\)) The first overtone of the tuning fork is \\(s^{2} = (\\frac{4.694}{1.875})^{2} \\approx 6.27\\) times higher than the fundamental\u0026mdash;meaning its significantly higher energy so it dissipates significantly faster; it is also not an integer multiple, which means its much less likely to be confused to be a harmonic; making a tuning fork essentially a pure-frequency oscillator Given equal conditions, only the thickness in one dimension (the one perpendicular to the bending axis) matters But, enough idling, onto our main event. Using standard reference values for aluminum, as well as our measured length and thickness of a \\(C\u0026rsquo;\\ 512hz\\) tuning fork, we have that\n# measured values---- # thickness h = 0.0065 # meters # length L0 = 0.09373 # meters L1 = 0.08496 # meters # theoretical values--- # elastic modulus E = 46203293995 # pascals = kg/m^2 # density p = 2597 # kg/m^3 # our solved characteristic value (s) # mode to index nth_mode = 0 s = characteristic_solutions[nth_mode] zero = (((s^2)/(2*pi*L0^2))*((E*h^2)/(12*p))^(1/2)).n() one = (((s^2)/(2*pi*L1^2))*((E*h^2)/(12*p))^(1/2)).n() zero, one mean([zero,one]) (504.123425101814, 613.571395642254) 558.847410372034 Close enough for a night. Thank you sorry about everything.\ntemperature # mode to index nth_mode = 0 # s = characteristic_solutions[nth_mode] s = var(\u0026#34;s\u0026#34;) # change to L and h (distances measures) by increases in degrees C d(t,x) = (2.4e-5)*x rho(t,x) = ((2.4e-5))*x a,b = var(\u0026#34;a b\u0026#34;) # a = -3.9 # b = 0.0033 Ed(t) = ((a*b)*e^(b*t))*1e9 f(E, L, h, p) = (((s^2)/(2*pi*L^2))*((E*h^2)/(12*p))^(1/2)) E,L,h,p = var(\u0026#34;E L h p\u0026#34;) # (a * b * /g)c t = var(\u0026#34;t\u0026#34;) # diff(t, dt, E,L,h,p) = sqrt((f.diff(E)*Ed(t)*dt)^2 + (f.diff(E)*Ed(t)*dt)^2 + (f.diff(L)*d(t)*dt)^2 + (f.diff(h)*d(t)*dt)^2) # diff(10, 1, 42661456706, 0.09833, 0.00643, 2545.454545).n() subdict = { a: -3.9, b:0.0033, L:0.09833, h:0.00643, p:2545.454545, E:42661456706, s:characteristic_solutions[nth_mode], t:50 } (f.diff(E)*Ed(t)).subs(subdict).full_simplify().n() (f.diff(L)*d(t,L)).subs(subdict).full_simplify().n() (f.diff(h)*d(t,h)).subs(subdict).full_simplify().n() (f.diff(p)*rho(t,p)).subs(subdict).full_simplify().n() -0.0782394489394635 -0.0211103561467420 0.0105551780733710 -0.00527758903668550 expansion = var(\u0026#34;x\u0026#34;) l,w,h,m = var(\u0026#34;l w h m\u0026#34;) density(l,w,h) = (l*w*h)/m density.diff(l)*expansion + density.diff(w)*expansion + density.diff(h)*expansion (l, w, h) |--\u0026gt; h*l*x/m + h*w*x/m + l*w*x/m ","permalink":"https://www.jemoka.com/posts/kbhdivide_by_2pi/","tags":null,"title":"Divide by 2pi"},{"categories":null,"contents":"Dopamine optical sensor. When dopamine is bound, it floreses and can detect micromolar changes and dopamine concentration.\n","permalink":"https://www.jemoka.com/posts/kbhdlight_1/","tags":null,"title":"dLight 1"},{"categories":null,"contents":"See also Software Development Methodologies\ndocumentation Comments Readme Wiki specification UX UI High-Level Architecture (libraries, external APIs) Low-Level Architecture (modules, functions, internal APIs) commenting Almost anything hardcoded (constants, strings, etc.) Anything confusing, tricky, nonstandard Historical notes: if something is added/removed, write it down TODO for bugs or hacks README Files Best used as a quick-start guide What are key pieces of info they will pieces of info they will need? What is your code supposed to do? How does someone run your code? How does a new engineer get set up? General overview of how things are laid out, with links to wiki pages with details Wiki In-depth explanation of subsystems and modules Separate pages for each subsystem Include decisions of their design decisions Discussions of why systems are not designed differently UI/UX Spec How do we know what the software is supposed to do? Varying levels of resolution User stories All the way up to granular details of UI elements Don\u0026rsquo;t forgot to document defaults!\n","permalink":"https://www.jemoka.com/posts/kbhdocumentation_and_specification/","tags":null,"title":"Documentation and Specification"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdopamine/","tags":null,"title":"dopamine"},{"categories":null,"contents":"The dopamine circuitry in NF1.\nGenetically encoded \u0026ldquo;sensors\u0026rdquo; to measure circuits.\n","permalink":"https://www.jemoka.com/posts/kbhdopamine_circuitry_in_nf1/","tags":null,"title":"dopamine circuitry in NF1"},{"categories":null,"contents":"orthogonality test The dot product is an orthogonality test. If the dot product between the two vectors is \\(0\\), they are definitely orthogonal.\ngeometric interpretation of the dot product Well, we have some shape between two vectors; then, we can first write out the law of cosines. Then, we can see that, for two vectors from the same origin, we can say that the projection of vector \\(\\vec{A}\\) onto \\(\\vec{B}\\) is written as:\n\\begin{equation} |\\vec{A}||\\vec{B}|\\cos \\theta \\end{equation}\nwhere, \\(\\theta\\) is the angle between the two vectors.\n","permalink":"https://www.jemoka.com/posts/kbhdot_product/","tags":null,"title":"dot product"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdouble_slit_experiment/","tags":null,"title":"double slit experiment"},{"categories":null,"contents":"A human gene similar to the gene PreTA found in E. Coli, a bacterial found in microbiome. See effects of PreTA on Fluoropyrimidine, and by proxy Capecitabmine for implications on cancer treatment.\n","permalink":"https://www.jemoka.com/posts/kbhdpyd/","tags":null,"title":"DPYD"},{"categories":null,"contents":"Gah I have to do this. Not for public consumption. California laws 2022 DL600 R7 2022.\nConsequences Not licensed If unlicensed person is drivnig your car, it maybe impounded for 30 days Hired to drive interstate commercially need to be older than 21, also need to be older than 21 to transport hazardous materials Class C License Driving #knw Two axle vehicle with a GVWL of 26,000 lbs or less Three axle vehicle weighing 6,000 lbs or less House car \u0026lt; 40 feet or less Three wheel motocycles Vanpool vehicle designed to carry between 10 and no more than 15 people Towing #knw Single vehicle of 10,000 or less Vehicle weighing 4000 lbs or more unladen Trailer coach under 10,000 lbs Fifth wheel trailer exceeding 10,000 lbs but under 15,000 lbs, with endorsement Mor ethings Class C drivers can\u0026rsquo;t tow more than one Motor vehile weigning under 4000 lbs cannot tow more than 6000 lbs Getting in trouble Get a traffic ticket and fail to show up to court: suspend driving One at fault collision or one at fault traffic violation: may take action? Two of either at fault collision or violation conviction: no driving for 30 days unless accompanied by 25 year old adult Three of \u0026ldquo;\u0026rdquo;: no driving for 6 months, on probation for a year. Drugs or alcohol between 13-21: suspension for a year Minor driving Not sure if this applies\nPractice for 50 hours, 10 hours at night #knw\nPass knowledge test\nPass driving test\nCannot drive between 11P and 5A during the first year #knw\nCannot drive with under 20 Y/O unless 25 Y/O licensed accompanied #knw\nUnless\u0026mdash;\nMedical need with doctor\u0026rsquo;s note and end date School and dean\u0026rsquo;s note Work and employer\u0026rsquo;s note and employment status Family need and parent\u0026rsquo;s note Minors can\u0026rsquo;t use a phone while driving.\nSafe car #knw Working driver\u0026rsquo;s window, brake lights, horn, parking brake, turn signals Safe tire (1/32 inch tread) Full windshield Two rear view mirrors, incl. one on left side Working seatbelts Check: clean windows and mirrors, adjust seat and mirrors, check tires.\nSafe personage Vision Hearing Not tired Not medicated Health: no Lapses of conciseness AD \u0026ldquo;related disorders\u0026rdquo; \u0026mdash; anything the doctor reports to DMV Steering Hand to Hand hands 9/3 or 8/4 oclock Push and pull, hands stay where they are Hand over hand Start 9/3 or 8/4 Turn, but leave wheel sliding under Sliding under hand reach over, pull the wheel up One-hand Turning or backing up to turn back Hand at 12 oclock Limeted use Signaling Arm signals when lights are hard to see because of bright sun\nMotorcyclists use these signals, and bikers point their hand straight up to turning direction\nWhen to signal #knw Signal when: turn, change lanes, slow down, stop.\n100 feet before turning Before every lane change: look over and check blind spot 5 seconds before lane change on highway Pulling next to or away curb Signal even if no cars around you Horning \u0026ldquo;It is safer to slow down or stop instead of honking your horn.\u0026rdquo;\nWhen to horn #knw Avoid collisions Alert hazard Alert oncoming traffic on narrow mountain roads when you cannot see at least 200 feet in front of vehicle Don\u0026rsquo;t use horn to move people along, or \u0026ldquo;express anger.\u0026rdquo; The more ya know.\nHeadlights They are bright.\nWhen to headlight #knw When its too dark to see: if you can\u0026rsquo;t see a person 1000 feet away Beginning 30 minutes after sunset until 30 minutes before sunrise Adverse weather: windshield wipers on = low-beam headlights on Clouds dust smoke or fog prevent seeing other cars On sunny days on country or mountain roads When a white regulatory sign says so To help others see your car, when sun is low on horizon When not to high-beam headlight Dim when 500 feet of car coming towards you or 300 feet of a car you are following Emergency flashers If you can see a collision ahead, do:\nTurn on flashers #knw Lightly tap brake pedal three/four times Use hand signals How to stop in a middle of the road during an emergency #knw Start breaking early.\nGive drivers warning\nTurn on emergency flashers if you earn\u0026rsquo;t moving, or use turn signals\nPull off the road\nStop not on the road or, if isn\u0026rsquo;t possible, stop where people can see\nDon\u0026rsquo;t stop just over a hill\nLift the hook to signal an emergency\nPlace emergency triangles 200-300 feet behind vehicle; use flares if needed but be careful b/c they may cause fire\nCall for roadside assistance\n63, 92\nLanes! Reading \u0026rsquo;em Yellow: different directions Single yellow is the center of the road; cannot cross into oncoming traffic Double solid yellow line: not to be crossed \u0026hellip;except hov entrace lane which has a left entrance Instructed to cross because the road is blocked Entering or exiting a driveway, private road, or making a u-turn 2 double yellow line groups spaced 2 feet or more apart are considered a barrier; under no circumstance is to cross Broken yellow line: you may pass if the broken line is next to you White: same directions Single solid white line: traffic lanes in the same direction Double solid white lines: not to be crossed, regular use vs. preferential use lanes (carpool, etc.) Broken white lines: separate roads with two or more lines in the same direction White triangles: yield lines A line where you should yield. Triangles point to the direction of oncoming traffic (\u0026ldquo;towards you.\u0026rdquo;).\nChoosing \u0026rsquo;em Leftmost lane is lane 1, rightmost is lane n\nUse the left lane to pass or turn left Use the right lane to enter or exit traffic Change lanes when Moving from one lane to another Entering freeway Exiting freeway Entering the road from curb or shoulder Protocol for lane change #knw Signal Look in all mirrors Check traffic beside and behind you Look over solder in direction of desired lane change Check blind spots for other vehicles, motorcyclists, and bicycilsts Ensure room Tips stay in one lane don\u0026rsquo;t weave if you start a change, finish it Types of them Lane closest to the center divider is the \u0026ldquo;passing lane\u0026rdquo; HOV lanes is for high occupancy Center left turn lanes The center of some two-way streets has a left turn lane; marked on both sides by two painted lines. Inner line is broken and outer line is solid.\nYou may only drive 200 feet in the center left turn lane #knw\nProtocol for using this lane\nLook for other vehicles coming towards you in the center left turn lane Signal Look over shoulder Merge completely into the center left turn lane Turn when its safe. Turnouts Areas or lanes for turning that are marked? Use when:\nDriving slowly on a two-lane road where passing is unsafe, AND There are 5 or more vehicles following #knw Bike lanes Bike lanes\nBuffered bike lanes: uses chevrons or diagonals to buffer the bikes\nBike route: shared road markings to designate a preferred route\nBike boulevard: bike travel on streets with cars\nSeperated bikeways: completely different\nBikes share the road \u0026ldquo;sharrows!\u0026rdquo;\nCannot drive in bike lane unless\u0026hellip;.\nParking Entering or leaving road Turning (within 200 feet of intersection) Turning Right Drive close to the edge Drive in a bike lane, wait until about 200 feet to make turn #knw Watch for everybody Signal about 100 feet before #knw Look over sholder Stop behind limit line (/before entering crosswalk or intersection) Look both ways and turn when its safe; don\u0026rsquo;t turn into another lane Complete turn Details:\nCan\u0026rsquo;t turn when red arrow, but you can turn against red light You could cross a bus lane to make a right turn, but you can\u0026rsquo;t drive in it There could be designated right turn lanes which let you make a \u0026ldquo;free right turn\u0026rdquo; Left Drive close to the center divider or left turn lane Signal about 100 feed Look over sholder Stop behind limit line (/before entering crosswalk or intersection) Look left, right, then left Turn Details\nonly can turn against light when single-lane-to-single-lane U Conditions Across double-yellow line In a residential district No cars for 200 feet Whenever a sign or light protects against approachng cars At an intersection On a divided driveway, if opening provided Anticonditions WHen \u0026ldquo;no u-turn\u0026rdquo; is posted\nAt a railroad crossing\nOn a divided highway if needed to cross things\nCannot see 200 feet in each direction\nWhen other cars may hit you\nOn a one-way street\nIn front of a fire station\nooo. scary\nIn business districts, including churches apartments and buildings (except for schools); turn only at an intersection or opening if allowed. Merging Highways Enter at or near traffic speed Merge onto highway when safe to do so, don\u0026rsquo;t stop unless needed Merge into a space large enough for your car to join the lane Use mirrors and turn signals Watch for cars Leave three seconds of space (\u0026ldquo;three second rule\u0026rdquo;) between you and the car in front of you Exiting Know the exist Signal, look over sholder, etc. Change lanes Signal intention for 5 seconds Leave Space for entering You will need about a half a block on city streets Or, a full block on the highway Passing If anybody wants to pass, let them pass\nSpace for passing Don\u0026rsquo;t pass if\u0026hellip;\nYou are approaching a hill and cannot see oncoming traffic Within 100 feet of an intersection #knw At crossroads or driveways Condition of Passing You pass on the left, unless\u0026hellip;\nOpen highway with two or more lanes going in your direction Driver ahead of you is turning left, and you don\u0026rsquo;t have to drive off the road to pass You are on a one-way street Never drive off the road to pass.\nProtocol for passing Signal Shoulder Turn Speed up and pass Retturn Parking Find a space #knw three feet longer that your vehicle Turn on turn signal Pull up alongside the vehicle in front; leave about two feet between you and the car to your right. Stop when you rear bumper is aligned with the front of the space Check rearview mirror, look over sholder, keep foot on break and reverse Back up, 45% When rear view is within 18 inches from the curb, straighten out Set parking break. Leave when safe. Parking on a hill \u0026ldquo;Your car may roll when you breaks fail.\u0026rdquo;\nDownhill: wheels towards the curb Uphill: wheels away from curb No curb: turn towards the sholder of the road \u0026ldquo;towards the sholder, except when uphill with curb\u0026rdquo;\nColors White curb: stop for picking up or dropping off passengers or mails Green curb: park for limited time Yellow: load and unload, staying in the vehicle Red: no stopping Blue: disabled\u0026mdash;fine of $1,000, 6 months in county jail #knw Can\u0026rsquo;t park when No marking Unmarked or marked crosswalk Sidewalk, partially blocking sidewalk, or in front of driveway Within 3 feet of disabled sidewalk ramp #knw On diagnal lines next to disabled space Within 15 feet of a fire hydrant #knw Double parking On the wrong side of the street or freeway, except: 1) emergency 2) law enforcement officer 3) specificaly permitted stop.\nTo stop and park then, park off the pavement, stay with the car and lock the doors until help arrives; visibility is 200 feet in each direction required. #knw\nLights Flashing red: stop sign\u0026ndash;stop and go when its safe Flashing yellow: yield sign\u0026mdash;proceed with caution Flashing yellow arrow: unprotected turn Broken traffic lights become a four way stop sign.\nSigns Stop sign is stop; there should be a limit line; if no limit line, stop before intesection Yield sign is to yield; slow down Right of Way Without stop/yield signs Whomever gets to the intersection first has right of way T intersection without stop/yield signs The through road have right of way Stop signs Stop first, then follow right of way rules as if no intersection Turning left Right of way to anyone approaching that\u0026rsquo;s \u0026ldquo;close enough to be dangerous\u0026rdquo; Turning right Check for pedestrians crossing the street, and bikes and motors next to you Green light Pedestrians Divided highways Vehicles coming in the lane you are about to enter Entering traffic The traffic you are entering Roundabouts The logistics of using a roundabout\nSlow down Yield to traffic Watch for signs Travel in counter-clockwise direction, don\u0026rsquo;t stop or pass Signal when you change lanes or exit If you miss your exit, try again Choosing lane Rightmost for turning right Either lane (\u0026ldquo;middle\u0026rdquo;, if exists) for straight Innermost for left turn or u turn Pedestrians Pedestrians have right-of-way Pedestrian crossing need to cross first, you yield or slow to them Which means\u0026hellip;\nDo not pass a stopped vehicle Don\u0026rsquo;t drive on a sidewalk except to cross it or enter/exit it Don\u0026rsquo;t stop in a crosswalk If people make eyecontact, they are crossing the street Obey pedestrian\u0026rsquo;s signs Watch for seniors, people with disabilities, young children.\nCrosswalks Crosswalks are marked (but not all) School crossings have yellow lines Pedestrians have right of wall in all crosswalks Flashing light crosswalks exists to, just be prepared to stop regardless Blind White canes and guide dogs have absolute right of way Stop at all stop walks Don\u0026rsquo;t stop in the middle of stop walk Don\u0026rsquo;t give verbal directions to blind pedestrian Don\u0026rsquo;t turn right w/o looking for pedestrians Don\u0026rsquo;t honk at a blind person Don\u0026rsquo;t block sidewalk Pulling in cane + stepping away: you may go Mountain roads Uphill car has right of way Downhill car has more control backing up the hill Roadsharing Large cars Average passenter car at 55mph has 400 feet before stopping Large car takes 800 feet Don\u0026rsquo;t move in front of a large car and suddenly stop.\nLook at turn signals: large vehicles may swing their back, say, left in order to turn right.\nDon\u0026rsquo;t\nChange lanes in front of them to reach an exit or turn (tight spaces around large vehicles is dangerous) Drive next to them (unless passing); after you pass, move ahead of it Follow too closely: that\u0026rsquo;s tailgating. Give more space Underestimate the size and speed of the vehicle \u0026ldquo;If you can\u0026rsquo;t see a truck\u0026rsquo;s side mirrors, it can\u0026rsquo;t see you.\u0026rdquo;\nalways pass it on the left side\nBuses and rails when loading is happening without a safety zone, stop behind the nearest door Stopped busses can only be passed at 10mph Don\u0026rsquo;t pass on the left side, unless\u0026hellip; you are on a one-way street tracks are so close to the right you can\u0026rsquo;t pass on the right traffic officer directs you to Never turn in front of a light rail vehicle Check for traffic lights (light rails can interrupt them) Motocycles 4 second following distance Given a motocycle a full lane; its legal to share but its unsafe Don\u0026rsquo;t try to pass a motorcycle in the same lane When possible, move to one side of your lane Check for motocyclists Emergency vehicles Give them right of way: drive to the edge until they\u0026rsquo;ve passed \u0026hellip;except in intersections: never stop in an intersection (continue through and stop) Obey loudspeaker orders Illegal to follow 300 feet of any emergency vehicles with flashing siren Slow cars Slow down for them NEV LSV Like gold carts\nThey have max speed 25mph They can\u0026rsquo;t drive in roads with speed limit larger than 35 mph Bikes Front lamp with white light visible for 300 feet Rear red reflector (visible from 500 feet) White or yellow reflector on each pedal (visible for 200 feet) Travel lanes Must ride to the curb if slow, unless\nPassing in the same direction Preparing to turn left Avoiding a hazard/road condition Approaching right turn On a one way road with two or more lanes (if so, bikers may right next to left curb) Passing bikers 3 feet clearance\nSchool buses Yellow lights flashing is to slow Red lights flashing is to stop If you fail to stop, you can be fined up to $1,000 and driving maybe suspended for a year Workzone fines Traffic violations have fines of $1,000 or more Assulting a worker has a fine of $2,000 plus imprisonment for up to on year Some regions are double-fine zones Speed Limit \u0026ldquo;Basic speed law\u0026rdquo;: you may never drive faster than its safe.\n10mph to pas a roadcar\n15 mph in blind intersections (cannot see 100 in both directions when within 100 feet)\nif your view is blocked in a blind intersection, inch forward until you can see 15 mph also in some school, alleys (roads no wider 25 feet), 100 feet of railroad tracks if visiblity less then 400 feet\n25 mph when you are 500-1000 feet of a school, when crossing the street, residential\n55 mph on two lane undivided highway\nYou cannot block traffic flow\nDrive far-right lane of you are towing\nRailroad Look in both directions Except train anytime Don\u0026rsquo;t stop in traintracks Watch for other cars Stop between 15-50 feet from the neearest tracks Fines and Stuff Smoking with a minor: $100\nDumping animals: $1,000, six months in jail\nEvading law enforcement:\nstate prison up to 7 years, county jail for 1 year Fine between $2,000 and $10,000 Or both Evading law enforcement and commiting manslauter\nImprisonment for 4-10 years Speed content and reckless driving: fine and imprsionment\nTexting\nWear earplugs in bot hyears\nCarry anything that extends beyond the fenders on the left side, or more then 6 inches on the right side\nCargo more the 4 feet must display a 1 feet red or flourencesnt flag\nTransport animals unless secured\nAllow a person to be in a back of a pickup truck unless secured\nDrive a car with a video monitor except when it doesn\u0026rsquo;t face driver\nThrow a cig from the car\nCut signs that block the windshiled\nDon\u0026rsquo;t hang objects on the mirror\nDon\u0026rsquo;t sticker, unless\n7 inch square on lower corner of passengers or rear window 5 inch square on the lower corner of the driver window Side windows behind driver 5 inch located in the center uppermost portion Funeral pocessions have right of way\nPoints 36 month record Suspension when: 4 points in 12 months, 6 in 24, or 8 in 36 Once 18 months to earn back points via traffic school Best Practices Scan road 10-15 seconds ahead of you Don\u0026rsquo;t stare Don\u0026rsquo;t tailgate: 3 seconds between you and the car ahead passes Allow extra space when\u0026hellip; If you have a tailgator, (and move! if you can) The driver behind you wants to pass Slippery Following on icy or wet Towing a trailer Followiing a car that blocks you ahead Merging onto freeway Following Don\u0026rsquo;t stay in the blind spot Don\u0026rsquo;t driving alongside cars Make space when possible Keep space between you and parked cars Be careful when nearing motorcyclists and bicyclists At intersections Look both ways Look left first (vehicles coming from the left are closer) Look right Take one more look to the left 5-10mph on wet road, reduce speed by half on snow, tiny very slow on ice Don\u0026rsquo;t use breaks if starting to hydroplone If you can\u0026rsquo;t see farther than 100 feet, its unsafe to drive faster than 30mph Seat belts Click it or ticket Under 16 years old, you may also get ticket Child safety Under 2 years old: secure in a real facing child restraight system (unless child weighs more than 40 pounds or is more that 3 ft 4 inches taller)\nChilden under 8 years old, less than 4 feet 9 inches tall: secure in a front-facing restraight system\nCould use front seat if there\u0026rsquo;s no rear seat or if they are side facing jump seat\n8 years old or older, or 4 feet 9 inches tall: use seat belts\n6 y/o or younger unattended illegal to leave in car; supervision could be 12 year old.\nHot vehicle can kill\nEmergencies Skids Slippery surface Slowly remove foot from gas pedal Don\u0026rsquo;t use breaks Turn the steering wheel in the direction of the skid If your breaches get wet, dry them by pressing gas and brake at the same time.\nLock wheel Breaking too hard when going to fast: skid no matter steering wheel\nRemove foot from break Straighten front wheel If ABS not working, step on brake gradually until safe speed. If the brake petal sinks to the floor, bump the brakes.\nDriving off pavement Grip wheel slowly Remove your foot from gas Brake gently Check for traffic Steer back Accelerator mallfunction Shift to neutral Apply breakes Look for traffic Honk horn and emergency flashers Drive car off the road Turn of ignition Collision If collision causes more than $1000 in property damage, you msut report to DMV Driving is suspended for 4 years of no insurance Disabled Vehicle Safely pull over Exit on the right side Find assistance Return no vehicle Stay inside with your seat belt Uuse flashers Railroad If a train is coming, get out and run in a 45 degree away from the train and tracks. Dial 911 If train not coming, exit vehicle, dial emergency number on the railroad crossing box, and then call 911 DUI Don\u0026rsquo;t drink and drive Don\u0026rsquo;t take drugs Use any combination of drugs Illegal to drink alcohol or smoke or eat cannabis products while in a car, whether self or passenger. If you are carrying it, it must be full and unopened. If its open, keep it in the trunk.\nLimits 0.08% over 21 0.01% under 21 0.01% under DUI probation 0.04% if commercial 0.04% if driving for hire DUI Arrests Hold license for 30 days Hearing from 10 days DUI Convictions Completion of DUI program Install Ignition Interlock Device 6 months in jail $390-$1000 May inpound vehicle Carrying under 21 May not carry unless someone older Fine up to $1000 and impound for 30 days, suspencion for 1 year 0.01% or higher you have to complete program, 0.05% suspension ","permalink":"https://www.jemoka.com/posts/kbhdriving/","tags":null,"title":"Driving"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdriving_practice/","tags":null,"title":"Driving Practice"},{"categories":null,"contents":"Dup15q Syndrome is an autistic syndrome associated with a gain of variant function in UBE3A. It is the opposite of Angelman Syndrome, which is a loss of function result on UBE3A.\n","permalink":"https://www.jemoka.com/posts/kbhdup15q/","tags":null,"title":"Dup15q Syndrome"},{"categories":null,"contents":"dynamic programming is a three-step algorithm to tackle large, multi-step problems; high level idea: guessing + caching + recursion.\ndynamic programming can sometimes not be good enough, and it doesn\u0026rsquo;t really give us fast enough to get what we need to use. That\u0026rsquo;s when we need to deal with relaxation, or possibly greedy programming.\nmain steps of dynamic programming Break a hard problem into sub-problems Guess what sub-problem to solve Solve the sub-problem and store the solution Repeat #2 and #3 Combine sub-problem solutions to solve the hard problem analyzing runtime of dynamic programming To analyze runtime of dynamic programming problems, you ask:\nHow many sub-problems are there? How long does it take to solve each sub-problem? How long does it take to combine sub-problems? fibonacchi numbers: dynamic programming here\u0026rsquo;s an example top-down dynamic programming problem:\nThere are \\(n\\) sub-problems: \\(F_1, F_2, \\ldots, F_{n-1}\\). Solve a sub-problem, then store the solution \\(F_{n-1} = F_{n-2}+F_{n-3}\\) Continue until \\(F_1 =1\\). Now, we can recurs back up (popping the call stack) and cache all calculated results So then we can just look up any \\(F_k\\). shortest path: dynamic programming here\u0026rsquo;s a graph! how do we get to node \\(6\\)?\nGuess that the shortest path goes through 10 Go recursively until you get to root, cache the solution Do it again until you got to all subproblems Look up cached result ","permalink":"https://www.jemoka.com/posts/kbhdynamic_programming/","tags":null,"title":"dynamic programming"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhe_coli/","tags":null,"title":"E. Coli"},{"categories":null,"contents":"Presented Project80 Talks Person Society Keywords Email Chhavi Chauhuan, PhD, ELS ASIP AI Ethics, Pathology cchauhan@asip.org J. Elliott Robinson, PhD, MD ASBMB NF1, Dopamine, ADHD elliott.robinson@cchmc.org Jason Yi, PhD ASBMB UBE3A, Recklinghaus, Dup15q domain Erica Korb, PhD ASBMB Autism, Chromatin ekorb@pennmedicine.upenn.edu Catherine Wang AAA student approach to learning ??? Megan Fagalde, PhD Candidate AAA anatomy learning mfagalde@iu.edu Michelle A. Sveistrup AAA haptic abilities, HAT msveistr@uwo.ca AAA anatomy learning Alam Boyd AAA partner vs. individual work Magnus ??? AAA Orna Issler ASBMB IncRNA, LINC00473, FEDORA orna.issler@mssm.edu Kaushik Ragunathan ASBMB whimsical adaptations ragunath@med.umich.edu Tracy l. Bale ASBMB i think like P80 scary tbale@som.umaryland.edu Gregory Morton APS thermoregulation, glucose gjmorton@uw.edu Peter Turnbaugh ASBMB Fluoropyrimidine, PreTA, DPYD peter.turnbaugh@ucsf.edu Ralph DeBernandis ASBMB metabolic alterations, LIPT1 People Meeters Person Place Email Job Followup Jay Pieczynski Rollins jpieczynski@rollings.edu Assist. Prof. P80, College Apps Sebastian Hernandez Rollins shernandez1@rollings.edu Undergrad \u0026quot;\u0026quot; Bryson Arnett U of Kentucky Undegrad Jennifer Pousont Pingry Eric P. Chang Pace U echang@pace.edu Assist. Prof P80 ","permalink":"https://www.jemoka.com/posts/kbheb_emails/","tags":["index"],"title":"EB2022 Index"},{"categories":null,"contents":"Slightly nontraditional Ted class, which is that it is in complete modular architecture: no large group lectures, work is done in 2-3 week sprints.\nFirst two days, we will be doing intro together. There are 12 modules, and you do 6. There will be core modules and branches.\nThere are 3 symposiums which the groups share out. This class is very hard; we are using a graduate school textbook. We will be sidestepping some depth: main idea is to show the big area.\nTracks 1 =\u0026gt; {2,4,5} 4 =\u0026gt; 5 7 =\u0026gt; 8 8 =\u0026gt; {11,12} 3 =\u0026gt; 6 6 =\u0026gt; 9 9 =\u0026gt; 10 Good to learn MatLab.\nLogistics Create a portfolio journal; supply one entry a week.\nIntroductory Reading How Did Economists Get It So Wrong?\n","permalink":"https://www.jemoka.com/posts/kbhecon320_architecture/","tags":null,"title":"ECON320 Architecture"},{"categories":null,"contents":"The economy of credit is an effect where credit is being traded liberally, and people are buying stocks on large margins and unable to pay back.\n","permalink":"https://www.jemoka.com/posts/kbheconomy_of_credit/","tags":null,"title":"economy of credit"},{"categories":null,"contents":" Many Mexican-Americans worked as migratory laborers + outside programs Indian Reorganization Act of 1934 Woman were paied less Environmental cost of damns and public projects commentary on the effects of the New Deal Incorporating aspects of Arthur M. Schlesinger\u0026rsquo;s Appraisal of the New Deal, William E. Leuchtenburg\u0026rsquo;s Appraisal of the New Deal, Anthony Badger\u0026rsquo;s Appraisal of the New Deal.\nThrough the analysis of the New Deal programs, what was particularly salient was Anthony Badger\u0026rsquo;s framing of the event as not one that is ultimately \u0026ldquo;successful\u0026rdquo; or \u0026ldquo;failed\u0026rdquo; but instead one which focuses on its long-term effects in context with the future policies. The equivocal labeling allows nuance that places the Deal properly in its historical content. According to Badger, helping the poor, a significant policy goal of the deals, were left as \u0026ldquo;unfinished business\u0026rdquo; when going to war. This idea contrasts with William E. Leuchtenburg\u0026rsquo;s framing of the same event\u0026mdash;that it was never the true intention of the deal to assist in subsidies on a humane level, but that which supported the economy and incidentally those that reaped benefits on it.\nThis new frame is much more useful when analyzing the deal. In fact, Leuchtenburg took this a step further and claimed that the New Deal didn\u0026rsquo;t work largely because it was impossible for it to have repaired the damage by the Hoover administration. Furthermore, according to Schlesinger, programs like the NRA were created with already the clear assumption that there were not enough policy tools in place to actually achieve it to the fullest extent. Under this mind frame, then, it is not difficult to see the New Deal as one that intentionally brought a failing US economy\u0026mdash;and those participating in it\u0026mdash;to full swing whilst ignoring those that didn\u0026rsquo;t have an economic influence. It was, therefore, never about helping \u0026ldquo;people\u0026rdquo;: it is a policy and economic tool like any other.\nThrough this somewhat revisionist view, it is much easier to place into perspective New Deal\u0026rsquo;s zealot focus on young men, strange deficiency in some areas, and central focus on infrastructure. In that regard, the New Deal worked very well to bring a failing economy back to a semblance of normalcy for the privileged few.\n","permalink":"https://www.jemoka.com/posts/kbheffects_of_the_new_deal/","tags":null,"title":"effects of the New Deal"},{"categories":null,"contents":"the eigenspace is the invariant subspace formed by a particular eigenvalue\n","permalink":"https://www.jemoka.com/posts/kbheigenspace/","tags":null,"title":"eigenspace"},{"categories":null,"contents":"The Elastic Modulus is a measurement of how much deformation takes place given some force on the system. Formally, it is the slope of the stress-strain curve, defined by:\n\\begin{equation} E = \\frac{stress}{strain} \\end{equation}\nThe units in pascals as it is: force per area (pascals) divided by deformation (dimensionless, as it is a fraction of old shape over new shape \\(\\frac{V}{V}=1\\)).\nDepending on how its measured, it is called different things:\nYoung\u0026rsquo;s Modulus: tensile elasticity\u0026mdash;tendency for object to deform along an axis with force applied (usually that is just called the Elastic Modulus) Shear\u0026rsquo;s Modulus: shear elasticity\u0026mdash;tendency of an object to shear (deform in shape with the constant volume) with force applied Bulk Modulus: volumetric elasticity\u0026mdash;tendency for an object to deform in all directions when uniformly loaded ","permalink":"https://www.jemoka.com/posts/kbhelastic_modulus/","tags":null,"title":"Elastic Modulus"},{"categories":null,"contents":"Eleanor Roosevelt is the first lady of the US.\nCreated minimum wage Wrote a weekly column named My Day, in 135 newspapers 2x a week broadcast ","permalink":"https://www.jemoka.com/posts/kbheleanor_roosevelt/","tags":null,"title":"Eleanor Roosevelt"},{"categories":null,"contents":"Though Coulomb\u0026rsquo;s Law allow us to calculate the force between any two individual charges, one can note that most of it is independent of the second test charge. In fact, each charge emits a field around itself of the shape:\n\\begin{equation} \\vec{E( r)} = k \\frac{q}{r^{2}} = \\frac{1}{4\\pi \\epsilon_{0}} \\frac{q}{r^{2}} \\end{equation}\nOr, you can think of it as moving a test charge \\(q\\) around the charge of interest, then calculating:\n\\begin{equation} \\vec{E} = \\frac{\\vec{F_{e}}}{q} \\end{equation}\nAs you can see, if the source charge were to be positive, you have a positive \\(E\\) which will point away from the charge, and negative charge \\(E\\) will point towards the charge.\nOne institution here that the above statement provides is that electric fields are drawn positive to negative; so, if you placed a positive test charge in the field, it will experience a force tangent to and in the same direction that traced out by the field lines. If you have a negative test change, then it will experience a force in the opposite direction.\nadditional information tracing electric field lines By tracing out electric fields and observing the lines\u0026rsquo; density, we can make a guess about how long the fields are.\ncomposing electric fields Unsurprisingly, superposition matters here as well:\nIf your system has multiple charges, then \\(E_{total} = E_{q_1} + E_{q_2}\\). No surprise here.\n","permalink":"https://www.jemoka.com/posts/kbhelectric_field/","tags":null,"title":"electric field"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhelectron/","tags":null,"title":"electron"},{"categories":null,"contents":"A civil rights movement organizer that founded SNICK.\n","permalink":"https://www.jemoka.com/posts/kbhella_baker/","tags":null,"title":"Ella Baker"},{"categories":null,"contents":"Your brain maintaing a stable level of energy. Closely related to glucose homeostatis.\nmethods to achive energy homeostasis by the CNS regulation of the brain AgRP signaling is activated to stimulate food intake when hypoglycemic. ","permalink":"https://www.jemoka.com/posts/kbhenergy_homeostasis/","tags":null,"title":"energy homeostasis"},{"categories":null,"contents":"english is a great language.\nFor Orthello:\nPronouns: 23 \u0026ldquo;Moor:\u0026rdquo;: 18 Orthello: 7\n","permalink":"https://www.jemoka.com/posts/kbhenglish/","tags":null,"title":"english"},{"categories":null,"contents":"motivating entanglement file:///Users/houliu/Documents/School Work/The Bible/Quantum/Leonard Susskind, Art Friedman - Quantum Mechanics_ The Theoretical Minimum-Basic Books (2014).pdf\nTake two actors, Alice \\(A\\) and Bob \\(B\\). They each have a space \\(S_A\\) and \\(S_B\\). What if, for instance, we want to create a composite system out of Alice and Bob?\nWe will define elements in the Alice space as being defined by bases \\(H\\) and \\(T\\), where each element \\(a \\in S_a\\) is defined as:\n\\begin{equation} \\alpha_H | H \\big\\} + \\alpha_T | T \\big\\} \\end{equation}\nWhy the weird kets? We will use different kets to be aware of where bases came from; as in, elements in Alicespace is not elements in Bobspace.\nLet\u0026rsquo;s take Bobspace to be a higher dimension, as in, using normal ket vectors:\n\\begin{align} |1\\big\u0026gt; \\\\ |2\\big\u0026gt; \\\\ |3\\big\u0026gt; \\\\ \\cdots \\\\ |6\\big\u0026gt; \\end{align}\n","permalink":"https://www.jemoka.com/posts/kbhentangled/","tags":null,"title":"entanglement"},{"categories":null,"contents":"epigenetics is the ability to make identical cells present distinct phenotipic states.\nWhy? DNA is packaged by charged histone proteins, and they wrap around the nucleosome. Upon acute changes in the environment, cells can change their epigenic states.\nwhimsical adaptations Epigenetic adaptive states in organisms with no clear path adaptation. For instance, a certain lung cancer cell has this ability. So, how do cells decide what genes they would activate?\nAnother example: treating fisheries in caffine\nGrowing in caffine will trigger caffine resistance Remove the caffine would cause some of them to default back, some of them to stay the same way ","permalink":"https://www.jemoka.com/posts/kbhepigenetics/","tags":null,"title":"epigenetics"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhequal_rights_act/","tags":null,"title":"Equal Rights Act"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhetf/","tags":null,"title":"ETF"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbheugene_wigner/","tags":null,"title":"Eugene Wigner"},{"categories":null,"contents":"A type of cell.\nSample eukareotyic cell gene:\nTATA box promoter 5\u0026rsquo; non-coding sequence Non-coding introns interlaced between exons, unique to eukareotyic cells. Bacteria (prokateotic cells don\u0026rsquo;t contain introns or have small them) 3\u0026rsquo; non-coding sequence ","permalink":"https://www.jemoka.com/posts/kbheukareotyic_cell/","tags":null,"title":"eukareotyic cell"},{"categories":null,"contents":"The Euler-Bernoulli Theory is a theory in dynamics which describes how much a beam deflect given an applied load.\nAssumptions For Euler-Bernoulli Theory to apply in its basic form, we make assumptions.\nThe \u0026ldquo;beam\u0026rdquo; you are bending is modeled as a 1d object; it is only long and is not wide For this page, \\(+x\\) is \u0026ldquo;right\u0026rdquo;, \\(+y\\) is \u0026ldquo;in\u0026rdquo;, and \\(+z\\) is \u0026ldquo;up\u0026rdquo; Probably more, but we only have this so far. the general form of the Euler-Bernoulli Theory assumes a freestanding beam Basic Statement The most basic for the Euler-Bernoulli Equation looks like this:\n\\begin{equation} \\dv[2]x \\qty(EI\\dv[2]{w}{x}) =q(x) \\end{equation}\nwhere, \\(w(x)\\) is the deflection of the beam at some direction \\(z\\) at position \\(x\\). \\(q\\) is the load distribution (force per unit length, similar to pressure which is force per unit area, at each point \\(x\\)). \\(E\\) is the Elastic Modulus of the beam, and \\(I\\) the second moment of area of the beam\u0026rsquo;s cross section.\nNote that \\(I\\) must be calculated with respect to the axis perpendicular to the load. So, for a beam placed longside by the \\(x\\) axis, and pressed down on the \\(z\\) axis, \\(I\\) should be calculated as: \\(\\iint z^{2}\\dd{y}\\dd{z}\\).\nPretty much all the time, the Elastic Modulus \\(E\\) (how rigid your thing is) and second moment of area \\(I\\) (how distributed are the cross-section\u0026rsquo;s mass) are constant; therefore, we factor them out, making:\n\\begin{align} \u0026amp;\\dv[2]x \\qty(EI\\dv[2]{w}{x}) =q(x) \\\\ \\Rightarrow\\ \u0026amp; EI \\qty(\\dv[2]x \\dv[2]{w}{x} )=q(x) \\\\ \\Rightarrow\\ \u0026amp; EI \\dv[4]{w}{x} = q(x) \\end{align}\nThis is also apparently used everywhere in engineering to figure out how bendy something will be given some \\(q\\) put along the beam.\nOk, let\u0026rsquo;s take the original form of this equation and take some integrals to see the edges of this thing:\n\\begin{equation} \\dv[2]{x} \\qty(EI \\dv[2]{w}{x}) = q(x) \\end{equation}\nFirst things first, let\u0026rsquo;s take a single integral:\n\\begin{equation} \\dv{x} \\qty(EI \\dv[2]{w}{x}) = -Q \\end{equation}\nThis is the total shear force on the material (the sum of all forces applied to all points \\(\\int q(x)\\).) We have a sign difference\nold notes\nLet\u0026rsquo;s take some transverse load \\(q(x,t)\\), applied at time \\(t\\) at location \\(x\\). To model the load/bending/vibration of the rod, we first have to know a few more things.\nFirst, figure the Young\u0026rsquo;s Modulus \\(E\\) of the thing that you are bending.\nOf course, we also want to know what shape our thing is; more specifically, we want to know how the point masses in our thing is distributed. So we will also need the second moment of area \\(I\\).\nFinally, we should have \\(m\\) mass per unit length of the rod we are bending.\nThe Euler-Bernoulli Theory tells us that the deflection (distance from the neutral-axis) each point \\(x\\) in the material should get is:\n\\begin{equation} EI \\pdv[4]{w}{x} + m \\pdv[2]{w}{t} = q(x,t) \\end{equation}\nSolving this lovely Differential Equation would tell you how far away each point diverges from the neutral point.\nTracing this out over \\((x,t)\\), we can get some trace of how the thing vibrates by measuring the behavior of \\(\\omega\\).\nfree vibrations in Euler-Bernoulli Theory If no time-varying \\(q\\) exists, we then have:\n\\begin{equation} EI \\pdv[4]{w}{x} + m \\pdv[2]{w}{t} = 0 \\end{equation}\nAnd then some magical Differential Equations happen. I hope to learn them soon.\nThe result here is significant: if we can figure the actual rate of vibrations which we expect.\nHowever, this doesn\u0026rsquo;t really decay\u0026mdash;but funing torks do. How?\nApparently because air resistance\u0026mdash;Zachary Sayyah. So Sasha time.\n","permalink":"https://www.jemoka.com/posts/kbheuler_bernoulli_theory/","tags":null,"title":"Euler-Bernoulli Theory"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbheurope/","tags":null,"title":"Europe"},{"categories":null,"contents":"The correlation is a relation between two random variables.\nStandardize variables to \\(z\\) by dividing The correlation is simply their \u0026ldquo;product\u0026rdquo;: means of positive and negative groups ","permalink":"https://www.jemoka.com/posts/kbhexpectation/","tags":null,"title":"expectation"},{"categories":null,"contents":"\\(\\mathbb{F}^n\\) is the set of all lists of length \\(n\\) with elements of \\(\\mathbb{F}\\). These are a special case of matricies.\nFormally\u0026mdash;\n\\begin{equation} \\mathbb{F}^n = \\{(x1,\\ldots,x_n):x_j\\in\\mathbb{F}, \\forall j =1,\\ldots,n\\} \\end{equation}\nFor some \\((x_1,\\ldots,x_n) \\in \\mathbb{F}^n\\) and \\(j \\in \\{1,\\ldots,n\\}\\), we say \\(x_j\\) is the \\(j^{th}\\) coordinate in \\((x_1,\\ldots,x_n)\\).\nadditional information addition in \\(\\mathbb{F}^n\\) Addition is defined by adding corresponding coordinates:\n\\begin{equation} (x1,\\ldots,x_n) + (y_1,\\ldots,y_n) = (x_1+y_1, \\ldots,x_n+y_n) \\end{equation}\naddition in \\(\\mathbb{F}^n\\) is commutative If we have \\(x,y\\in \\mathbb{F}^n\\), then \\(x+y = y+x\\).\nThe proof of this holds because of how addition works and the fact that you can pairwise commute addition in \\(\\mathbb{F}\\).\n\\begin{align} x+y \u0026amp;= (x_1,\\ldots,x_n) + (y_1,\\ldots,y_n)\\\\ \u0026amp;= (x_1+y_1,\\ldots,x_n+y_n)\\\\ \u0026amp;= (y_1+x_1,\\ldots,y_n+x_n)\\\\ \u0026amp;= (y_1,\\ldots,y_n) + (x_1,\\ldots,x_n)\\\\ \u0026amp;= y+x \\end{align}\nThis is a lesson is why avoiding explicit coordinates is good.\nadditive inverse of \\(\\mathbb{F}^n\\) For \\(x \\in \\mathbb{F}^n\\), the additive inverse of \\(x\\), written as \\(-x\\) is the vector \\(-x\\in \\mathbb{F}^n\\) such that:\n\\begin{equation} x+(-x) = 0 \\end{equation}\nWhich really means that its the additive inverse of each of the coordinates.\nscalar multiplication in \\(\\mathbb{F}^n\\) At present, we are only going to concern ourselves with the product of a number \\(\\lambda\\) and a vector \\(\\mathbb{F}^n\\). This is done by multiplying each coordinate of the vector by \\(\\lambda\\).\n\\begin{equation} \\lambda (x_1,\\ldots,x_n) = (\\lambda x_1, \\lambda, \\lambda x_n) \\end{equation}\nwhere, \\(\\lambda \\in \\mathbb{F}\\), and \\((x_1,\\ldots,x_n) \\in \\mathbb{F}^n\\).\nThe geometric interpretation of this is a scaling operation of vectors.\n","permalink":"https://www.jemoka.com/posts/kbhlists_over_fields/","tags":null,"title":"F^n"},{"categories":null,"contents":"We define a set \\(\\mathbb{F}^{s}\\), which is the set of unit functions that maps from any set \\(S\\) to \\(\\mathbb{F}\\).\ncloseness of addition \\begin{equation} (f+g)(x) = f(x)+g(x), \\forall f,g \\in \\mathbb{F}^{S}, x \\in S \\end{equation}\ncloseness of scalar multiplication \\begin{equation} (\\lambda f)(x)=\\lambda f(x), \\forall \\lambda \\in \\mathbb{F}, f \\in \\mathbb{F}^{S}, x \\in S \\end{equation}\ncommutativity inherits \\(\\mathbb{F}\\) (for the codomain of functions \\(f\\) and \\(g\\))\nassociativity inherits \\(\\mathbb{F}\\) for codomain or is just \\(\\mathbb{F}\\) for scalar\ndistribution inherits distribution in \\(\\mathbb{F}\\) on the codomain again\nadditive identity \\begin{equation} 0(x) = 0 \\end{equation}\nadditive inverse \\begin{equation} (-f)(x) = -f(x) \\end{equation}\nmultiplicative identity \\(1\\) hee hee\n","permalink":"https://www.jemoka.com/posts/kbhfs_is_a_vector_space/","tags":null,"title":"F^s is a Vector Space Over F"},{"categories":null,"contents":"A New Deal program to help long-term families to have home. Tho program lowered down-payment for homes down from \\(50\\%\\) down to only \\(\u0026lt;10\\%\\). This is part of Roosevelt\u0026rsquo;s New Deal to lower interest rates and increased national home ownership rates. This could have been attributed to programs to stabilize home prices. This specifically helped white families: favoured single-family homes.\n","permalink":"https://www.jemoka.com/posts/kbhfederal_housing_administration/","tags":null,"title":"Federal Housing Administration"},{"categories":null,"contents":"The Federal Project Number One is a branch of projects under the WPA which created opportunities for writers, musicians, artists, writers, etc.\n","permalink":"https://www.jemoka.com/posts/kbhfederal_project_number_one/","tags":null,"title":"Federal Project Number One"},{"categories":null,"contents":"A field is a special set.\nconstituents distinct elements of at least \\(0\\) and \\(1\\) operations of addition and multiplication requirements closed commutativity associativity identities (both additive and multiplicative) inverses (both additive and multiplicative) distribution Therefore, \\(\\mathbb{R}\\) is a field, and so is \\(\\mathbb{C}\\) (which we proved in properties of complex arithmetic).\nadditional information Main difference between group: there is one operation is group, a field has two operations.\n","permalink":"https://www.jemoka.com/posts/kbhfield/","tags":null,"title":"field"},{"categories":null,"contents":"We have a system of differential equations:\n\\begin{equation} \\begin{cases} \\dv{I}{t} = -0.73 U(t) + 0.0438 + 0.4 \\dv{M}{t} \\\\ \\dv{U}{t} = 0.4I-0.012 \\\\ \\dv{G}{t} = \\dv{M}{t} - I(t) \\end{cases} \\end{equation}\nwhere, \\(M\\) is a sinusoidal function which we can control.\nWe hope for this system to be as stable as possible.\nFirst, let\u0026rsquo;s try to get a general solution of the system. The linearized(ish) solution takes the shape of:\n\\begin{equation} \\dv t \\mqty(I \\\\ U \\\\ G) = \\mqty(0 \u0026amp; -x_1 \u0026amp; 0 \\\\ x_4 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; 0 \u0026amp; 0 ) \\mqty(I \\\\ U \\\\ G)+ \\dv{M}{t}\\mqty(x_3 \\\\ 0 \\\\ 1) + \\mqty(x_2 \\\\ x_5 \\\\ 0) \\end{equation}\nWith:\n\\begin{equation} \\begin{cases} x_1 = 0.73 \\\\ x_2 = 0.0438 \\\\ x_3 = 0.4 \\\\ x_4 = 0.4 \\\\ x_5 = 0.012 \\end{cases} \\end{equation}\nas input parameters. We will follow the method of underdetermined coefficients: taking the homogeneous solution first and then using it to get the general solution.\nHomogeneous System To get the characteristic equation of the homogeneous system, we take the eigenvalue of the system:\nx1,x2,x3,x4,x5 = var(\u0026#34;x1 x2 x3 x4 x5\u0026#34;) matrix = matrix([[0, -x1, 0], [x4, 0,0], [-1, 0,0]]) matrix.eigenvalues() [-sqrt(-x1*x4), sqrt(-x1*x4), 0] Awesome. So we can see that our characteristic equation will be:\n\\begin{align} \\mqty(I \\\\ U \\\\ G)_{h} \u0026amp;= \\vec{c_0} e^{0t} + \\vec{c_1} e^{\\sqrt{-x_1x_4}t} + \\vec{c_2} e^{-\\sqrt{-x_1x_4}t} \\\\ \u0026amp;= \\vec{c_0} + \\vec{c_1}e^{i \\sqrt{x_1x_4}t} + \\vec{c_2} e^{-i \\sqrt{x_1x_4}t} \\end{align}\nNow, the two \\(e^{ix}\\) functions, one positive and one negative, inspires us to the following results:\n\\begin{equation} \\begin{cases} \\cos x = \\frac{e^{ix}+e^{-ix}}{2}\\\\ \\sin x = \\frac{e^{ix}-e^{-ix}}{2i}\\\\ \\end{cases} \\end{equation}\nTreating \\(\\frac{1}{2}\\) and \\(\\frac{1}{2i}\\) (which we can do, because the constants can be defined on any space desired), we have:\n\\begin{align} \\cos x + \\sin x \u0026amp;= \\frac{e^{ix}+e^{-ix}}{2} + \\frac{e^{ix}-e^{-ix}}{2i} \\\\ \u0026amp;= A_1e^{ix}+A_2e^{-ix} \\end{align}\nfor some constant scalars \\(A_1\\) and \\(A_2\\)\n\u0026ldquo;Wait, doesn\u0026rsquo;t the \\(e^{-ix}\\) and \\(-e^{-ix}\\) subtract each other out on the numerator? No, notice the denominator is different, so we will have \\((A-B)e^{-ix}\\) after we add the two expressions for some constants \\(A\\) and \\(B\\), it doesn\u0026rsquo;t cancel out.\u0026rdquo;\nPerforming this substitution allows us to reveal the sinusoidal nature of our characteristic equation, and get rid of those pesky \\(i\\).\n\\begin{align} \\mqty(I \\\\ U \\\\ G)_{h} \u0026amp;= \\vec{c_0} e^{0t} + \\vec{c_1} e^{\\sqrt{-x_1x_4}t} + \\vec{c_2} e^{-\\sqrt{-x_1x_4}t} \\\\ \u0026amp;= \\vec{c_0} + \\vec{c_1}e^{i \\sqrt{x_1x_4}t} + \\vec{c_2} e^{-i \\sqrt{x_1x_4}t} \\\\ \u0026amp;= \\vec{c_0} + \\vec{c_1\u0026rsquo;} \\cos (\\sqrt{x_1x_4} t)+ \\vec{c_2\u0026rsquo;} \\sin (\\sqrt{x_1x_4} t) \\end{align}\nThe primes here indicate that \\(\\vec{c_1} \\neq \\vec{c_1\u0026rsquo;}\\) because the initial conditions shift when we move to sinusoidal functions.\nWriting this out completely, ditching the vector expressions, we have\n\\begin{equation} \\begin{cases} I_{h}(t) = I_0 + I_1\\cos(\\sqrt{x_1x_4}t) + I_2\\sin (\\sqrt{x_1x_4}t) \\\\ U_{h}(t) = U_0 + U_1\\cos(\\sqrt{x_1x_4}t) + U_2\\sin (\\sqrt{x_1x_4}t) \\\\ G_{h}(t) = G_0 + G_1\\cos(\\sqrt{x_1x_4}t) + G_2\\sin (\\sqrt{x_1x_4}t) \\end{cases} \\end{equation}\nas the homogenous solutions for the equation.\nUnderdetermined Coefficients Recall the expression we are trying to solve is:\n\\begin{equation} \\begin{cases} \\dv{I}{t} = -0.73 U(t) + 0.0438 + 0.4 \\dv{M}{t} \\\\ \\dv{U}{t} = 0.4I-0.012 \\\\ \\dv{G}{t} = \\dv{M}{t} - I(t) \\end{cases} \\end{equation}\nWe dealt with the homongenous part\u0026hellip; but not the next two parts! Let\u0026rsquo;s do that.\nIn order to do that, we will use the method of underdetermined coefficients. Recall that:\n\\begin{equation} \\dv t \\mqty(I \\\\ U \\\\ G) = \\mqty(0 \u0026amp; -x_1 \u0026amp; 0 \\\\ x_4 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; 0 \u0026amp; 0 ) \\mqty(I \\\\ U \\\\ G)+ \\dv{M}{t}\\mqty(x_3 \\\\ 0 \\\\ 1) + \\mqty(x_2 \\\\ x_5 \\\\ 0) \\end{equation}\nFor now, we will add the extra \\(\\dv{M}{t}\\) term explicitly later. Let us solve for the undetermined coefficients based on the assumption that each function (except for the attenuation by \\(M\\)) is linear:\n\\begin{equation} y(t) = at \\end{equation}\n(\u0026ldquo;it linearly changes over time\u0026rdquo;)\nIts derivative by time is:\n\\begin{equation} y\u0026rsquo;(t) = a \\end{equation}\nPlugging that into our expressions above:\n\\begin{equation} \\mqty(a_{I} \\\\ a_{U} \\\\ a_{G} ) = \\mqty(0 \u0026amp; -x_1 \u0026amp; 0 \\\\ x_4 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; 0 \u0026amp; 0 ) \\mqty(a_{I}t \\\\ a_{U}t \\\\ a_{G} t) + \\dv{M}{t} \\mqty(x_3 \\\\ 0 \\\\ 1) + \\mqty(x_2 \\\\ x_5 \\\\ 0) \\end{equation}\nAnd now, arranging the right expressions such that we can clearly see each coefficient line up, relegating \\(M\\) to the side, and actually multiplying:\n\\begin{equation} \\mqty(a_{I} \\\\ a_{U} \\\\ a_{G} ) = \\mqty(0 \u0026amp; -x_1 \u0026amp; 0 \\\\ x_4 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; 0 \u0026amp; 0 ) \\mqty(a_{I}t + x_2 \\\\ a_{U}t + x_5 \\\\ a_{G} t) + \\dv{M}{t} \\mqty(x_3 \\\\ 0 \\\\ 1) \\end{equation}\n\\begin{equation} \\mqty(a_{I} \\\\ a_{U} \\\\ a_{G} ) = \\mqty(-x_1 (a_{U}t + x_5) \\\\ x_4 (a_{I}t + x_2 )\\\\ 1 a_{G} t) + \\dv{M}{t} \\mqty(x_3 \\\\ 0 \\\\ 1) \\end{equation}\nAwesome, so now, matching coefficients, we have:\n\\begin{equation} \\begin{cases} a_{I} = -x_1x_5 \\\\ a_{U} = x_4x_2 \\\\ a_{G} = 0 \\end{cases} \\end{equation}\nWhich I honestly could have told you by\u0026hellip;. Just staring at the equations. furthermore, we will add the requisite shift of \\(\\dv{M}{t}\\) to the right equations when appropriate.\nSo, adding the \\(M(t)\\) in place, our particular solutions are:\n\\begin{equation} \\begin{cases} I_{p}(t) = -x_1x_5 t + x_3 M(t) \\\\ U_{p}(t) = x_4x_2t \\\\ G_{p}(t) = M(t) \\end{cases} \\end{equation}\nas the homogenous solutions for the equation.\nGeneral Solution Let us know put the general and particular solutions together:\nRecall that:\n\\begin{equation} \\begin{cases} I_{h}(t) = I_0 + I_1\\cos(\\sqrt{x_1x_4}t) + I_2\\sin (\\sqrt{x_1x_4}t) \\\\ U_{h}(t) = U_0 + U_1\\cos(\\sqrt{x_1x_4}t) + U_2\\sin (\\sqrt{x_1x_4}t) \\\\ G_{h}(t) = G_0 + G_1\\cos(\\sqrt{x_1x_4}t) + G_2\\sin (\\sqrt{x_1x_4}t) \\end{cases} \\end{equation}\n\\begin{equation} \\begin{cases} I_{p}(t) = -x_1x_5 t + 0.4M(t) \\\\ U_{p}(t) = x_4x_2t \\\\ G_{p}(t) = M(t) \\end{cases} \\end{equation}\nSo, by linear additivity, we have:\n\\begin{equation} \\begin{cases} I}(t) = I_0 + I_1\\cos(\\sqrt{x_1x_4}t) + I_2\\sin (\\sqrt{x_1x_4}t) -x_1x_5 t + 0.4M(t) \\\\ U}(t) = U_0 + U_1\\cos(\\sqrt{x_1x_4}t) + U_2\\sin (\\sqrt{x_1x_4}t) + x_4x_2t\\\\ G}(t) = G_0 + G_1\\cos(\\sqrt{x_1x_4}t) + G_2\\sin (\\sqrt{x_1x_4}t) + M(t) \\end{cases} \\end{equation}\nSimplification Recall that our function \\(M(t)\\) is a sinusoidal function. And it is being added to some linear combination of sinusoidal functions in each term of our general solution above. Meaning, each of our equations are of the shape:\n[some vertical shift] + [cosine something] + [sine something] + [optional linear drift] + [M(t)]\nFor us to use \\(M(t)\\) to attenuate/stabilize the system, the best we can do is to dampen the sinusoidal part (because \\(M\\) itself is sinusoidal). We can\u0026rsquo;t do much of anything else.\nTo do this, we want ideally \\(M(t)\\) be \\(\\pi\\) ahead of the \\(\\cos + \\sin\\) waves in each of the functions; that is, we want \\(M\\) to be out of phase exactly.\n\\(\\cos +\\sin\\) is harder to be out of phase than just \\(\\sin\\); if the latter, we can just figure out its frequency and shift, and be \\(\\pi\\) ahead of it.\nFortunately, our \\(\\cos\\) and \\(\\sin\\) terms have exactly the same contents; therefore, their sum form just another shifted sine wave (don\u0026rsquo;t believe me, plot it!). Therefore, we will now endeavor to combine them.\nAside: \\(A\\cos (x)+B\\sin (x)\\) Here\u0026rsquo;s how you go about the combination. We desire that \\(A\\cos (x) + B \\sin (x)\\) be a single shifted sine function; we know this is true (by plottingish or using imaginary numbers), so we will set the sum to some arbitrary sine function and solve for its correct coefficients to mimic the sum; that is:\n\\begin{equation} r \\sin (x + \\alpha) := A\\cos (x) + B \\sin (x) \\end{equation}\nwe know desire the coefficients \\(r, \\alpha\\) that would make this true.\nRecall \\(\\sin a+b = \\cos a\\sin b + \\sin a\\cos b\\); so:\n\\begin{align} r \\sin (x+\\alpha) \u0026amp; = r(\\cos x \\sin \\alpha + \\sin x \\cos \\alpha ) \\\\ \u0026amp;= r \\sin x \\cos \\alpha + r \\cos x \\sin \\alpha \\\\ \u0026amp;= (r \\sin \\alpha) \\cos x + (r \\cos \\alpha) \\sin x \\end{align}\nNow, we have:\n\\begin{equation} (r \\sin \\alpha) \\cos x + (r \\cos \\alpha) \\sin x := A\\cos (x) + B \\sin (x) \\end{equation}\nTherefore:\n\\begin{equation} \\begin{cases} r\\sin \\alpha = A \\\\ r \\cos \\alpha = B \\end{cases} \\end{equation}\nAnd we desire correct coefficients \\(r, \\alpha\\) in terms of \\(A, B\\).\nDividing the two expressions:\n\\begin{equation} \\frac{\\sin \\alpha }{\\cos \\alpha } = \\frac{A}{B} \\end{equation}\nTherefore, \\(\\alpha = \\tan^{-1}\\qty(\\frac{A}{B})\\).\nFinally, recall that \\(\\sin^{2} x +\\cos^{2} x =1\\) for any \\(x\\). We will use this fact to get \\(r\\).\n\\begin{align} \u0026amp;\\sin^{2} \\alpha + \\cos^{2} \\alpha = 1 \\\\ \\Rightarrow\\ \u0026amp; \\qty(\\frac{A}{r})^{2} + \\qty(\\frac{B}{r})^{2} =1 \\end{align}\nBy rearranging our pair of expressions above to get \\(\\sin \\alpha\\) and \\(\\cos \\alpha\\) by itself.\nFinally, we have:\n\\begin{align} 1 \u0026amp;= \\qty(\\frac{A}{r})^{2} + \\qty(\\frac{B}{r})^{2} \\\\ \u0026amp;= \\frac{A^{2} + B^{2}}{r^{2}} \\end{align}\nSo:\n\\begin{equation} r^{2} = \\sqrt{A^{2}+B^{2}} \\end{equation}\nFinally, we have that:\n\\begin{equation} A\\cos (x)+B\\sin (x) = \\sqrt{A^{2}+B^{2}} \\sin \\qty(x + \\tan^{-1}\\qty(\\frac{A}{B})) \\end{equation}\nUsing the above result Recall we are working with:\n\\begin{equation} \\begin{cases} {I}(t) = I_0 + I_1\\cos(\\sqrt{x_1x_4}t) + I_2\\sin (\\sqrt{x_1x_4}t) -x_1x_5 t + 0.4M(t) \\\\ {U}(t) = U_0 + U_1\\cos(\\sqrt{x_1x_4}t) + U_2\\sin (\\sqrt{x_1x_4}t) + x_4x_2t\\\\ {G}(t) = G_0 + G_1\\cos(\\sqrt{x_1x_4}t) + G_2\\sin (\\sqrt{x_1x_4}t) + M(t) \\end{cases} \\end{equation}\nAnd we desire to use the above to simplify it. Plugging this expression directly in, for instance, to the first expression, we have:\n\\begin{equation} I(t) = I_0 + \\sqrt{ {I_{1}}^{2} + {I_{2}}^{2} } \\sin \\qty(\\sqrt{x_1x_4}t + \\tan^{-1} \\qty(\\frac{I_1}{I_2})) -x_1x_5 t + 0.4M(t) \\end{equation}\nNotice! Even if the shift changes based on each function, the frequency of the oscillation of each function is the same\u0026mdash;\nas each \\(\\cos x + \\sin x\\) sinusoidal, after applying the identity derived above, takes the form of:\n\\begin{equation} A\\sin (\\sqrt{x_1x_4}t + \\tan^{-1}(B)) \\end{equation}\nwe can see that they all oscillate with frequency of\n\\begin{equation} \\frac{\\sqrt{x_1x_4}}{2\\pi} \\end{equation}\n\u0026ldquo;how many \\(2\\pi\\) can our function go in \\(1\\) second?\u0026rdquo;\nTherefore, the control mechanism must work in frequencies of \\(\\frac{\\sqrt{x_1x_4}}{2\\pi}\\) (and best be exactly or as best as possible out of phase by being phase shifted by \\(\\tan^{-1}(B) + \\pi\\)) to be able to attenuate the sinusoidal the best.\nWe can allow \\(M(t)\\) to go to any sinusoidal function, and compose them together:\n\\begin{equation} I(t) = I_0 + \\sqrt{ {I_{1}}^{2} + {I_{2}}^{2} } \\sin \\qty(\\sqrt{x_1x_4}t + \\tan^{-1} \\qty(\\frac{I_1}{I_2})) -x_1x_5 t + 0.4 (c \\sin(ax+b)) \\end{equation}\nOk, let us now spend another aside to figure out the frequency and amplitude of this new curve, which will be our target upon which we are optimizing:\nAttenuating the Sums of Sinusoidals We now have:\n\\begin{equation} a_1\\sin (b_1t + c_1) + a_2 \\sin (b_2t+c_2) \\end{equation}\nThe question is how we can make the first wave destructively interfere with the second one.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_finance_eigen/","tags":null,"title":"Finance (Eigen)"},{"categories":null,"contents":"Why do we have a market? Basically: it allows society to make decisions about the value of things\u0026mdash;with the wisdom of the crowd. The stock market is how we (as people) decide what to make and how to make it.\nMisc. Questions About the Market Misc. Financial Market Questions\nKnowledge price Random Walk Hypothesis Brownian Motion Arbitrage Pricing Derivative Pricing options CAPM Stochastic Discount Factor GARCH ETF accounting price stock indicies VWAP short selling darkpool fundamental investing Second-Level Thinking stock market survey OTC markets NBBO LiquidNet ","permalink":"https://www.jemoka.com/posts/kbhfinancial_markets_intro/","tags":["index"],"title":"Financial Market"},{"categories":null,"contents":"We define:\n\\begin{equation} \\mathbb{F}^{\\infty} = \\{(x_1, x_2, \\dots): x_{j} \\in \\mathbb{F}, \\forall j=1,2,\\dots\\} \\end{equation}\nclosure of addition We define addition:\n\\begin{equation} (x_1,x_2,\\dots)+(y_1,y_2, \\dots) = (x_1+y_1,x_2+y_2, \\dots ) \\end{equation}\nEvidently, the output is also of infinite length, and as addition in \\(\\mathbb{F}\\) is closed, then also closed.\nclosure of scalar multiplication We define scalar multiplication:\n\\begin{equation} \\lambda (x_1,x_2, \\dots) = (\\lambda x_1, \\lambda x_2, \\dots ) \\end{equation}\nditto. as above\ncommutativity extensible from commutativity of \\(\\mathbb{F}\\)\nassociativity extensible from associativity of \\(\\mathbb{F}\\), for both operations\ndistribution \\begin{align} \\lambda ((x_1,x_2,\\dots)+(y_1,y_2, \\dots)) \u0026amp;= \\lambda (x_1+y_1,x_2+y_2, \\dots ) \\\\ \u0026amp;= (\\lambda (x_1+y_1),\\lambda (x_2+y_2), \\dots ) \\\\ \u0026amp;= (\\lambda x_1+\\lambda y_1,\\lambda x_2+\\lambda y_2, \\dots) \\\\ \u0026amp;= (\\lambda x_1, \\lambda x_2, \\dots) + (\\lambda y_1, \\lambda y_2, \\dots) \\\\ \u0026amp;= \\lambda (x_1, x_2, \\dots) + \\lambda (y_1, y_2, \\dots) \\end{align}\nditto. for the other direction.\nadditive ID \\begin{equation} (0,0, \\dots ) \\end{equation}\nadditive inverse extensive from \\(\\mathbb{F}\\)\n\\begin{equation} (-a, -b, \\dots ) + (a,b, \\dots ) = 0 \\end{equation}\nscalar multiplicative ID \\(1\\)\n","permalink":"https://www.jemoka.com/posts/kbhfinfty_is_a_vector_space_over_f/","tags":null,"title":"Finfinity is a Vector Space over F"},{"categories":null,"contents":"The Finite Difference Method is a method of solving partial Differential Equations. It follows two steps:\nDevelop discrete difference equations for the desired expression Algebraically solve these equations to yield stepped solutions https://www.youtube.com/watch?v=ZSNl5crAvsw\nFollow Along We will try to solve:\n\\begin{equation} \\pdv{p(t,x)}{t} = \\frac{1}{2}\\pdv[2]{p(t,x)}{x} \\end{equation}\nTo aid in notation, let us:\n\\begin{equation} p(t_{i}, x_{j}) := p_{i,j} \\end{equation}\nto represent one distinct value of our function \\(p\\).\nLet\u0026rsquo;s begin by writing our expression above via our new notation:\n\\begin{equation} \\pdv{p_{i,j}}{t}= \\frac{1}{2} \\pdv[2]{p_{i,j}}{x} \\end{equation}\nGreat. Now, let\u0026rsquo;s think about the left side and try to turn it into a difference eqn:\nWhat exactly is\u0026mdash;\n\\begin{equation} \\pdv{p_{i,j}}{t} \\end{equation}\nas a finite difference? Well, it is just:\n\\begin{equation} \\frac{p_{i+1,j}-p_{i,{j}}}{\\Delta t} \\end{equation}\nWhat about second partials?\nWell, what is\u0026mdash;\n\\begin{equation} \\pdv[2]{p_{i,j}}{x} \\end{equation}\nIt is:\n\\begin{equation} \\frac{\\pdv{p_{i,j+1}}{x}- \\pdv{p_{i,j}}{x}}{\\Delta x} \\end{equation}\nExpanding the top expressions even more difference expressions:\n\\begin{equation} \\frac{\\frac{p_{i,{j+2}}-p_{i,{j+1}}}{\\Delta x}- \\frac{p_{i,{j+1}}-p_{i,{j}}}{\\Delta x}}{\\Delta x} \\end{equation}\nThis equals to:\n\\begin{equation} \\frac{\\frac{p_{i,{j+2}}-p_{i,{j+1}} - p_{i,{j+1}}+p_{i,{j}}}{(\\Delta x)^{2}} \\end{equation}\nFinally, substitute this into our expression, then solve for some \\(p_{{i+1}, j}\\) in terms of \\(p_{i, ?}\\). We will treat the entire \u0026ldquo;row\u0026rdquo; of \\(p_{i,?}\\) as our initial condition, then solve for the rest + propagate forward.\n","permalink":"https://www.jemoka.com/posts/kbhfinite_difference_method/","tags":null,"title":"Finite Difference Method"},{"categories":null,"contents":"A graph of states which is closed and connected.\nAlso relating to this is a derived variable. One way to prove reaching any state is via Floyd\u0026rsquo;s Invariant Method.\n","permalink":"https://www.jemoka.com/posts/kbhfinite_state_machine/","tags":null,"title":"Finite State Machine"},{"categories":null,"contents":"A finite-dimensional vector space is a vector space where some actual list (which remember, has finite length) of vectors spans the space.\nAn infinite-demensional vector space is a vector space that\u0026rsquo;s not a finite-dimensional vector space.\nadditional information every finite-dimensional vector space has a basis Begin with a spanning list in the finite-dimensional vector space you are working with. Apply the fact that all spanning lists contains a basis of which you are spanning. Therefore, some elements of that list form a basis of the finite-dimensional vector space you are working with. \\(\\blacksquare\\)\nfinite-dimensional subspaces finite-dimensional subspaces\n","permalink":"https://www.jemoka.com/posts/kbhfinite_dimensional_vector_space/","tags":null,"title":"finite-dimensional vector space"},{"categories":null,"contents":"Fireside Chats are a group of broadcasts by Franklin D. Roosevelt (FDR) which allowed him to speak directly to the people.\n","permalink":"https://www.jemoka.com/posts/kbhfireside_chats/","tags":null,"title":"Fireside Chats"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhflexua/","tags":null,"title":"flexua"},{"categories":null,"contents":"To prove properties on Finite State Machines, we can construct a proof:\nstating an invariant proving that the invarient is true for all states for all transitions: assume invarient is true before transition and prove that its true after So, essentially induction.\n","permalink":"https://www.jemoka.com/posts/kbhfloyd_s_invariant_method/","tags":null,"title":"Floyd's Invariant Method"},{"categories":null,"contents":"Abstract Alzheimer\u0026rsquo;s Disease (AD) is a demonstrativeness disease marked by declines in cognitive function. Despite early diagnoses being critical for AD prognosis and treatment, currently accepted diagnoses mechanisms for AD requires clinical outpatient testing with a medical professional, which reduces its accessibility. In this work, we propose a possible feature extraction mechanism leveraging the previously demonstrated errors of Hidden Markov-based forced alignment (FA) tools upon cognitively impaired patients as an automated means to quantify linguistic disfluency.\nBackground Annotated linguistic disfluency features, used in combination with semantic features, have been shown ((Antonsson et al. 2021)) to improve the accuracy of AD classification systems. However, manual annotation of disfluency hinders the throughput of AD detection systems. Furthermore, there is a dearth ((Guo et al. 2021)) of data provided with preexisting annotated results.\nExisting acoustic-only approaches ((Lindsay, Tröger, and König 2021; Shah et al. 2021)) frequently places focus on the actual speech features such as silence, energy, rate, or loudness. While this approach has returned promising results ((Wang et al. 2019)), it renders the acoustic data features extracted independent of actual linguistic disfluency. Of course, some approaches (including that in (Wang et al. 2019)) perform separate, manual annotation on both aspects and treat them jointly with late fusion. However, no existing approaches have an effective feature representation that bridges the acoustic-linguistic gap.\nAn incidental effect of Hidden Markov Model (HMM) based Viterbi forced alignment (FA) tools (such as P2FA) is that its quality is shown ((Saz et al. 2009)) to be lowered in cognitively impaired speakers, resulting from a roughly \\(50\\%\\) decrease in power of discrimination between stressed and unstressed vowels. Other ASR and FA approaches ((Tao, Xueqing, and Bian 2010)) has since been designed discriminate against such changes more effectively.\nProposal By encoding FA results of HMM based approaches in embedding space, we introduce a novel feature representation of acoustic information. As FA requires an existing transcript, this method is considered semi-automated because the test must be either administered via a common-transcript, transcribed manually later, or transcribed using ASR techniques. After encoding, the proposed feature can be used in a few ways.\nEuclidean distance The Euclidean Distance approach compares the embedding of the HMM FA vector with a \u0026ldquo;reference\u0026rdquo; benchmark via pythagoras in high dimension.\nThere are two possible modalities by which the \u0026ldquo;reference\u0026rdquo; can be acquired; if the data was sourced via the patient sample reading a standardized transcript, a reference FA sample could be provided via the audio of another individual reading the same transcript screened traditionally screened without AD. Therefore, the \u0026ldquo;deviation from reference\u0026rdquo; would be used as an input feature group to any proposed model architectures.\nAlternatively, as stated before, other FA approaches are less susceptible to lexical hindrances with decreased discriminatory power. Therefore, we could equally take the Euclidean distance between embedded results of two different FA mechanisms\u0026mdash;one shown to be more sustainable to cognitively impaired speakers and one not\u0026mdash;as input features to training architectures.\nCross-Attention One key issue with the Euclidean Distance approach is that the difference between \u0026ldquo;normal\u0026rdquo; pauses, changes in speaker pace, etc. which would be variable between different speakers even controlling for AD prognoses.\nIn computer vision, few-shot classification cross-attention ((Hou et al. 2019)) has shown promising results in discrimination; furthermore, trainable cross-attention ensures more flexible control to non-prognostic verbal disturbances such as a normal change in pace which would otherwise cause a large difference in the Euclidean Distance approach.\nIn practice, a model similar to that proposed by ((Hou et al. 2019)) would be used as the basis to encode (or even discriminate) between pairwise samples of different FA approaches or against a non-AD control, as per highlighted in the section above.\nAs input features Of course, the raw FA embedding can be used as an input feature. There are less prior work on this front as this project would be, as far as we know, proposing the use of forced aligner outputs as a feature input heuristic.\nReferences Antonsson, Malin, Kristina Lundholm Fors, Marie Eckerström, and Dimitrios Kokkinakis. 2021. “Using a Discourse Task to Explore Semantic Ability in Persons with Cognitive Impairment.” Frontiers in Aging Neuroscience 12 (January): 607449. doi:10.3389/fnagi.2020.607449. Guo, Yue, Changye Li, Carol Roan, Serguei Pakhomov, and Trevor Cohen. 2021. “Crossing the ‘Cookie Theft’ Corpus Chasm: Applying What BERT Learns from Outside Data to the ADReSS Challenge Dementia Detection Task.” Frontiers in Computer Science 3 (April): 642517. doi:10.3389/fcomp.2021.642517. Hou, Ruibing, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. 2019. “Cross Attention Network for Few-Shot Classification.” Advances in Neural Information Processing Systems 32. Lindsay, Hali, Johannes Tröger, and Alexandra König. 2021. “Language Impairment in Alzheimer’s Disease—Robust and Explainable Evidence for AD-Related Deterioration of Spontaneous Speech through Multilingual Machine Learning.” Frontiers in Aging Neuroscience 13 (May): 642033. doi:10.3389/fnagi.2021.642033. Saz, Oscar, Javier Simón, W Ricardo Rodr\\’ıguez, Eduardo Lleida, and Carlos Vaquero. 2009. “Analysis of Acoustic Features in Speakers with Cognitive Disorders and Speech Impairments.” Eurasip Journal on Advances in Signal Processing 2009. Springer: 1–11. Shah, Zehra, Jeffrey Sawalha, Mashrura Tasnim, Shi-ang Qi, Eleni Stroulia, and Russell Greiner. 2021. “Learning Language and Acoustic Models for Identifying Alzheimer’s Dementia from Speech.” Frontiers in Computer Science 3 (February): 624659. doi:10.3389/fcomp.2021.624659. Tao, Ye, Li Xueqing, and Wu Bian. 2010. “A Dynamic Alignment Algorithm for Imperfect Speech and Transcript.” Computer Science and Information Systems 7 (1): 75–84. doi:10.2298/CSIS1001075T. Wang, Tianqi, Chongyuan Lian, Jingshen Pan, Quanlei Yan, Feiqi Zhu, Manwa L. Ng, Lan Wang, and Nan Yan. 2019. “Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese.” In Interspeech 2019, 3880–84. ISCA. doi:10.21437/Interspeech.2019-2414. ","permalink":"https://www.jemoka.com/posts/kbhdementiabank_acoustics_project_proposal/","tags":null,"title":"Forced-Alignment Error for Feature Extraction for Acoustic AD Detection"},{"categories":null,"contents":"FDR is an American president.\nFDR and Teddy Roosevelt is Got Polio, which played in his favor =\u0026gt; press agree to not photograph him when he was in a wheelchair Created the New Deal Models himself after his cousin Teddy Roosevelt, and believed that charisma and moral leadership work. \u0026ldquo;Above all, try something\u0026hellip; let the court shoot it if need to.\u0026rdquo;\nHe was able to gain single party control, wh.\nCreated Fireside Chats.\nHis wife, Eleanor Roosevelt, was very controversial.\nlegacy of FDR Never spent enough to end the depression Expanded government regulation, government size, and social welfare Modernization of presidency: sets agenda, initiates legislation Realigned the democratic party (created the progressive democrats) Maintained democracy \u0026lt;=== compared to Authoritarianism ","permalink":"https://www.jemoka.com/posts/kbhfdr/","tags":null,"title":"Franklin D. Roosevelt (FDR)"},{"categories":null,"contents":"Saltwater economists are economists from coastal schools that are mostly classical Keynsians\nFreshwater economists are economists who are mostly Neoclassical Economists\n","permalink":"https://www.jemoka.com/posts/kbhfreshwater_economists/","tags":null,"title":"Freshwater economists"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhfunction/","tags":null,"title":"function"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhfunctor/","tags":null,"title":"functor"},{"categories":null,"contents":"If your style is not math driven, fundamental investing are the strategies you can use.\nBTW: the game of investing in the stock market has gotten harder because we have too much data with the historical data. (Efficient) markets tend to eliminate opportunities to make a profit.\nLooking at 300 banks would be a good idea.\nValue Investing Value Investing is a fundamental investing strategy to value each company by going through the accounting/books of the company and buying companies that are theoretically undervalued.\nDistressed Investing Distressed Investing is the extreme version of Value Investing. Buy something that has a bad asset, a bad structure, and a bad holders: buy things when \u0026ldquo;what do they care what price they sell at\u0026rdquo;, which means you may be able to buy it at less than its worth.\nGrowth Investing Growth Investing is a fundamental investing strategy to bet in the future growth of a company given its performance and technology: Tesla, Teledoc, etc.\nQuality Investing Quality Investing is a fundamental investing strategy to buy stocks even despite high prices that has the most dependable market share: Coke, P\u0026amp;G, etc.\nSecond-Level Thinking A meta-level way of looking at decisions: \u0026ldquo;how many people know what I know.\u0026rdquo; Your strategy has to be both 1) DIFFERENT and 2) BETTER than what other people are doing.\n\u0026ldquo;The correctness of a decision cannot be judged by the outcome.\u0026rdquo;\n","permalink":"https://www.jemoka.com/posts/kbhfundimental_investing/","tags":null,"title":"fundamental investing"},{"categories":null,"contents":"The dimension of the null space plus the dimension of the range of a Linear Map equals the dimension of its domain.\nThis also implies that both the null space (but this one\u0026rsquo;s trivial b/c the null space is a subspace of the already finite-dimensional domain) and the range as well is finite-dimensional.\nconstituents \\(T \\in \\mathcal{L}( V,W )\\) finite-dimensional \\(V\\) (otherwise commenting on computing its dimension doesn\u0026rsquo;t make sense) requirements \\begin{equation} \\dim V = \\dim null\\ T + \\dim range\\ T \\end{equation}\nfor \\(T \\in \\mathcal{L}(V,W)\\)\nproof We desire that \\(\\dim V = \\dim null\\ T + \\dim range\\ T\\) for \\(T \\in \\mathcal{L}(V,W)\\).\nLet us construct a basis of the null space of \\(T\\), \\(u_1, \\dots u_{m}\\). This makes \\(\\dim null\\ T = m\\).\nWe can extend this list to a basis of \\(V\\), the domain, with some vectors \\(v_1, \\dots v_{n}\\). This makes the \\(\\dim V = m+n\\).\nWe now desire that \\(\\dim range\\ T = n\\). We show this by showing \\(Tv_{1}, \\dots Tv_{n}\\) is a basis of \\(range\\ T\\).\nRecall that \\(u_1, \\dots u_{m}, v_1, \\dots v_{n}\\) is a basis of \\(V\\) the domain of \\(T\\). This means that any element that can go into \\(T\\) takes the shape of:\n\\begin{equation} v = a_1u_1+ \\dots +a_{m}u_{m} + b_{1}v_1 + \\dots + b_{n}v_{n} \\end{equation}\nRecall also that the definition of the range of \\(T\\) is that:\n\\begin{equation} range\\ T = \\{Tv: v \\in V\\} \\end{equation}\nTherefore, every element of the range of \\(T\\) takes the shape of \\(Tv\\): meaning:\n\\begin{equation} Tv = a_1Tu_1+ \\dots +a_{m}Tu_{m} + b_{1}Tv_1 + \\dots + b_{n}Tv_{n} \\end{equation}\nby additivity and homogeneity of Linear Maps.\nNow, \\(Tu_{j}=0\\), because each \\(u_{j}\\) is a basis (and so definitely at least an element of) the null space of \\(T\\). This makes the above expression:\n\\begin{equation} Tv = 0 + b_{1}Tv_1 + \\dots + b_{n}Tv_{n} = b_{1}Tv_1 + \\dots + b_{n}Tv_{n} \\end{equation}\nOk. Given that all elements of the range can be constructed by a linear combination of \\(Tv_{1} \\dots Tv_{n}\\), we declare that the list spans the range of \\(T\\). Notably, as \\(V\\) is finite-dimensional and \\(v_1, \\dots v_{n}\\) is a sublist of its basis, \\(n \u0026lt; \\infty\\) and so the range of \\(T\\) is also finite-dimensional.\nTo finish showing \\(Tv_{1}, \\dots, Tv_{n}\\) to be a basis of \\(range\\ T\\), we have to show that its linearly independent.\nSuppose:\n\\begin{equation} c_1Tv_{1} + \\dots + c_{n}Tv_{n} = 0 \\end{equation}\nBy homogeneity and additivity, we have that:\n\\begin{equation} T(c_1v_{1} + \\dots + c_{n}v_{n}) = 0 \\end{equation}\nthis makes \\(c_1v_1 + \\dots\\) a member of the null space of \\(T\\). Recall that \\(u_1, \\dots u_{m}\\) were a basis thereof, this means that the linear combination of \\(v_{j}\\) can be written as a linear combination of \\(u_{j}\\):\n\\begin{equation} c_1 v_1 + \\dots + c_{n}v_{n} = d_1 u_{1} + \\dots + d_{m} u_{m} \\end{equation}\nOf course, the list \\(u_1, \\dots u_{m}, v_1, \\dots v_{n}\\) is linearly independent as it is a basis of \\(V\\). This makes \\(c_{j}=d_{j}=0\\) (to see this, move all the \\(d_{j}u_{j}\\) to the left and apply definition of linear independence).\nWe have therefore shown that, given\n\\begin{equation} c_1Tv_{1} + \\dots + c_{n}Tv_{n} = 0 \\end{equation}\n\\(c_1 = \\dots = c_{n} =0\\), satisfying the definition of linear independence of the list of \\(Tv_{j}\\).\nHaving shown that \\(Tv_{j}\\) to be a linearly independent spanning list of \\(range\\ T\\), we can conclude that it is indeed a basis of \\(range\\ T\\).\nThis makes the \\(\\dim range\\ T = n\\), as desired. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhfundamental_theorem_of_linear_maps/","tags":null,"title":"fundamental theorem of linear maps"},{"categories":null,"contents":"fusion in machine learning is the process of adding features or encoding.\nlate fusion late fusion adds features together to a model in a multi-modal approach by first embedding the features separately\nearly fusion early fusion adds features together to a model in a multi-modal approach by concatenating the features first then embedding\n","permalink":"https://www.jemoka.com/posts/kbhfusion/","tags":null,"title":"fusion (machine learning)"},{"categories":null,"contents":"The GARCH model is a model for the heteroskedastic variations where the changes in variance is assumed to be auto correlated: that, though the variance changes, it changes in a predictable manner.\nIt is especially useful to\nGARCH 1,1 Conditional mean:\n\\begin{equation} y_{t} = x\u0026rsquo;_{t} \\theta + \\epsilon_{t} \\end{equation}\nThen, the epsilon parameter:\n\\begin{equation} \\epsilon_{t} = \\sigma_{t}z_{t} \\end{equation}\nwhere:\n\\begin{equation} z_{t} \\sim \\mathcal{N}(0,1) \\end{equation}\nand:\nconditional variance\n\\begin{equation} {\\sigma_{t}}^{2} = \\omega + \\lambda {\\sigma_{t-1}}^{2} + \\beta {\\sigma_{t-1}}^{2} \\end{equation}\nFinally, with initial conditions:\n\\begin{equation} w\u0026gt;0; \\alpha \u0026gt;0; \\beta \u0026gt;0 \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhgarch/","tags":null,"title":"GARCH"},{"categories":null,"contents":"The point of Gaussian elimination is to solve/identiy-ify a linear equation. Take, if you have a matrix expression:\n\\begin{equation} Ax = b \\end{equation}\nWe can apply \\(A^{-1}\\) to both side, we then have:\n\\begin{equation} A^{-1}Ax = A^{-1} b \\end{equation}\nApplying the definition of the identity:\n\\begin{equation} Ix = A^{-1}b \\end{equation}\nTherefore, to solve for some \\(A^{-1}\\), which would yield \\(x\\).\n","permalink":"https://www.jemoka.com/posts/kbhgaussian_elimination/","tags":null,"title":"Gaussian elimination"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgeneral_relativity/","tags":null,"title":"general relativity"},{"categories":null,"contents":"a hissyfight with the transformational generative syntax.\ngenerative semantics states that structure is in support of meaning, rather than the other way around that transformational generative syntax suggests.\nThis means that you need to first come up with a meaning then imbew the best structure to support the expression of that meaning.\nThis (along with distributed morphology) is the main opposition of the Lexicalist Hypothesis, and because proof for the existence of semantic primes, also the main opposition of the existence of semantic primes.\n","permalink":"https://www.jemoka.com/posts/kbhgenerative_semantics/","tags":null,"title":"generative semantics"},{"categories":null,"contents":" A genetic algorithm is a search heuristic that is inspired by Charles Darwin\u0026rsquo;s theory of natural evolution.\nIts what Grey\u0026rsquo;s video says. The picking and chucking iterative thing.\n","permalink":"https://www.jemoka.com/posts/kbhgenetic_algorithum/","tags":null,"title":"genetic algorithm"},{"categories":null,"contents":"A Geometric Brownian Motion is a Brownian Motion with a drift.\nIt is determined by:\n\\begin{equation} \\dd{S_{t}} = \\mu S_{t} \\dd{t} + \\sigma \\dd{S_{t}} \\dd{W_{t}} \\end{equation}\nwhere, \\(S_{t}\\) is a Geometric Brownian Motion, \\(\\mu\\) is its drift, \\(\\sigma\\) the volatility, and \\(W_{t}\\) a centered Brownian Motion.\n","permalink":"https://www.jemoka.com/posts/kbhgeometric_brownian_motion/","tags":null,"title":"Geometric Brownian Motion"},{"categories":null,"contents":"(Py)Torch is a great C++/Python library to construct and train complex neural networks. It has taken over academia over the last few years and is slowly taking over industry. Let\u0026rsquo;s learn about how it works!\nThis document is meant to be read cover-to-cover. It makes NO SENSE unless read like that. I focus on building intuition about why PyTorch works, so we will be writing unorthodox code until the very end where we put all ideas together.\nThe chapters below take you through large chapters in a machine-learning journey. But, to do anything, we need to import some stuff which we will need:\nimport numpy as np import torch Autograd source\nI believe that anybody learning a new ML framework should learn how its differentiation tools work. Yes, this means that we should first understand how it works with not a giant matrix, but with just two simple variables.\nAt the heart of PyTorch is the built-in gradient backpropagation facilities. To demonstrate this, let us create two such variables.\nvar_1 = torch.tensor(3.0, requires_grad=True) var_2 = torch.tensor(4.0, requires_grad=True) (var_1, var_2) (tensor(3., requires_grad=True), tensor(4., requires_grad=True)) There is secretly a lot going on here, so let\u0026rsquo;s dive in. First, just to get the stickler out of the way, torch.tensor (used here) is the generic variable creator, torch.Tensor (capital!) initializes a proper tensor\u0026mdash;which you will never need.\nWhat is a tensor? A tensor is simply a very efficient matrix that can updates its own values dynamically but keep the same variable name. The above commands creates two such tensor, both being 1x1 matrices.\nNote that, for the initial values, I used floats! instead of ints. The above code will crash if you use ints: this is because we want the surface on which the matrix changes value to be smooth to make things like gradient descent to work.\nLastly, we have an argument requires_grad=True. This argument tells PyTorch to keep track of the gradient of the tensor. For now, understand this as \u0026ldquo;permit PyTorch to change this variable if needed.\u0026rdquo; More on that in a sec.\nNaturally, if we have two tensors, we would love to multiply them!\nvar_mult = var_1*var_2 var_mult tensor(12., grad_fn=\u0026lt;MulBackward0\u0026gt;) Wouldyalookatthat! Another tensor, with the value \\(12\\).\nNow. Onto the main event. Back-Propagation! The core idea of a neural network is actually quite simple: figure out how much each input parameter (for us var_1, var_2) influence the output, then adjust the inputs accordingly to get the output to be \\(0\\).\nTo see what I mean, recall our output tensor named:\nvar_mult tensor(12., grad_fn=\u0026lt;MulBackward0\u0026gt;) How much does changing var_1 and var_2, its inputs, influence this output tensor? This is not immediately obvious, so let\u0026rsquo;s write what we are doing out:\n\\begin{equation} v_1 \\cdot v_2 = v_{m} \\implies 3 \\cdot 4 = 12 \\end{equation}\nwith \\(v_1\\) being var_1, \\(v_2\\) being var_2, and \\(v_{m}\\) being var_mult.\nAs you vary var_1, by what factor does the output change? For instance, if var_1 (the \\(3\\)) suddenly became a \\(2\\), how much less will var_mult be? Well, \\(2\\cdot 4=8\\), the output is exactly \\(4\\) less than before less than before. Hence, var_1 influences the value of var_mult by a factor of \\(4\\); meaning every time you add/subtract \\(1\\) to the value of var_1, var_mult gets added/subtracted by a value of \\(4\\).\nSimilarly, as you vary var_2, by what factor does the output change? For instance, if var_2 (the \\(4\\)) suddenly became a \\(5\\), how much less will var_mult be? Well, \\(3\\cdot 3=5\\), the output is exactly \\(3\\) more than before less than before. Hence, var_2 influences the value of var_mult by a factor of \\(3\\); meaning every time you add/subtract \\(1\\) to the value of var_3, var_mult gets added/subtracted by a value of \\(3\\).\nThose of you who have exposure to Multi-Variable Calculus\u0026mdash;this is indeed the same concept as a partial derivative of var_mult w.r.t. var_1 and var_2 for the previous two paragraphs respectively.\nThese relative-change-units (\\(4\\) and \\(3\\)) are called gradients: the factor by which changing any given variable change the output.\nNow, gradient calculation is awfully manual! Surely we don\u0026rsquo;t want to keep track of these tiny rates-of-change ourselves! This is where PyTorch autograd comes in. Autograd is the automated tool that helps you figure out these relative changes! It is built in to all PyTorch tensors.\nIn the previous paragraphs, we figured out the relative influences var_1 and var_2 on var_multi. Now let\u0026rsquo;s ask a computer to give us the same result, in much less time.\nFirst, we will ask PyTorch to calculate gradients for all variables that contributed to var_mult.\nvar_mult.backward() The backward function is a magical function that finds and calculates these relative-change-values of var_multi with respect to every variable that contributed to its values. To view the actual relative values, we will use .grad now on the actual variables:\nvar_1.grad tensor(4.) Recall! We used our big brains to deduce above that changing var_1 by \\(1\\) unit will change var_mult by \\(4\\) units. So this works!\nThe other variables works as expected:\nvar_2.grad tensor(3.) Yayyy! Still what we expected.\nGradient Descent Relative changes are cool, but it isn\u0026rsquo;t all that useful unless we are actually doing some changing. We want to use our epic knowledge about the relative influences of var_1 and var_2, to manipulate those variables such that var_mult is the value we want.\nTHE REST OF THIS DOCUMENT IS IN CONSTRUCTION\nimport torch.optim as optim To start an optimizer, you give it all the variables for which it should keep track of updating.\noptim = torch.optim.SGD([var_1, var_2], lr=1e-2, momentum=0.9) And then, to update gradients, you just have to:\noptim.step() # IMPORTANT optim.zero_grad() What\u0026rsquo;s that zero_grad? That clears the gradients from the variables (after applying them with .step()) so that the next update doesn\u0026rsquo;t influence the current one.\nYour First Neural Network import torch.nn as nn Layers m = nn.Linear(20, 30) input = torch.randn(128, 20) output = m(input) output, output.size() Explain what the \\(20, 30\\) means.\nOk one layer is just lame. What if you want a bunch of layers?\nm1 = nn.Linear(20, 30) m2 = nn.Linear(30, 30) m3 = nn.Linear(30, 40) input = torch.randn(128, 20) # function call syntax! Functions call from rigth to left! output = m3(m2(m1(input))) output, output.size() And guess what? If you want to adjust the values here, you would just do:\nm1 = nn.Linear(20, 30) m2 = nn.Linear(30, 30) m3 = nn.Linear(30, 40) input = torch.randn(128, 20) # function call syntax! Functions call from rigth to left! output = m3(m2(m1(input))) (output.sum() - 12).backward() None But wait! What are the options you give to your optimizer?\noptim = torch.optim.SGD([m1.weight, m1.bias ... ... ], lr=1e-2, momentum=0.9) That\u0026rsquo;s a lot of variables!! Each linear layer has a \\(m\\) and a \\(b\\) (from \\(y=mx+b\\) fame), and you will end up with a bajillon one of those! Also, that function call syntax, chaining one layer after another, is so knarly! Can we do better? Yes.\nAn Honest-to-Goodness Neural Network PyTorch makes the module framework to make model creator\u0026rsquo;s lives easier. This is the best practice for creating a neural network.\nLet\u0026rsquo;s replicate the example above with the new module framework:\nclass MyNetwork(nn.Module): def __init__(self): # important: runs early calls to make sure that # the module is correct super().__init__() # we declare our layers. We don\u0026#39;t use them yet. self.m1 = nn.Linear(20,30) self.m2 = nn.Linear(30,30) self.m3 = nn.Linear(30,40) # this is a special function that is called when # the module is called def forward(self, x): # we want to pass our input through to every layer # like we did before, but now more declaritively x = self.m1(x) x = self.m2(x) x = self.m3(x) return x Explain all of this.\nBut now, we essentially built our entire network in own \u0026ldquo;layer\u0026rdquo; (actually we literally did, all =Layer=s are just =torch.Module=s) that does the job of all other layers acting together. To use it, we just:\nmy_network = MyNetwork() input = torch.randn(128, 20) # function call syntax! Functions call from rigth to left! output = my_network(input) output tensor([[-0.1694, 0.0095, 0.4306, ..., 0.1580, 0.2644, 0.1509], [-0.2346, -0.0269, -0.1191, ..., 0.0229, -0.0819, -0.1452], [-0.4871, -0.2868, -0.2488, ..., 0.0637, 0.1832, 0.0619], ..., [-0.1323, 0.2531, -0.1086, ..., 0.0975, 0.0426, -0.2092], [-0.4765, 0.1441, -0.0520, ..., 0.2364, 0.0253, -0.1914], [-0.5044, -0.3263, 0.3102, ..., 0.1938, 0.1427, -0.0587]], grad_fn=\u0026lt;AddmmBackward0\u0026gt;) But wait! What are the options you give to your optimizer? Surely you don\u0026rsquo;t have to pass my_network.m1.weight, my_network.m1.bias, etc. etc. to the optimizer, right?\nYou don\u0026rsquo;t. One of the things that the super().__init__() did was to register a special function to your network class that keeps track of everything to optimize for. So now, to ask the optimizer to update the entire network, you just have to write:\noptim = torch.optim.SGD(my_network.parameters(), lr=1e-2, momentum=0.9) optim SGD ( Parameter Group 0 dampening: 0 differentiable: False foreach: None lr: 0.01 maximize: False momentum: 0.9 nesterov: False weight_decay: 0 ) TODO make students recall original backprop example, backprope and step and zero_grad with this new optim.\nLook! Optimizing an entire network works in the exact same way as optimizing two lone variables.\nPutting it together TODO\ntraining loop (zero first, call model, get diff/loss, .backward(), .step()) best practices saving and restoring models GPU ","permalink":"https://www.jemoka.com/posts/kbhgetting_started_with_pytorch/","tags":["guide"],"title":"Getting Started with PyTorch"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgolden_gate_bridge/","tags":null,"title":"Golden Gate Bridge"},{"categories":null,"contents":"a:2:{i:0;s:2:\u0026ldquo;f2\u0026rdquo;;i:1;s:2:\u0026ldquo;f3\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;e2\u0026rdquo;;i:1;s:2:\u0026ldquo;e3\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;e1\u0026rdquo;;i:1;s:2:\u0026ldquo;e2\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;b2\u0026rdquo;;i:1;s:2:\u0026ldquo;b3\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;c2\u0026rdquo;;i:1;s:2:\u0026ldquo;d8\u0026rdquo;;}\n","permalink":"https://www.jemoka.com/posts/kbhgoogle_nerd_snipe/","tags":null,"title":"Google Nerd Snipe"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgorup/","tags":null,"title":"gorup"},{"categories":null,"contents":"Using constructor theory to test whether or not gravity in quantum theory is just entanglement.\nThis solves problem with gravity.\n","permalink":"https://www.jemoka.com/posts/kbhgravitational_entanglement/","tags":null,"title":"gravitational entanglement"},{"categories":null,"contents":"The Great Depression is a period of time of American depression.\n","permalink":"https://www.jemoka.com/posts/kbhgreat_depression/","tags":null,"title":"Great Depression"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgreedy_programming/","tags":null,"title":"greedy programming"},{"categories":null,"contents":"Participate in Demo Day.\nGetting something:\nOpportunity to get partnering Networking opportunities, having access to contract manufacturing =\u0026gt; Conrad Challenge, $600 each, $1200\nUser conversation\nSpoke again with CompassionKind: wanting to get 20 units shipped out Spoke with SustainableEnergy for All: started by one of the UN reps. of an African country; wanted to have us featured on Social Media Wanted to connect Start diving into user connections Hiring requests\nFulfilling orders MechE ","permalink":"https://www.jemoka.com/posts/kbhgreenswing_april_checkin/","tags":null,"title":"GreenSwing April Checkin"},{"categories":null,"contents":"In this experiment, an efficient and accurate network of detecting automatically disseminated (bot) content on social platforms is devised. Through the utilisation of parallel convolutional neural network (CNN) which processes variable n-grams of text 15, 20, and 25 tokens in length encoded by Byte Pair Encoding (BPE), the complexities of linguistic content on social platforms are effectively captured and analysed. With validation on two sets of previously unexposed data, the model was able to achieve an accuracy of around 96.6% and 97.4% respectively — meeting or exceeding the performance of other comparable supervised ML solutions to this problem. Through testing, it is concluded that this method of text processing and analysis proves to be an effective way of classifying potentially artificially synthesized user data — aiding the security and integrity of social platforms.\n","permalink":"https://www.jemoka.com/posts/kbhgregarious_abstract/","tags":null,"title":"Gregarious Abstract"},{"categories":null,"contents":"grid search is a hyperparameter tuning technique by trying pairs of all hyperparemeters sequentially\n","permalink":"https://www.jemoka.com/posts/kbhgrid_search/","tags":null,"title":"grid search"},{"categories":null,"contents":"components a set of constituent objects an operation requirements for group closed existence of identity existence of inverses associative additional information identity in group commutates with everything (which is the only commutattion in groups ","permalink":"https://www.jemoka.com/posts/kbhgroup/","tags":null,"title":"group"},{"categories":null,"contents":"The Guilded Age is a period in history between 1877 and 1900. This period deepened divide in racism, deepened the split between poor and rich, and the fluidity of American social classes became more set in this time.\nLinks to Organize Imperialism New American South Why is the \u0026ldquo;Guilded Age\u0026rdquo; \u0026ldquo;Guilded\u0026rdquo;? Guilded: Outside Lined with Gold, Inside Contains Metal and is Less Valuable.\nThe Guilded Age consists of three different sections:\nBusiness (Top!) Labour Government Contributors to the Guilded Age There are three pieces\n\u0026ldquo;Homestead Act\u0026rdquo;: legal way to give people land in the west \u0026ldquo;National Banking Act\u0026rdquo;: unified a uniform economic system, and to connect markets \u0026ldquo;Pacific Railroad Act\u0026rdquo;: expansion of connection; also formed the first \u0026ldquo;Corporations\u0026rdquo; based on railroad organization structures. Issues of the Guilded Age Immigration \u0026ldquo;They are coming to take our jobs!\u0026rdquo; (Irish Edition.)\nUSCIS processed people in Ellis (Irish processing, took about 2 days to process) and Angel (Chinese processing, took about 6 months to process) islands: beginning having racial immigrant discrimination.\nUrbanization Populations in the United States tripled in about 50 years. Immigrants were stuffed into Tennaments. The Guilded age saw the beginning of skyscrapers.\nSocial Activism Because of the issues began during the Guilded Age, more people essentially stepped instead of the governement to play a role in supporting welfare.\nIndustrialization \u0026ldquo;Pulling yourself up by your bootstraps.\u0026rdquo; Steel is invented. All of the technology has been moved to provide maximum output; this comes, of course, at an environmental cost. Large unions are starting to take off. Railroad Strike: federal troops fired upon railroad workers; argued the case for union but also for corp. influence on politics.\nPolitics Democrats: racist, states rights, limited federal government. Republicans: supported businesses, immigrations. Yes, they are still flipped.\nBut either way, democracy was rampant: 80% turnout! This is, however, one of the most corrupt time in American politics. The party system: local political bosses would provide favours for a vote \u0026mdash; in an absence of welfare, in exchange for a vote, wolud provide protection and social welfare. At this time, there was mass lack of reliance.\nCulture Victorianism! Proper manners, conservatism, etc. Bikes and contraceptives! There is also a fear that \u0026ldquo;manliness was declining\u0026rdquo;: that no more farming means need for more sports, body building, etc. Also, \u0026ldquo;name brands\u0026rdquo;, \u0026ldquo;sears catalogue\u0026rdquo;, and consumerism is taking hold.\nCorportization Corporations, as an idea, took hold. That the owners of a group is seperated by management, it allows the expansion of the size of companies. Monopolies in industries run while: concentrated wealth in addition to corrupted politics.\nTaylorism: Taylor decided to make a shovel for each type of movement \u0026mdash; which makes people repeat the same task over again but increased efficiency. \u0026ldquo;Taylor-made\u0026rdquo; comes from this.\nOmaha Platform Expanding Credit Bracketed income tax Social reforms Lays the groundwork for the progressive moment. This was a socialist movement!\nThe West Transcontinental railroad: power over towns and concessions Rise of Cowboys and \u0026ldquo;cattle bonanza\u0026rdquo; Prairies settled with new farming equipment and new Russian wheat strands: \u0026ldquo;Americanlization\u0026rdquo; The \u0026ldquo;turner thesis\u0026rdquo;: American democracy is formed at the frontier. However, Western Expansion is actually much of a tragedy, and this is actually leads to Imperialism.\nIndian Removal Policy of Indian removal to force into treaties + reservation Sioux Wars (crazy horse, etc.): Native American resistance Native Americans of California extreme violence; as well as slave labour Dawes Act of 1887 and forced \u0026ldquo;assimilation\u0026rdquo;: forced the breakup of many reservations Guilded Age Commentary Historians Rebekah Edwards The late 19th century was not entirely laissez faire \u0026ldquo;Progressive Era\u0026rdquo;: not always progressive Issues that lead to the \u0026ldquo;Guilded age\u0026rdquo; name that was not specific to the Guilded age \u0026ldquo;Guilded age\u0026rdquo;: \u0026ldquo;eh, nothing else to deal with, so let\u0026rsquo;s deal with racism!\u0026rdquo;\nRichard John Guilded age was a period of rapid industrialization Very charactured, unequal + vulgar time The resulting changes are very concentrated; all of the changes that are 80 years apart This is super disconnected to social, political aspects of life. It doesn\u0026rsquo;t talk about how the economy effects the social standings and ladders that people lived in =\u0026gt; that movement comes from a lot of social change.\nMade a point about the positive/negatives effects of the guilded age: don\u0026rsquo;t focus the individuals but instead the structures.\nHe did not want the \u0026ldquo;progressive era\u0026rdquo; as a classification in line with the guilded age. \u0026ldquo;Guilded age\u0026rdquo; is the only pejorative term for an era: so one negative description does not do it justice.\nRichard Benzel Richard Benzel claims that the textbook industry primes people; that a title for an age shoehorns the age and changes the reflection of the reader.\n","permalink":"https://www.jemoka.com/posts/kbhguilded_age/","tags":null,"title":"Guilded Age"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.642517\nOne-Liner Used WLS data to augment CTP from ADReSS Challenge and trained it on a BERT with good results.\nNovelty Used WLS data with CTP task to augment ADReSS DementiaBank data Notable Methods WLS data is not labeled, so authors used Semantic Verbal Fluency tests that come with WLS to make a presumed conservative diagnoses. Therefore, control data is more interesting:\nKey Figs Table 2 Data-aug of ADReSS Challenge data with WSL controls (no presumed AD) trained with a BERT. As expected the conservative control data results in better ferf\nNew Concepts ADReSS Challenge is small so use WLS to augment it ","permalink":"https://www.jemoka.com/posts/kbhguo_2021/","tags":["ntj"],"title":"Guo 2021"},{"categories":null,"contents":"Gut bacteria are both adversly affected by 5-Fluoropyrimidine, and but they mtaybe able to inactivate synthesized Fluoropyrimidine.\nPreTA in E. Coli is an example of a bacterial that can do this. See implications of PreTA deactivating Fluoropyrimidine.\n","permalink":"https://www.jemoka.com/posts/kbh5_fluoropyrimidine_maybe_inactivated_by_gut_microbiome/","tags":null,"title":"gut microbiome deactivating Fluoropyrimidine"},{"categories":null,"contents":"Hello Internet is a podcast hosted by Brady Haran and CGP Grey.\n","permalink":"https://www.jemoka.com/posts/kbhhello_internet/","tags":null,"title":"Hello Internet"},{"categories":null,"contents":"Herber Hoover is an American president.\nHerber Hoover\u0026rsquo;s response to the Great Depression Hoover\u0026rsquo;s Programs: too little, too late Makes business pledge to maintain wages, tax cuts, Smoot-halwey Tariff, bank financial support Builds Golden Gate Bridge and the Hoover Dam Rejects the idea of the direct federal relief, which is against FDR\u0026rsquo;s thoughts ","permalink":"https://www.jemoka.com/posts/kbhherber_hoover/","tags":null,"title":"Herber Hoover"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhheteroskedastic/","tags":null,"title":"heteroskedasticity"},{"categories":null,"contents":" Reading Date Notes New Deal Flip-book \u0026lt;2022-03-24 Thu\u0026gt; New Deal Historian Flipbook Legacy of McCarthyism \u0026lt;2022-04-25 Mon\u0026gt; Legacy of McCarthyism Soviet Perspective on the Cold War \u0026lt;2022-04-29 Fri\u0026gt; Soviet Perspective on Cold War MLK and Malcom X \u0026lt;2022-05-10 Tue\u0026gt; MLK and Malcom X Reading Origins of American Conservatism \u0026lt;2022-05-27 Fri\u0026gt; Origins of American Conservatism ","permalink":"https://www.jemoka.com/posts/kbhhistory_readings_index/","tags":["index"],"title":"History Readings Index"},{"categories":null,"contents":"Homestead Act is the legal colonization of the west that was a Contributor to the Guilded Age\n","permalink":"https://www.jemoka.com/posts/kbhhomestead_act/","tags":null,"title":"Homestead Act"},{"categories":null,"contents":"statistical context Homogeneity is a measure of how similar many things are.\nLinear Algebra context \u0026hellip;of linear maps homogeneity is a property of Linear Maps to describe the ability to \u0026ldquo;factor out\u0026rdquo; scalars\n\u0026hellip;of linear equations A homogenous linear equation is one which the constant term on the right of the equations are \\(0\\).\nhomogenous system with more variables than equations has nonzero solutions Proof: You can imagine the system as a matrix equation:\n\\begin{equation} Av = 0 \\end{equation}\nwhere, \\(v\\) is a list of input variables, and \\(A\\) is a coefficient matrix. Note that \\(A = \\mathbb{F}^{n} \\to \\mathbb{F}^{m}\\), where \\(n\\) is the number of variables, and \\(m\\) the number of equations.\nNow, the input variables \\(v\\) of the above expression is in the null space of \\(A\\). The question of \u0026ldquo;whether is there non-zero solutions\u0026rdquo; can be rephrased as given \\(Av=0\\), does \\(v=0\\)?\u0026quot; Otherwise known as \u0026ldquo;is \\(null\\ A=\\{0\\}\\)?\u0026rdquo;: that is, \u0026ldquo;is \\(A\\) injective?\u0026rdquo;\nGiven the fact that map to smaller space is not injective, if \\(m \u0026lt;n\\), the map is not going to be injective. Therefore, we want \\(m\u0026lt;n\\), meaning we want more variables (\\(n\\)) than equations (\\(m\\)) to have non-zero solutions.\ninhomogenous system with more equations than variables has no solutions for an arbitrary set of constants Proof: You can imagine the system as a matrix equation:\n\\begin{equation} Av = C \\end{equation}\nwhere, \\(v\\) is a list of input variables, and \\(A\\) is a coefficient matrix. Note that \\(A = \\mathbb{F}^{n} \\to \\mathbb{F}^{m}\\), where \\(n\\) is the number of variables, and \\(m\\) the number of equations.\nNow, a valid solution of the above expression means that \\(Av=C\\) for all \\(v\\) (as they are, of course, the variables.) If we want the expression to have a solution for all choices of \\(C\\), we desire that the range of \\(A\\) to equal to its codomain\u0026mdash;that we desire it to be surjective.\nGiven the fact that map to bigger space is not surjective, if \\(m \u0026gt; n\\), the map is not going to be surjective. Therefore, we want \\(m\u0026gt;n\\), meaning we want more equations (\\(m\\)) than variables (\\(n\\)) to have no solutions for arbitrary \\(C\\).\n","permalink":"https://www.jemoka.com/posts/kbhhomogeneity/","tags":null,"title":"homogeneity"},{"categories":null,"contents":"Honoré\u0026rsquo;s Statistic is a statistical measure of vocabulary complexity, it is a test of Semantic Verbal Fluency and is commonly used for cognitive impairment detection.\nThe statistic is defined as:\n\\begin{equation} HS = 100 \\log \\frac{N}{1-\\frac{N_{uni}}{U}} \\end{equation}\nwhere, \\(N\\) is the total number of words, \\(U\\) the total number of distinct words, \\(N_{uni}\\) the number of total distinct words used only once.\nThe idea here is that a higher diversity of vocabulary shows higher Semantic Verbal Fluency.\n","permalink":"https://www.jemoka.com/posts/kbhhonore_s_statistic/","tags":null,"title":"Honoré's Statistic"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhhoover_dam/","tags":null,"title":"Hoover Dam"},{"categories":null,"contents":"Hoovervile are homeless encampments named after Herber Hoover, where homeless people band together after loosing jobs in the Great Depression.\n","permalink":"https://www.jemoka.com/posts/kbhhooverviles/","tags":null,"title":"Hoovervile"},{"categories":null,"contents":"me\n","permalink":"https://www.jemoka.com/posts/kbhhoujun_liu/","tags":null,"title":"Houjun Liu"},{"categories":null,"contents":"A reading: (Krugman 2009)\nReflection The discussion here of the conflict between \u0026ldquo;saltwater\u0026rdquo; and \u0026ldquo;freshwater\u0026rdquo; (Keynesian and Neoclassical) economists is very interesting when evaluated from the perspective of our recent impending recession.\nOne particular statement that resonated with me in the essay was the fact that a crisis simply \u0026ldquo;pushed the freshwater economists into further absurdity.\u0026rdquo; It is interesting to see that, once a theory has been well-established and insulated in a community, it becomes much more difficult to parcel out as something that could be wrong.\nAs the same time, the forcibly-correcting \u0026ldquo;fudge\u0026rdquo; inconsistencies of the Keynesian model is also a strong weakness which perhaps further exacerbated the freshwater economists\u0026rsquo; dissent into their models. Modeling human behavior has been consistently quite messy, so it is unsurprising that both neoclassical and Keynesian economists strayed away from those models.\nCircling back to the COVID-trigger economic downturn: we definitely see a push towards increased \u0026ldquo;absurdity\u0026rdquo; in terms of increased polarization in the US; but not only that, the deeply rooted idea of \u0026ldquo;pandemics don\u0026rsquo;t affect the States\u0026rdquo; or at least \u0026ldquo;the Feds/our supply chain have preparation for absurd events\u0026rdquo; is again shown to be false\u0026mdash;despite the Obaman re-discovery of Keynesian management earlier.\nThis all raises a question: under what circumstances is a tangibly \u0026ldquo;better\u0026rdquo; result going to surface and be accepted when one model is tangibly perfect yet wrong, the other requiring flawed corrections or unrigorous analysis. Must we reject one model completely before the other one can be used?\nI don\u0026rsquo;t believe behavioral economics, though providing a partial solution as Krugman outlines, is the be-and-end-all of macroeconomic models during a depression. All of the models which were theorized (bar pure neoclassicalist \u0026ldquo;perfect agents\u0026rdquo;) ostensibly do one thing: trying to \u0026ldquo;rationally\u0026rdquo; model the \u0026ldquo;irrational\u0026rdquo; behavior of market participants. I don\u0026rsquo;t believe that this is ultimately going to be feasible on a macroeconomic scale to create models that will last (sans repeated, empirical testing\u0026mdash;but there are not enough depressions to go around.) Perhaps, then, the basic Keynesian idea of simply creating fiscal corrections may very well be the best second thing.\nReading notes the main problem was the fact that nobody saw a catastrophie coming More important was the profession’s blindness to the very possibility of catastrophic failures in a market economy.\npeople either believed that the market would never go wrong or the Fed fixes everything free-market economies never go astray and those who believed that economies may stray now and then but that any major deviations from the path of prosperity could and would be corrected by the all-powerful Fed.\nThe economists thought the humans are perfectly rational, and the fact that they are not is what leads to failures Unfortunately, this romanticized and sanitized vision of the economy led most economists to ignore all the things that can go wrong. They turned a blind eye to the limitations of human rationality that often lead to bubbles and busts\nKeynsian Economics was not trying to entirely replace markets Keynes did not, despite what you may have heard, want the government to run the economy. \u0026hellip; He wanted to fix capitalism, not replace it.\nMilton Friedman lead the return to Neoclassical Economics The neoclassical revival was initially led by Milton Friedman of the University of Chicago, who asserted as early as 1953 that neoclassical economics works well enough as a description of the way the economy actually functions\nNeoclassical Economics with the monetarist theory under Milton asserted that keeping the money supply growing is all that needed Monetarists asserted, however, that a very limited, circumscribed form of government intervention — namely, instructing central banks to keep the nation’s money supply, the sum of cash in circulation and bank deposits, growing on a steady path — is all that’s required to prevent depressions.\nMilton Freedman believes that large-scale expansion would lead to inflation and high unimployment excessively expansionary policies, he predicted, would lead to a combination of inﬂation and high unemployment\nAnti-Keynesian seniments overtook Freedman\u0026rsquo;s original proposition Eventually, however, the anti-Keynesian counterrevolution went far beyond Friedman’s position, which came to seem relatively moderate compared with what his successors were saying.\n#question why is this obvious? for obvious reasons\nBecause the new economists beliefed that the market is right, the advise was for business to max stock price ﬁnance economists believed that we should put the capital development of the nation in the hands of what Keynes had called a “casino.”\nMajor stock events didn\u0026rsquo;t blunt the disregard to Keynesian policy These events, however, which Keynes would have considered evidence of the unreliability of markets, did little to blunt the force of a beautiful idea.\nNew \u0026ldquo;perfect\u0026rdquo; economic models earned large respect in industry mild-mannered business-school professors could and did become Wall Street rocket scientists, earning Wall Street paychecks.\nNew models often analyzed financial systems independently of their real-world worth Finance economists rarely asked the seemingly obvious (though not easily answered) question of whether asset prices made sense given real-world fundamentals like earnings. Instead, they asked only whether asset prices made sense given other asset prices\nMacro split into two factions: the Keynes recessionists or the anti-Keynesians macroeconomics has divided into two great factions: “saltwater” economists (mainly in coastal U.S. universities), who have a more or less Keynesian vision of what recessions are all about; and “freshwater” economists (mainly at inland schools), who consider that vision nonsense.\nFreshwater economists\u0026rsquo; theory: recessions were just people confused? Nobel laureate Robert Lucas, argued that recessions were caused by temporary confusion: workers and companies had trouble distinguishing overall changes in the level of prices\nUnder freshwater theories, unemployment is just people electing not to work due to unfavorable environment ampliﬁed by the rational response of workers, who voluntarily work more when the environment is favorable and less when it’s unfavorable. Unemployment is a deliberate decision by workers to take time off.\n\u0026hellip;\nPut baldly like that, this theory sounds foolish — was the Great Depression really the Great Vacation?\nThe new Keysians still kept more or less to non-dramatic thinking They tried to keep their deviations from neoclassical orthodoxy as limited as possible. This meant that there was no room in the prevailing models for such things as bubbles and banking-system collapse.\nNew Keysians believed entirely in the Fed, without need for large fiscal policy They believed that monetary policy, administered by the technocrats at the Fed, could provide whatever remedies the economy needed.\nPeople just thought that there can\u0026rsquo;t be a bubble in housing What’s striking, when you reread Greenspan’s assurances, is that they weren’t based on evidence — they were based on the a priori assertion that there simply can’t be a bubble in housing.\nObama\u0026rsquo;s economic policies are much more on the Keynes side Such Keynesian thinking underlies the Obama administration’s economic policies — and the freshwater economists are furious.\nFailure of neoclassicalist theory is that breaking Keynsian economical behavior requires perfect rationality, which is absurd if you start from the assumption that people are perfectly rational and markets are perfectly efﬁcient, you have to conclude that unemployment is voluntary and recessions are desirable.\nEconomists thought that economics would have been perfect Economics, as a ﬁeld, got in trouble because economists were seduced by the vision of a perfect, frictionless market system.\nBehavioral Economics Behavioral Economics is a study of economics which hinges on the irrationality of human behavior. Its an answer to both the Neoclassical Economics\u0026rsquo; poor assumption that humans and markets are perfect, but also Keynsian Economics\u0026rsquo;s increasingly large need for a random \u0026ldquo;fudge\u0026rdquo; to get their models working right.\npillars of Behavioral Economics \u0026ldquo;Many real-world investors bear little resemblance to the cool calculators of efﬁcient-market theory: they’re all too subject to herd behavior, to bouts of irrational exuberance and unwarranted panic.\u0026rdquo; \u0026ldquo;even those who try to base their decisions on cool calculation often ﬁnd that they can’t, that problems of trust, credibility and limited collateral force them to run with the herd.\u0026rdquo; Good arbitrageurs are just forced out of the economy in large downward spirals As a result, the smart money is forced out of the market, and prices may go into a downward spiral.\n","permalink":"https://www.jemoka.com/posts/kbhhow_did_economists_get_it_so_wrong/","tags":null,"title":"How Did Economists Get It So Wrong?"},{"categories":null,"contents":"hypothesis testing is the mechanism by which a hypothesis is tested statistically.\nThe core logic of hypothesis testing: have a metric, do tests, calculate probability that the outcome could have happened given the metric is true.\nExamples include\nt-test (for sample means) z-test (for sample proportions) chi-square test (for sample categories) Common to all hypothesis tests are the following terms.\nnull hypothesis A null hypothesis is a \u0026ldquo;no difference\u0026rdquo; hypothesis created as a part of hypothesis testing. It is usually stated as an equality.\nalternative hypothesis The alternative hypothesis is the \u0026ldquo;new news\u0026rdquo; hypothesis created as a part of hypothesis testing, whereby the confirmation would introduce new information.\np-value the p-value of a hypothesis test is the probability of the results acquired taking place given if the null hypothesis. That is:\n\\begin{equation} p(\\hat{p} | H_0\\ true) \\end{equation}\nTo figure out the above probability, you could either simulate the occurrence and look at a histogram (more common for AP Statistics anyways) or measure a few other statistics. We will talk about them later.\nTo use p-value as a hypothesis test, the sample has to meet the conditions for inference.\nType I Error A Type I Error takes place when you reject the null hypothesis during hypothesis testing even while its true: i.e., a false positive.\nThe probability of having a Type I Error is the significance level of the test.\nType II Error A Type II Error takes place when you accept the null hypothesis during hypothesis testing even while its false.\nThe probability of having a Type II Error is the conjugate of the power of a test.\nsignificance level significance level is the level by which one would accept a p-value is being indicative of the success of a test. We usually use the letter \\(\\alpha\\) to denote this.\npower (statistics) power is a statistic calculable during hypothesis testing. Its the probability of rejecting the null hypothesis given the null hypothesis is false. Also known as the conjugate of the Type II Error.\npower increases as significance level increases, but then the probability of a Type I Error increases as well.\n","permalink":"https://www.jemoka.com/posts/kbhhypothesis_testing/","tags":null,"title":"hypothesis testing"},{"categories":null,"contents":"identities allows another number to retain its identity after an operation.\nWhat identities are applicable is group dependent. Identities are almost always object dependent.\n","permalink":"https://www.jemoka.com/posts/kbhidentity/","tags":null,"title":"identity"},{"categories":null,"contents":"\u0026lt;\u0026gt; NUS-HIST301 American History\nThe idea of identity politics is proposed, that politics became associated with sub-population of identities:\nBlack Pride Movement Chicano Activism The American Indian movement Termination of reservation system Pan-Indian Rights Alcatraz and Wounded Knee Occupations LGBT movement Stonewall GLF starts marching Asian American Yellow Peril Model minority movement NOW Femanism Acts The Equal Rights Act almost possible, and then Phyllis Schlafly happened Environmental Movement Silent Spring Cuyahoga River on fire Richard Nixon creates the EPA Earth Day ","permalink":"https://www.jemoka.com/posts/kbhactivism_during_the_1970s/","tags":null,"title":"identity politics"},{"categories":null,"contents":"to prove that something goes both ways: given \\(A\\Rightarrow B\\), and \\(A \\Leftarrow B\\), \\(A \\Leftrightarrow B\\).\n","permalink":"https://www.jemoka.com/posts/kbhequivalence/","tags":null,"title":"if and only if"},{"categories":null,"contents":"Imperialism: a policy of extending a country\u0026rsquo;s power and influence though diplomacy or military force.\nColonies Protectorate \u0026mdash; nations has own government legally controlled by outside power Sphere of influence U.S. Imperialism, why?\n\u0026ldquo;Desire for Military strength\u0026rdquo;: for a nation to be an international player, you have to have a strong navy \u0026ldquo;Thirst for new markets\u0026rdquo;: if we continue to expand, we will have more economic power \u0026ldquo;Belief in supernatural superiority\u0026rdquo;: trust that own culture is better Alaska \u0026mdash; \u0026ldquo;Seward\u0026rsquo;s Ice Box\u0026rdquo;, purchased from czarist Russia.\nHawaii \u0026mdash; Annexed 1898, a sugar company, to get around import taxes, asked the US to annex Hawaii.\nSpanish-American War \u0026mdash;- newspaper receive letter sent by Spanish minister to not protect Cuba. The US then proceeded to fight for the territories.\nFilipino rejected treaty of Paris, America fights. America burned food and crops to starve rebels, and built infrastructure earning elite support due to infrastructure.\n","permalink":"https://www.jemoka.com/posts/kbhimperialism/","tags":null,"title":"Imperialism"},{"categories":null,"contents":"The Inbox is an Inbox for mobile org captures.\n","permalink":"https://www.jemoka.com/posts/kbhinbox/","tags":null,"title":"Inbox"},{"categories":null,"contents":"Here\u0026rsquo;s a list of all indexes:\nProjects Index Research Index Production Index About This should be reflected on a fancier way on my home page.\n","permalink":"https://www.jemoka.com/posts/kbhindex_index/","tags":["index"],"title":"Index Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhinflectional_words/","tags":null,"title":"inflectional words"},{"categories":null,"contents":"Information Units are unique entities mentioned during an utterance; for a sentence like \u0026ldquo;There is a boy. The boy is a brother. He is stealing a cookie. The sister is watching.\u0026rdquo;, \u0026ldquo;boy, cookie, sister\u0026rdquo; are possible IUs.\n","permalink":"https://www.jemoka.com/posts/kbhiu/","tags":null,"title":"Information Units (Linguistics)"},{"categories":null,"contents":"An injective function is one which is one-to-one: that it maps distinct inputs to distinct outputs.\nconstituents A function \\(T: V \\to W\\) requirements \\(T\\) is injective if \\(Tu = Tv\\) implies \\(u=v\\).\nadditional information injectivity implies that null space is \\(\\{0\\}\\) Proof: let \\(T \\in \\mathcal{L}(V,W)\\); \\(T\\) is injective IFF \\(null\\ T = \\{0\\}\\).\ngiven injectivity Suppose \\(T\\) is injective.\nNow, we know that \\(0\\), because it indeed gets mapped by \\(T\\) to \\(0\\), is in the null space of \\(T\\).\nBecause linear maps take \\(0\\) to \\(0\\), \\(T0=0\\). Now, because \\(T\\) is injective, for any \\(v\\) that \\(Tv = 0 = T 0\\) implies \\(v=0\\).\nSo \\(0\\) is the only thing that an injective \\(T\\) can map to \\(0\\), and it is indeed in the null space, so the null space is just \\(\\{0\\}\\).\ngiven \\(null\\ T=\\{0\\}\\) Suppose we have some \\(Tu = Tv\\), we desire to proof that \\(u=v\\) to show that \\(T\\) is injective.\nGiven \\(Tu=Tv\\), we have that \\(Tu-Tv\\). Given additivity, \\(T(u-v) = 0\\). This makes \\((u-v) \\in\\ null\\ T\\).\nGiven only \\(0\\) is in the null space of \\(T\\), \\(u-v = 0\\), so \\(u=v\\), as desired. \\(\\blacksquare\\).\nmap to smaller space is not injective See map to smaller space is not injective\n","permalink":"https://www.jemoka.com/posts/kbhinjectivity/","tags":null,"title":"injectivity"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhinjectivity_implies_that_null_space_is_0/","tags":null,"title":"injectivity implies that null space is {0}"},{"categories":null,"contents":"insertion sort is an algorithm that solves the sorting problem.\nconstituents a sequence of \\(n\\) numbers \\(\\{a_1, \\dots a_{n}\\}\\), called keys\nrequirements Insertion sort provides an ordered sequence \\(\\{a_1\u0026rsquo;, \\dots a_{n}\u0026rsquo;\\}\\) s.t. \\(a_1\u0026rsquo; \\leq \\dots \\leq a_{n}\u0026rsquo;\\)\nimplementation I don\u0026rsquo;t know why, but it seems like CLRS\u0026rsquo; implementation is back-to font. But perhaps I\u0026rsquo;m just mistaken.\nvoid insertion_sort(int length, int *A) { for (int j=1; j\u0026lt;length; j++) { int key = A[j]; // insert the key correctly into the // sorted sequence, when appropriate int i = j-1; while (i \u0026gt; 0 \u0026amp;\u0026amp; A[i] \u0026gt; key) { // if things before had // larger key // move them A[i+1] = A[i]; // move it down // move our current value down i -= 1; } // put our new element into the correct palace A[i+1] = key; } } additional information proof We use loop invariant method to show that our algorithm is correct. Our invariant is that the array \\(A[0, \\dots, j-1]\\) is sorted \\(\\forall j 0 \\dots L+1\\).\nInitialization: at the first step, \\(j=1\\) (second element), the subarray of \\(A[0, \\dots j-1]\\) (namely, only the first element), is sorted trivially Maintenance: during each loop, we move \\(j\\) to the right, only being done when the subarray to the left is correctly sorted because of \\(j\\) is moving forward until length, it will terminate As \\(j\\), by the end, covers the entire loop, our loop terminates at \\(L+1\\) and invariant (sortedness) is maintained between \\(A[0, \\dots j]\\).\n","permalink":"https://www.jemoka.com/posts/kbhinsertion_sort/","tags":null,"title":"insertion sort"},{"categories":null,"contents":"an integer (\\(\\mathbb{Z}\\)) is the natural numbers, zero, and negative numbers: \u0026hellip;,-4,-3,-2,-1,0,1,2,2,3\n","permalink":"https://www.jemoka.com/posts/kbhinteger/","tags":null,"title":"integer"},{"categories":null,"contents":"The integrating factor \\(\\rho(x)\\) is a value that helps undo the product rule. For which:\n\\begin{equation} log(\\rho(x)) = \\int P(x)dx \\end{equation}\nfor some function \\(P(x)\\).\nSeparating the \\(\\rho(x)\\) out, we have therefore:\n\\begin{equation} e^{\\int P dx} = \\rho(x) \\end{equation}\nWhy is this helpful and undoes the product rule? This is because of a very interesting property of how \\(\\rho(x)\\) behaves.\n","permalink":"https://www.jemoka.com/posts/kbhintegrating_factor/","tags":null,"title":"integrating factor"},{"categories":null,"contents":"Goal We are going to solve the inter-temporal choice problem, for ten time stamps, and perform some numerical optimization of the results\nMain Methods We do this by solving backwards. We will create a variable \\(k\\) to measure asset, and \\(k_{t}\\) the remaining asset at time \\(t\\).\nLet us first declare the function for power utility. \\(k\\) is our asset holding, \\(\\gamma\\) our relative margin of risk, and \\(U\\) the power utility.\nThe power utility function is defined by:\n\\begin{equation} U( C) = \\frac{c^{1-\\gamma}-1}{1-\\gamma} \\end{equation}\nImplementing Power Utility # risk aversion y = var(\u0026#34;y\u0026#34;, latex_name=\u0026#34;\\gamma\u0026#34;, domain=\u0026#39;real\u0026#39;) # discount factor d = var(\u0026#34;d\u0026#34;, latex_name=\u0026#34;\\delta\u0026#34;, domain=\u0026#39;real\u0026#39;) # final value at time t=f k_f = var(\u0026#34;k_f\u0026#34;, latex_name=\u0026#34;k_f\u0026#34;, domain=\u0026#39;real\u0026#39;) # the instrument\u0026#39;s percentage return over a period: i.e. (1+mu)*I_t = k_{t+1} m = var(\u0026#34;m\u0026#34;, latex_name=\u0026#34;\\mu\u0026#34;, domain=\u0026#39;real\u0026#39;) # boundary conditions assume(y\u0026gt;0) assume(y\u0026lt;1) assume(d\u0026gt;0) assume(d\u0026lt;1) # power utility u(c) = ((c^(1-y)-1)/(1-y)) u c |--\u0026gt; -(c^(-y + 1) - 1)/(y - 1) End Boundary Conditions At the final time stamp, we desire to consume all of our assets. Therefore, we will seed our investment amount at \\(I=0\\). We will optimize for eventual global utility, therefore, we will talley our utility; starting this talley at \\(0\\).\n# at the final time, leave nothing for investment I=0; u_total = 0 Bottom-Up Dynamic Programming From every step from here, we will discount this utility by \\(d\\), then solve for the previous step\u0026rsquo;s target consumption that would maximize utility. That is, at every step, we desire:\n\\begin{equation} k_{t-1} = I_{t} + c_{t} \\implies c_{t} = k_{t-1}-I_{t} \\end{equation}\n\u0026ldquo;we want to consume all we have that needed\u0026rsquo;t to be invested\u0026rdquo;\nand\n\\(\\max u(c_{t})\\)\nRecall also that \\((1+\\mu)I_{t} = k_{t+1}\\) (as \\(\\mu\\) is the mean log-return, 1+that times \\(I\\), how much was invested at time \\(t\\), is the expected capital one time period from then.) Therefore, to make sure that \\(k_{f}\\) gets back in the final period, we solve for our seed value for \\(I\\), how much to invest would be:\n\\begin{equation} I_{t-1} = \\frac{k_t}{(1+m)} \\end{equation}\nActual implementation # create an dictionary to keep track of all the capital variables k = {} # we will iterate time stamps 1-10 T = 10 # a variable for captial at that time for i in range(T): k_t = var(f\u0026#34;k_{T-i}\u0026#34;, latex_name=f\u0026#34;k_{T-i}\u0026#34;) # t-i becasue we are solving backwards; i0 = T10 # what can be consumed at every time stamp # is the k of the previous timestamp, minus # what needs to be left over # we multiply here by d because we want to # discount future utility u_total = d*u_total + u(k_t-I) # add the current variable to dictionary k[T-i] = k_t # recall again i0=T10 because backwards # solve for the next investment amount I = k_t/(1+m) u_total -(((((((d*(d*(k_10^(-y + 1) - 1)/(y - 1) + ((k_9 - k_10/(m + 1))^(-y + 1) - 1)/(y - 1)) + ((k_8 - k_9/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_7 - k_8/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_6 - k_7/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_5 - k_6/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_4 - k_5/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_3 - k_4/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_2 - k_3/(m + 1))^(-y + 1) - 1)/(y - 1))*d - ((k_1 - k_2/(m + 1))^(-y + 1) - 1)/(y - 1) Optimization with some constants We can now use the scipy numerical optimizer to minimize this target. Recall that we can recover the actual value of consumption at each step as \\(c=k-\\frac{k}{m+1}\\).\nWe will set some initial conditions:\n_m = 0.01 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.8 # generally risk averse _d = 0.9 # the future matters slightly less Optimization Target Function Recall that the scipy optimizer MINIMIZES, so we will make the loss the negative of utility. Before we finally start, we need to make the actual, numerical loss function that performs the substitution.\nThe code is actually just doing some function substitution, so its not very exciting.\n# we reverse the k_* variables because it is stored in the dictionary # in reverse, because we knew the reverse condition first optim_variables = list(k.values()) optim_variables.reverse() # this function is also the callback, so it returning # True terminates execution def u_total_loss(x): # the optimizer\u0026#39;s current step # we want to take [1:], because we need to keep k1 the same at _k the # initial value substitution_dict = {key: val for key, val in zip(optim_variables[1:], x)} # initial conditions substitution_dict[m] = _m substitution_dict[y] = _y substitution_dict[d] = _d substitution_dict[d] = _d # we want to keep the initial value k1 the same substitution_dict[k[1]] = _k try: # get value content = (-1*u_total).subs(substitution_dict) # recall we multiply by -1 because we are MINIMIZING, so the loss is # the inverse of the maximization utility target return float(content.n()), False except: return 0, True Optimize! Finally, we are ready to start. We will now create the other initial conditions k1\u0026hellip;k10 and : we will set the initial value to all be 1000 (i.e. do nothing) and have the optimizer work it out from there:\nfrom scipy.optimize import minimize target = minimize(lambda x:u_total_loss(x)[0], [_k for _ in range(T-1)], callback=lambda x:u_total_loss(x)[1]) target fun: -50.71592850322347 hess_inv: array([[ 9518.97596212, 7617.14636381, 5964.42171873, 4433.87331935, 4253.91810669, 3528.72923763, 2329.61846616, 1769.85078017, 1126.51562458], ... [ 1126.51562458, 1359.86586059, 1639.19265606, 2255.16306648, 2598.27394651, 2293.45506703, 2657.8129831 , 2422.66762041, 2911.30717272]]) jac: array([ 0.00000000e+00, -3.81469727e-06, 2.38418579e-06, 0.00000000e+00, 8.58306885e-06, -5.24520874e-06, 2.86102295e-06, -1.43051147e-06, -5.24520874e-06]) message: \u0026#39;Optimization terminated successfully.\u0026#39; nfev: 1360 nit: 130 njev: 136 status: 0 success: True x: array([841.22097906, 699.82556541, 573.89912346, 461.63474591, 361.51493714, 272.10309839, 192.29084196, 120.94057011, 57.12129925]) Recovering Actual Dollar Consumption Amount Awesome! We now can recover \\(c\\) at each point by a nice helpful function:\nc(k0, k1) = k0 - k1/(_m+1) \u0026ldquo;Consumption is how much we have, minus how much we would be investing\u0026rdquo;\nSo, let us translate our list to the actual values consumed:\ncapital_over_time = [_k]+target.x.tolist() # we need to add the initial condition _k back to the # inventory list consumption_over_time = [c(i,j) for i,j in zip(capital_over_time, capital_over_time[1:])] consumption_over_time [167.107941529699, 148.324379643989, 131.608611479784, 116.835018601197, 103.699164584812, 92.1059288329209, 81.7161261514775, 72.5477032414004, 64.3848282779261] Examples of Output The next set of slides show examples of possible optimization outputs\u0026mdash;how decisions by the inter-temporal choice problem changes based on the inputs.\nRisk Averse _m = 0.01 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.8 # generally risk averse _d = 0.9 # the future matters slightly less [167.107941529699, 148.324379643989, 131.608611479784, 116.835018601197, 103.699164584812, 92.1059288329209, 81.7161261514775, 72.5477032414004, 64.3848282779261] More Return Winning Game _m = 0.1 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.8 # generally risk averse _d = 0.9 # the future matters slightly less [154.860597149863, 152.989432556196, 151.010433069881, 149.201249715528, 147.329750167852, 145.539019666462, 143.739371599600, 141.984228587213, 140.243839963791] More Risk _m = 0.01 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.2 # generally risky _d = 0.9 # the future matters slightly less [388.525041338376, 241.124420093987, 149.632568775223, 92.8644259086613, 57.6330459746870, 35.7667230511026, 22.1970017374152, 13.7754327365677, 8.54930907023498] Loosing Game _m = -0.01 # this is a loosing stock _k = 1000 # $1000 capital _y = 0.9 # very safe _d = 0.9 # the future matters fun: 0 hess_inv: array([[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1]]) jac: array([0., 0., 0., 0., 0., 0., 0., 0., 0.]) message: \u0026#39;Optimization terminated successfully.\u0026#39; nfev: 10 nit: 0 njev: 1 status: 0 success: True x: array([1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.]) Evidently: do nothing if we have a loosing cause.\nWinning Game _m = 1.00 # this is SUPER winning stock _k = 1000 # $1000 capital _y = 0.9 # very safe _d = 0.9 # the future matters [125.667556437602, 241.474827418105, 460.068836905327, 868.972817783791, 4540.45893314523, 4219.93058738029, 3988.05775624984, 3996.89431939885, 3615.74982832315] We made so much money that we are spending a lot of it and still spending it.\n","permalink":"https://www.jemoka.com/posts/risk_apetite_preso/","tags":null,"title":"Inter-Temporal Choice"},{"categories":null,"contents":" psycoacoustics ","permalink":"https://www.jemoka.com/posts/kbhintersession_2023/","tags":null,"title":"Intersession 2023"},{"categories":null,"contents":"invariant subspaces are a property of operators; it is a subspace for which the operator in question on the overall space is also an operator of the subspace.\nconstituents an operator \\(T \\in \\mathcal{L}(V)\\) a subspace \\(U \\subset V\\) requirements \\(U\\) is considered invariant on \\(T\\) if \\(u \\in U \\implies Tu \\in U\\)\n(i.e. \\(U\\) is invariant under \\(T\\) if \\(T |_{U}\\) is an operator on \\(U\\))\nadditional information nontrivial invariant subspace (i.e. eigenstuff)\nA proof is not given yet, but \\(T \\in \\mathcal{L}(V)\\) has an invariant subspace that\u0026rsquo;s not \\(V\\) nor \\(\\{0\\}\\) if \\(\\dim V \u0026gt; 1\\) for complex number vector spaces and \\(\\dim V \u0026gt; 2\\) for real number vector spaces.\n","permalink":"https://www.jemoka.com/posts/kbhinvariant_subspace/","tags":null,"title":"invariant subspace"},{"categories":null,"contents":"the inverse is the the opposite of an operation. As in, if you apply the inverse of an operation to the result of applying the original with the same operation it will cancel it.\nThat is,\n\\begin{equation} A * B * B^{-1} = A \\end{equation}\n\\(B^{-1}\\) is then the inverse of \\(B\\) for the \\(*\\) operation. This is operation dependent.\n","permalink":"https://www.jemoka.com/posts/kbhinverses/","tags":null,"title":"inverse"},{"categories":null,"contents":"A Linear Map is invertable if it can be undone. It is called a nonsingular matrix\nconstituents A linear map \\(T \\in \\mathcal{L}(V,W)\\)\nrequirements A Linear Map \\(T \\in \\mathcal{L}(V,W)\\) is called invertable if \\(\\exists T^{-1} \\in \\mathcal{L}(W,V): T^{-1}T=I \\in \\mathcal{L}(V), TT^{-1} = I \\in \\mathcal{L}(W)\\).\n\u0026ldquo;a map is invertable if there is an inverse\u0026rdquo;: that combining the commutable inverse and itself will result in the identity map.\nadditional information matrix invertability Matrices whose determinants are not \\(0\\) (i.e. it is invertable) is called \u0026ldquo;nonsingular matrix\u0026rdquo;. If it doesn\u0026rsquo;t have an inverse, it is called a singular matrix.\nlinear map inverse is unique An invertable Linear Map has an unique inverse:\nProof:\nSuppose \\(T \\in \\mathcal{L}(V,W)\\), and \\(\\exists S_1, S_2\\) which are both inverses of \\(T\\). We desire \\(S_1=S_2\\).\nSo:\n\\begin{equation} S_1 = S_1(TS_2) = (S_1T)S_2 = IS_{2} = S_2 \\end{equation}\ngiven Product of Linear Maps is associative.\n\\(S_1=S_2\\), as desired. \\(\\blacksquare\\)\ninjectivity and surjectivity implies invertability Suppose \\(T \\in \\mathcal{L}(V,W)\\); we desire that \\(T\\) is invertable IFF it is both injective and surjective.\nFirst, suppose \\(T\\) is invertible; that is, \\(\\exists T^{-1}: T^{-1}T=I, TT^{-1}=I\\) We desire that \\(T\\) is both injective and surjective.\nInjectivity: Suppose \\(Tv=Tu\\); we desire \\(u=v\\). \\(u = T^{-1}(Tu) = T^{-1}(Tv) = v\\) . We essentially to use the fact that \\(T^{-1}\\) is a function to \u0026ldquo;revert\u0026rdquo; the map of \\(T\\); as \\(T^{-1}\\) is a map, we know it has to revert to the same result.\nSurjectivity: Recall \\(T: V\\to W\\). WLOG let \\(w \\in W\\), \\(w=T(T^{-1}w)\\). Therefore, all \\(w\\) is in range of \\(T\\).\nSecond, suppose \\(T\\) is both injective and surjective. Define a transition \\(S\\) such that \\(T(Sw) = w\\) for all \\(w \\in W\\) (i.e. it hits just the right element to hit \\(w\\) as an input of \\(T\\).) This is made possible because \\(T\\) is surjective (because you can hit all \\(W\\)) and injective (which makes \\(S\\) not need to hit two different things or have two non-equal things accidentally map to the same thing.)\nEvidently, \\(T(Sw)=w \\forall w \\in W \\implies (TS) = I\\) by definition.\nWe now desire \\(ST = I\\). We have \\((TSTv) = (TS)(Tv) = ITv = Tv\\) by associativity of map multiplication. Now, \\((TSTv) = Tv \\implies T(ST)v = Tv\\) by associativity again. This implies that \\((ST)v=v\\) again because \\(T\\) is injective: so the same input will not produce two unique outputs.\nWe then can show \\(S\\) is a linear map in the usual way.\nHaving constructed the desired result, \\(\\blacksquare\\)\nAlternate Proof for Finite Dimensional \\(T\\) So given map to bigger space is not surjective and map to smaller space is not injective, we have that the dimension of \\(W = V\\), we leverage the basis of each and build the using the basis of domain.\n","permalink":"https://www.jemoka.com/posts/kbhinvertability/","tags":null,"title":"invertability"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhiob/","tags":null,"title":"iob"},{"categories":null,"contents":"irrational numbers are real numbers that are not rational numbers.\nFormally:\n\\begin{equation} \\mathbb{C} = \\mathbb{R} \\backslash \\mathbb{Q} \\end{equation}\nwhere, \\(\\backslash\\) is subtracting two sets.\n","permalink":"https://www.jemoka.com/posts/kbhirrational_number/","tags":null,"title":"irrational number"},{"categories":null,"contents":"An isomorphism is an invertable Linear Map. Two vector spaces are called isomorphic if there is an isomorphism from one to another.\n\u0026ldquo;A linear map that maintain the correct structure of the structure.\u0026rdquo;\nThis makes the vector spaces that are isomorphic \u0026ldquo;equivalent\u0026rdquo;, because the isomorphism is the equivalence relationship. Of course, they are still not equal.\nGenerally, isomorphisms can only be built between vector spaces over the same field.\nadditional information matrices We know we can represent Linear Maps as matricies.\nSo, given some \\(A\\), we have an inverse \\(A^{-1}\\).\nSo:\n\\begin{equation} A A^{-1} = I = A^{-1} A \\end{equation}\nIn this case, the \\(I\\) is the identity map: \\(Iv = v\\).\ntwo vector spaces are isomorphic IFF they have the same dimension note: this relationship works over the SAME field \\(\\mathbb{F}\\), otherwise lin comb can\u0026rsquo;t work\nGiven vector spaces \\(I,W\\) isomorphic, we desire \\(dim V = dim W\\) Suppose \\(V\\) and \\(W\\) are finite-dimensional vector spaces that are isomorphic. There means that there is an isomorphism, an invertable Linear Map between them which we will name \\(T \\in \\mathcal{L}(V,W)\\).\nBecause \\(T\\) is invertable, and injectivity and surjectivity implies invertability, so \\(null\\ T = \\{0\\}\\) and \\(range\\ T = W\\).\nLastly, we have that:\n\\begin{align} \\dim V \u0026amp;= \\dim null\\ T + \\dim range\\ T \\\\ \u0026amp;= 0 + dim\\ W \\\\ \u0026amp;= dim\\ W \\end{align}\nas desired.\nGiven \\(dim V = dim W\\), show the vector spaces are isomorphic Take \\(v_1, \\dots v_{n}\\) a basis of \\(V\\), and \\(w_1 \\dots w_{n}\\) a basis of \\(W\\).\nDefine a map by basis of domain mapping \\(Tv_{j} = w_{j}\\), that is, \\(T(c_1v_1 + \\dots + c_{n}v_{n}) = c_1 w_1 + \\dots + c_{n} w_{n}\\).\nBecause \\(w_1 \\dots w_{n}\\) spans \\(W\\) (it is a basis after all), \\(T\\) is surjective.\nAn input with some set of \\(c_{j}\\) is in the null space of \\(T\\) if \\(c_1 w_1 + \\dots + c_{n}w_{n}\\) adds up to \\(0\\) (by definition, as that\u0026rsquo;s the output of \\(T\\)).\nBecause \\(w_1 \\dots w_{n}\\) is a basis, the only linear combination thereof which makes \\(0\\) is by taking all \\(c_1 = \\dots c_{n} = 0\\). This make it so that the only valid input to \\(T\\) that will map to \\(0\\) requires \\(c_1=\\dots c_{n} = 0\\), making \\(null\\ T = \\{0\\}\\), showing that \\(T\\) is injective.\nHaving shown \\(T\\) is injective and surjective, it is an isomorphism, as desired. \\(\\blacksquare\\)\nmatricies and Linear Maps from the right dimensions are isomorphic Formally: suppose \\(v_1 \\dots v_{n}\\) is a basis of \\(V\\), and \\(w_1 \\dots w_{m}\\) is a basis of \\(W\\), then, \\(\\mathcal{M}\\) the matrixify operation that takes Linear Maps and turn them into matricies is an isomorphism between \\(\\mathcal{L}(V,W)\\) and \\(\\mathbb{F}^{m,n}\\).\nThe matrixify operation \\(\\mathcal{M}\\) is linear, because matricies are linear. The only thing that \\(\\mathcal{M}\\) will turn into the zero matrix is the zero Linear Map (i.e. \\(\\mathcal{M}(t)=0 \\implies T v_{k} = 0\\ \\forall k 1 \\dots n\\) by construction of matricies, and because the \\(v_{k}\\) are a basis, \\(T v_{k} =0 \\implies T=0\\)), so the null space of \\(\\mathcal{M}\\) is \\(\\{0\\}\\), making \\(\\mathcal{M}\\) injective.\nNow, because of the fact one can construct a matrix based on the scalars applied to map the input basis to the output basis; i.e. that, for any map \\(T \\in \\mathcal{L}(V,W)\\):\n\\begin{equation} Tv_{k} = \\sum_{j=i}^{m}A_{j,k} w_{j} \\end{equation}\nfor some matrix \\(\\mathcal{M}(T) = A \\in \\mathbb{F}^{m,n}\\), we have that \\(\\mathcal{M}\\) can be used to produce any map between \\(V\\) and \\(W\\). This makes \\(\\mathcal{M}\\) surjective.\n\\(\\dim \\mathcal{L}(V,W) = (\\dim V)(\\dim W)\\) \\(\\mathcal{L}(V,W)\\) is isomorphic to the set of matricies \\(\\mathbb{F}^{m,n}\\) where \\(w_1 \\dots w_{m}\\) is a basis for \\(W\\) and \\(v_1 \\dots v_{n}\\) is a basis for \\(V\\). two vector spaces are isomorphic IFF they have the same dimension, so \\(\\dim \\mathcal{L}(V,W) = \\dim \\mathbb{F}^{m,n} = m\\cdot n\\) (see \\(\\mathbb{F}^{m,n}\\)).\nHaving claimed that \\(w_1 \\dots w_{m}\\) is a basis of \\(W\\) and \\(v_1 \\dots v_{n}\\) is a basis of \\(V\\), \\(W\\) and \\(V\\) have dimensions \\(m\\) and \\(n\\) respectively. So \\((\\dim V)(\\dim W) = n \\cdot m = m\\cdot n = \\dim \\mathbb{F}^{m,n} = \\dim \\mathcal{L}(V,W)\\), as desired.\n","permalink":"https://www.jemoka.com/posts/kbhisomorphism/","tags":null,"title":"isomorphism"},{"categories":null,"contents":" Date Notes \u0026lt;2022-04-13 Wed\u0026gt; PCP April Checkin \u0026lt;2022-04-13 Wed\u0026gt; Alivio April Checkin \u0026lt;2022-04-16 Sat\u0026gt; GreenSwing April Checkin \u0026lt;2022-04-25 Mon\u0026gt; Pollen April Checkin \u0026lt;2022-04-30 Sat\u0026gt; Logan\u0026rsquo;s Team Checkin \u0026lt;2022-05-02 Mon\u0026gt; Anna\u0026rsquo;s Team Checkin TODO Stack Get asthma kids leads for Alivio GreenSwing Hiring: Fufilling Orders, MechE Conrad money? Get Mentors for Pollen =\u0026gt; Figma lady ","permalink":"https://www.jemoka.com/posts/kbhistudio_meeting_notes/","tags":null,"title":"iStudio Meeting Notes"},{"categories":null,"contents":" https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma\nfor integrating Differential Equations with Brownian Motion.\n","permalink":"https://www.jemoka.com/posts/kbhito_intergral/","tags":null,"title":"Itô Intergral"},{"categories":null,"contents":"Punchlines Screw you, I\u0026rsquo;m not Stupid\u0026hellip;. I\u0026rsquo;m just Chinese. Joke\u0026rsquo;s on you, I\u0026rsquo;m both Chinese AND stupid. Setups Why is it that toilets have a refractory period? The satanic church fights back against the Texas abortion ban. Completed jokes Where did Texas gun control funding go? its illegal to own more than 6 d*l**s there ","permalink":"https://www.jemoka.com/posts/kbhjokes/","tags":null,"title":"jokes"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.642633\nOne-Liner Developed a kitchen sink of diagnoses tools and correlated it with biomarkers.\nNovelty The kitchen sink of data collection (phones, tablet, eye tracker, microphone, wristband) and the kitchen sink of noninvasive data imaging, psych, speech assesment, clinical metadata.\nNotable Methods Here\u0026rsquo;s their kitchen sink\nI have no idea why a thermal camera is needed\nKey Figs Here are the features they extracted\nDeveloped the features collected via a method similar to action research, did two passes and refined/added information after preliminary analysis. Figure above also include info about whether or not the measurement was task specific.\nand there are the biomarkers and medical data they collected\nAnd then they correlated their kitchen sink with biomarker from the tap\nNew Concepts spinal tap Notes ","permalink":"https://www.jemoka.com/posts/kbhjonell_2021/","tags":["ntj"],"title":"Jonell 2021"},{"categories":null,"contents":" Orbits of planetary bodies are ellipses with the sun at one of the two foci Drawing a line from the sun to the orbiting body, they would sweep out equal areas Planets that are closer to the sun have much shorter periods than that Squares of the periods of the planets is equal to the cubes of the distance from the planet to the sun ","permalink":"https://www.jemoka.com/posts/kbhkepler_s_laws_of_planetary_motion/","tags":null,"title":"Kepler's Laws of Planetary Motion"},{"categories":null,"contents":"Keynsian Politics is a economy strategy to support large projects via the government to boost economic output (i.e. that the economy needs a minder, but is generally free-sustaining.)\nSee also: Keynsian Economics was not trying to entirely replace markets\n","permalink":"https://www.jemoka.com/posts/kbhkeynsian_politics/","tags":null,"title":"Keynsian Politics"},{"categories":null,"contents":"in algorithms, key\n","permalink":"https://www.jemoka.com/posts/kbhkeys/","tags":null,"title":"keys (algorithms)"},{"categories":null,"contents":"KLA is a semiconductor process control company. https://www.kla.com/ Rick Wallace is the CEO.\n135000 employees 8.2B of revenue 72-300 tools 15% of revenue in R\u0026amp;D Their main business is in automatically inspecting chips and wafers in time.\n","permalink":"https://www.jemoka.com/posts/kbhkla/","tags":null,"title":"KLA"},{"categories":null,"contents":"knowledgebase testing is a space to test the knowledgebase! Other utility and maintained pages include random and Index.\nGenerate a Valid Token! 1:nah 2:\nwhat is there to do? oenutonsetuhaoeu o ooensutahoeu\ntype faces for everybody everywhere\nyes indeed we stan type faces\nHere\u0026rsquo;s the characteristic equation again:\nMath! What? we do math here?\n\\begin{equation} \\pdv[2] x \\qty(EI \\pdv[2]{w}{x}) = -\\mu \\pdv{w}{t}+q(x) \\end{equation}\nAfter Fourier decomposition, we have that:\n\\begin{equation} EI \\dv[4]{\\hat{w}}{x} - \\mu f^{2}\\hat{w} = 0 \\end{equation}\nLet\u0026rsquo;s solve this!\nE,I,u,f = var(\u0026#34;E I u f\u0026#34;) x, L = var(\u0026#34;x L\u0026#34;) w = function(\u0026#39;w\u0026#39;)(x) _c0, _c1, _c2, _c3 = var(\u0026#34;_C0 _C1 _C2 _C3\u0026#34;) fourier_cantileaver = (E*I*diff(w, x, 4) - u*f^2*w == 0) fourier_cantileaver -f^2*u*w(x) + E*I*diff(w(x), x, x, x, x) == 0 And now, we can go about solving this result.\nMore Math! We have one equation, four unknowns. However, we are not yet done. We will make one more simplifying assumption\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\n\\begin{equation} _{C_{1}} e^{\\left(b x\\right)} + _{C_{0}} e^{\\left(i \\, b x\\right)} + _{C_{2}} e^{\\left(-i \\, b x\\right)} + _{C_{3}} e^{\\left(-b x\\right)} \\end{equation}\nWe have one equation, four unknowns. However, we are not yet done. We will make one more simplifying assumption\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\nEven 0More Math! Recall that:\n\\begin{equation} \\begin{cases} \\cosh x = \\frac{e^{x}+e^{-x}}{2} \\\\ \\cos x = \\frac{e^{ix}+e^{-ix}}{2}\\\\ \\sinh x = \\frac{e^{x}-e^{-x}}{2} \\\\ \\sin x = \\frac{e^{ix}-e^{-ix}}{2i}\\\\ \\end{cases} \\end{equation}\nWe have one equation, four unknowns. However, we are not yet done. We will make one more simplifying assumption\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\nSo much more math!\nWe have one equation, four unknowns. However, we are not yet done. We will make one more simplifying assumption\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\nWe have one equation, four unknowns. However, we are not yet done. We will make one more simplifying assumption\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\nerror prop for tuning forks E = var(\u0026#34;E\u0026#34;) h = var(\u0026#34;h\u0026#34;) p = var(\u0026#34;p\u0026#34;) L = var(\u0026#34;L\u0026#34;) s = var(\u0026#34;s\u0026#34;) Ee = var(\u0026#34;Ee\u0026#34;) he = var(\u0026#34;he\u0026#34;) pe = var(\u0026#34;pe\u0026#34;) Le = var(\u0026#34;Le\u0026#34;) f = (s^2)/(2*pi*L^2)*((E*h^2)/(12*p))^(1/2) fd = ((f.diff(E)*Ee)^2 + (f.diff(h)*he)^2 + (f.diff(pe)*pe)^2 + (f.diff(Le)*Le)^2)^(1/2) fd.subs(E==46203293995, h==0.0065, p==2597, L==0.11604, s==1.8751040687120917, Ee==4620329399, he==5e-5, pe==1, Le==5e-5).n() 16.6390442077208 ","permalink":"https://www.jemoka.com/posts/kbhknowledgebase_testing/","tags":null,"title":"knowledgebase testing page"},{"categories":null,"contents":"A KS test is a hypothesis test that measures if two groups of samples are drawn from the same distribution.\n","permalink":"https://www.jemoka.com/posts/kbhkolmogorov_smirnov_test/","tags":null,"title":"Kolmogorov-Smirnov test"},{"categories":null,"contents":"Want mechanics? No. You get energy.\nFirst, recall the stationary-action principle. To define a system in Lagrangian Mechanics, we define a smooth function \\(L\\), called the \u0026ldquo;Lagrangian\u0026rdquo;, and some configuration space (axis) \\(M\\).\nBy convention, \\(L=T-V\\). \\(T\\) is the kinetic energy in the system, and \\(V\\) is the potential energy in the system.\nBy the stationary-action principle, then, we require \\(L\\) to remain at a critical point (max, min, saddle.) This fact allows us to calculate the equations of motion by hold \\(L\\) at such a point, and evolving the \\((T,V)\\) pair to remain at that point.\nThe notion of solving for optimal \\((T,V)\\), which will give us the equations of motion, is why Lagrangian multipliers were invented.\nNow, here\u0026rsquo;s a few results which help you deal with the Lagrangian.\nConservation of Momentum Note that momentum is always conserved.\nRecall that:\n\\begin{equation} F = m a = m \\dv{v}{t} = \\dv{mv}{t} \\end{equation}\nwhen \\(m\\) is constant, which it almost certainly is.\nRecall the definition of momentum:\n\\begin{equation} p := mv \\end{equation}\nTherefore, we have that:\n\\begin{equation} F = \\dv{p}{t} \\end{equation}\nGreat, now let\u0026rsquo;s recall what energy is:\n\\begin{equation} W = \\int F\\dd{x} \\end{equation}\nSubstituting our definitions of force:\n\\begin{equation} W = \\int \\dv{p}{t}\\dd{x} = \\int \\dd{p}\\dv{x}{t} = \\int v \\dd{p} \\end{equation}\n[something something something ask leonard]\nWe end up with:\n\\begin{equation} \\pdv{W}{v} = p \\end{equation}\nhow? IDK. But you then usually would use this by taking the derivative of the Lagrangian by velocity, then figuring it lingual to \\(0\\).\nBeam Theory We begin with this Euler-Lagrange expression:\nthese are a series of expressions derived to semiautomatically solve Largrangian expressions of expressions derived to semiautomatically solve Largrangian expressions: they are the pre-figured-out stationary-action principle \u0026ldquo;stationary points\u0026rdquo; with the least energy.\nWe want to create a Lagrangian of our system, and plug it in there.\nWe define the Lagrangian for this system to be\nRecall that the Lagrangian is defined by all kinetic energy sum minus all potential energy sum. Will investigate deeper later, but the first term is obviously the kinetic energy (1/2 mass-density velocity squared), then the subtracted potential energy term is the spring potential of the system (1/2 kx^2).\nThen there\u0026rsquo;s this third term. No idea.\nWe then try to plug stuff into that Euler-Lagrange expression. We can calculate for ourselves that:\nFinally, then:\nliterally\u0026hellip; the end. We just move stuff around and that\u0026rsquo;s literally it.\n","permalink":"https://www.jemoka.com/posts/kbhlagrangian_mechanics/","tags":null,"title":"Lagrangian Mechanics"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624694\nOne-Liner Proposed a large multimodal approach to embed auditory info + biomarkers for baseline classification.\nNovelty Developed a massively multimodal audio-to-embedding correlation system that maps audio to biomarker information collected (mood, memory, respiratory) and demonstrated its ability to discriminate cough results for COVID. (they were looking for AD; whoopsies)\nNotable Methods Developed a feature extraction model for AD detection named Open Voice Brain Model Collected a dataset on people coughing and correlated it with biomarkers Key Figs Figure 2 This is MULTI-MODAL as heck\nThis figure tells us the large network the came up with.\nTable 2 and 3 The descriminator tacked on the end of the network is transfer-trained to different tasks. It shows promising results for cough-to-COVID classification\nNew Concepts OVBM Lyu 2018 Notes Biomarker correlation Is biomarker data something that is commonly used as a feature extraction/benchmark tool?\n","permalink":"https://www.jemoka.com/posts/kbhlaguarta_2021/","tags":["ntj"],"title":"Laguarta 2021"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhlaplae/","tags":null,"title":"laplae"},{"categories":null,"contents":"the\n","permalink":"https://www.jemoka.com/posts/kbhlaw_of_cosines/","tags":null,"title":"law of cosines"},{"categories":null,"contents":"LOOCV is a cross validation method whereby the entire dataset bar one sample is used for training; then, validation is ran on one sample. This is repeated \\(N\\) times (with a fresh model and a fresh item left out) to get a distribution of one-shot validation results that is an approximately-normal curve centered around the mean validation result from many one-shot samples.\n","permalink":"https://www.jemoka.com/posts/kbhloo/","tags":null,"title":"Leave-One-Out Cross Validation"},{"categories":null,"contents":"Reading notes :claim: Mccarthyism was a process that de-politicized America Since political activities could get you in trouble, prudent folk avoided them\nSocial conformaty became standard middle-class Americans became social conformists\nCommunism serves as a form of balance checking, which Mccathyism lost With their demise, the nation lost the institutional network that had created a public space where serious alternatives to the status quo could be presented.\nModerate-left was also diminished Moreover, with the disappearance of a vigorous movement on their left, moderate reform groups were more exposed to right-wing attacks and thus rendered less effective.\nMccarthyism also diminshed America\u0026rsquo;s liberal modernization Measures like national health insurance, a social reform embraced by the rest of the industrialized world, simply fell by the wayside.\nCold-war opposition became quelled by mccarthism Opposition to the cold war had been so thoroughly identified with communism that it was no longer possible to challenge the basic assumptions of American foreign policy without incurring suspicions of disloyalty\nThat there may have been more international collaboration if mccarthism was not done early on American policymakers feared to acknowledge the official existence of the People\u0026rsquo;s Republic of China until Richard Nixon, who was uniquely impervious to charges of being soft on communism, did so as president in 1971\nControvercial issues were avoided intellecturally and artistically Similarly, the blacklist contributed to the reluctance of the film industry to grapple with controversial social or political issues. In the intellectual world, cold war liberals also avoided controversy.\nThat \u0026ldquo;ideology\u0026rdquo; became irrelavent, pure pragmatism took hold They celebrated the \u0026ldquo;end of ideology,\u0026rdquo; claiming that the United States\u0026rsquo; uniquely pragmatic approach to politics made the problems that had once concerned left- wing ideologists irrelevant.\nState power became expanded federal agents attacked individual rights and extended state power into movie studios, universities, labor unions, and many other ostensibly independent institutions.\nThat Mccarthism produced a threat to demcrocy in itself McCarthyism alone did not cause these outrages; but the assault on democracy that began during the 1940s and 1950s with the collaboration of private institutions and public agencies in suppressing the alleged threat of domestic communism was an important early contribution.\n","permalink":"https://www.jemoka.com/posts/kbhlegacy_of_mccarthyism/","tags":null,"title":"Legacy of McCarthyism"},{"categories":null,"contents":"Any two basis of finite-dimensional vector space have the same length.\nconstituents A finite-dimensional vector space \\(V\\) Basis \\(B_1\\), \\(B_2\\) be bases in \\(V\\) requirements Given \\(B_1\\), \\(B_2\\) are basis in \\(V\\), we know that they are both linearly independent and spans \\(V\\). We have that the length of linearly-independent list \\(\\leq\\) length of spanning list.\nLet\u0026rsquo;s take first \\(B_1\\) as linearly independent and \\(B_2\\) as spanning:\nWe have then \\(len(B_1) \\leq len(B_2)\\)\nSwapping roles:\nWe have then \\(len(B_2) \\leq len(B_1)\\)\nAs both of this conditions are true, we have that \\(len(B_1)=len(B_{2})\\). \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhlength_of_basis_doesn_t_depend_on_basis/","tags":null,"title":"Length of Basis Doesn't Depend on Basis"},{"categories":null,"contents":"The Lexicalization Hypothesis is a hypothesis proposed by Chomsky that states that syntactic transformations can only apply on syntatic constituents; therefore, the rules of putting words together is different from the rules that puts phrases together. This theory stands in opposition to generative semantics.\nThere are two versions of the Lexicalization Hypothesis:\nStrong Lexicalization Hypothesis The Strong Lexicalization Hypothesis states that both derivational words (changes meaning, bench=\u0026gt;benching) or inflectional words (changes grammar, eat=\u0026gt;eating) cannot be put together via syntatical rules. (Geeraerts 2009)\nWeak Lexicalization Hypothesis Weak Lexicalization Hypothesis states that semantic rules cannot work in the formation of derivational words only.\n","permalink":"https://www.jemoka.com/posts/kbhlexicalization_hypothesis/","tags":null,"title":"Lexicalization Hypothesis"},{"categories":null,"contents":" Poster-modern search for individualism ","permalink":"https://www.jemoka.com/posts/kbhliberal_center/","tags":null,"title":"Liberal Center"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhlina/","tags":null,"title":"lina"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.642033\nOne-Liner Proposed cross-linguistic markers shared for AD patients between English and French; evaluated features found with standard ML.\nNovelty Multi-lingual, cross-linguistic analysis.\nNotable Methods Looked at common patters between the two languages Linguistic results scored by IUs on CTP task Key Figs Figure 1 This figure tells us the various approaches measured.\nTable 2 Here\u0026rsquo;s a list of semantic features extracted\nTable 3 Here\u0026rsquo;s a list of NLP features extracted. Bolded items represent P \u0026lt;0.001 correlation for AD/NonAD difference between English and French.\nSame thing but semantic features\nsame thing but acoustic features. As we can see, acoustic features didn\u0026rsquo;t do much.\nNew Concepts CTP IU Notes ","permalink":"https://www.jemoka.com/posts/kbhlindsay_2021/","tags":["ntj"],"title":"Lindsay 2021"},{"categories":null,"contents":"Gaussian Elimination Quiz Demonstrate that matrices\u0026rsquo; multiplication are not commutative (error: didn\u0026rsquo;t consider \\(m\\times m\\)) Which \\(2\\times 2\\) matrices under multiplication form a group? (error: closure need to proved on invertable matrices under multiplication, not just \\(2\\times 2\\)) Deriving Rotation matrices (error: clockwise vs counter-clockwise) Linear Independence Quiz Connection between linear independence and systems equations (error: beated around the bush) \u0026mdash; the matrix of an nxn system of equations has a solution if the matrix\u0026rsquo;s column vectors is linearly independent Basis and Dimension Quiz put 0 into a basis AAAA not lin. indep; figure out what the basis for a polynomial with a certain root is: it is probably of dimension m (instead of m+1), because scalars doesn\u0026rsquo;t work in the case of p(3)=0; so basis is just the scalars missing some inequality about basis? \u0026mdash; its just that lin.idp sets is shorter or equal to basis and spanning sets is longer or equal to basis Final, part 1 definition of vector space: scalar multiplication is not an operation straight forgot \\(dim(U+V) = dim U + dim V - dim (U\\cap V)\\) plane containing \\((1,0,2)\\) and \\((3,-1,1)\\): math mistake proof: det A det B = det AB Final, part 2 Counterproof: If \\(v_1 \\dots v_4\\) is a basis of \\(V\\), and \\(U\\) is a subspace of \\(V\\) with \\(v_1, v_2 \\in U\\) and \\(v_3, v_4\\) not in \\(U\\), \\(v_1, v_2\\) is a basis of \\(U\\) Counterproof: if \\(T \\in \\mathcal{L}(V,V)\\) and \\(T^{2}=0\\), then \\(T=0\\) Counterproof: if \\(s,t \\in \\mathcal{L}(V,V)\\), and \\(ST=0\\), then \\(null\\ s\\) is contained in \\(range\\ T\\) Product Spaces Quiz Prove that \\(\\mathcal{L}(V_1 \\times V_2 \\times \\dots \\times V_{m}, W)\\) and \\(\\mathcal{L}(V_1, W) \\times \\dots \\times \\mathcal{L}(V_{m}, W)\\) are isomorphic\nerror: didn\u0026rsquo;t do it\nQuotient Spaces Quiz Couldn\u0026rsquo;t prove that the list in linearly independent: the linear combinations is some \\(c_1v_1 + \\dots c_{m}v_{m} + U\\); as \\(v_1 \\dots v_{m}\\) is a basis of \\(V / U\\), \\(c_1 \\dots c_{m} = 0\\), now the second part is also a basis so they are \\(0\\) too. The spanning proof: \\(v + U =\\) , rewrite as basis, etc. ","permalink":"https://www.jemoka.com/posts/kbhlinear_algebra_errors-1/","tags":null,"title":"Linear Algebra Errors"},{"categories":null,"contents":"Gaussian Elimination Quiz Demonstrate that matrices\u0026rsquo; multiplication are not commutative (error: didn\u0026rsquo;t consider \\(m\\times m\\)) Which \\(2\\times 2\\) matrices under multiplication form a group? (error: closure need to proved on invertable matrices under multiplication, not just \\(2\\times 2\\)) Deriving Rotation matrices (error: clockwise vs counter-clockwise) Linear Independence Quiz Connection between linear independence and systems equations (error: beated around the bush) \u0026mdash; the matrix of an nxn system of equations has a solution if the matrix\u0026rsquo;s column vectors is linearly independent Basis and Dimension Quiz put 0 into a basis AAAA not lin. indep; figure out what the basis for a polynomial with a certain root is: it is probably of dimension m (instead of m+1), because scalars doesn\u0026rsquo;t work in the case of p(3)=0; so basis is just the scalars missing some inequality about basis? \u0026mdash; its just that lin.idp sets is shorter or equal to basis and spanning sets is longer or equal to basis Final, part 1 definition of vector space: scalar multiplication is not an operation straight forgot \\(dim(U+V) = dim U + dim V - dim (U\\cap V)\\) plane containing \\((1,0,2)\\) and \\((3,-1,1)\\): math mistake proof: det A det B = det AB Final, part 2 Counterproof: If \\(v_1 \\dots v_4\\) is a basis of \\(V\\), and \\(U\\) is a subspace of \\(V\\) with \\(v_1, v_2 \\in U\\) and \\(v_3, v_4\\) not in \\(U\\), \\(v_1, v_2\\) is a basis of \\(U\\) Counterproof: if \\(T \\in \\mathcal{L}(V,V)\\) and \\(T^{2}=0\\), then \\(T=0\\) Counterproof: if \\(s,t \\in \\mathcal{L}(V,V)\\), and \\(ST=0\\), then \\(null\\ s\\) is contained in \\(range\\ T\\) Product Spaces Quiz Need more specific description: explain why we use product and quotient to describe product and quotient spaces? Prove that \\(\\mathcal{L}(V_1 \\times V_2 \\times \\dots \\times V_{m}, W)\\) and \\(\\mathcal{L}(V_1, W) \\times \\dots \\times \\mathcal{L}(V_{m}, W)\\) are isomorphic. Error: didn\u0026rsquo;t do it correctly for infinite dimensional Quotient Spaces Quiz Couldn\u0026rsquo;t prove that the list in linearly independent: the linear combinations is some \\(c_1v_1 + \\dots c_{m}v_{m} + U\\); as \\(v_1 \\dots v_{m}\\) is a basis of \\(V / U\\), \\(c_1 \\dots c_{m} = 0\\), now the second part is also a basis so they are \\(0\\) too. The spanning proof: \\(v + U =\\) , rewrite as basis, etc. she graded wrong: what\u0026rsquo;s the importance of \\(\\widetilde{T}\\)? Give two statements equivalent to \\(v+U = w+U\\), prove equivalence betewen this statement and the others didn\u0026rsquo;t prove both directions! ","permalink":"https://www.jemoka.com/posts/kbhlinear_algebra_errors/","tags":null,"title":"Linear Algebra Errors"},{"categories":null,"contents":"The bible stays the same: (Axler 1997)\nWe will be less exploratory, Axler will pretty much tell us. However, we should try to say stuff in the class every single class period.\nThere is a ban on numbers over 4 on this class.\nBest Practices Ask questions Talk to each other Make mistakes From Riley: know the Proof Design Patterns Non-Axler but Important Things we explicitly are told to know, but is not immediately in Axler. You bet you determinants are going to be here.\ngroup matricies dot product and cross product solving systems binary operation modular arithmetic quotient group Homework See NUS-MATH530 Homework Index\nQuiz Errors Linear Algebra Errors\n1 Axler 1.A Axler 1.B Axler 1.C 2 Axler 2.A Axler 2.B Axler 2.C 3 Axler 3.A Axler 3.B Axler 3.C Axler 3.D Axler 3.E 4 Thoughts on Axler 4\n5 Axler 5.A Misc Knowledge algebra vector integer additive identity Axler, Sheldon. 1997. Linear Algebra Done Right. Undergraduate Texts in Mathematics. Springer New York. doi:10.1007/b97662. ","permalink":"https://www.jemoka.com/posts/kbhlinear_algebra_index/","tags":["index"],"title":"Linear Algebra Index"},{"categories":null,"contents":"A Linear Combination of vectors is a\u0026hellip; guess what? Any vector formed by a combination of vectors at arbitrary scales.\nconstituents A list of vectors \\(v_1, \\dots,v_{m}\\) Scalars \\(a_1, \\dots, v_{m} \\in \\mathbb{F}\\) requirements A Linear Combination is defined formally by:\n\\begin{equation} v = a_1v_1+\\dots+a_{m}v_{m} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhlinear_combination/","tags":null,"title":"linear combination"},{"categories":null,"contents":"Here it is:\n\\begin{equation} a\\frac{dy}{dx} + by = c \\end{equation}\nFor some constants \\(a,b,c\\). The name is pretty obvious, because we have constants and the highest power on everything is \\(1\\). Its first-order because the derivative is only the first-order derivative.\nlinear (diffeq) We technically call it \u0026ldquo;linear\u0026rdquo; because: if there are two possible solutions \\(y_1(x)\\) \\(y_2(x)\\), a linear combination \\(Ay_1(x)+By_2(x)\\) should also be a solution. Its \u0026ldquo;linear\u0026rdquo; because linear combinations work.\nsolving separable differential equations A separable differential equation means that we can separate the derivative by itself and separate its two components. For the example above, we have that:\n\\begin{equation} \\frac{dy}{dx} = \\frac{c-by}{a} \\end{equation}\nWe can naturally separate this:\n\\begin{equation} \\frac{a}{c-by}dy = dx \\end{equation}\nAnd then we can finally take the integral on both sides:\n\\begin{equation} \\int \\frac{a}{c-by}dy = \\int dx \\end{equation}\nWait wait wait but why is this possible? Why is it that we can separate a \\(\\frac{dy}{dx}\\) such that \\(dy\\) and \\(dx\\) is isolatable? Remember:\n\\begin{equation} \\frac{dy}{dx} = \\lim_{h\\to 0} \\frac{y(x+h)-y(x)}{h} \\end{equation}\nno where is the differentials seperatable! Apparently Ted\u0026rsquo;s undergrads didn\u0026rsquo;t know this either. So here\u0026rsquo;s a reading on it.\nWhat if its non-seperable? See Linear Non-Seperable Equation\n","permalink":"https://www.jemoka.com/posts/kbhlinear_constant_coefficient_equation/","tags":null,"title":"Linear Constant-Coefficient Equation"},{"categories":null,"contents":"Linear Dependence Lemma is AFAIK one of the more important results of elementary linear algebra.\nstatement Suppose \\(v_1, \\dots v_{m}\\) is an linearly dependent list in \\(V\\); then \\(\\exists j \\in \\{1, 2, \\dots m\\}\\) such that\u0026hellip;\n\\(v_{j} \\in span(v_1, \\dots, v_{j-1})\\) the span of the list constructed by removing \\(v_{j}\\) from \\(v_1, \\dots v_{m}\\) equals the span of \\(v_1, \\dots v_{m}\\) itself intuition: \u0026ldquo;in a linearly dependent list of vectors, one of the vectors is in the span of the previous ones, and we can throw it out without changing the span.\u0026rdquo;\nproof By definition of linear dependence, given the list \\((v_1, \\dots v_{m}\\)) is linearly dependent, there exists some not-all-zero \\(a_1, \\dots, a_{m} \\in \\mathbb{F}\\) such that:\n\\begin{equation} a_1v_1+\\dots +a_{m}v_{m} = 0 \\end{equation}\nLet \\(a_{j}\\) be the last non-zero scalar in the expression (making the term actually exist). You can, in this circumstance, chuck everything to the right and divide by \\(a_{j}\\) to recover \\(v_{j}\\):\n\\begin{equation} v_{j}= -\\frac{a_1}{a_{j}} v_1 - \\dots -\\frac{a_{j-1}}{a_{j}}v_{j-1} \\end{equation}\nWe were able to construct \\(v_{j}\\) as a linear combination of \\(v_{1}, \\dots v_{j-1}\\), therefore:\n\\begin{equation} v_{j} \\in span(v_1, \\dots, v_{j-1}) \\end{equation}\nshowing \\((1)\\).\nFor \\(2\\), the intuition behind the proof is just that you can take that expression for \\(v_{j}\\) above to replace \\(v_{j}\\), therefore getting rid of one vector but still keeping the same span.\nFormally, \\(\\forall u \\in span(v_1, \\dots v_{m})\\), we can write it as some:\n\\begin{equation} u = c_1v_1 + \\dots c_{j}v_{j} + \\dots + c_{m}v_{m} \\end{equation}\nnow we replace \\(v_{j}\\) with the isolated expression for \\(v_{j}\\) above.\nException: if \\(j=1\\) and \\(v_1=0\\), note that you can just replace \\(v_1\\) with \\(0\\) without doing any special substitution.\nHaving written all arbitrary \\(u \\in span(v_1, \\dots v_{m})\\) as a linear combination of \\(v_1\\dots v_{m}\\) without \u0026hellip; \\(v_{j}\\), we see that the renaming vectors span the same space. \\(\\blacksquare\\)\nissue note that if we chose \\(j=1\\) in the above result, \\(v_1=0\\). Contrapositively, if \\(v_1 \\neq 0\\), \\(j\\neq 1\\). This is because of the fact that:\nif \\(j=1\\), the lemma tells us that \\(v_{1} \\in span(v_{1-1}) \\implies v_1 \\in span()\\). As per definition, the span of the empty set is \\(\\{0\\}\\). Therefore, \\(v_1 \\in \\{0\\} \\implies v_1=0\\).\n","permalink":"https://www.jemoka.com/posts/kbhlinear_dependence_lemma/","tags":null,"title":"Linear Dependence Lemma"},{"categories":null,"contents":"A linearly independent list is a list of vectors such that there is one unique choice of scalars to be able to construct each member of their span.\nBased on the same technique as in the proof that a sum of subsets is a direct sum IFF there is only one way to write \\(0\\), we can show that in a linearly independent list, there is (IFF) only one way to write the zero vector as a linear combination of that list of vectors \u0026mdash;namely, the trivial representation of taking each vector to \\(0\\). In fact, we will actually use that as the formal definition of linear independence.\nThis definition of linear independence is the result of the definition for direct sum.\nSee also Linear Dependence Lemma.\nconstituents A list of vectors \\(v_1, \\dots, v_{m}\\) in \\(V\\) requirements Formally, a linearly independent list is defined by there being only one choice of scalars \\(a_1, \\dots, a_{m} \\in \\mathbb{F}\\) to write \\(0\\) as a linear combination of \\(v_{1},\\dots, v_{m}\\): namely, by taking each \\(a_1, \\dots a_{m}\\) to \\(0\\).\nWe also declare \\(()\\) to be linearly independent.\nadditional information linearly dependent a list is linearly dependent if\u0026hellip;. its not linearly independent.\noh. my. god.\nBased on the same formal definition, this means that a linearly dependent list is defined by the fact that there can be more than one way of writing \\(0\\) as a linear combination of that list of vectors, where one of the ways makes it so that writing \\(0\\) does not require all zero scalars.\nlength of linearly-independent list \\(\\leq\\) length of spanning list A linearly independent list should be smaller or equal in length to a spanning list.\nThe canonical proof is one by induction.\nSuppose \\(u_1, \\dots u_{m}\\) is an linearly independent list in \\(V\\). Take also a list \\(w_1, \\dots w_{n}\\) spans \\(V\\). We desire that \\(m\\leq n\\). We create a list of length \\(n\\) containing all of the \\(w\\) thus far. Our invariant is that \\(len(B) = n\\). This proof essentially uses Floyd\u0026rsquo;s Invariant Method (compsci topic for Jack\u0026rsquo;s understanding only.)\nbase case take the spanning list of \\(V\\) we declared named \\(w_1, \\dots w_{n}\\). Given it spans, adding any other vector in \\(V\\), if \\(w_1, \\dots w_{n}\\) isn\u0026rsquo;t already linearly dependent, will make it linearly dependent. This is because you can write the new vector \\(v \\in V\\) which you add as a linear combination of the previous vectors already as they already span \\(V\\).\nBy the Linear Dependence Lemma, you can remove one of the vectors in the new linearly dependent list while keeping the list still spanning \\(V\\).\nNow, construct the list:\n\\begin{equation} u_1, w_1, \\dots w_{n} \\end{equation}\nwhere, \\(u_{1} \\in V\\) is taken from that linearly independent list in \\(V\\). By the statement above, via applying the Linear Dependence Lemma, we can create a list that spans the same space by taking away one of the \\(w_{j}\\) (we can\u0026rsquo;t take \\(u_1\\) because it is at the first position, and we can\u0026rsquo;t grantee its $0$\u0026mdash;see the issue with the Linear Dependence Lemma). We now have a list \\(B\\) with length \\(n\\) with \\(u_1\\) and the rest of the \\(w\\) not taken away which span \\(V\\)\ncase number \\(j\\) Given a spanning list \\(B\\) of \\(V\\) with length \\(n\\), with some parts \\(u_1, \\dots, u_{j-1}, w_{j}, \\dots w_{n}\\). We now include \\(u_{j}\\) in the list, placing it after \\(u_{j-1}\\). As the list pre-inclusion is already a spanning list of \\(V\\), any new vectors from \\(V\\) added will necessarily be able to be written as a linear combination of the other vectors already in the list. Therefore, we know that\u0026mdash;if not already pre-inclusion\u0026mdash;the list is linearly dependent.\nBecause the first half (\\(u_1,\\dots u_{j}\\)) of this new list is linearly independent (given), the bit that \u0026ldquo;causes\u0026rdquo; the linear dependence is in the \\(w\\) (i.e. each \\(u\\) cannot be written by other \\(u\\).) Therefore, we can say that the first condition of Linear Dependence Lemma allows us to remove one of the \\(w\\) while spanning the same space, creating again a spanning list of length \\(n\\).\ninduction repeat the procedure \\(m\\) times, resulting in all the \\(u_{j}\\) being included in our new list \\(B\\) of length still \\(n\\). Given we contained a list of length \\(m\\) in a list of length \\(n\\), \\(m \\leq n\\).\n","permalink":"https://www.jemoka.com/posts/kbhlinear_independence/","tags":null,"title":"linear independence"},{"categories":null,"contents":"A Linear Map (a.k.a. Linear Transformation) is a function which maps elements between two vector space that follows linear properties.\nconstituents vector spaces \\(V\\) and \\(W\\) (they don\u0026rsquo;t have to be subspaces) A function \\(T: V \\to W\\) (when we put something in, it only goes to one place) requirements \\(T\\) is considered a Linear Map if it follows\u0026hellip; (properties of \u0026ldquo;linearity\u0026rdquo;)\nadditivity \\begin{equation} T(u+v) = Tu+Tv,\\ \\forall u,v \\in V \\end{equation}\nhomogeneity \\begin{equation} T(\\lambda v) = \\lambda (Tv),\\ \\forall \\lambda \\in \\mathbb{F}, v \\in V \\end{equation}\nadditional information note on notation The \u0026ldquo;application\u0026rdquo; of a Linear Map \\(T\\) on a vector \\(V\\) is written as:\n\\begin{equation} \\begin{cases} Tv \\\\ T(v) \\end{cases} \\end{equation}\nboth are acceptable.\n\\(\\mathcal{L}(V,W)\\) The set of all Linear Maps from \\(V\\) to \\(W\\) is denoted as \\(\\mathcal{L}(V,W)\\).\nsome fun linear maps zero There is, of course, the Linear Map that maps everything to the \\(0\\) \u0026mdash; as the zero exists in all vector spaces.\nThat is:\n\\begin{equation} 0 \\in \\mathcal{L}(V,W) \\implies 0v = 0, v\\in V \\end{equation}\nadditivity\nLet \\(v_1+v_2= v \\in V\\).\n\\begin{equation} 0(v_1+v_2) = 0(v) = 0 = 0+0 = 0v_1+0v_2 \\end{equation}\nhomogeneity\nLet \\(\\lambda v = u \\in V\\).\n\\begin{equation} 0(\\lambda v) = 0(u) = 0 = \\lambda 0 = \\lambda 0v \\end{equation}\nidentity Another classic. \\(I\\), the identity map, is denoted as (for some \\(v \\in V\\) and \\(I \\in \\mathcal{L}(V,V)\\)):\n\\begin{equation} Iv = v \\end{equation}\ni.e. it does nothing\nadditivity\nLet \\(v_1,v_2 \\in V\\):\n\\begin{equation} I(v_1+v_2) = v_1+v_2 = Iv1+Iv2 \\end{equation}\nhomogeneity\n\\begin{equation} I(\\lambda v) = \\lambda v = \\lambda Iv \\end{equation}\nany map from \\(\\mathbb{F}^{n}\\) to \\(\\mathbb{F}^{m}\\) turns out any map that follows a specific pattern of polynomials between two vector spaces are Linear Maps.\nDefine some two vector spaces \\(\\mathbb{F}^{n}\\) and \\(\\mathbb{F}^{m}\\), some set of scalars \\(a_{jk} \\in \\mathbb{F}: j=1, \\dots m; k=1, \\dots n\\).\nWe construct \\(T \\in \\mathcal{L}(\\mathbb{F}^{n}, \\mathbb{F}^{m})\\) by: \\(T(x_1, \\dots x_{n}) = a_{11} x_1+ \\cdots + a_{1n} x_{n}, \\dots, a_{m1} x_1 + \\cdots + a_{mn} x_{n}\\) (i.e. a combination of linear combinations).\nadditivity\nLet \\(x,y \\in \\mathbb{F}^{n}\\), with \\(x_{j}\\) being each coordinate of \\(x\\) and the same goes for \\(y\\).\n\\begin{align} T((x_1, \\dots x_{n}) + (y_1, \\dots y_{n}))) \u0026amp;= T(x_1+y_1 \\dots x_{n}+y_{n}) \\\\ \u0026amp;= a_{11}(x_1+y_1) + \\cdots, \\dots, \\cdots + a_{mn} (x_{n} + y_{n}) \\\\ \u0026amp;= (a_{11}x_1 + a_{11}y_{1}) + \\cdots, \\dots, \\cdots + (a_{mn} x_{n} + a_{mn} y_{n}) \\\\ \u0026amp;= (a_{11}x_1 + \\cdots) + (a_{11}y_{1}+ \\cdots ), \\dots, (\\cdots + a_{mn}x_n) + (\\cdots + a_{mn}y_{n}) \\\\ \u0026amp;= ((a_{11}x_1 + \\cdots), \\dots, (\\cdots + a_{mn}x_n)) = ((a_{11}y_{1}+ \\cdots ), \\dots,(\\cdots + a_{mn}y_{n})) \\\\ \u0026amp;= T (x_1, \\dots, x_{n}) + T (y_1, \\dots, x_{n}) \\end{align}\nhomogeneity\nProof left to the reader. Pretty much just expand and more algebra.\nmatricies to encode Linear Map we can use matricies to represent Linear Maps. See matrix of Linear Map\n\u0026ldquo;basis of domain\u0026rdquo; This result tells us that we can find a Linear Map for wherever we want to take the basis of a vector space, and that a Linear Map\u0026rsquo;s behavior on basis uniquely determines that Linear Map.\nSee basis of domain.\naddition and scalar multiplication on \\(\\mathcal{L}(V,W)\\) Suppose \\(S,T \\in \\mathcal{L}(V,W); \\lambda \\in \\mathbb{F}\\).\n\u0026ldquo;Sum\u0026rdquo; and \u0026ldquo;Product\u0026rdquo; are defined in the way that one would expect:\n\\begin{equation} (S+T)(v) = Sv+Tv \\end{equation}\nand\n\\begin{equation} (\\lambda T)(v) = \\lambda (Tv) \\end{equation}\nfor all \\(v \\in V\\).\nThese two operations make \\(\\mathcal{L}(V,W)\\) a vector space (\\(1Tv = Tv\\), \\(0+Tv=Tv\\), \\(Tv + (-1)Tv = 0\\), associativity, commutativity, distributive inherits from \\(V\\).)\nlinear maps take \\(0\\) to \\(0\\) We desire that \\(T(0) = 0\\) for any linear map \\(T\\)\nProof:\n\\begin{equation} T(0) = T(0+0) \\end{equation}\nThen, by additivity:\n\\begin{equation} T(0) = T (0 + 0) = T (0) + T (0) \\end{equation}\nGiven \\(\\mathcal{L}(V,W)\\) is a vector space for any \\(V,W\\), \\(\\exists -T(0)\\) such that \\(T(0)+(-T(0)) = 0\\). Applying that here:\n\\begin{equation} T(0) = T(0)+T(0) \\implies T(0) -T(0) = T(0)+T(0)-T(0) \\implies 0 = T(0) \\end{equation}\nProduct of Linear Maps See Product of Linear Maps\n\u0026ldquo;sizes\u0026rdquo; of maps map to smaller space is not injective Suppose \\(V,W\\) are finite-dimensional vector spaces, and \\(\\dim V \u0026gt; \\dim W\\). Then, all \\(T \\in \\mathcal{L}(V,W)\\) are not injective.\nWe first have that:\n\\begin{align} \u0026amp;\\dim V = \\dim null\\ T + \\dim range\\ T \\\\ \\Rightarrow\\ \u0026amp; \\dim null\\ T = \\dim V - \\dim range\\ T \\end{align}\nrecall at this point that \\(\\dim range\\ T \\leq \\dim W\\) (the range is a subspace of the codomain.) Therefore, subtracting a bigger value means that the value will be smaller. So we have that:\n\\begin{align} \u0026amp; \\dim null\\ T = \\dim V - \\dim range\\ T \\\\ \\Rightarrow\\ \u0026amp; \\dim null\\ T \\geq \\dim V - \\dim W \\end{align}\nNow, recall that \\(\\dim V \u0026gt; \\dim W\\). Therefore, \\(\\dim V - \\dim W\\) is strictly bigger than \\(0\\). So:\n\\begin{align} \\dim null\\ T \u0026amp;\\geq \\dim V - \\dim W \\\\ \u0026amp;\u0026gt; 0 \\end{align}\nAnd so, the dimension of the null space of \\(T\\) is not \\(0\\). Therefore, the null space of \\(T\\) can\u0026rsquo;t have been \\(\\{0\\}\\) because that does have dimension \\(0\\). This makes the map not injective because injectivity implies that null space is \\(\\{0\\}\\)\nmap to bigger space is not surjective Its basically the same thing as the one above. Suppose \\(V,W\\) are finite-dimensional vector spaces, and \\(\\dim V \u0026lt; \\dim W\\). Then, all \\(T \\in \\mathcal{L}(V,W)\\) are not injective.\nWe first have that:\n\\begin{align} \u0026amp;\\dim V = \\dim null\\ T + \\dim range\\ T \\\\ \\Rightarrow\\ \u0026amp; \\dim range\\ T = \\dim V - \\dim null\\ T \\end{align}\nBecause the dimension of \\(null\\ T\\) is larger than \\(0\\) (or, for that matter, the dimension of anything), \\(\\dim V - \\dim\\ null\\ T \\leq \\dim\\ V\\). Hence:\n\\begin{align} \u0026amp; \\dim range\\ T = \\dim V - \\dim null\\ T \\\\ \\Rightarrow\\ \u0026amp; \\dim range\\ T \\leq \\dim V \\end{align}\nNow, recall that \\(\\dim V \u0026lt; \\dim W\\).\n\\begin{align} \u0026amp; \\dim range\\ T = \\dim V - \\dim null\\ T \\\\ \\Rightarrow\\ \u0026amp; \\dim range\\ T \\leq \\dim V \u0026lt; \\dim W \\end{align}\nGiven the range of \\(T\\) is smaller than the codomain of \\(T\\), they cannot be equal spaces. So, \\(T\\) is not surjective.\n","permalink":"https://www.jemoka.com/posts/kbhlinear_map/","tags":null,"title":"Linear Map"},{"categories":null,"contents":"general form of First-Order Differential Equations This will depend on both unknown function \\(x\\), and the independent variable \\(t\\). These could and could not be separable.\n\\begin{equation} \\dv{x}{t} = F(t,x),\\ x(t_{0}) = x_{0} \\end{equation}\nLet\u0026rsquo;s imagine \\(F\\) is \u0026ldquo;bounded\u0026rdquo; and \u0026ldquo;continuous\u0026rdquo; on \\(I \\times \\omega\\), where \\(I\\) is an open interval about \\(t_{0}\\) and \\(\\omega\\) is an open subset of \\(\\mathbb{R}^{n}\\), containing \\(x_{0}\\). \\(F\\) is bounded; the results are bounded??\nfunctions embedded in vector spaces We understand that such First-Order Differential Equations will describe a subset of an infinite dimensional vector space.\nGiven we are dealing with First-Order Differential Equations, each function is a basis (if linear, otherwise, not quite the basis) of the subspace of the larger vector space; \\(+C\\) is how you create parametried variations However, our function is not linear, not all functions would suffice here: non-linear equations are difficult to deal with beacuse the arc length follows a certain pattern General form of a first order linear differential equation A general linear, first-order, first-degree differential equation of the form:\n\\begin{equation} \\dv{y}{x} + P(x)y = Q(x) \\end{equation}\nhas a solution:\n\\begin{equation} y(x) = e^{-\\int P\\dd{x}}\\int e^{\\int P\\dd{x}} Q(x) \\dd{x} \\end{equation}\nthe more general solution (for definite integrals):\n\\begin{equation} x(t) = e^{-A(t)}x_{0} + e^{-A(t)}\\int_{t_{0}}^{t}e^{A(s)}b(s)\\dd{s} \\end{equation}\ngiven the initial condition that \\(x(0) = 0\\). This is from the textbook.\nBefore you go ham and start solving, though, make sure that pesky \\(y\\) term is actually there. If its not, you maybe better served using the seperable methods to solve these things.\nthis is bad This is difficult to deal with this! What?? How?? Why does this work?? See below.\nsolving differential equations The following technique works for ALL first-order linear differential equations:\nTo solve, first put your equation into the standard form:\n\\begin{equation} \\frac{dy}{dx} + P(x)y = Q(x) \\end{equation}\nIf you have an equation like:\n\\begin{equation} a(x) \\dv{y}{x} + b(x)y = c(x) \\end{equation}\na good way to do this is to apply \\(\\frac{1}{a(x)}\\) to both sides, resulting in:\n\\begin{equation} \\dv{y}{x} + \\frac{b(x)}{a(x)} y = \\frac{c(x)}{a(x)} \\end{equation}\nAnd then you can carry on solving like its an equation in standard form.\nTo solve such a generic equation, we here are trying to UNDO the product rule.\nWe first multiply the entire expression by something called the intergrating factor \\(\\rho(x)\\).\n\\begin{equation} \\rho(x) \\left(\\frac{dy}{dx} + P(x)y\\right) = \\rho(x)Q(x) \\end{equation}\nA note on how this \\(\\rho(x)\\) works. This intergrating factor is actually defined with the following rule:\n\\begin{equation} \\log (\\rho (x)) = \\int P(x) \\dd{x} \\end{equation}\n(notably, \\(\\log\\) is actually \\(\\ln\\) in this case.)\nWhy so weird of an expression? This all springs from the fact that \\(\\dv x e^{x} = e^{x}\\). See below on how this fact is stretched (to great lengths) to solve diffeqs.\nFrom the above expression containing \\(\\rho (x)\\), we naturally have that (based on the definition of the natural log, just expanding it out):\n\\begin{equation} e^{\\int P(x)\\dd{x}} = \\rho (x) \\end{equation}\nWhy is this useful? Remember, we are trying to undo the product rule. Let\u0026rsquo;s replace our new definition for \\(\\rho (x)\\) into the above expression we are trying to solve and see what happens!\n\\begin{align} \u0026amp;\\rho (x)\\qty (\\dv{y}{x} + P(x)y) = \\rho (x)Q(x) \\\\ \\Rightarrow\\ \u0026amp; e^{\\int P\\dd{x}} \\qty (\\dv{y}{x} + P(x)y) = e^{\\int P\\dd{x}} Q(x) \\end{align}\nFor a second now, let\u0026rsquo;s just take an aside and deal with the left side. We are starting to almost clearly see the product rule at play here. Let\u0026rsquo;s finish the job by finishing up the rest of the product rule. Remember, we want to go opposite the product rule at the next steps.\n\\begin{align} e^{\\int P\\dd{x}} \\qty (\\dv{y}{x} + P(x)y) \u0026amp;= \\dv{y}{x}e^{\\int p\\dd{x}} + yPe^{\\int P\\dd{x}} \\\\ \u0026amp;= \\dv x \\qty (ye^{\\int P\\dd{x}}) \\end{align}\nWoah! Now we have something clearly in the favor of \\(y\\) separated out. Let\u0026rsquo;s put this back to our original expression.\n\\begin{align} \u0026amp;e^{\\int P\\dd{x}} \\qty (\\dv{y}{x} + P(x)y) = e^{\\int P\\dd{x}} Q(x) \\\\ \\Rightarrow\\ \u0026amp; \\dv x \\qty (ye^{\\int P\\dd{x}}) = e^{\\int P\\dd{x}} Q(x) \\end{align}\nNice. Now, do you see the clear step to isolate \\(y\\) by itself? I do.\n\\begin{align} \u0026amp;\\dv x \\qty (ye^{\\int P\\dd{x}}) = e^{\\int P\\dd{x}} Q(x) \\\\ \\Rightarrow\\ \u0026amp; \\int \\dv x \\qty (ye^{\\int P\\dd{x}}) \\dd{x}= \\int e^{\\int P\\dd{x}} Q(x) \\dd{x}\\\\ \\Rightarrow\\ \u0026amp; ye^{\\int P\\dd{x}} = \\int e^{\\int P\\dd{x}} Q(x) \\dd{x} \\end{align}\nAnd finally, naturally and lastly, we divide the \\(e^{\\int P\\dd{x}}\\) to both sides.\n\\begin{align} \u0026amp; ye^{\\int P\\dd{x}} = \\int e^{\\int P\\dd{x}} Q(x) \\dd{x}\\\\ \\Rightarrow\\ \u0026amp; y = e^{-\\int P\\dd{x}}\\int e^{\\int P\\dd{x}} Q(x) \\dd{x}\\ \\blacksquare \\end{align}\nAnd there you have it. That\u0026rsquo;s the general solution to our diffeq.\n","permalink":"https://www.jemoka.com/posts/kbhlinear_non_seperable_equation/","tags":null,"title":"Linear Non-Seperable Equation"},{"categories":null,"contents":"CAPM, a Review Note that we will be using the Sharpe-Linter version of CAPM:\n\\begin{equation} E[R_{i}-R_{f}] = \\beta_{im} E[(R_{m}-R_{f})] \\end{equation}\n\\begin{equation} \\beta_{im} := \\frac{Cov[(R_{i}-R_{f}),(R_{m}-R_{f})]}{Var[R_{m}-R_{f}]} \\end{equation}\nRecall that we declare \\(R_{f}\\) (the risk-free rate) to be non-stochastic.\nLet us begin. We will create a generic function to analyze some given stock.\nData Import We will first import our utilities\nimport pandas as pd import numpy as np Let\u0026rsquo;s load the data from our market (NYSE) as well as our 10 year t-bill data.\nt_bill = pd.read_csv(\u0026#34;./linearity_test_data/10yrTBill.csv\u0026#34;) nyse = pd.read_csv(\u0026#34;./linearity_test_data/NYSEComposite.csv\u0026#34;) nyse.head() Date Close 0 11/7/2013 16:00:00 9924.37 1 11/8/2013 16:00:00 10032.14 2 11/11/2013 16:00:00 10042.95 3 11/12/2013 16:00:00 10009.84 4 11/13/2013 16:00:00 10079.89 Excellent. Let\u0026rsquo;s load in the data for that stock.\ndef load_stock(stock): return pd.read_csv(f\u0026#34;./linearity_test_data/{stock}.csv\u0026#34;) load_stock(\u0026#34;LMT\u0026#34;).head() Date Close 0 11/7/2013 16:00:00 136.20 1 11/8/2013 16:00:00 138.11 2 11/11/2013 16:00:00 137.15 3 11/12/2013 16:00:00 137.23 4 11/13/2013 16:00:00 137.26 Raw Data And now, let\u0026rsquo;s load all three stocks, then concatenate them all into a big-ol DataFrame.\n# load data df = { \u0026#34;Date\u0026#34;: nyse.Date, \u0026#34;NYSE\u0026#34;: nyse.Close, \u0026#34;TBill\u0026#34;: t_bill.Close, \u0026#34;LMT\u0026#34;: load_stock(\u0026#34;LMT\u0026#34;).Close, \u0026#34;TWTR\u0026#34;: load_stock(\u0026#34;TWTR\u0026#34;).Close, \u0026#34;MCD\u0026#34;: load_stock(\u0026#34;MCD\u0026#34;).Close } # convert to dataframe df = pd.DataFrame(df) # drop empty df.dropna(inplace=True) df Date NYSE TBill LMT TWTR MCD 0 11/7/2013 16:00:00 9924.37 26.13 136.20 44.90 97.20 1 11/8/2013 16:00:00 10032.14 27.46 138.11 41.65 97.01 2 11/11/2013 16:00:00 10042.95 27.51 137.15 42.90 97.09 3 11/12/2013 16:00:00 10009.84 27.68 137.23 41.90 97.66 4 11/13/2013 16:00:00 10079.89 27.25 137.26 42.60 98.11 ... ... ... ... ... ... ... 2154 10/24/2022 16:00:00 14226.11 30.44 440.11 39.60 252.21 2155 10/25/2022 16:00:00 14440.69 31.56 439.30 39.30 249.28 2156 10/26/2022 16:00:00 14531.69 33.66 441.11 39.91 250.38 2157 10/27/2022 16:00:00 14569.90 34.83 442.69 40.16 248.36 2158 10/28/2022 16:00:00 14795.63 33.95 443.41 39.56 248.07 [2159 rows x 6 columns] Log Returns Excellent. Now, let\u0026rsquo;s convert all of these values into daily log-returns (we don\u0026rsquo;t really care about the actual pricing.)\nlog_returns = df[[\u0026#34;NYSE\u0026#34;, \u0026#34;TBill\u0026#34;, \u0026#34;LMT\u0026#34;, \u0026#34;TWTR\u0026#34;, \u0026#34;MCD\u0026#34;]].apply(np.log, inplace=True) df.loc[:, [\u0026#34;NYSE\u0026#34;, \u0026#34;TBill\u0026#34;, \u0026#34;LMT\u0026#34;, \u0026#34;TWTR\u0026#34;, \u0026#34;MCD\u0026#34;]] = log_returns df Date NYSE TBill LMT TWTR MCD 0 11/7/2013 16:00:00 9.202749 3.263084 4.914124 3.804438 4.576771 1 11/8/2013 16:00:00 9.213549 3.312730 4.928050 3.729301 4.574814 2 11/11/2013 16:00:00 9.214626 3.314550 4.921075 3.758872 4.575638 3 11/12/2013 16:00:00 9.211324 3.320710 4.921658 3.735286 4.581492 4 11/13/2013 16:00:00 9.218298 3.305054 4.921877 3.751854 4.586089 ... ... ... ... ... ... ... 2154 10/24/2022 16:00:00 9.562834 3.415758 6.087025 3.678829 5.530262 2155 10/25/2022 16:00:00 9.577805 3.451890 6.085183 3.671225 5.518577 2156 10/26/2022 16:00:00 9.584087 3.516310 6.089294 3.686627 5.522980 2157 10/27/2022 16:00:00 9.586713 3.550479 6.092870 3.692871 5.514879 2158 10/28/2022 16:00:00 9.602087 3.524889 6.094495 3.677819 5.513711 [2159 rows x 6 columns] And now, the log returns! We will shift this data by one column and subtract.\nreturns = df.drop(columns=[\u0026#34;Date\u0026#34;]) - df.drop(columns=[\u0026#34;Date\u0026#34;]).shift(1) returns.dropna(inplace=True) returns NYSE TBill LMT TWTR MCD 1 0.010801 0.049646 0.013926 -0.075136 -0.001957 2 0.001077 0.001819 -0.006975 0.029570 0.000824 3 -0.003302 0.006161 0.000583 -0.023586 0.005854 4 0.006974 -0.015657 0.000219 0.016568 0.004597 5 0.005010 -0.008476 0.007476 0.047896 -0.005622 ... ... ... ... ... ... 2154 0.005785 0.004940 -0.023467 -0.014291 0.001349 2155 0.014971 0.036133 -0.001842 -0.007605 -0.011685 2156 0.006282 0.064420 0.004112 0.015402 0.004403 2157 0.002626 0.034169 0.003575 0.006245 -0.008100 2158 0.015374 -0.025590 0.001625 -0.015053 -0.001168 [2158 rows x 5 columns] Risk-Free Excess Recall that we want to be working with the excess-to-risk-free rates \\(R_{T}-R_{f}\\), where \\(R_{T}\\) is some security. So, we will go through and subtract everything by the risk-free rate (and drop the RFR itself):\nrisk_free_excess = returns.drop(columns=\u0026#34;TBill\u0026#34;).apply(lambda x: x-returns.TBill) risk_free_excess NYSE LMT TWTR MCD 1 -0.038846 -0.035720 -0.124783 -0.051603 2 -0.000742 -0.008794 0.027751 -0.000995 3 -0.009463 -0.005577 -0.029747 -0.000307 4 0.022630 0.015875 0.032225 0.020254 5 0.013486 0.015952 0.056372 0.002854 ... ... ... ... ... 2154 0.000845 -0.028406 -0.019231 -0.003591 2155 -0.021162 -0.037975 -0.043738 -0.047818 2156 -0.058138 -0.060308 -0.049017 -0.060017 2157 -0.031543 -0.030593 -0.027924 -0.042269 2158 0.040964 0.027215 0.010537 0.024422 [2158 rows x 4 columns] Actual Regression It is now time to perform the actual linear regression! We will use statsmodels\u0026rsquo; Ordinary Least Squares API to make our work easier, but we will go through a full regression in the end.\nimport statsmodels.api as sm CAPM Regression: Lockheed Martin Let\u0026rsquo;s work with Lockheed Martin first for regression, fitting an ordinary least squares. Remember that the OLS functions reads the endogenous variable first (for us, the return of the asset.)\n# add a column of ones to our input market excess returns nyse_with_bias = sm.add_constant(risk_free_excess.NYSE) # perform linreg lmt_model = sm.OLS(risk_free_excess.LMT, nyse_with_bias).fit() lmt_model.summary() OLS Regression Results ============================================================================== Dep. Variable: LMT R-squared: 0.859 Model: OLS Adj. R-squared: 0.859 Method: Least Squares F-statistic: 1.312e+04 No. Observations: 2158 AIC: -1.263e+04 Df Residuals: 2156 BIC: -1.262e+04 Df Model: 1 Prob (F-statistic): 0.00 Covariance Type: nonrobust Log-Likelihood: 6318.9 ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0004 0.000 1.311 0.190 -0.000 0.001 NYSE 0.9449 0.008 114.552 0.000 0.929 0.961 ============================================================================== Based on the constants row, we can see that\u0026mdash;within \\(95\\%\\) confidence\u0026mdash;the intercept is generally \\(0\\) and CAPM applies. However, we do see a slight positive compared to the market. Furthermore, we can see that the regression has a beta value of \\(0.9449\\) \u0026mdash; according the CAPM model, it being slightly undervarying that the market.\nCAPM Regression: MacDonald\u0026rsquo;s # perform linreg mcd_model = sm.OLS(risk_free_excess.MCD, nyse_with_bias).fit() mcd_model.summary() OLS Regression Results ============================================================================== Dep. Variable: MCD R-squared: 0.887 Model: OLS Adj. R-squared: 0.887 Method: Least Squares F-statistic: 1.697e+04 No. Observations: 2158 AIC: -1.310e+04 Df Residuals: 2156 BIC: -1.309e+04 Df Model: 1 Prob (F-statistic): 0.00 Covariance Type: nonrobust Log-Likelihood: 6551.1 ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0003 0.000 1.004 0.315 -0.000 0.001 NYSE 0.9651 0.007 130.287 0.000 0.951 0.980 ============================================================================== Same thing as before, we are within \\(95\\%\\) confidence having a intercept of \\(0\\) (with a slight positive edge), and it looks like MacDonald\u0026rsquo;s vary a little bit more than Lockheed Martin. The food industry is probably a tougher business than that in defense.\nCAPM Regression: Twitter Lastly, to analyze the recently delisted Twitter!\n# perform linreg twtr_model = sm.OLS(risk_free_excess.TWTR, nyse_with_bias).fit() twtr_model.summary() OLS Regression Results ============================================================================== Dep. Variable: TWTR R-squared: 0.522 Model: OLS Adj. R-squared: 0.522 Method: Least Squares F-statistic: 2357. No. Observations: 2158 AIC: -8610. Df Residuals: 2156 BIC: -8599. Df Model: 1 Prob (F-statistic): 0.00 Covariance Type: nonrobust Log-Likelihood: 4307.1 ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0002 0.001 -0.346 0.730 -0.002 0.001 NYSE 1.0173 0.021 48.549 0.000 0.976 1.058 ============================================================================== Evidently, Twitter is much more variable. It looks like it has a nontrivial bias (the intercept being -0.001 being within the \\(95\\%\\) confidence band \u0026mdash; that the security is possibly significantly underperforming the CAPM expectation in the market.) Furthermore, we have a positive beta value: that the asset is more variable than the market.\nManual Checking We can also use the betas formula to manually calculate what we expect for the beta values (i.e. as if they were one IID random variable.)\nrisk_free_cov = risk_free_excess.cov() risk_free_cov NYSE LMT TWTR MCD NYSE 0.001143 0.001080 0.001163 0.001103 LMT 0.001080 0.001188 0.001116 0.001083 TWTR 0.001163 0.001116 0.002264 0.001155 MCD 0.001103 0.001083 0.001155 0.001200 Finally, to construct the beta values. Recall that:\n\\begin{equation} \\beta_{im} := \\frac{Cov[(R_{i}-R_{f}),(R_{m}-R_{f})]}{Var[R_{m}-R_{f}]} \\end{equation}\nand that:\n\\begin{equation} Var[X] = Cov[X,X], \\forall X \\end{equation}\n# get the market variance (covariance with itself) market_variation = risk_free_cov.NYSE.NYSE # calculate betas betas = {\u0026#34;LMT\u0026#34;: (risk_free_cov.LMT.NYSE/market_variation), \u0026#34;TWTR\u0026#34;: (risk_free_cov.TWTR.NYSE/market_variation), \u0026#34;MCD\u0026#34;: (risk_free_cov.MCD.NYSE/market_variation)} # and make dataframe betas = pd.Series(betas) betas LMT 0.944899 TWTR 1.017294 MCD 0.965081 dtype: float64 Apparently, all of our assets swing less than the overall NYSE market! Especially Lockheed\u0026mdash;it is only \\(94.4\\%\\) of the market variation. Furthermore, it is interesting to see that Twitter swings much more dramatically compared to the market.\nEqual-Part Fund We will now create two funds with the three securities, one with equal parts and one which attempts to maximizes the bias (max returns) while minimizing the beta variance value compared to the market.\nFirst, let\u0026rsquo;s create a baseline fund in equal parts. Here it is:\nfund_1_returns = returns.LMT + returns.TWTR + returns.MCD fund_1_returns 1 -0.063167 2 0.023420 3 -0.017149 4 0.021384 5 0.049750 ... 2154 -0.036409 2155 -0.021132 2156 0.023917 2157 0.001720 2158 -0.014596 Length: 2158, dtype: float64 We will calculate the excess returns of this fund:\nfund_1_excess = fund_1_returns-returns.TBill fund_1_excess 1 -0.112813 2 0.021600 3 -0.023310 4 0.037041 5 0.058226 ... 2154 -0.041349 2155 -0.057265 2156 -0.040503 2157 -0.032449 2158 0.010994 Length: 2158, dtype: float64 Performance of the Equal-Part Fund # perform linreg fund_1_model = sm.OLS(fund_1_excess, nyse_with_bias).fit() fund_1_model.summary() OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.473 Model: OLS Adj. R-squared: 0.473 Method: Least Squares F-statistic: 1935. No. Observations: 2158 AIC: -7735. Df Residuals: 2156 BIC: -7724. Df Model: 1 Prob (F-statistic): 3.01e-302 Covariance Type: nonrobust Log-Likelihood: 3869.5 ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0007 0.001 0.841 0.401 -0.001 0.002 NYSE 1.1290 0.026 43.993 0.000 1.079 1.179 ============================================================================== Surprisingly, we have now created a significantly riskier investment that, though riskier, generates a much higher probability of reward (\\(+0.001\\) is now within the \\(99\\%\\) band!)\nA More Optimized Fund To me, this is the payoff of this assignment. We will now use CAPM to create the \u0026ldquo;best\u0026rdquo; fund combination\u0026mdash;given some variance, the funds which match CAPM. To do this, let\u0026rsquo;s create a generic linear combination of the assets.\nimport sympy as sym x = sym.Symbol(\u0026#39;x\u0026#39;) y = sym.Symbol(\u0026#39;y\u0026#39;) z = sym.Symbol(\u0026#39;z\u0026#39;) fund_2_returns = x*returns.LMT + y*returns.TWTR + z*returns.MCD fund_2_returns 1 0.0139260753744255*x - 0.0751364261353569*y - ... 2 -0.00697525170622448*x + 0.0295704573211193*y ... 3 0.000583132897928884*x - 0.0235859990058791*y ... 4 0.000218587198947517*x + 0.016568426347233*y +... 5 0.00747599199607762*x + 0.0478955096700351*y -... ... 2154 -0.0234665578621085*x - 0.0142913301107561*y +... 2155 -0.00184214468578059*x - 0.0076045993852194*y ... 2156 0.00411172646842317*x + 0.0154024001854269*y +... 2157 0.00357547337231878*x + 0.0062445563228315*y -... 2158 0.00162509910496933*x - 0.0150529686289622*y -... Length: 2158, dtype: object Excellent. We will also calculate the excess returns of this fund:\nfund_2_excess = fund_2_returns-returns.TBill Y = fund_2_excess.to_numpy() Y [0.0139260753744255*x - 0.0751364261353569*y - 0.00195664549320096*z - 0.0496463208073039 -0.00697525170622448*x + 0.0295704573211193*y + 0.000824317408861575*z - 0.00181917459665826 0.000583132897928884*x - 0.0235859990058791*y + 0.00585367525146019*z - 0.00616055581298536 ... 0.00411172646842317*x + 0.0154024001854269*y + 0.00440300114913317*z - 0.0644196927849867 0.00357547337231878*x + 0.0062445563228315*y - 0.0081004573348249*z - 0.0341688956152497 0.00162509910496933*x - 0.0150529686289622*y - 0.00116834209450634*z + 0.0255902303732043] We cast this type to a numpy array because we are about to perform some matrix operations upon it.\nOptimizing the Optimized Fund: Linreg Now, let us perform the actual linear regression ourselves. Recall that the pseudoinverse linear regression estimator is:\n\\begin{equation} \\beta = (X^{T}X)^{-1}X^{T}Y \\end{equation}\nWe have already our \\(Y\\) as a vector (above), and our \\(X\\) is:\nX = nyse_with_bias.to_numpy() X [[ 1.00000000e+00 -3.88457302e-02] [ 1.00000000e+00 -7.42217926e-04] [ 1.00000000e+00 -9.46284244e-03] ... [ 1.00000000e+00 -5.81378271e-02] [ 1.00000000e+00 -3.15429207e-02] [ 1.00000000e+00 4.09643405e-02]] We now have our matrices, let\u0026rsquo;s perform the linear regression!\nlinear_model = np.linalg.inv((X.transpose()@X))@X.transpose()@Y linear_model [0.000544056413840724*x - 6.62061061591867e-5*y + 0.000429966553373172*z - 0.000178620725465344 0.0457830563134785*x + 0.118178191274045*y + 0.0659651260604729*z + 0.899115719100281] Excellent. So we now have two rows; the top row represents the \u0026ldquo;bias\u0026rdquo;\u0026mdash;how much deviation there is from CAPM, and the bottom row represents the \u0026ldquo;rate\u0026rdquo;\u0026mdash;the \u0026ldquo;beta\u0026rdquo; value which represents how much excess variance there is.\nOptimizing the Optimized Fund: Picking Optimizing Parameters We can will solve for a combination of solutions to give us specific values of returns vs risk. For instance, we can fix the variance to 1 (i.e. we can vary as much as the market.) We subtract one here for the solver, which expects the expressions equaling to \\(0\\).\nrisk_expr = linear_model[1] - 1 risk_expr 0.0457830563134785*x + 0.118178191274045*y + 0.0659651260604729*z - 0.100884280899719 Now, we will set a certain earning value, and solve for possible solutions. We will try to get the largest possible bias without needing to short something (i.e. cause a negative solution). By hand-fisting a value, it seems 0.001 is a good bet.\ndeviance_expr = linear_model[0] - 0.001 deviance_expr 0.000544056413840724*x - 6.62061061591867e-5*y + 0.000429966553373172*z - 0.00117862072546534 Optimizing the Optimized Fund: Optimize! solution = sym.solvers.solve([deviance_expr, risk_expr], x,y,z) solution {x: 2.16803104555387 - 0.819584899551304*z, y: 0.0137520589394366 - 0.24067066980814*z} We have one degree of freedom here: how much MacDonald\u0026rsquo;s we want! Let\u0026rsquo;s say we want none (which would, according to this, be an equally efficient solution.)\nHow Does Our Fund Do? This would create the following plan:\n# for our case z_val = 0 # numerical solutions s_x = solution[x].subs(z,z_val) s_y = solution[y].subs(z,z_val) # solution fund_2_nobias_nomac = s_x*returns.LMT + s_y*returns.TWTR + z_val*returns.MCD fund_2_nobias_nomac.mean() 0.001185050286566688 Recall that this is the performance of the balanced portfolio:\nfund_1_returns.mean() 0.0009224705380695683 So, for market-level risk (\\(\\beta =1\\), instead of the balanced portfolio\u0026rsquo;s \\(\\beta =1.1290\\)), this is a pretty good deal!\nSome Plots Finally, let\u0026rsquo;s plot the prices of our various funds:\nimport matplotlib.pyplot as plt import matplotlib.dates as mdates import seaborn as sns from datetime import datetime sns.set() fund_2_price = s_x*df.LMT + s_y*df.TWTR + z_val*df.MCD fund_1_price = df.LMT + df.TWTR fund_l_price = df.LMT fund_t_price = df.TWTR dates = df.Date.apply(lambda x:datetime.strptime(x, \u0026#34;%m/%d/%Y %H:%M:%S\u0026#34;)) sns.lineplot(x=dates, y=fund_2_price.apply(sym.Float).astype(float)) sns.lineplot(x=dates, y=fund_1_price.apply(sym.Float).astype(float)) sns.lineplot(x=dates, y=fund_l_price.apply(sym.Float).astype(float)) sns.lineplot(x=dates, y=fund_t_price.apply(sym.Float).astype(float)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.gca().set_ylabel(\u0026#34;Price\u0026#34;) plt.show() Recall that we didn\u0026rsquo;t actually buy any MacDonald\u0026rsquo;s. So, we have\u0026mdash;blue, being our \u0026ldquo;optimal\u0026rdquo; portfolio, yellow, our balanced portfolio, green, being Lockheed, and red, being Twitter.\nOur portfolio works surprisingly well!\n","permalink":"https://www.jemoka.com/posts/linearity_tests_preso/","tags":null,"title":"Linearity Tests"},{"categories":null,"contents":" Using P2P to trade stocks in a darkpool Sweep across LiquidNet and send normall if not needed ","permalink":"https://www.jemoka.com/posts/kbhliquidnet/","tags":null,"title":"LiquidNet"},{"categories":null,"contents":"A list is an ordered collection of \\(n\\) elements.\nrequirements as list length cannot be negative list length cannot be \\(\\infty\\) repetition matters order matters additional info two lists are equal IFF they have same \\(n\\) same elements same order they are different from sets because order matters (therefore, because in/out is no longer a binary) number of entries of the same object matters length is finite ","permalink":"https://www.jemoka.com/posts/kbhlist/","tags":null,"title":"list"},{"categories":null,"contents":" Number Name 31 Herber Hoover 32 Franklin D. Roosevelt (FDR) ","permalink":"https://www.jemoka.com/posts/kbhlist_of_american_presidents/","tags":null,"title":"list of American presidents"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhlog_laws/","tags":null,"title":"log laws"},{"categories":null,"contents":"TODO: connect Logan with a few fire departments\n","permalink":"https://www.jemoka.com/posts/kbhlogan_s_team_check_in/","tags":null,"title":"Logan's Team Checkin"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhloop_invariant/","tags":null,"title":"loop invariant"},{"categories":null,"contents":"DOI: 10.1101/2021.03.24.21254263\nOne-Liner Review paper presenting the \\(ADReSS_o\\) challenge and current baselines for three tasks\nNotes Three tasks + state of the art:\nClassification of AD: accuracy \\(78.87\\%\\) Prediction of MMSE score: RMSE \\(5.28\\) Prediction of cognitive decline: accuracy \\(68.75\\%\\) Task 1 AD classification baseline established by decision tree with late fusion\n(LOOCV and test)\nTask 2 MMSE score prediction baseline established by grid search on parameters.\nSVR did best on both counts; results from either model are averaged for prediction.\nTask 3 Same thing here, DT does better but notably its F1 is smaller; data trained with final late fusion\n","permalink":"https://www.jemoka.com/posts/kbhluz_2021/","tags":["ntj"],"title":"Luz 2021"},{"categories":null,"contents":"Seed: walking, loving\nWalking\nSkipping\nShoes\nRoad\nRunning\nForward\nSpeed\nPlane\nTravel\nUnique\nCold\nHouse\nLoving\nCuddling\nKissing\nHolding\nTogether\nStaring\nLonging\nEstablish\nSpending time\nWaving\nWelling\nWalking together, staring forward longing you\nLoving together, skipping forward, a cold house\nCuddling down the avenue, spending time there, Waving by\nEstablish what it\u0026rsquo;s like,\n","permalink":"https://www.jemoka.com/posts/kbhlyrics_ping/","tags":null,"title":"Lyrics: Ping"},{"categories":null,"contents":"Seed: explore, wild\nexplore\nlearn\nresources\nmineral\ndetail\nfeature\nfact\npolice\nduty\nparticulars\ndeposit\nassign\nundertake\nnatural\nenvironment\ncultivate\nregion\nharshly\nuntrusting\nnervous\nincreasing\nchanging\nperiod\nbecome greater\nWe go explore, changing times, parting ways.\nWanting no praise, become greater Than ever\nWe go explore, shining lights moving stars\nFinding no target, we cannot expect to see\nHow can we explore if we can\u0026rsquo;t even feed? Ourselves? Our families? Our digitaries?\nHow can we explore if we can\u0026rsquo;t even seek. Unatural exploration Touching the depths with our feet\nWe go explore, wondrous depths, random seas\nWanting someone, reminicing the never\nWe go explore, purple skies acid rain\nFinding the target, we didn\u0026rsquo;t know to see\nHow can we explore if we can\u0026rsquo;t even feed? Ourselves? Our families? Our digitaries?\nHow can we explore if we can\u0026rsquo;t even seek. Unatural exploration Probing the depths with our feet\n","permalink":"https://www.jemoka.com/posts/kbhlyrics_laws/","tags":null,"title":"Lyrics: Unnatural Exploration"},{"categories":null,"contents":"DOI: 10.1109/CISP-BMEI.2018.8633126\nA dataset paper with which auditory info about people talking is collected.\nHere are the state-of-the-art as of Laguarta 2021 on the dataset proposed.\n","permalink":"https://www.jemoka.com/posts/kbhlyu_2018/","tags":null,"title":"Lyu 2018"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.623607\nOne-Liner Trained a bimodal model on speech/text with GRU on speech and CNN-LSTM on text.\nNovelty A post-2019 NLP paper that doesn\u0026rsquo;t use transformers! (so faster (they used CNN-LSTM) lighter easier) \u0026ldquo;Our work sheds light on why the accuracy of these models drops to 72.92% on the ADReSS dataset, whereas, they gave state of the art results on the DementiaBank dataset.\u0026rdquo; Notable Methods Bi-Modal audio and transcript processing vis a vi Shah 2021, but with a CNN-LSTM and GRU on the other side.\nKey Figs Figure 1: Proposed Architecture The figure highlights the authors\u0026rsquo; proposed architecture\nFigure 2: confusion matrix In addition to validating prior work by Karlekar 2018 and Di Palo 2019, proposed model C and got accuracy of \\(73.92\\%\\).\n","permalink":"https://www.jemoka.com/posts/kbhmahajan_2021/","tags":["ntj"],"title":"Mahajan 2021"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmahatma_ghandi/","tags":null,"title":"Mahatma Ghandi"},{"categories":null,"contents":"Suppose \\(T \\in \\mathcal{L}(V)\\), and \\(U \\subset V\\), an invariant subspace under \\(T\\). Then:\n\\begin{equation} T|_{U}(u) = Tu,\\ \\forall u \\in U \\end{equation}\nwhere \\(T|_{U} \\in \\mathcal{L}(U)\\)\n","permalink":"https://www.jemoka.com/posts/kbhmap_restriction_operator/","tags":null,"title":"map restriction operator"},{"categories":null,"contents":"MapReduce is an distributed algorithm.\nMap: \\((in\\_key, in\\_value) \\Rightarrow list(out\\_key, intermediate\\_value)\\). Reduce: Group map outputs by \\(out\\_key\\) \\((out\\_key, list(intermediate\\_value)) \\Rightarrow list(out\\_value)\\) example of MapReduce Say, if you want to count word frequencies in a set of documents.\nMap: \\((document\\_name, document\\_contents) \\Rightarrow list(word, #\\ occurrences)\\) You can see that this can be distributed to multiple processors. You can have each processor count the word frequencies in a single document. We have now broken the contents into divide and conquerable groups.\nReduce: \\((word, list\\ (occurrences\\_per\\_document)) \\Rightarrow (word,sum)\\) We just add up the occurrences that each of the nodes\u0026rsquo; output for word frequency.\n","permalink":"https://www.jemoka.com/posts/kbhmapreduce/","tags":null,"title":"MapReduce"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmarkovian_process/","tags":null,"title":"markovian process"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmartin_luther_king/","tags":null,"title":"Martin Luther King"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.642647\nOne-Liner Combined bag-of-words on transcript + ADR on audio to various classifiers for AD; ablated BERT\u0026rsquo;s decesion space for attention to make more easy models in the future.\nNovelty Pre-processed each of the two modalities before fusing it (late fusion) Archieved \\(93.75\\%\\) accuracy on AD detection The data being forced-aligned and fed with late fusion allows one to see what sounds/words the BERT model was focusing on by just focusing on the attention on the words Notable Methods Used classic cookie theft data bag of words to do ADR but for words multimodality but late fusion with one (hot-swappable) classifier Key Figs How they did it This is how the combined the forced aligned (:tada:) audio and transcript together.\nBertbelation Ablated BERT results.\nThe model overall tends to focus on early parts of sentences. y is attention weight, x is position in sentence, blue is TD, red is AD.\nNew Concepts Active Data Representation ","permalink":"https://www.jemoka.com/posts/kbhmartinc_2021/","tags":["ntj"],"title":"Martinc 2021"},{"categories":null,"contents":"The Martingale Model states: if we observed the closing price of the market yesterday, we expect that the market is going to open at the close price yesterday.\nFormally:\n\\begin{equation} E\\qty [X_{k}|X_{k-1}, X_{k-2},\\ldots] = X_{k-1} \\end{equation}\n\u0026ldquo;irrespective of what you know, no matter how long the history, the best expectation of today\u0026rsquo;s price is yesterday\u0026rsquo;s price.\u0026rdquo;\nThis is not a for sure! modeling statement: this is simply the expected value!! That means, after \\(\\infty\\) times of re-running the universe starting \u0026ldquo;yesterday\u0026rdquo;, the new opening price will converge to the last closing price.\nTwo important conclusions:\nIf we know the closing price yesterday (it is observed), the price today will be DETERMINED and not!!! a random variable If the closing price yesterday is a random variable, the price today will be IN-DETERMINED and also a random variable Therefore, the \u0026ldquo;randomness is fair\u0026rdquo;, and therefore the \u0026ldquo;market is not drifting in favor/against you.\u0026rdquo;\nThe Martingale Model comes from the idea that \u0026ldquo;true gambling is true equal conditions (money, opponents, bystanders, situations, die, and dice.)\u0026rdquo; Therefore, any amount of bias towards one direction/party is advantageous for that person.\nIn fact, it was theorized that an efficient market should follow exactly this behavior.\nchanges in history Of course, the difference between the expression:\n\\begin{equation} E\\qty [X_{k}|X_{k-1}, X_{k-2},\\ldots] = X_{k-1} \\end{equation}\nversus\n\\begin{equation} E\\qty [X_{k}|X_{k-1}] = X_{k-1} \\end{equation}\nis pretty big. The two will only be the same if the markets is assumed to be a markovian process.\nMartingale historical conditioning Ok, if we are told that the process is Martingale, but we only have two days ago, what do we have?\ni.e. what if we want to know:\n\\begin{equation} E\\qty [X_{k} | X_{k-2}] = ? \\end{equation}\nTurns out, there\u0026rsquo;s a small trick you can do. Without even Martingale, we can claim that:\n\\begin{equation} E\\qty [X_{k} | X_{k-2}] = \\sum_{x} E\\qty [X_{k} | X_{k-1}, X_{k-1} = x] \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\end{equation}\nThat, the price today is just the sum of all possible prices for day \\(k-1\\) we name small \\(x\\) times the probability \\(Pr\\) that it actually happens given the existing \\(k-2\\) observation.\nOf course, given the Martingale Model now, given some possible price in day \\(k-1\\) named \\(x\\), price in \\(k\\) is also \\(x\\). Therefore:\n\\begin{equation} E[X_{k}|X_{k-1},X_{k-1} = x] =x \\end{equation}\nApplying this, then, we have\n\\begin{equation} \\sum_{x} E\\qty [X_{k} | X_{k-1}, X_{k-1} = x] \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) = \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\end{equation}\nThe right sum, then, is just the expected value of \\(X_{k-1}\\) given \\(X_{k-2}\\)!! Meaning:\n\\begin{equation} \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) = E[X_{k-1} | X_{k-2}] \\end{equation}\nNow, we are in a Martingale Model. Therefore:\n\\begin{equation} \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) = E[X_{k-1} | X_{k-2}] = X_{k-2} \\end{equation}\nAnd so, putting it all together, we have:\n\\begin{align} E\\qty [X_{k} | X_{k-2}] \u0026amp;= \\sum_{x} E\\qty [X_{k} | X_{k-1}, X_{k-1} = x] \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\\\ \u0026amp;= \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\\\ \u0026amp;= E[X_{k-1} | X_{k-2}] \\\\ \u0026amp;= X_{k-2} \\end{align}\nAmazing. So Martingale holds over time\n","permalink":"https://www.jemoka.com/posts/kbhmartingale_model/","tags":null,"title":"Martingale Model"},{"categories":null,"contents":"matricies are like buckets of numbers. ok, ok, seriously:\nmatricies are a way of encoding the basis of domain proof: that if Linear Maps are determined uniquely by where they map the basis anyways, why don\u0026rsquo;t we just make a mathematical object that represents that to encode the linear maps.\ndefinition Let \\(n\\), \\(m\\) be positive integer. An \\(m\\) by \\(n\\) matrix \\(A\\) is a rectangular array of elements of \\(\\mathbb{F}\\) with \\(m\\) rows and \\(n\\) columns:\n\\begin{equation} A = \\mqty(A_{1,1} \u0026amp; \\dots \u0026amp; A_{1,n} \\\\ \\vdots \u0026amp;\u0026amp; \\vdots \\\\ A_{m,1} \u0026amp; \\dots \u0026amp; A_{m,n}) \\end{equation}\nthe matrix representing a Linear Map \\(T\\) is noted as \\(\\mathcal{M}(T)\\). This maybe basis specific; see matrix of Linear Map for more.\nadditional information matrix of Linear Map This result codifies the claim that matricies represent Linear Maps by what they do to the basis of the space of concern.\nSuppose \\(T \\in \\mathcal{L}(V,W)\\), and \\(v_1, \\dots, v_{n}\\) is a basis of \\(V\\); and \\(w_1, \\dots w_{m}\\) is a basis of \\(W\\). Then, the matrix of \\(T\\) with respective to these basis is the \\(m\\) by \\(n\\) (rows by columns!) where:\n\\begin{equation} Tv_{k} = A_{1,k}w_1 + \\dots + A_{m,k}w_{m} \\end{equation}\nQuick memory of this result: inputs across columns, outputs across rows; think about how matrix is multiplied: you smash the input vector horizontally, across the top columns and down. Therefore, a matrix is written as: each columns contains the instructions of where to send each input basis, written as a linear combination down each row of that column of the output basis\nIF the basis being used in the matrix is unclear (i.e. if we had a change of basis, so didn\u0026rsquo;t use the standard basis, etc.), then the matrix of a SPECIFIC set of basis is written as: \\(\\mathcal{M}(T, (v_1, \\dots, v_n), (w_1, \\dots, w_{m}))\\).\nmatrix of a vector The matrix of a vector is just an encoding of scalars which needed to scale the basis of the space to add up to that vector.\nMore formally\u0026mdash;\nSuppose \\(v \\in V\\), and \\(v_1 \\dots v_{n}\\) is a basis of \\(V\\). The matrix representing vector \\(v\\) is the n-by-1 matrix:\n\\begin{equation} \\mathcal{M}(v) = \\mqty(c_1 \\\\ \\dots \\\\ c_{n}) \\end{equation}\nwhere \\(c_1 \\dots c_{n}\\) are the scalars such that:\n\\begin{equation} v = c_1v_1 + \\dots +c_{n}v_{n} \\end{equation}\ncolumn notation One can use a dot to index matricies\u0026rsquo; columns and rows.\nSuppose \\(A\\) is an \\(m\\) by \\(n\\) matrix.\nAT \\(1 \\leq j \\leq m\\), \\(A_{j ,.}\\) denotes the \\(1\\) by \\(n\\) matrix consisting only row \\(j\\) of \\(A\\) AT \\(1 \\leq k \\leq n\\), \\(A_{. ,k}\\) denotes the \\(m\\) by \\(k\\) matrix consisting only column \\(k\\) of \\(A\\) sums and scalar multiplication of matricies According to Jana, a third grader can add and scalar multiply matricies. So I am not going to write them here.\nHowever, what\u0026rsquo;s interesting is the fact that they actually work:\nSuppose \\(S,T \\in \\mathcal{L}(V,W)\\), then \\(\\mathcal{M}(S+T) = \\mathcal{M}(S)+\\mathcal{M}(T)\\) Suppose \\(\\lambda \\in \\mathbb{F}, T \\in \\mathcal{L}(V,W)\\), then \\(\\mathcal{M}(\\lambdaT) = \\lambda \\mathcal{M}(T)\\) The verification of this result, briefly, is that:\nRecall that matricies encode where each input basis get sent, as a linear combination of the output basis, down each column; recall that \\((S+T)v = Sv+Tv\\); now, write the sum of the matrix without performing the sum; apply the basis to the matrix; distribute the basis choordinates across the sum, seperate into two matricies. Now we have the sum of the matrix is equal to \\(Sv + Tv\\); then invoke definition of sum of Linear Map.\nscalar multiplication works in the same darn way.\nmatrix multiplication See matrix multiplication\n\\(\\mathbb{F}^{m,n}\\) For \\(m\\) and \\(n\\) positive integers, the set of all \\(m,n\\) matricies with entries in \\(\\mathbb{F}\\) is called \\(\\mathbb{F}^{m,n}\\).\nThis is a vector space! \u0026ldquo;obviously\u0026rdquo; its basis is the set of all matrix with \\(1\\) in one slot and \\(0\\) in all others. There are \\(m\\cdot n\\) of those matricies so \\(\\dim \\mathbb{F}^{m,n}=m\\cdot n\\).\ninvertability See matrix invertability\nelementary matrix elementary matricies are slight variations from the identity matrix which performs the elementary row operations:\nswap rows add a row to another scale rows determinants See determinants\nGaussian elimination See Gaussian elimination\n","permalink":"https://www.jemoka.com/posts/kbhmatricies/","tags":null,"title":"matricies"},{"categories":null,"contents":"matrix multiplication is defined such that the expression \\(\\mathcal{M}(ST) = \\mathcal{M}(S)\\mathcal{M}(T)\\) holds:\n\\begin{equation} (AC)_{j,k} = \\sum_{r=1}^{n}A_{j,r}C_{r,k} \\end{equation}\nWhile matrix multiplication is distributive and associative, it is NOT!!!!!!!!!!! commutative. I hope you can see that \\(ST\\neq TS\\).\nmemorization its always row-by-column, move down rows first then columns multiply element-wise and add (row times column and add) other ways of thinking about matrix multiplication it is \u0026ldquo;row times column\u0026rdquo;: \\((AC)_{j,k} = A_{j, .} \\cdot C_{., k}\\) it is \u0026ldquo;matrix times columns\u0026rdquo;: \\((AC)_{. , k} = A C_{., k}\\) matrix as a linear combinator Suppose \\(A\\) is an \\(m\\) by \\(n\\) matrix; and \\(c = \\mqty(c_1\\\\ \\vdots\\\\ c_{0})\\) is an \\(n\\) by \\(1\\) matrix; then:\n\\begin{equation} Ac = c_1 A_{., 1} + \\dots + c_{n} A_{., n} \\end{equation}\n(i.e. you can use a vector to linearly combinate the column vectors.)\nlinear maps are like matrix multiplication \\begin{equation} \\mathcal{M}(Tv) = \\mathcal{M}(T)M(v) \\end{equation}\n\u0026ldquo;the matrix of a vector formed by applying some Linear Map \\(T\\) onto \\(v\\) is the same as the product of the matrix of \\(T\\) and the matrix of a vector of \\(v\\)\u0026rdquo;\nProof:\nLet \\(v_1 \\dots v_{n}\\) be a basis of \\(v\\).\nSo, we have that \\(Tv = c_1Tv_{1} + \\dots + c_{n}T v_{n}\\) by the additivity and homogeneity of \\(T\\).\nThen, converting it all to matricies:\n\\begin{align} \\mathcal{M}(Tv) \u0026amp;= c_1 \\mathcal{M}(Tv_{1}) + \\dots + c_{n} \\mathcal{M}(Tv_{n}) \\\\ \u0026amp;= c_1 \\mathcal{M}(T)_{.,1} + \\dots + c_{n}\\mathcal{M}(T)_{.,n} \\end{align}\nbecause the columns of a matrix represent where each basis vector gets taken in the new space.\nYou will notice now that \\(c_1 \\dots c_{n}\\) are the scalars needed to construct \\(v\\), and that \\(\\mathcal{M}(T)_{.,1} \\dots\\) are the vectors needed to construct \\(\\mathcal{M}(T)\\).\nSo:\n\\begin{equation} c_1 \\mathcal{M}(T)_{.,1} + \\dots + c_{n}\\mathcal{M}(T)_{.,n} = \\mathcal{M}(T) \\mathcal{M}(v) = \\mathcal{M}(Tv) \\end{equation}\nas desired. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhmatrix_multiplication/","tags":null,"title":"matrix multiplication"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624558\nOne-Liner analyzed spontaneous speech transcripts (only!) from TD and AD patients with fastText and CNN; best was \\(83.33\\%\\) acc.\nNovelty threw the NLP kitchen sink to transcripts fastText CNN (with vary n-gram kernel 2,3,4,5 sizes) Notable Methods embeddings seaded by GloVe fastText are much faster, but CNN won out Key Figs the qual results PAR (participant), INV (investigator)\nNotes Hey look a review of the field:\n","permalink":"https://www.jemoka.com/posts/kbhmeghanani_2021/","tags":["ntj"],"title":"Meghanani 2021"},{"categories":null,"contents":"Applying the MFA aligner upon the Pitt (cookie only) data and performing statistics upon the calculated disfluency information. The ultimate goal is to replicate Wang 2019.\nThe code is available here.\nThe (unvalidated, draft) results are reported below:\nMean value reported, standard deviation in parens. For our data, \\(N=422\\), cases balanced.\nVariable AD (Pitt, ours) MCI (Wang) Control (ours) Control (Wang) Silence Duration 28.10 (21.28) 13.55 (5.53) 18.06 (12.52) 7.71 (5.03) Speech Duration* 23.77 (14.11) 46.64 (5.79) 27.23 (15.3) 53.63 (7.82) Voice-Silence Ratio 1.79 (4.88) 4.43 (2.78) 5.78 (31.95) 10.11 (6.05) Verbal Rate 1.59 (0.61) 1.56 (0.40) 1.989 (0.51) 1.91 (0.43) *speech duration would obviously vary with file length\nFurther statistical quantification also tells us some more things. Although the data does not make a good classifier, I performed two tests: a Kolmogorov-Smirnov test for goodness of fit, and a good \u0026lsquo;ol Pearson\u0026rsquo;s correlation with AD/control target. p-values are reported below.\nKS test silence duration: \\(1.31 \\times 10^{-5}\\) speech duration: \\(2.98 \\times 10^{-3}\\) voice-silence ratio: \\(2.01 \\times 10^{-7}\\) verbal rate: \\(4.32 \\times 10^{-10}\\) Pearson\u0026rsquo;s silence duration: \\(4.15 \\times 10^{-8}\\) speech duration: \\(0.164\\) voice-silence ratio: \\(0.732\\) verbal rate: \\(1.22 \\times 10^{-12}\\) As per the values reported in Wang 2019, we can see that\u0026mdash;apart from audio metadata\u0026mdash;verbal rate is a strongly correlated indicator against MCI/AD. We can reasonably say that Wang 2019\u0026rsquo;s data collection can be automated with reasonable success using batchalign + MFA.\nBroken ML I applied an RBF Support-Vector machine to classify AD/control based only on the two most highly correlated variables: verbal rate and silence duration. The results were disappointing.\nOn test data, N=42, balanced labels:\nSVC: \\(61.9\\%\\) Random forest: also \\(61.9\\%\\) We have fairly disappointing results. Here\u0026rsquo;s my hypothesis of why:\nif you take a look at this figure, we can see two main distributions\nSo, if we, like Wang 2019, used statistics on independence (they used chi-square, I used KS test), we will come up that the distributions are different.\nHowever, if you take a look at a randomly sampled set of validation data (crosses on the figure), you can see that a lot of them lands in the \u0026ldquo;mostly control\u0026rdquo; area: making the classifier not super useful.\nWe can therefore catch a lot of the \u0026ldquo;slow talking, long pausing\u0026rdquo; patients, but most speaking fluently will possibly need semantic information for prediction.\nI have some preliminary results on Pitt+ERNIE (a kind of BERT) that indicate that a key semantic factor is \u0026ldquo;on-topicness.\u0026rdquo; However, Pitt does not contain a lot of off-topic control data (say, the fluency task, which it has for dementia) for me to validate those claims easily. I will continue work on that front.\n","permalink":"https://www.jemoka.com/posts/kbhmfa_disfluency_measurement/","tags":null,"title":"MFA Disfluency Measurement"},{"categories":null,"contents":" Lanzi WNL (August 12) 1%. Selection Seed 7. Houjun. 82.64% ± 4.48% with a 95% confidence. Lanzi MCI (August 12) 1%. Selection Seed 7. Houjun. 78.70% ± 7.85% with a 95% confidence. Lanzi WNL (August 13) 1%. Selection Seed 7; syllabic balanced. Houjun. Within which, 90.97%±3.40% of multi-syllabic words were correctly identified 86.28%±4.08% of mono-syllabic words were correctly identified 88.63%±2.65% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test. Lanzi MCI (August 13) 1%. Selection Seed 7; syllabic balanced. Houjun. Within which, 76.85%±8.08% of multi-syllabic words were correctly identified 72.22%±8.58% of mono-syllabic words were correctly identified 74.54%±5.86% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test. Lanzi WNL (August 13) 1%. Selection Seed 7; syllabic balanced; 3-tier labeling. Houjun. Within which, 96.75%±2.10% of multi-syllabic words were correctly identified 90.61%±3.46% of mono-syllabic words were correctly identified 93.68%±2.03% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test.\nWithin sucesseses, 16.57% are partial.\nLanzi MCI (August 13) 1%. Selection Seed 7; syllabic balanced; 3-tier labeling. Houjun. Within which, 91.67%±5.30% of multi-syllabic words were correctly identified 78.70%±7.85% of mono-syllabic words were correctly identified 85.19%±4.78% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test.\nWithin sucesseses, 18.48% are partial.\n","permalink":"https://www.jemoka.com/posts/kbhmfa_performance_statistics/","tags":null,"title":"MFA Performance Statistics"},{"categories":null,"contents":"Mia is a student at the Nueva School\n","permalink":"https://www.jemoka.com/posts/kbhmia_tavares/","tags":null,"title":"Mia Tavares"},{"categories":null,"contents":"Micah Brown is a student at The Nueva School, also the host of Project80, among other things.\n","permalink":"https://www.jemoka.com/posts/kbhmicah_brown/","tags":null,"title":"Micah Brown"},{"categories":null,"contents":"Milton Freedman is an economist.\n","permalink":"https://www.jemoka.com/posts/kbhmilton_freedman/","tags":null,"title":"Milton Freedman"},{"categories":null,"contents":"MMSE is not mean squared error! It is a short mental state test to measure one\u0026rsquo;s neuralpsycological capabilities; frequently used as a first line by a psycologist.\n","permalink":"https://www.jemoka.com/posts/kbhmmse/","tags":null,"title":"Mini-Mental State Examination"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhminimum_spanning_tree/","tags":null,"title":"minimum spanning tree"},{"categories":null,"contents":"How many disturbance users can coveather take without crashing? Let\u0026rsquo;s find out.\nCode Util function to mapreduce a list:\ndef multiplyList(l) : # Multiply elements one by one result = 1 for x in l: result = result * x return result We first set a user count:\nN = var(\u0026#34;N\u0026#34;) # Pool size val_percent = var(\u0026#34;val_percent\u0026#34;) # Pools val_pool = N*val_percent user_pool = N*(1-val_percent) # Disturbance disturbance_percent = var(\u0026#34;disturbance_percent\u0026#34;) # Validation Pools + Disburbance val_disturbance_pool = disturbance_percent*val_pool val_normal_pool = (1-disturbance_percent)*val_pool # Chance of three or more disturbance attestors # which is equal to one minus chance of zero, one, or two disturbance attesors no_disturbance_attestor = (val_normal_pool/val_pool)*((val_normal_pool-1)/(val_pool-1))*((val_normal_pool-2)/(val_pool-2))*((val_normal_pool-3)/(val_pool-3)) one_disturbance = [] for disturbance_point in range(0,4): res = [] res.append((val_disturbance_pool)/(val_pool-disturbance_point)) for pre_disturbance in range(0,disturbance_point): res.append((val_normal_pool-pre_disturbance)/(val_pool-pre_disturbance)) for post_disturbance in range(disturbance_point+1,4): res.append((val_normal_pool-post_disturbance)/(val_pool-post_disturbance)) one_disturbance.append(multiplyList(res)) one_disturbance_attestor = sum(one_disturbance) two_disturbance = [] for disturbance_point_i in range(0,4): for disturbance_point_j in range(disturbance_point_i+1,4): res = [] res.append((val_disturbance_pool)/(val_pool-disturbance_point_i)) res.append((val_disturbance_pool-1)/(val_pool-disturbance_point_j)) for pre_i_disturbance in range(0,disturbance_point_i): res.append((val_normal_pool-pre_disturbance)/(val_pool-pre_disturbance)) for sandwich in range(disturbance_point_i+1,disturbance_point_j): res.append((val_normal_pool-post_disturbance)/(val_pool-sandwich)) for post_j_disturbance in range(disturbance_point_j+1,4): res.append((val_normal_pool-post_disturbance)/(val_pool-post_j_disturbance)) two_disturbance.append(multiplyList(res)) two_disturbance_attestor = sum(two_disturbance) distubrance_chance(N, val_percent, disturbance_percent) = expand(1-(no_disturbance_attestor+one_disturbance_attestor+two_disturbance_attestor)) # no_disturbance_attestor (N*(disturbance_percent - 1)*val_percent + 3)*(N*(disturbance_percent - 1)*val_percent + 2)*(N*(disturbance_percent - 1)*val_percent + 1)*(disturbance_percent - 1)/((N*val_percent - 1)*(N*val_percent - 2)*(N*val_percent - 3)) z = var(\u0026#34;z\u0026#34;) val_dist(val_percent, disturbance_percent) = distubrance_chance(100, val_percent, disturbance_percent) implicit_plot3d(val_dist-z, (val_percent,0.1,1), (disturbance_percent, 0,1), (z, 0,1) ,frame=True,axes_labels=[\u0026#39;Validation\u0026#39;,\u0026#39;Disturbance\u0026#39;, \u0026#39;Chance\u0026#39;],axes=False, color=(val_dist,colormaps.Blues)) Launched html viewer for Graphics3d Object z = var(\u0026#34;z\u0026#34;) n_dist(N, disturbance_percent) = distubrance_chance(N, 0.1, disturbance_percent) show(implicit_plot3d(n_dist-z, (N,100,100000), (disturbance_percent, 0,1), (z, 0,1) ,frame=True,axes_labels=[\u0026#39;N\u0026#39;,\u0026#39;Disturbance\u0026#39;, \u0026#39;Chance\u0026#39;],axes=False, color=(n_dist,colormaps.Blues)), aspect_ratio=[1,100000,100000], plot_points=100) Launched html viewer for Graphics3d Object n_dir(N) = distubrance_chance(N, 0.1, 0.1) # plot(n_dir, (N,100,100000),axes_labels=[\u0026#39;N\u0026#39;, \u0026#39;Disturbance\u0026#39;], thickness=1) # solve(distubrance_chance(100, N, 0.1)==0.01, N, to_poly_solve=True) # implicit_plot(distubrance_chance(100, N, 0.1)==0.01, (N, 0,1), (z, 0, # solve(distubrance_chance(N, val_perc, 0.1)==0.01, val_perc, to_poly_solve=True) # implicit_plot(solve(distubrance_chance(N, val_perc, 0.1)==0.01, val_perc)[0]) # val_perc = var(\u0026#34;var_perc\u0026#34;) show(implicit_plot(distubrance_chance(N, val_perc, 0.1)==0.01, (N, 15, 1000), (val_perc, 0,1), plot_points=300,axes_labels=[\u0026#39;N\u0026#39;,\u0026#39;Val Ratio\u0026#39;],axes=False), aspect_ratio=800) # solve(distubrance_chance(800, val_perc, 0.1)==0.01, val_perc, to_poly_solve=True) \u0026lt;/Users/houliu/.sage/temp/baboon.jemoka.com/64368/tmp_9bdcu2si.pn\u0026gt;\n","permalink":"https://www.jemoka.com/posts/kbhminimum_user_base_requirements_for_coveather/","tags":null,"title":"minimum user base requirements for coveather"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhminimum_wage/","tags":null,"title":"minimum wage"},{"categories":null,"contents":"Why so many stock exchanges? Because the FTC just allows you to make\u0026rsquo;em as desired.\nWhy doesn\u0026rsquo;t the market trade 24 hours a day? Because the institutional traders can only trade 2 hours a day: the beginning of the day, or the end of the day. Otherwise, there are not enough volume for the institutional traders to be able to trade at their size. See Volume Profile.\nWhat\u0026rsquo;s a good \u0026ldquo;full view\u0026rdquo; of the stock? The order book! You can actually see it by paying money to the exchange. You want to subscribe to every order for every exchange. How to large traders strategically break stocks? \u0026ldquo;How long should I take?\u0026rdquo;\nWhy are some Ethernet ports worth a lot more than others? Some amount of trading (10-20%) is done at light speed. Cable lengths of about a foot change the stock dramatically.\n","permalink":"https://www.jemoka.com/posts/kbhmisc_financial_market_questions/","tags":null,"title":"Misc. Financial Market Questions"},{"categories":null,"contents":"Reading notes Malcom X\u0026rsquo;s father was an active prechear in the scene Malcom X and MLK are both made mostly charactures out of context Malcom X had a belligent upbringing with a belligent father, whereas MLK lived in relative comfort as a son of a successful minister Malcom was sent into white foster families as his mother became institutionalized Becasue of his experience in foster system, Malcom tried to pass/be white King\u0026rsquo;s nonviolent priciples not understood and became conflicted with ideas of local leaders Malcom found a father figure in the Nation of Islam, changing his name in prison MLK had more positive African American role models in life Malcom X disallusioned with the policy of nonengagement by the nation of islam Malcom X had support over racial seperatism Nation of Islam wanted to create a completely seperate Black state, promoting Black Nationalism secret Malcom X wanted break because of skeptism again Eli Mohammed Malcom charged MLK with infiltration Martin believes that the process of voilence is a form of naïve expression King believes that the \u0026ldquo;strong demagogic oratory\u0026rdquo; of Malcom was detrimental and extremist Martin believes that the personal nature of assults from Malcom maybe result in physical assult Malcom was suspended during 1963, and became independent\u0026mdash;wanted to combine religion and politics like King Malcom began forging ties with millitan Black movement Martin regretted that integration has not proceeded, but believed it would have been difficult anyways Rejected nonviolent and intergrational movement People saw King and X\u0026rsquo;s ideas inrecosiliable But, King and X themselves made a possible shared ending by the end Believes that suicides were cut short Racial pride was a centering point: while Malcom saw it as something to be harbored, Martin saw it as inate ","permalink":"https://www.jemoka.com/posts/kbhmlk_and_malcom_x_reading/","tags":null,"title":"MLK and Malcom X Reading"},{"categories":null,"contents":"modalization is the\n","permalink":"https://www.jemoka.com/posts/kbhmodalization/","tags":null,"title":"modalization"},{"categories":null,"contents":"Clock math.\nRecall the real numbers: \\(\\dots, -2, -1, 0, 1, 2, 3, \\dots\\)\nThat\u0026rsquo;s so many numbers! Instead, let\u0026rsquo;s create a circle of these values. For instance, what if you only want \\(5\\):\n\\begin{equation} \\mathbb{Z}_{5} = \\{0,1,2,3,4\\} \\end{equation}\nThis is a group under addition.\n","permalink":"https://www.jemoka.com/posts/kbhmodular_arithmetic/","tags":null,"title":"modular arithmetic"},{"categories":null,"contents":"Monetarist theory is a theory of economics proposed by Milton Freedman which asserts that Keynsian economics only applies in the limited case that central bank need to keep the money supply growing; otherwise, the free market can handle itself.\nTherefore the Monetarist theorists propose that the stock market crash of 1929 was caused that the US monetary fund did a bad job of actually controlling the funds, and didn\u0026rsquo;t inject enough money into economy.\nSee also the opposite: demand-driven theory.\n","permalink":"https://www.jemoka.com/posts/kbhmonetarist_theory/","tags":null,"title":"Monetarist theory"},{"categories":null,"contents":"The fallout of the Rosa Parks incident, which is when many of Montgomery residents.\nThe boycotts were developed by Martin Luther King.\n","permalink":"https://www.jemoka.com/posts/kbhmontomery_bus_boycott/","tags":null,"title":"Montgomery Bus Boycott"},{"categories":null,"contents":"The multiplicative identity allows another number to retain its identity after multiplying. Its \\(1\\) [for fields?].\n","permalink":"https://www.jemoka.com/posts/kbhmultiplicative_identity/","tags":null,"title":"multiplicative identity"},{"categories":null,"contents":"TBD\n","permalink":"https://www.jemoka.com/posts/kbhmultiplying/","tags":null,"title":"multiplying"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmy_day/","tags":null,"title":"My Day"},{"categories":null,"contents":"NACC is a large, longitudinal dataset for neurodegentitive disease as a project in collaboration with Dr. Alyssa Weakley at UC Davis.\nDr. Alyssa Weakley is interested in\nEarly Cognitive Change Mild Cognitive Impairment (MCI) \u0026ldquo;How early can we detect, using NACC, change?\u0026rdquo;\ndataset construction Participants are given a battery of mental capacity tests, these values are tracked over time There are also family member questionnaire Neuroimaging and biomarker data Other things tracked in the data\u0026mdash;\nAmyloid levels of spinal fluid Detecting even earlier focus good to focus on specifically alzheimer\u0026rsquo;s type dementia (so, ignore things on lewy body disease) Using clinical diagnosis as the dependent variable, but good to see the autopsy results Items 3 and 7 are independent codes; if alzhimer\u0026rsquo;s is measured, MCI is not measured. visa versa.\n","permalink":"https://www.jemoka.com/posts/kbhnacc/","tags":null,"title":"NACC"},{"categories":null,"contents":"The National Banking Act unified Financial Markets.\n","permalink":"https://www.jemoka.com/posts/kbhnational_banking_act/","tags":null,"title":"National Banking Act"},{"categories":null,"contents":"natural numbers (\\(\\mathbb{N}\\)) are the counting numbers: 1,2,3,4\u0026hellip;.\nZero is not part of it; this produces interesting results like set of natural number under addition is not a group because there is no identity (tbh nor inverse (inverse of 1 is -1 which is not in the set.))\n","permalink":"https://www.jemoka.com/posts/kbhnatural_numbers/","tags":null,"title":"natural number"},{"categories":null,"contents":"The nsm theory is a theory that\u0026hellip;\nclaims that there exists a set of semantic primes and logic universal across languages which is indefinable by other words within the language which, as a corollary, resolves the epistemological problem that if all words are defined by other words in the language there will (why?) be no connection to the real world the theory of NSM rests on\u0026hellip;\ntwo pillars of NSM theory existence of semantic primes The existence of semantic primes is codified more formally as the strong version of the Lexicalization Hypothesis.\nIssues with it: problems with semantic primes\nthe ability to perform the act of reductive paraphrase Issues with that: problems with reductive paraphrasing\noh cool! (Bohnemeyer 2004)\nAlso the fact that NSM is first found in English means that there is a certain anglo-centrism that comes with the language.\n","permalink":"https://www.jemoka.com/posts/kbhnatural_semantic_metalanguage/","tags":null,"title":"Natural Semantic Metalanguage"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhnatural_transformations/","tags":null,"title":"natural transformation"},{"categories":null,"contents":"NBBO is the composite best bid/ask nationally, across all of the exchanges.\nIt allows the average person on the street to get a price for the asset. The government system is actually SLOWER from the fastest exchange: you can know, within microseconds, the difference.\n","permalink":"https://www.jemoka.com/posts/kbhnbbo/","tags":null,"title":"NBBO"},{"categories":null,"contents":"needfinding as a process of finding need.\nneedfinding with Rick Wallace needfinding with Rick Wallace. You don\u0026rsquo;t find out what they need, but you find what they need and how to fix it. (duh?)\n","permalink":"https://www.jemoka.com/posts/kbhneedfinding/","tags":null,"title":"needfinding"},{"categories":null,"contents":"Neoclassical Economics is a view of economics that disregards the Keynsian Politics theory of the economy needs a minder started by Milton Freedman. It believes that free market economy will prevail.\n","permalink":"https://www.jemoka.com/posts/kbhneoclassical_economics/","tags":null,"title":"Neoclassical Economics"},{"categories":null,"contents":"a neutral stability ( mar ) condition in Differential Equations means that a function is neither stable nor unstable: it does not\nSee: https://en.wikipedia.org/wiki/Marginal_stability\n","permalink":"https://www.jemoka.com/posts/kbhneutral_stability/","tags":null,"title":"neutral stability"},{"categories":null,"contents":"Election between Hayes vs Tildon was very close. Democrats gave Republicans Hayes, but then asked the Republican millitary to leave the South and hence they have no way of enforcing the rights.\nRedeemer Governments Democrats put in systems to relegate African Americans to second-class citizenship into the south. Lynchings became the weapon of choice of enforcing Jim Crow.\nWithin 20 years, Jim Crow became implemented by every state 1896 Plessy vs Ferguson upholding the process of segregation Convict leasing: convicts\u0026rsquo; labour was leased to create infrastructure Economic transformation: put in sharecropping (crops in lieu or in addition to rent) and convict leasing. This is essentially modern slavery because debt is used as a process to enslave people as they will never actually be paid enough to pay back debt.\nPush for Civil Rights \u0026ldquo;Booker T. Washington\u0026rdquo;: help promote Southern society will gain equality. Founded the \u0026ldquo;Tuskegee Institute\u0026rdquo;.\n\u0026ldquo;W.E.B. Dubois\u0026rdquo;: make the most talented and artistic people push for civil rights. \u0026ldquo;Civil rights by copyright.\u0026rdquo;\n","permalink":"https://www.jemoka.com/posts/kbhnew_american_south/","tags":null,"title":"New American South"},{"categories":null,"contents":"A set of policy by Franklin D. Roosevelt (FDR) which helped saving the economy during the Great Depression.\nSaving the Banks Unemployment Relief Industrial Recovery Agriculture Creates the WPA. Also the Social Security Administration. Also created Rural Electrification Administration\nMany people were still left out.\n","permalink":"https://www.jemoka.com/posts/kbhnew_deal/","tags":null,"title":"New Deal"},{"categories":null,"contents":"A reformist, counterculture movement during the \u0026rsquo;80s lead by Ronald Reagan. Its a new response to the neoliberalism which aligned the blocks of Evangelical Christians (25% of voters) and Business leaders (powerful leaders.)\nAmerican liberalism expands under the new right as well.\nPresident as a party leader: Reagan is often shown as shining beaken of the Republican Party Leadership\u0026mdash;won every single state except Georgia .\n","permalink":"https://www.jemoka.com/posts/kbhnew_right/","tags":null,"title":"New Right"},{"categories":null,"contents":"Complex System\nLanguage Model A Language Model is a large neural network trained to predict the next token given some context.\n\u0026ldquo;Language models can discriminate behavior that they can\u0026rsquo;t reliably generate.\u0026rdquo;\nCoherence Generative REVOLUTION\nWhy probability maximization sucks Its expensive!\nBeam Search Take \\(k\\) candidates Expand \\(k\\) expansions for each of the \\(k\\) candidates Choose the highest probability \\(k\\) candidates Branch and Bound Keep following the highest probability token by multpliying probabiltiies together, and select until the joint probability is lower.\n\u0026hellip;but the model would just. like end the sentence. So longer the string the lower the chance is. Because all the other possibility is lower.\nDirect Sampling Its sucks. It sucks. Just sampling from the distribution sucks. This has to do with the fact that assigning slightly lower scores \u0026ldquo;being less confident\u0026rdquo; is exponentially worse.\nThe model has to therefore be VERY conservative about giving low confidences; so, it is over confident about worst tokens.\nTop-K Top-k is too broad, and top\nNucleaus Sampling Find the smallest set of tokens that make up to \\(p\\) probability.\nCorrectness The highest probability answer isn\u0026rsquo;t always right Generative models consider every answer, so we want another model to compute the correct answer Surface Form Competition The Surface Form Competition problem results when top probabity token \u0026ldquo;steals\u0026rdquo; probability from the other tokens.\nThe predicted frequency of a possible string is a main comfounder. And so we can use models to decompose their own predictions:\nTurns out:\n\\(P(answer|question) \\approx P(answer\\ is\\ valid)P(answer|domain)\\)\nSo\u0026hellip;\n\\begin{equation} P(answer\\ is\\ valid) = \\frac{P(answer|question)}{P(answer|domain)} \\end{equation}\nThis is better :point_up:. Futher reading: (Holtzman et al. 2021)\nDomain Domain is the context in which that the text may occur.\nCoverage Why aren\u0026rsquo;t models controllable\nHallucination Language models predict what\u0026rsquo;s most likely We hope to control them with natural-language semantics In-Context Learning If we show the model some context which has example input output pairs, it can output. (Language Model model are few shot learners)\nCorrect Scoring We can reverse the output to predict the input to prevent model from loosing information, and use that to rerank the info. Of course, if the model can\u0026rsquo;t generate the desired input, the output is probably missing information.\nSmaller models can be made better because of info reranking.\nTh Degenerative Discriminative Gap.\nFuture Work The fact that the single comma shift the input. What we need is a language to control language behavior.\nThe Ability to Control a Model are the Goal of Understand the Model\nWe should only claim to understand a model when we can make a theory map about it: \u0026ldquo;when X is fed into the model, we get Y\u0026rdquo;\nSo: we should look at what the model is biased about (Surface Form Competition, for instance) we would be closer to prime behaviors such that they mimic the human behavior (in pieces, not just \u0026ldquo;complete these tokens\u0026rdquo;) in completion We see success as the actual evaluation metrics; we can use machines vs. other machines as the the results Questions ahai@uw.edu\nMarcel Just\nanthropic ai papers\npercy liang\n","permalink":"https://www.jemoka.com/posts/kbhnlp/","tags":null,"title":"NLP"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhchomsky/","tags":null,"title":"Noam Chomsky"},{"categories":null,"contents":"In this project, we aim to derive situations for the existence of a differential equation for when a family of functions do not intersect. We were able to derive a full solution for the result in linear equations, and we offer an exploration of a partial solution for non-linear cases.\nFunction Families Fundamentally, function families are functions parameterized by some \\(C\\), which has the shape:\n\\begin{equation} y(x, \\dots, c) = f(x, \\dots)+c \\end{equation}\nThrough this result, we can figure a statement for \u0026ldquo;intersection.\u0026rdquo; If two functions intersect, their difference will be \\(0\\); if there is a non-trivial solution (that \\(c_1\\neq c_2\\) \u0026mdash; that, they are not the same function\u0026mdash;still makes \\(y_{C_1} = y_{C_2}\\)), the function family interact.\nWe can test this by subtracting two arbitrary members from the desired family. If it results that \\(c_1-c_2=0 \\implies c_1=c_2\\), we can say that the family does not intersect: that there are no non-trivial solutions to the function having no difference.\nSingle-Order Linear Differential Equations Here, we prove the fact that single-order linear differential equations do not produce solutions that intersect. We have the following single-order linear differential equation:\n\\begin{equation} \\dv{y}{x} + P(x) = Q(x) \\end{equation}\nIf, as desired, our function has a analytical solution (without an integral), we will make both terms differentiable.\n\\begin{equation} \\dv{y}{x} + P\u0026rsquo;(x) = Q\u0026rsquo;(x) \\end{equation}\nRecall the general solution of this expression:\n\\begin{align} y \u0026amp;= e^{-\\int P\u0026rsquo;(x)\\dd{x}} \\int e^{\\int P\u0026rsquo;(x)\\dd{x}} Q\u0026rsquo;(x)\\dd{x} \\\\ \u0026amp;= e^{-(P(x)+C_1)} \\int e^{P(x)+C_1} Q\u0026rsquo;(x)\\dd{x} \\end{align}\nOf course, we can separate the constants \\(e^{C_1}\\) out.\n\\begin{align} y \u0026amp;= e^{-(P(x)+C_1)} \\int e^{P(x)+C_1} Q\u0026rsquo;(x)\\dd{x} \\\\ \u0026amp;= e^{-P(x)} \\int e^{P(x)} Q\u0026rsquo;(x)\\dd{x} \\end{align}\nNow, it is the case that, for the most part, \\(e^{P(x)}Q\u0026rsquo;(x)\\) may not be integral-differentiable. Applying the fundamental theorem, we still have that as the integral function, with some \u0026ldquo;differentiated\u0026rdquo; term which we will call \\(a(x)\\): below\n\\begin{align} y \u0026amp;= e^{-P(x)}(a(x) +C) \\\\ \u0026amp;= e^{-P(x)}a(x) +Ce^{-P(x)} \\end{align}\nExcellent. Now, let\u0026rsquo;s do the subtraction test devised above; if we have that \\(C_1-C_2=0\\) given \\(y_1-y_2=0\\), then we can ensure that the function family do not intersect.\n\\begin{align} y_1 - y_2 =0 \u0026amp;= (e^{-P(x)}a(x) +C_{1}e^{-P(x)})-(e^{-P(x)}a(x) +C_{2}e^{-P(x)}) \\\\ \u0026amp;= C_{1}e^{-P(x)}-C_{2}e^{-P(x)} \\\\ \u0026amp;= (C_{1}-C_{2})e^{-P(x)} \\end{align}\nWe now have that:\n\\begin{equation} 0 = (C_1+C_2)e^{-P(x)} \\end{equation}\nNotably, the codomain of \\(e^{x}\\) is \\((0, \\infty)\\). Having never reached \\(0\\), we have that \\(0=C_1-C_2\\), as desired. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnon_intersecting_graphs/","tags":null,"title":"Non-Intersecting Graphs (Single Order)"},{"categories":null,"contents":"\u0026ldquo;Chaotic Dynamics\u0026rdquo; Because the word is sadly nonlinear.\nmotivating non-linearity \\begin{equation} \\dv t \\mqty(x \\\\ y) = f\\qty(\\mqty(x\\\\y)) \\end{equation}\nThis function is a function from \\(f: \\mathbb{R}^{2}\\to \\mathbb{R}^{2}\\). All the work on Second-Order Linear Differential Equations, has told us that the above system can serve as a \u0026ldquo;linearization\u0026rdquo; of a second order differential equation that looks like the follows:\n\\begin{equation} \\dv t \\mqty(x \\\\y) = A \\mqty(x \\\\ y) +b \\end{equation}\nActually going about deriving a solution to this requires powers of \\(A\\) to commute. If \\(A\\) has a independent variable in it, or if its a time-varying function \\(A(t)\\), you can\u0026rsquo;t actually perform the linearization technique (raising diagonalized \\(A\\) to powers) highlighted here.\nSo we need something new.\nSudden Review of Vector Functions Let\u0026rsquo;s take some function:\n\\begin{equation} f: \\mathbb{R}^{2} \\to \\mathbb{R}^{2} \\end{equation}\nIt will output a vector:\n\\begin{equation} f(x,y) = \\mqty(f_1(x,y)\\\\ f_{2}(x,y)) \\end{equation}\nSolving Non-Linear Systems, actually Let\u0026rsquo;s take a non-linear system:\n\\begin{equation} \\begin{cases} \\dv{x}{t} = F(x,y) \\\\ \\dv{y}{t} = G(x,y) \\end{cases} \\end{equation}\nOverarching Idea: To actually solve this, we go about taking a Taylor Series (i.e. linearize) the functions next to its critical points. Then, we use an epsilon-delta proof to show that the linearization next to those critical points are a good approximation.\nSo! Let us begin.\nLet \\((x*,y*)\\) be a critical point of \\(F\\). Naturally, \\(d 0=0\\), so it is also a critical point of \\(G\\).\nSo we have:\n\\begin{equation} F(x*,y*)=G(x*,y*) = 0 \\end{equation}\nNow, we will begin building the \u0026ldquo;slope\u0026rdquo; of this function to eliminate the independent variable wholesale\u0026mdash;by dividing:\n\\begin{equation} \\dv{y}{x} = \\dv{y}{t} / \\dv{x}{t} = \\frac{G(x,y)}{F(x,y)} \\end{equation}\na divergence into epsilon delta proof\nstable A critical point is considered \u0026ldquo;stable\u0026rdquo; because, for each \\(\\epsilon \u0026gt;0\\), \\(\\exists \\delta \u0026gt;0\\), such that:\n\\begin{equation} |x_0-x*| \u0026lt; \\delta \\implies |x(t)-x*| \u0026lt; \\epsilon \\end{equation}\nasymptotically stable For every trajectory that begins close to the critical point, it will end up at the critical point as time increases. That is, \\(\\exists \\delta \u0026gt;0\\) such that:\n\\begin{equation} |x-x*| \u0026lt; \\delta \\implies \\lim_{t \\to \\infty } x(t)=x* \\end{equation}\nThis is essentially epsilon delta, but the limit traces out the entire process descending so the critical point is stable through the whole descend.\n","permalink":"https://www.jemoka.com/posts/kbhnon_linear_systems/","tags":null,"title":"Non-Linear System"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhnonsingular_matricies/","tags":null,"title":"nonsingular matricies"},{"categories":null,"contents":"The nonviolence movement a method of protest which is developed by Mahatma Ghandi and leveraged by Martin Luther King in the civil rights movement.\nThe idea is to achieve civil disobedience and allowing oneself to be punished so egregiously without inciting violence so at to elicit sympathy across the nation.\nThe civil rights movement leveraged training tactics and training to ensure its participants would be completely nonviolent and so elicit the correct response.\n","permalink":"https://www.jemoka.com/posts/kbhnonviolence_movement/","tags":null,"title":"nonviolence movement"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhnormal_distribution/","tags":null,"title":"normal distribution"},{"categories":null,"contents":"Foreword Hi there, internet traveler.\nThe time is 2015/2016, I was either in 5th or 6th grade. At that time, I was barely beginning to be actually comfortable using the language of English.\nOne of the ways I practiced English, which is also a habit I continue to do today, is to write. I write mostly expository prose now, but, back then, shining with childish naïvete, I decided to write a multi-part story as a means of practicing English.\nAt the time, I was fortunately supported by four very helpful adults\u0026mdash;ESL instructors, teachers from the local government\u0026rsquo;s ESL program, local students, family friends\u0026mdash;who have supported me and edited this silly story as a means of helping me better my command of English.\nIronically, this story is set in 2018, I think, two years after when I wrote it. Its now 2022, almost 7 years after. Make of that what you will.\nTherefore\u0026mdash;\nNorman, an epic tale told in N parts\nWritten by yours truly, Houjun Liu, circa 2016.\nEdited by: Lynne Zummo, Dorit Hahn, Susan Cole, and Jennifer Fan.\nTypesetted: May 10th, 2022. Menlo Park, California.\nPrologue: James Peter On a sunny day, in a small house at 1623 Wesson Ave, James lay on a dirty, tiny bed. Suddenly a dog was in James’ sight. James stood up, stared at the dog. It was a small, brown, white, fuzzy dog with a tiny stump. The dog walked around James’ bed, looking silly.\n“Let’s call you Norman! It is a good name for you!”\n“There is no dog allowed in my house, get’em out! RIGHT now, or I will get YOU out!” shouted Mr. Miller.\n“Dude,” a voice came from James’ mind. Mr. Miller, the owner of the Wacky Hair Salon, who is James’ uncle, barged into James’ room, continuously shouting.\n“Get’em out, RIGHT NOW! NOW! YOU HEAR ME?”\nJames, staring at Norman, just didn’t care.\nNorman seemed to not understand all this. He followed Mr. Miller to the window, and \u0026hellip; just as suddenly as he had come in, he was thrown out by Mr. Miller.\nWhile Norman was wandering around, James started crying.\nMonths passed…\nPart 1 On a cold winter afternoon, Mr. Miller is sending James to an orphanage as punishment for doing “bad” things. James just doesn’t understand this. He SIMPLY wants Norman to come back!\nWhen they arrive, James finally realizes why he didn’t have parents. The truth is dreadful: his dad went crazy from programming in binary code.\n“I will go crazy, too,” James thinks. “It is not an easy job, no sir.” His mom’s situation was even worse, for she was killed by the African disease Ebola.\nHe trudges into the front building with Dr. Brains, and sees children that had been starved, gone mad, and even had been wandering around hopelessly! Many questions flew into James’ mind: Will I go crazy, too? Will I be starving, too? Will I also be wandering around like a zombie?! Feeling scared, he starts to wander, feel hungry, and starve like the other kids ……\n“Wait ! NO, I can’t do that,” thought James.\nDr. Brains takes James to walk around the orphanage, he realizes it is actually a better place to be rather than 1623 Wesson Ave. He sees cats, he sees ducks, he sees horses, he sees a playground, and he sees…\nNORMAN!\nPart 2 Dr. Brains, who looks bewildered, is staring at him.\n“How can you know him? You just arrived here!”.\n“Long story…,” explains James. “I once met the dog, and he was thrown out by Mr. Miller from where I used to live.”\n“So, this is PART of our orphanage. As you see, it is big. We now should thank the donor, who passed away, Dr. James Rover Peter…” There is a little pause, then Dr. Brains continues.\n“Who is YOUR dad!”\nThey continue walking until they get to a building labeled ‘EDU_K-4’.\n“This is the K-4 grade educational multifunctional building,” explains Dr. Brains, “where you will be staying for half a year. Then you will move to this building for study.”\nDr. Brains is now pointing at a building labeled ‘EX-EDU_5-12’.\nThey continue to walk until they get to another building labeled ‘OPH_LV #20312’. It is a small, lovely building, much like an apartment. “This is where you will live, in room # 20312_004,” says Dr. Brains while he hands James a key. Then he gives him a packet, which reads: Vanilla orphanage grand rules and schedule.\n“This is all you need, good day! I will leave you here.”\nJames watches Dr. Brains until he is out of sight.\nHe walks straight into the room. It looks clean, neat, like a 3-star hotel. There is a twin size bed, a desk, and a restroom. He sits down and starts to read the packet:\nChapter 1: Grand rules Welcome to Vanilla Orphanage! This is a place where you can enjoy yourself, explore yourself, and get prepared for the world!\nBut, there MUST be some rules in our orphanage to keep you and your classmates safe.\nFirst of all, you MUST not run in the front building.\nSecond, no talking is allowed while a grand meeting is taking place (see chapter 2 for more info).\nThird, follow your schedule all the time.\nFourth, if you have an emergency, use the emergency call phone (You don’t need to dial it, it will automatically connect to Vanilla Orphanage Hospital). But if you can walk and speak normally, go to Vanilla Orphanage Hospital for more help.\nFifth, the use of a regular telephone is only allowed three times a day. If your teacher calls you, it won’t count. You can only use a regular telephone for calling inside-the-orphanage friends, no outside call is allowed. To see the interior telephone numbers, see chapter 3.\nChapter 2: Grand schedule + your personal schedule Grand schedule\nYour personal schedule\nMeet me every OTHER Sunday at 15:00 at grand office starting 1/2/2019.\nGrand meeting will take place every first day of the month at the big hall in the front building. Everyone will attend the grand meeting; it lasts the whole day.\nDinner, Lunch, and Breakfast will be served at Front Building.\nDr. Flynn (k-4 Sciences) 4242-5000-2525 Dr. Jones (5-12 Sciences) 2134-1000-1045 Dr. Foster (k-4 Math) 2456-6206-6200 Ms. Garcia (5-12 Math) 1341-4000-4012 Mrs. Newman (k-4 Talk-it-out assistant) 2563-6374-7407 Mrs. Willems (5-12 Talk-it-out assistant) 8908-6997-9000 Dr. Brains (Headmaster) 2563-0035-3526\nPart 3 A brief day, he does whatever he is told, follows the schedule, does the work. But, something that amazes James is that the food is actually YUMMY.\nHe does enjoy eating at vanilla orphanage. Normally, it is like a buffet, but a limited one. You only can have one serving of meat, 2 vegetables, a delicious main dish (e.g. cooked rice, cooked noodles …).\nBy the table, James sees students laugh at each other, talk with each other, and, from far away, he sees a little brown-white puppy is running to a girl with curly hair, and stops.\nNorman!!!!!!!!!!!!!\nIt is funny that the girl asks exactly the same thing as Dr. Brains asked:” How can you know him?” He explains the whole story why he knows Norman and asks his very own and very first question to the very first student he meets at the vanilla orphanage: “How did he get here?”.\n“Long story,” says the girl, “he first arrived here because of our save the dogs project, Calvin and I found him.”\n“And who are you? I’m James”.\n“Sorry, I forgot about that, my name is Amelia!”.\nA tall, black haired student comes and joins them. “Hi there, what’s up? I heard someone mention my name.”\n“Oh, we were just talking about the dog. Our new friend, James, gave him a name: Norman,” responds Amelia.\n“Guess what?” asked Calvin, “I taught him Chinese!”.\n“Oh interesting, show us!” says Amelia.\n“狗儿，请来一下; I told him to come.” Suddenly, Norman comes and starts running around Calvin. “你的名字叫做 Norman; I told him that his name is Norman,” says Calvin. The dog starts moving around in a funny way, which James feels weird about. “Oh, don’t worry about that, that’s the Funny-Brown-Hula-Stump-Wiggle-Wag-Dance that I taught him,” Says Amelia.\nPart 4 Dong, dong, dong, dong…… The school bell rings, everyone gets up to do everything they need. It’s Sunday. According to Dr. Brains, James needs to meet Dr. Brains at the grand office.\nWhen he arrives, Dr. Brains says nothing but a greeting, and he hands James a slip of paper that says:\nThe organization of Brainiacs: 52345 Brainful way, North town, CA 94780\n“What is the org…”. “Stop! I will explain everything right after!” explains Dr. Brains. “Just remember what this parchment says!”\nHe hands him a telephone, and says, “Dial 52325, when you hear a beep, dial 900. Answer every question it’s asking you.”\nHe does what he is told, then a girl’s voice says: “Welcome to the new member registration center of TOFB, or the Organization of Brainiacs. Please answer the question: What is your address? “James states the orphanage’s address. “What is your reason to join?” Dr. Brains says quietly,” Invited.”\nWho invited you?” James answers:”Dr. Brains.” “ Welcome, again, new member. Please take the blood needle that appears in front of you and use it to poke your left ring finger.” James does this, and the voice says, ”Thanks for joining! Please hang up the phone!”\n“Understand this?” Dr. Brains says, ”Let’s go!”. “But, go where?” asks James. ”T-O-F-B,” replies Dr. Brains. They walk straight into a box, where James spots a device. Dr. Brains pushes a button on the device, and suddenly, James feels dizzy. They are spinning. They spin faster and faster. Finally, he hears a pop, then, suddenly, he falls into another device which is like a poison chamber. He and Dr. Brains open the door, and he sees a small, transparent house that reads T-O-F-B.\nPart 5 They walk straight into the house, and see a small elevator that is made out of glass. While they walk into the elevator, James feels something is seriously wrong. First, this is a one-story building, and unlike the 5th avenue apple retail store, it has no underground floor. Second, the elevator has NO button, how can Dr. Brains go anywhere with this elevator?\nDr. Brains seems solemn, he carefully looks at the emergency speaker, then, suddenly, James hears a loud CRACK. Then the elevator starts getting darker and darker. After 5 seconds, it is not transparent anymore.\nThe elevator starts to go down deeper and deeper. Then a screen pops up.”Hello, WELCOME to the Organization of Brainiacs. Please scan your card…” says a voice. He doesn’t have a card!! He looks around to find Dr. Brains, but, he is gone!\n“Where else can he be?”James thinks,”there is no way out!”. Suddenly, smoke fills the elevator, James first doesn’t realize what it is, but suddenly, he knows it.”Oh oh!” thinks James, ”IT IS GAS!!!!!”\nChapter 2: T-O-F-B Way underground, Dr. Brains hesitates. “OOOOOOOOOOPS! I forgot James in the elevator..” , he thinks, ”and the killer gas X03-00 would be deadly.”\nHe rushes to the “hacker center”, and shouts, ”You guys! STOP the elevator! And STOP the gas! Open the doors! Clear out the gas! He is NOT a criminal!”\nEverybody freezes, and some whisper, ”Oops, x03-00 gas can knock a human out in 10 seconds”.\nPart 6 Back in the elevator, James barely has time to call the emergency. “Does Dr. Brains mean to kill me?” he thinks, ”or is this a test for me?” He has more things to worry about than that. However, the good news is that Dr.Brains and his team hurry to the elevator just in time, which is when he gets knocked off. They give him the medicine that will neutralize the effect of the gas, and then they hurry to prepare the WELCOME event of the new T-O-F-B members in this season.\nSoon after, James wakes up, safe and sound. Dr. Brains is right by him.\n“Sorry for the accident, but here, welcome to T-O-F-B”, Dr. Brains says with a little smile.\nThere is a little awkward moment when he and Dr. Brains both try to say something, but no sounds come out. It doesn’t last long, just a few seconds. Then Dr. Brains continues, ”The Organization of Brainiacs is a little like what you see in the movie M-I-B. We basically are the only legal group in human and alien law that can meet, communicate with, and study the aliens from outer universe. You know one of our aliens: Norman. He actually can speak Hidoeneese AND English.”\n“But what is Hidoeneese?” James asks.\n“Hidoeneese is the language of the Hidonipothan.” Dr.Brains says.\n“And what is Hidonipothan?” he asks, again.\n“Long story short, it’s kind of an alien tribe. Later at breakfast, Norman will explain. By the way, he likes his name Norman.” Dr. Brains responds.\n“What? Breakfast? It’s already morning?” James asks.\n“Well yes, you have been knocked out by gas for almost 12 hours, and now it is 6:00 in the morning,” Dr. Brains says, ”you still can get about 3 hours rest. Everyone in T-O-F-B sleeps late and wakes up late. And one last thing, I will give you the NEW MEMBER #04 packet so you can learn more about T-O-F-B.”\nHe hands him a packet, just like the packet in the orphanage. But it is hand written.\nWelcome, new member, we are proud that you are here. As the founder of T-O-F-B, I will introduce you to the few basics of daily life.\nFirst, you all have an outside “job”, which you will still perform. Since you are a child, AS I KNOW, we will just keep you up-to-date and call you via the headphones that we will give you. We won’t interrupt your class, unless it is an emergency, I promise. You will be meeting once a month so it won’t affect any of your grades.\nSecond, in T-O-F-B, we treat any child like an adult. It means a large work load, but you can also access any part of our centre freely with your BNPS. But in some areas, we want you to have adult supervision.\nYour supervisor is:\nGrave Hono ( Dr. Brains, as a substitute name in the human world)\nWe will give you a map and what you should do later.\nDr. Ranboro\n9/23/2018\nPart 7 He falls asleep……… He dreams about aliens attacking the centre, and only Dr. Brains, Dr. Ranboro, Norman, Amelia, Calvin and a guy who he didn’t know survived. He thinks it’s just a dream, but what he doesn’t know is, this day is coming closer and closer.\n“Wakey, Wakey!” Dr. Brains shouts, laughing” JJJJJJJJAAAAAAMMMMMMMEEEEEEEESSSSSSSSSSSS!!!!!!!!!!!!”. James finally wakes up, and mumbles, ”What the heck in the world was this?”\nDr. Brains seems to be confused. “You didn’t recognize my voice? Wake up, Buddy! Get dressed! The welcoming party is waiting!!”.\nHe gets dressed, hurries to follow Dr. Brains, and they go outside to a “secretive” room that is labeled “G—CHECK, BNPS ROOM”. They go in, and he sees a bunch of devices that are new to him. He sits down, just as Dr. Brains ordered, and Dr. Brains brings a needle to his face, straight into his eyes. “Watch out!” James shouts. He doesn’t even have time to think, as the needle goes in and out of his eyes. Dr. Brains says, “Good, we already got the DNA, scanned the iris, scanned the brain map. Ok, 2 last things, then we are good to go!” He does a bunch of scans on James’ finger, and he enters a password into a machine. “Ok, one last thing. Print your BNPS and tattoo it to your shoulder!” Dr. Brains says. The machine reads “bring human to the tattoo station …… step 3/5”. Dr. Brains orders him to put his shoulder into a cylinder. He feels a little pressure and his shoulder pops out of the machine. He sees a little piece of metal on his shoulder and it reads ”TOFB.1029358612.JP/////////” The machine also prints out a metal card. “Don’t lose it!” Dr. Brains says, “it is your ID here!”\nThey walk out of the room and into the elevator. It is an elevator like the one in the TOFB’s entrance. The one that changes color and transparency, only much more slowly. When it tells him to scan the card, he knows better than to not do so. The elevator seems smart, and it asks “Homo, and James! Morning! Which level area do you want to go to?”. Dr. Brains responds, “Dining room number three, formal, both of us”. The elevator responds with a “TOFB wishes you a pleasant day!” When the door opens again, they enter a large area, like the first level of a 5 star hotel. Everything is white: people’s clothes, the ground, the staircase, the light, etc,. He sees Dr.Brains’ clothes change to white! He says, “Dr.Brains! Your outfit changed color!” “Yours did, too!” Dr. Brains responds. James looks down at his clothes. His had actually, as Dr. Brains said, changed color and texture.\nThey eat their breakfast—salmon, soup and broccoli, and Dr. Brains announces to him, “OK, now let’s do some work stuff”. They head back to the living area, and they wash themselves. Then they head to the meeting area. Norman, Dr. Ranboro and the other guy James sees in his dream are waving to James-and-Dr.Brains-in-the-black-suit-and-a-tie.\nPart 8 “So”, Dr. Ranboro says, “Welcome! Thank you for joining the organiza………?!!!\u0026gt;?*\u0026amp;%*\u0026amp;^%\u0026amp;^%%∆˙ßå˚µß∂˙”. FFF! A small arrow flies though the walls and hits Dr. Ranboro, making his words into nonsense. “å∆∆ß¬—å˚å!!!!!!¡¡¡¡¡¡?¿!¡……Jams…….main sq……is com…..tel..hom……nnnoor…….¡¡¡!!!???¿¿¿å∂ß˚˚˚˚∆ƒå˙”, he says. James can barely understand, but he knows one thing, they will tell him about the main sq…whatsoever.\n“Let’s jump into the topic,” Norman says. “The main sq… is actually an attack called The Main Square Rattle, or what we call TMSR. It’s started because another kind of alien, The Froakan, wants to use humans as slaves, own the TOFB AND the Hidonipothan.The only way to stop that is to get the battle-rattler and rattle it. But if The F’s got the rattler and rattle it, well, we will all freeze and do what they want, like a bunch of zombies. The state of being a zombie is called ratling. Sadly, there isn’t a known cure yet for ratle. But Dr. Brains is working on it! Lastly, the battle-rattler is locked in the Ratle Mountains. And the only way to enter the Ratle Mountain is by using Dr. Ranboro’s key. Otherwise, you will have very little chance to get out alive! And that’s why they shot Dr. Ranboro. As a matter of fact, the arrow is poisonous. If we don’t send him to hospital now, he will become a baby in 72 hours.”\nTalking about Dr. Ranboro, James notices Dr. Ranboro’s hair getting darker and darker from the old-man-white. They send him to the hospital about 5 minutes later.\nChapter 3:That’s called war After another ride on the “TOTP—0111”, which is the “squeeze box” to get to the North Town, they are back at Dr. Brains’ office. But something weird has happened, only students in OPH_LV # 40000 - OPH_LV # 49999 are still in the orphanage. Dr. Brains tries to find out why, but he can’t. And that’s when all of the humans in the orphanage hear a gigantic laugh coming from nowhere.”HHHAAAHHHAAAAHAHHAAA! This is your day, Homo, your death ceremony!HHAAHAHA!”\nMonths passed again……..\nPart 9 The daily live is almost the same as before, just that a part of the students in the orphanage is missing. But live is still very simple. Tasty food, friendly teachers, and visits to TOFB every other week.\nOne day, James is in his math class.\n“So when 2 is raised to the……”\n“Beep! Beep!”\nHis secret headphones from TOFB send a message request to him.\n“Beep! Beep!”\n“Didn’t that Ranboro guy say they won’t interrupt our classes?” James thought.\n“And let’s do some prob….”\n“Beep! Beep!”\nJames requests a bathroom break and answers the headphones in the boys’ restroom.\n“It’s an emergency!!! The Froakans are getting closer to the rattler!!! Help!!!! James, take Homo and get here now!” Norman cries.\nAs fast as he can, he rushes to Dr. Brains’ office, grabs Dr. Brains and locks him and himself into the TOTB-0111.\nAnd as fast as lightning, they are here, in the North town.\nThey rush into the elevator, he swipes his and Dr. Brains’ card and rushes to Dr. Ranboro’s office.\n“Quick! They will rattle it in like…like 20 minutes and we will all ratle!!!”, Norman hollers.\nAnd again, as fast as lightning, they get war-dressed and get into the fastest transport system in TOFB.\nAs James looks down, he is wearing a strong iron chest plate that reads ’T-O-F-B///////The Smarter one’. And on his shoulder, there is a cord which extends from his Digital ID to the chest plate. There is a screen in his chest plate that is unbreakable. There is a soft protection layer, then there is a swimming layer, then the pressure layer, an iron pad, an air supply on his side if the enemy spreads poisonous gas, and an armor on the outside.\nAmazingly, these things only weigh 1 pound and fit perfectly.\nHe is war-trained, so he knows exactly what to do with this fancy outfit. The screen is the main control, the outfit will detect the environment and change to the perfect layer.\nUploaded ate 10/25/2015 [sic.]\nPart 10 The ride seems to be long, but it’s actually only 5 minutes. They will enter the Ratle Mountains from the North End, which is the second-safest route into the mountains without Dr. Ranboro’s key.\nAnd there they are, in the Ratle Mountains. They are led by Mr. Giose, who was the other guy in his dream when he came to the TOFB the first time. The other four warriors are Norman, Dr. Ranboro, Dr. Brains and James. The first 20 miles are short and boring. Nothing happens. But after the 29th mile mark, they enter a cave.\nThe cave is dark. There are only few lights flashing. They are not worried, until they hear a scream.\n“OOOOOOO! Eeeek!”\n“Ahhhhhhhhhhhh! ZZZ! ZZZ…ZZZ..ZZZ…ZZZ…ZZZ…zzz…” The voice is getting smaller and smaller.\n“It is the sleeping spider! It will knock out a human in NO time!” Mr. Giose shouts.\nJames and the whole crew know what to do. They press a few buttons on their screens, and their helmets of their armor dissolve into the air. What is left behind, is the air filtering system.\n“Three! Two! One! It’s gas,” Norman says, playfully.\nDr. Brains spreads out the SSG gas, which will, hopefully, knock out the sleeping spider.\nThat wastes a LOT of time. Before they know it, they all starts to ratle.\nIt is James who feels it first. He feels extremely and uncontrollably happy. He starts running around and talking to other people in a rude way. To himself, it feels like as if he is drifting into unconsciousness.\nThen the same happens to Dr. Brains, and then Norman, followed by Mr. Giose. Luckily, Dr. Ranboro called the TOFB’s team 2 to come for help before he changes, too.\nI never knew what happened after this incident until the year of 2021. Since James was ratling, he couldn’t remember the whole year of 2020. He recovered on the day of 10/26/2021. Dr. Foster, who works at the orphanage AND at TOFB found a cure using Chinese Herbal Tea.\nWell, let’s jump into the time machine. Backward to 2014!!\nChapter 4:Childhood We jump into the time machine, and swoosh. Here we are, in the year of 2014. We are standing in front of 1623 Wesson Ave. It is a sunny day. The Peters are getting ready for a trip to Africa. James greets his uncle, who will look after the house while they are gone. Mrs. Peter is packing hastily. And Mr. Peter is bringing his computer, because, weirdly, he is starting to like CODING in BINARY CODE. Nothing more to say, so here the story goes.\nPart 11 “Hurrrrrryyyy!” Mr. Peter shouts. “Or else we will be late for the plane!”\nThe Peters hurry to the bus stand, waiting for the airport express.\nAfter about an hour ride, they finally arrive at the San Francisco International Airport.\nThey check in. And they hurry to the security check. At the security check, Mrs. Peter thinks she forgot something. Yes, she forgot to bring ANY medication for the disease Ebola.\n","permalink":"https://www.jemoka.com/posts/kbhnorman_an_epic_tale_in_n_parts/","tags":null,"title":"Norman: An Epic Tale in N Parts"},{"categories":null,"contents":"\u0026ldquo;Doing NSM analysis is a demanding process and there is no mechanical procedure for it. Published explications have often been through a dozen or more iterations over several months\u0026rdquo; \u0026mdash; (Heine, Narrog, and Goddard 2015)\nApproach and XD Introduction and Theory The Natural Semantic Metalanguage (NSM) approach (Wierzbicka 1974) is a long-standing hypothetical theory in structural semantics which claims that all human languages share a common set of primitive lexical units\u0026mdash;usually words, but, in some languages, short connected phrases\u0026mdash;through which all other words in each language can be defined.\nFor NSM to hold, two main results must be demonstrated. (Heine, Narrog, and Goddard 2015) The theory\u0026rsquo;s validity hinges, first, upon the existence of semantic primes\u0026mdash;a series of primitive lexical units both indefinable via other words in the same language but also is universally lexicalized across all languages. Second, the theory\u0026rsquo;s confirmation requires the ability to perform \u0026ldquo;reductive paraphrasing\u0026rdquo;, the process of defining all other words in a language with respect to the universal semantic primes\u0026rsquo; manifest in that language.\nIf proven as fact, the NSM theory and its implications has reaching implications into the long-standing (footnote: not to mention often personally fierce) conflict between the newer theories of generative semantics\u0026mdash;where structure of language is created in support of meaning\u0026mdash;and Noam Chomsky\u0026rsquo;s transformational generative syntax\u0026mdash;where meaning is filled to precomputed structure, which NSM suggests (Harris 2021).\nThe difficulty of forming adequate investigations in the area of NSM is due the theory itself being exceedingly hard to falsify\u0026mdash;the principle method through which NSM is demonstrated is via the manual (i.e. non-standardized) lexicalization of semantic primes and a partial demonstration of their relations (Geeraerts 2009) to other words in the language. Whenever one irregularity in the theory is identified (Bohnemeyer 1998), the proponents of the theory simply respond with another update to the (non standardized) set of reductive paraphrasing rules to account for the irregularity (NO_ITEM_DATA:goddard1998bad.)\nYet, there are repeated empirical (again, non-standardized) confirmations of the existence of the original set (Wierzbicka 1974) of semantic primes in other languages (Chappell 2002; Peeters 1994; Travis 2002); there are also numerous demonstrations of the proposed applications (Goddard 2012) of the theory in structural semantics. These facts has therefore maintained the relevance of NSM in current linguistic study but rendered the theory without a very clear path forward. Due to this reason, recent research has placed larger focus on functional (cognitive linguistical) theories (Divjak, Levshina, and Klavan 2016) and largely overlooked structuralist arguments like the NSM.\nBroad Goals and Approach To complement the very large body of work already in the identification of semantic primes for NSM in numerous languages, we aim in this project to investigate the process of reductive paraphrasing to provide a baseline evaluation of the feasibility of NSM as a theory. The approach proposed below is intended to very generally test the practicality of the act of reductive paraphrasing from the published set of primes: whether paraphrasing from those primes is even broadly possible across the entire lexicon of the few languages for which it is purported to be possible. This test remains needed because, quite counter-intuitively, metalanguage theorists have been constructing lexicalizations for non-prime words on an \u0026ldquo;as-needed\u0026rdquo; basis such as in (Wierzbicka 2007). No lexicon-wide demonstrations of lexicalizability has been performed (i.e. reductive paraphrasing all words down to the primes) as the current approach of manual definition of words from primes is significantly time-consuming and requires careful consideration of NSM\u0026rsquo;s semantic grammar between primes.\nWe aim perform a lexicon-wide test of reductive paraphrasing computationally via much newer approaches in computational linguistics, specifically model-based Natural Language Processing (NLP).\nIn order to isolate the exact problem of reductive paraphrasing, we first will have to highlight a few key assumptions by the NSM theory and therefore this project.\nThe semantic metalanguage theory is itself built on the assumption that \u0026ldquo;each language is its own metalanguage\u0026rdquo; (Goddard 2002)\u0026mdash;that human languages are broadly lexicalizable by itself (i.e. one can write an English dictionary by only using English.) We believe that the examination of this assumption is not within scope of the study and\u0026mdash;given it is fairly universally true from a practical standpoint (i.e. English dictionaries exist)\u0026mdash;we will take it as fact. We will use this fact further as the control for the feasibility of the approach, as discussed in the section below.\nThe remaining assumptions of NSM to be tested here, then, is that 1) semantic primes exist and 2) the original set of NSM primes published (Wierzbicka 1974) (and in subsequent studies in various other languages highlighted before) are correct and, through reductive paraphrase, can lexicalize every word in the lexicon.\nAims and Experimental Design In this study, we aim to develop a computational protocol for lexicon-wide testing of the possibility of performing reductive paraphrasing for every word in the lexicon given a set of purported semantic primes. Practically, this means that we are trying create a model to test whether all words in a language is lexicalizable when restricted to only using a chosen subset of primes in the same language.\nTo create a truly replicable test for lexicalizability under restriction, we turn to probabilistic NLP approaches. We propose the following metric for lexicalizability: a word is \u0026ldquo;lexicalizable\u0026rdquo; under some set of semantic primes if there is a lossless mapping between a linear combination of the primes\u0026rsquo; latent embeddings to the word in lexicon space.\nUnder this model, all words in the lexicon are lexicalizable by the set of primes being tested if there is a lossless projection of the bases of the lexical space to the primes\u0026rsquo; latent embedding space.\nThat is, given we have a latent embedding space of \\(n\\) semantic primes \\(P^n\\) and some lexicon \\(W\\) with \\(m\\) words, we aim to identify a linear mapping \\(M\\) such that:\n\\begin{equation} Mp = e_{W_j}\\ |\\ p \\in P^n, \\forall j=1\\ldots m \\end{equation}\nwhere, \\(e_{W_j}\\) is the \\(j\\) th standard basis of \\(W\\) (i.e. \\(j\\) th word in the lexicon.)\nThis projection is not, in principle, impossible. In the high-dimensional space of the entire lexicon, individual lexicalized words represent only the basis vectors of the space (and indeed in one-hot encodings for deep learning they are shown as the standard-basis of the lexicon-wide space.) Whereas in the lower-dimensional subspace of primes, a linear combination of primes can be used to represent each lexicalized word in the full lexicon.\nSuccess in identifying a feasible \\(M \\in \\mathcal{L}(P, W)\\) for a given \\(P\\) and \\(W\\) indicates the feasibility of finding a linear combination in \\(P\\) which maps to all \\(w \\in W\\), which means reductive paraphrase of \\(w\\) to a set of primes in \\(P\\) is possible as there is a direct \u0026ldquo;translation\u0026rdquo; (namely, \\(W\\)) from \\(P\\) to \\(W\\).\nTo actually compute \\(W\\) given \\(P\\) and \\(M\\), we leverage the well-established Transformer encoder-decoder architecture for language modeling (Vaswani et al. 2017). Furthermore, we frame the problem as one of unsupervised multi-lingual translation without alignments.\nThe basis of the model proposed to be used to obtain \\(W\\) is (Artetxe et al. 2018), a unsupervised multi-lingual translation model.\nFigure from (Artetxe et al. 2018).\nAs we are performing the task with word embeddings, not sentences like that of (Artetxe et al. 2018), the cross-attention lookup vector will serve no purpose (be \\(0\\)) (Niu, Zhong, and Yu 2021) and hence removed.\nFor the sake of standardization, we will call \\(P\\) the primary language/lexicon \\(L1\\), and \\(W\\) the second language/lexicon \\(L2\\). The basic hypothesis provided by (Artetxe et al. 2018) is that, through alternating samples of \\(L1\\) and \\(L2\\) through the model against their corresponding decoders using a shared encoder and separate decoders, the shared encoder is trained to perform the task of autoencoding for both lexicons at once. Therefore, at prediction time, to get the \u0026ldquo;translation\u0026rdquo; of an input, one simply applies the decoder of the desired lexicon to obtain a result.\nDuring training, the input to the shared encoder can either be a word from either \\(P\\) or $W$\u0026mdash;sampled with equal probability. If the input is from \\(P\\), we connect the output of the shared encoder with the \\(L1\\) decoder and train with the objective of recovering the input. Essentially, we are using the model as a alternate method of training a variational auto-encoder (Klys, Snell, and Zemel 2018) with alternating decoders given the lexicon being analyzed.\nThis task is trivial if the embedding space after the shared encoder is exactly as wide as both lexicon. However, we will restrict the output dimension of the shared encoder to \\(dim(P)\\) which after training we will call the latent embedding space of \\(L1\\); this name is verified and justified as a part of the feasibility check below.\nWe will also use the backtranslation mechanism proposed by (Artetxe et al. 2018) during training: whereby the autoencoded output from \\(L1\\) is used as target for the same input as \\(L2\\) (as well as the reverse), mimicking the process of translation.\nAfter training, the \\(L2\\) decoder would then be the candidate \\(W\\), mapping from the (proposed) latent embedding space of \\(P\\) to the lexicon \\(W\\).\nFollowing both (Artetxe et al. 2018; Conneau and Lample 2019) we will use cross-entropy as the objective function of training.\nFeasibility Checkpoint We first need to show that, as expected, the model architecture proposed above\u0026mdash;upon convergence\u0026mdash;will create a latent embedding for \\(L1\\) after encoding if the output size for encoding is \\(dim(L1)\\) (defined to be equal to \\(dim(P)\\)).\nA trivial test of whether the encoding output is desirably the embedding space of \\(L1\\) is that, through training with a toy mapping \\(P=W=L1=L2\\), we would expect both decoders to be an one-to-one mapping that simply copies the input. That is, after training with \\(P=W\\), we should see that activating one input in the shared post-encoding space should activate one or close to one feature only in both decoder\u0026rsquo;s output space.\nNumerically, this means that the result obtained from taking the mean entropy of both outputs given a singular input activation should be statistically insignificantly different from \\(0\\).\nThat is, we expect that given trained decoders \\(L_1\\) and \\(L_2\\), and standard bases of \\(W=P\\) named \\(e\\), we should see that:\n\\begin{equation} \\frac{\\log(L_1e_j) + \\log(L_2e_j)}{2} \\approx 0: \\forall j = 1\\ldots dim(W) \\end{equation}\nWe expect this result because, through gradient-descent, the quickest minima reachable to capture variation in the input perfectly is the copying task; therefore, we should expect here that if the post-encoding distribution is the same distribution as the input, the model\u0026rsquo;s decoders will fit to the copying task. If the post-encoding distribution is different from the input, the model\u0026rsquo;s decoders would then have to actually perform nontrivial mappings to achieve the desired autoencoding result.\nCheckpoint 2 + Hypothesis 1 The following is the first novel result that we can show with the new architecture. We first hypothesize that the model should converge when training to the target of the (already linguistically accepted, as aforementioned) result that English words are themselves a metalanguage.\nFor \\(dim(W)\\) iterations (similar to (Webb et al. 2011)), we will leave a word chosen at random out of the lexicon of \\(P\\). This operation results in \\(dim(P) = dim(W)-1\\). We will then train the model until a local minima is reached and measure convergence.\nTo test this hypothesis, we will measure the cross-entropy performance of \\(L2\\) decoder upon the word that is left out. The resulting loss should be statistically insignificantly different from \\(0\\) if the word is successfully lexicalized via the \\(dim(W)-1\\) other words not left out in \\(P\\) in the latent embedding space after encoding.\nIf the hypothesis is not successful, the model cannot converge even on a large subset of the entire lexicon, much less in the limited subset of the 60-word NSM-proposed metalanguage; it is therefore imperative not to continue the study unless convergence at this point can be shown. Importantly, however, failures in this step does not show any claims about reductive paraphrasing as we are simply benchmarking the model against a control linguistic assumption we discussed earlier.\nIn any case, it would be valuable at this point to again perform analyze for post-encoding output to observe any reductive paraphrasing behavior.\nHypothesis 2 At this point, we will set the lexicons to the sets we are actually testing. We will set \\(P\\) to be the list of semantic primes established by (Heine, Narrog, and Goddard 2015), and \\(W\\) to the English lexicon.\nShould lexicalization of all of the English lexicon via the semantic primes only be possible, this model should again converge after training with cross-entropy inappreciably different from \\(0\\). This result would indicate the existence of a \\(W\\) (i.e. \\(L2\\) decoder), indicating the possibility of lexicon-wide reductive paraphrasing.\nInstitution and Experience The actual protocol proposed as a part of this study (namely, creating, training, and calculating metrics from the autoencoder) is a technical concept taught as a part of the regular curriculum of Advanced Machine Learning at Nueva; however, expertise and mentorship may still be required when implementing a complex model topology and training mechanism like the one proposed. The open-ended project structure of the Advanced Machine Learning course supports and sometimes necessitate implementing a model like the one proposed with the help of the CS faculty. Therefore, if additional mentorship is indeed required, there exists support available within the institution.\nThe more difficult skill-set to capture is the knowledge regarding the theories of NSM and the field of structuralist linguistics in general. As of writing, we are not aware of any students which has an active research interest in traditional linguistics; however, this knowledge constitute a far more insignificant portion of the actual mechanics of the project and is more importantly very easily taught. Mentorship is also available here from members of the Mathematics and CS faculty with prior research interest in computational linguistics.\nIn terms of equipment, the most important tool required in working with a large-scale neural network is a matrix-mathematics accelerator; this often takes the form of a consumer graphics card and typical desktop computing setup. For the Machine Learning course taught at Nueva, Google\u0026rsquo;s Colab (and their free graphics card addition) is frequently used to address this need and would at a minimum suffice here. Also, it is based on the personal experience of the author, though by no means definite, that a large selection of students at Nueva has comparable hardware for training available at home.\nProvided network access to the computing accelerator, this experiment can be done under any setting and definitely does not necessitate the use of the biology lab.\nImpact Academic Significance Within the short term, this experiment provides two major results. First, it establishes the use of a bifercated unsupervised encoder-decoder translation model like that proposed by (Artetxe et al. 2018) as a Conditional Variational Autoencoder (CVAE) (Klys, Snell, and Zemel 2018) with the ability to define and train the hidden latent representation after encoding. Although traditional CVAEs are frequently more suited for most output-aware generation tasks, this new scheme supports the direct influence of the latent representations of the encoder instead of using an additional input to both the encoder and decoder to influence such representations, like in traditional CVAEs. This difference is significant for as it creates the where dimensional projection is needed but the content of the latent representation itself is also relevant to the study.\nOf course, the short-term result also includes the direct result of the second tested hypothesis: a systemic, lexicon-wide evaluation of the feasibility of reductive paraphrasing. The study is to develop a computational protocol for lexicon-wide reductive paraphrasing by creating a lossless mapping between a linear combination of the primes\u0026rsquo; latent embeddings to the word in lexicon space.\nIf both initial metrics succeeds and the third, final reduction step with actual semantic primes fail, the result would indicate an inability to create such a lossless mapping, and therefore raise concerns about the lexicon-wide applicability of the reductive paraphrasing on the set of published semantic primes. That, there is not even a locally convergent linear combination of primes that will generally describe all of the lexicon, despite the hypothesis by NSM theorists. This result will be highly impactful for NSM theory in general which necessitates the possibilty of reductive paraphrase (Geeraerts 2009) (Vanhatalo, Tissari, and Idström, n.d.).\nOn the long term, demonstrations of reductive paraphrasing has wide-reaching implications into NSM theory is general (Heine, Narrog, and Goddard 2015; Geeraerts 2009), and the field of language learning. The paraphrasing capacity of the proposed embedding would hypothetically be able to create a semantic mapping between a set of words to one other word; in this way, it is not infeasible to create a language-learning tool with continually larger embedding size to slowly create a larger lexicon in the target user. Early results (Sharma and Goyal 2021) have shown a possible application of such an approach, using supervised machine translation techniques.\nLearning and Teaching One to two students, along with a facilitator, would be an ideal size for this experiment. Primarily, the three main roles will include model engineering, training and validation, and model ablation and testing. The last role requires the most amount of traditional linguistics knowledge as the student\u0026rsquo;s role would be to connect the weights in the model to the applicable theories being tested.\nThe study proposed is an extremely conventional empirical Machine Learning/NLP study. From a pedagogical standpoint for XRT, this study will be a diversion from the traditional wet-lab sciences or survey-based educational/social sciences commonly produced by the lab and lead a new avenue for the Lab\u0026rsquo;s expansion. Within Nueva, empirical research into machine learning is frequently done through independent study or the Intro/Advance machine learning courses\u0026mdash;which were recently expanded due to widening interest at the Upper School.\nParticipation in this project provides its constituent students an opportunity to practice publish-quality ML/NLP in a longer-term and multi-stage project previously not possible through semester-long courses. Students are trained to perform model construction, data selection and cleaning, collection of model validation metrics, as well as model ablation and interpretation: important concepts in ML operations taught but not formalized in the Machine Learning course as the course exercises, while open-ended, isolate only one skill and have expected outcomes.\nGiven the demand and rate of student progression between Intro/Advanced courses in ML each year, developing a suitable approach to propagate true machine-learning research will be relevant to upwards of 30 students each year.\nIncidentally, students also get an exposure to the practice of conventional linguistics and the new trend of applying empirical research NLP back against classic semantics; however, the demand for this exact skill is likely small at Nueva.\nThough the tool used and expanded upon by this experiment is applicable to the NLP research community, it is unfortunately difficult to predict its future applications to XRT or Nueva students without seeing more expansion into the area of ML and NLP by the XRT lab.\nSafety and Ethics The following are the responses to the safety and ethics checklist.\nThis project does not satisfy any triggers of the second-expert protocol. All data needed is from a dictionary (for the English lexicon, e.g. (Fellbaum 2010)) as well as the semantic primes listed in a figure on the article (Heine, Narrog, and Goddard 2015). The data is being generated during compute. The actual compute hardware will need to be stored in either in the cloud (not on-prem), physically in the iLab, or (for personal compute hardware), in students\u0026rsquo; homes. An internet connection and a model training acceleration scheme (such as the free Google Colab) would suffice. None foreseeable See below The experiment is done on the English lexicon. It is difficult to imagine a tangible harm from the experiment. This study provides students with an opportunity to conduct a full research study in ML; XRT has not had this from of projects before and approval would result in a new avenue of research being conducted with XRT. However, if the project is not approved, other ML projects may subsequently surface and students can leverage those opportunities to learn about the practice of empirical ML instead. As with most machine-learning projects, it is customary and appropriate to end with a statement on ML ethics and its implications. This study is a linguistics, lexicon-scale study, and the data sourced is available generally and not subject to copyright or any known data-protection laws. The inputs to the model are combinations of English words, and the model produces singular English words. The benefits of this model involves generating new knowledge about the English lexicon and semantic theories. The only known harm of the model involves the mis-intepretation of its results, creating overreaching generalizations to semantic primality analysis or NSM theories. The model and source code can be released to the general public without broad impact.\nAcknowledgments I would like to thank Brandon Cho at Princeton University and Ted Theodosopoulos at The Nueva School for the very interesting discussion/argument that resulted in this proposal almost a year ago. I would like to thank Klint Kanopka at Stanford University for his mentorship and discussion of the overall feasibility of the approach and pointing out the path that lead to the proposed model\u0026rsquo;s basis in machine translation. Finally, I would like to thank Prof. Brian MacWhinney at Carnegie Mellon University for pointing out discourse between structuralism/functionalism during our exchanges and for his mentorship in my exploration of computational linguistics.\nReferences Artetxe, Mikel, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2018. “Unsupervised Neural Machine Translation,” 12. Bohnemeyer, Jurgen. 1998. “Temporal Reference from a Radical Pragmatics Perspective: Why Yucatec Does Not Need to Express ’after’ and ’before’.” Walter de Gruyter, Berlin/New York Berlin, New York. Chappell, Hilary. 2002. “5. The Universal Syntax of Semantic Primes in Mandarin Chinese.” In Studies in Language Companion Series, 243–322. Studies in Language Companion Series. John Benjamins Publishing Company. doi:10.1075/slcs.60.12cha. Conneau, Alexis, and Guillaume Lample. 2019. “Cross-Lingual Language Model Pretraining,” 11. Divjak, Dagmar, Natalia Levshina, and Jane Klavan. 2016. Cognitive Linguistics 27 (4): 447–63. doi:doi:10.1515/cog-2016-0095. Fellbaum, Christiane. 2010. “Wordnet.” In Theory and Applications of Ontology: Computer Applications, 231–43. Springer. Geeraerts, Dirk. 2009. “Neostructuralist Semantics.” In Theories of Lexical Semantics, 124–78. Theories of Lexical Semantics. Oxford University Press. doi:10.1093/acprof:oso/9780198700302.003.0004. Goddard, Cliff. 2002. “The Search for the Shared Semantic Core of All Languages.” In Meaning and Universal Grammar: Theory and Empirical Findings. John Benjamins Publishing Company. ———. 2012. “Semantic Primes, Semantic Molecules, Semantic Templates: Key Concepts in the NSM Approach to Lexical Typology.” Linguistics 50 (3). doi:10.1515/ling-2012-0022. Harris, Randy Allen. 2021. The Linguistics Wars: Chomsky, Lakoff, and the Battle over Deep Structure. Oxford University Press. Heine, Bernd, Heiko Narrog, and Cliff Goddard. 2015. “The Natural Semantic Metalanguage Approach.” In The Oxford Handbook of Linguistic Analysis, edited by Bernd Heine and Heiko Narrog. Oxford University Press. doi:10.1093/oxfordhb/9780199677078.013.0018. Klys, Jack, Jake Snell, and Richard Zemel. 2018. “Learning Latent Subspaces in Variational Autoencoders,” 11. Niu, Zhaoyang, Guoqiang Zhong, and Hui Yu. 2021. “A Review on the Attention Mechanism of Deep Learning.” Neurocomputing 452 (September): 48–62. doi:10.1016/j.neucom.2021.03.091. Peeters, Bert. 1994. “16 Semantic and Lexical Universals in French.” In Studies in Language Companion Series, 423. Studies in Language Companion Series. John Benjamins Publishing Company. doi:10.1075/slcs.25.20pee. Sharma, Prawaal, and Navneet Goyal. 2021. “Zero-Shot Reductive Paraphrasing for Digitally Semi-Literate.” In Forum for Information Retrieval Evaluation, 91–98. Travis, Catherine E. 2002. “4. La Metalengua Semántica Natural.” In Studies in Language Companion Series, 173–242. Studies in Language Companion Series. John Benjamins Publishing Company. doi:10.1075/slcs.60.11tra. Vanhatalo, Ulla, Heli Tissari, and Anna Idström. n.d. “Revisiting the Universality of Natural Semantic Metalanguage: A View through Finnish,” 28. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” 11. Webb, Geoffrey I., Claude Sammut, Claudia Perlich, Tamás Horváth, Stefan Wrobel, Kevin B. Korb, William Stafford Noble, et al. 2011. “Leave-One-Out Cross-Validation.” In Encyclopedia of Machine Learning, edited by Claude Sammut and Geoffrey I. Webb, 600–601. Boston, MA: Springer US. doi:10.1007/978-0-387-30164-8_469. Wierzbicka, Anna. 1974. “Semantic Primitives.” Lingua 34 (4): 365–69. doi:10.1016/0024-3841(74)90004-7. ———. 2007. “Bodies and Their Parts: An NSM Approach to Semantic Typology.” Language Sciences 29 (1): 14–65. doi:10.1016/j.langsci.2006.07.002. NO_ITEM_DATA:goddard1998bad. ","permalink":"https://www.jemoka.com/posts/kbhnsm_proposal/","tags":null,"title":"NSM Proposal"},{"categories":null,"contents":"NUS Secondary School Other Duties AP Statistics Index AP Phys C Mech Index AP Phys C EM Index Tuning Forks bioinformatics PKM Intersession 2023 NUS-MATH580 QIC Date Topic \u0026lt;2022-04-05 Tue\u0026gt; physical qubits, manipulating physical qubits \u0026lt;2022-04-08 Fri\u0026gt; making qubits interact \u0026lt;2022-05-10 Tue\u0026gt; Chiara Marletto \u0026lt;2022-05-24 Tue\u0026gt; Strong Free Will NUS-CS223 Algorithms Backlog: Finite State Machine\nDate Topic \u0026lt;2022-04-07 Thu\u0026gt; stable matching problem, stable matching algorithm \u0026lt;2022-05-02 Mon\u0026gt; dynamic programming, relaxation \u0026lt;2022-05-23 Mon\u0026gt; distributed algorithum, randomized algorithum, complexity theory NUS-HIST301 American History Backlog: New Deal, Franklin D. Roosevelt (FDR), Works Progress Administration, effects of the New Deal, Great Depression, Herber Hoover, disinformation, Guilded Age\nDate Topic \u0026lt;2022-04-07 Thu\u0026gt; WWII, propaganda \u0026lt;2022-05-02 Mon\u0026gt; cold war \u0026lt;2022-05-09 Mon\u0026gt; civil rights \u0026lt;2022-05-26 Thu\u0026gt; Richard Nixon \u0026lt;2022-06-01 Wed\u0026gt; Ronald Raegan NUS-PHYS301 Mech Date Topic \u0026lt;2022-04-12 Tue\u0026gt; String Yo-Yo Problem, rotational energy \u0026lt;2022-05-24 Tue\u0026gt; Gyroscopes NUS-ENG401 English Date Topic \u0026lt;2022-04-15 Fri\u0026gt; secondary source comparison activity Essays Bluest Eye Essay Planning I, Tituba Essay Planning NUS-MATH530 Please refer to Linear Algebra Index\nNUS-ECON320 Financial Econometrics Date Topic \u0026lt;2022-08-25 Thu\u0026gt; Financial Markets Intro, ECON320 Architecture NUS-CS350 Software Studio Date Topic \u0026lt;2022-08-25 Thu\u0026gt; User Interviews, User Story \u0026lt;2022-09-07 Wed\u0026gt; Software Engineering, Prototyping \u0026lt;2022-09-12 Mon\u0026gt; Task Estimation \u0026lt;2022-09-15 Thu\u0026gt; Documentation and Specification \u0026lt;2022-09-19 Mon\u0026gt; Testing \u0026lt;2022-10-06 Thu\u0026gt; Defensive Programming \u0026lt;2022-11-03 Thu\u0026gt; Code Review \u0026lt;2022-12-01 Thu\u0026gt; UX Design NUS-MATH570 DiffEq Date Topic \u0026lt;2022-08-26 Fri\u0026gt; DiffEq Intro NUS-LANG250 Translation Translation Studies Index\nNUS-MATH575 CompBio Computational Biology Index\n","permalink":"https://www.jemoka.com/posts/kbhnueva_courses_index/","tags":["index"],"title":"Nueva Courses Index"},{"categories":null,"contents":"Boop.\nThank you so much for being the inspiring voice of reason(ing in Physics), for being the shoulder that I could lean on during YPT, for your unwavering intelligence charm that makes everything happen.\nAlso, thank you for adding bright moments of levity into my day. It really makes them better.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_1anafna/","tags":null,"title":"Nueva Kindness: 1anafna"},{"categories":null,"contents":"Hello Kian,\nWow. R@N, PERCH, XRT, UCSF\u0026hellip; What can\u0026rsquo;t you do? I am humbled by your work and your dedication\u0026mdash;especially your drive to chase what you want out of your high school experience. Thank you for seeing me through the tough time for my family together with me, and for continuing to champion your own work and to even support others at Nueva through your gracious leadership. For the last two years, it has been my unbelievable privilege and luck to call you my friend and collaborator.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_1cksali/","tags":null,"title":"Nueva Kindness: 1cksali"},{"categories":null,"contents":"Hello Michael,\nYour slice! You dice! You are a taskmaster that make R@N run, you are the leader that power Ferrofoot, you are the voice that sets the tone in JC. What more can I ask for in a collaborator? It has been my privilege to learn from how you think, how you lead, and how you make a lasting mark in any room you grace.\nSeriously. Thank you for making it all happen, and for keeping the same trademark sense of punchy levity that makes it all fun and worthwhile.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_1fmicsa/","tags":null,"title":"Nueva Kindness: 1fmicsa"},{"categories":null,"contents":"Hi William,\nThank you very much for being the constant voice of sanity and reason, the leader keeping me in emotional and physical check during YPT, and for your dedication to your craft in physics and beyond.\nThank you for giving me the opportunity to discuss ML, for humoring my numerous rants and terrible jokes, for listening to my mediocre music. It has been a privilege being your partner during YPT.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_21wfish/","tags":null,"title":"Nueva Kindness: 21wfish"},{"categories":null,"contents":"Flint!\nIts been what\u0026hellip; 6 years? Thank you for your friendship through the last many many years, and for adding levity to any room you inhabit. Thank you for the intelligence and willingness you have through our many conversations: going as far back as M. D\u0026rsquo;s 8th grade geometry!\nThank you for making R@N what it is, and for making the past years amazing!\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_2hflikz/","tags":null,"title":"Nueva Kindness: 2hflikz"},{"categories":null,"contents":"Oliver!\nYou are the talent that makes R@N work! Seriously\u0026hellip; We\u0026rsquo;d have no work and! no team if it weren\u0026rsquo;t for you. Thank you for being that incredible leader to make the dream work, and also for your artful engagement in important conversations (not the least of which, GPT!) on campus through your film-making. It has been amazing being your friend and collaborator; thank you for making it all happen.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_32olich/","tags":null,"title":"Nueva Kindness: 32olich"},{"categories":null,"contents":"Teddy,\nThank you for making it all happen! Your expertise and your skill made everything go around with Scalender; it has been my privilege to learn from you and how you approach things. Thank you for being the independent leader that keeps the team together, for helping to show the best sides of everyone.\nBest of luck with everything. Thank you for being a friend and a mentor for me. I am stoked to continue our collaborations well into the future.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_3utedga/","tags":null,"title":"Nueva Kindness: 3utedga"},{"categories":null,"contents":"Anping\u0026ndash;\nIt really has been a true privileged and pleasure working with you over the last years. The theoretical underpinnings and, truth be told, sheer levity in collaboration made funing torks what is is. We could not be here without you; thank you for making it happen.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_a3anpzh/","tags":null,"title":"Nueva Kindness: a3anpzh"},{"categories":null,"contents":"Lachlan,\nThank you! Its been an amazing few years collaborating together in the STEM classes which we seem to keep crossing paths in. Its been quite the privileged learning from how you think, how you approach challenges, and for your deep nuanced take on things.\nBest of luck with the novel. I\u0026rsquo;m super excited to read it.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_b3lachu/","tags":null,"title":"Nueva Kindness: b3lachu"},{"categories":null,"contents":"Thomas,\nIt has been a privilege working with you during YPT. Your intelligence, depth of knowledge (especially about everything in our darn physical world), and your coolness under immense pressure has continually inspired me during and after the competition.\nThank you for the opportunity to discuss ML together, and for putting up with me over the last year. It has been incredible.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_e2tomre/","tags":null,"title":"Nueva Kindness: e2tomre"},{"categories":null,"contents":"Misha\u0026mdash;\nThank you for always being there, for bringing Menlo Park together, and for being the voice of reason even despite my many maniacal machinations.\nThank you for making life at Nueva and beyond fun, and for being so always kind and supportive throughout the last four years for which I had the privilege of knowing you.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_e9misag/","tags":null,"title":"Nueva Kindness: e9misag"},{"categories":null,"contents":"Sasha!\nWow. It has been a year!! Thank you very much for sticking together with funing torks since the beginning, and for seeing this incredibly project through together. It has been an honour being your collaborator.\nI am super excited to dive head-first into QuantumNLP. I can\u0026rsquo;t wait to see what we create next together. Thank you for everything.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_easasco/","tags":null,"title":"Nueva Kindness: easasco"},{"categories":null,"contents":"Thank you Vinca,\nThank you so much for sticking through with R@N through its many thicks and thins; thank you for your leadership\u0026mdash;always providing a cool voice of reason and direction through all of the often manic and unpredictable changes. Thank you for creating the systems that allowed the team to happen: our team cannot be where it is without you.\nI am humbled by your depth of knowledge, and your consistent willingness to share it with others. I am super excited to dive head-first into QuantumNLP. I can\u0026rsquo;t wait to see what we create next together. Thank you for making the amazing happen.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_h3vinlu/","tags":null,"title":"Nueva Kindness: h3vinlu"},{"categories":null,"contents":"Charlotte,\nHOLY CRAP! Its been almost a whole year\u0026hellip; The Studio has come a looong way since you and Logan took the helm, and I daresay it is going to places far beyond what I could have done.\nThank you for being the cool thinker even despite my mania, for being the direct and effective communicator. Your people skills humble me and are a goalstone from which I am still far from achieving.\nThank you for the opportunity to be your friend and your collaborator, and for your leadership making it all happen.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_h4charo/","tags":null,"title":"Nueva Kindness: h4charo"},{"categories":null,"contents":"Logan,\nWow\u0026hellip; Its been an amazing year working together. I don\u0026rsquo;t know how you stay so cool under the almost literally billion things you have going on; watching you keep it all together humbles me, and I admire this skill greatly.\nThank you for making the Studio what it is, for keeping the panic in check, and for adding levity to everyone\u0026rsquo;s day. And of course, thank you for your leadership making the Studio what it is.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_hhlogra/","tags":null,"title":"Nueva Kindness: hhlogra"},{"categories":null,"contents":"Test\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_test/","tags":null,"title":"Nueva Kindness: test"},{"categories":null,"contents":"4 years?!?!?\nHOW?\nPeter\u0026mdash;what can you not do? Your cool and thoughtful leadership make JC and FRC happen, your intelligence and precision make XRT and Longitude what they are. Your sheer drive in every single day humble me; it is infectious.\nThank you for being the voice of reason, for staying so cool under pressure, and for many times keeping my sheer mania in check.\nSincerely yours,\nHoujun \u0026ldquo;Jack\u0026rdquo; Liu\n","permalink":"https://www.jemoka.com/posts/kbhnueva_kindness_u9petch/","tags":null,"title":"Nueva Kindness: u9petch"},{"categories":null,"contents":"The Null Space, also known as the kernel, is the subset of vectors which get mapped to \\(0\\) by some Linear Map.\nconstituents Some linear map \\(T \\in \\mathcal{L}(V,W)\\)\nrequirements The subset of \\(V\\) which \\(T\\) maps to \\(0\\) is called the \u0026ldquo;Null Space\u0026rdquo;:\n\\begin{equation} null\\ T = \\{v \\in V: Tv = 0\\} \\end{equation}\nadditional information the null space is a subspace of the domain It should probably not be a surprise, given a Null Space is called a Null Space, that the Null Space is a subspace of the domain.\nzero As linear maps take \\(0\\) to \\(0\\), \\(T 0=0\\) so \\(0\\) is in the Null Space of \\(T\\).\nclosure under addition We have that:\n\\begin{equation} 0+0 = 0 \\end{equation}\nso by additivity of the Linear Maps the map is closed under addition.\nclosure under scalar multiplication By homogeneity of linear maps, the same of the above holds.\nThis completes the subspace proof, making \\(null\\ T\\) a subspace of the domain of \\(T\\), \\(V\\). \\(\\blacksquare\\)\nthe null space of the zero map is just the domain I mean duh. The zero map maps literally everything to zero.\nInjectivity IFF implies that null space is \\(\\{0\\}\\) See injectivity IFF implies that null space is \\(\\{0\\}\\)\n","permalink":"https://www.jemoka.com/posts/kbhnull_space/","tags":null,"title":"null space"},{"categories":null,"contents":"A number can be any of\u0026hellip;\n\\(\\mathbb{N}\\): natural number \\(\\mathbb{Z}\\): integer \\(\\mathbb{Q}\\): rational number \\(\\mathbb{R}\\): real number \\(\\mathbb{P}\\): irrational number \\(\\mathbb{C}\\): complex number ","permalink":"https://www.jemoka.com/posts/kbhnumber/","tags":null,"title":"number"},{"categories":null,"contents":"Here\u0026rsquo;s the characteristic equation again:\n\\begin{equation} \\pdv[2] x \\qty(EI \\pdv[2]{w}{x}) = -\\mu \\pdv{w}{t}+q(x) \\end{equation}\nAfter Fourier decomposition, we have that:\n\\begin{equation} EI \\dv[4]{\\hat{w}}{x} - \\mu f^{2}\\hat{w} = 0 \\end{equation}\nLet\u0026rsquo;s solve this!\nE,I,u,f = var(\u0026#34;E I u f\u0026#34;) x, L = var(\u0026#34;x L\u0026#34;) w = function(\u0026#39;w\u0026#39;)(x) _c0, _c1, _c2, _c3 = var(\u0026#34;_C0 _C1 _C2 _C3\u0026#34;) fourier_cantileaver = (E*I*diff(w, x, 2) - u*f^2*w == 0) fourier_cantileaver -f^2*u*w(x) + E*I*diff(w(x), x, x) == 0 And now, we can go about solving this result.\nsolution = desolve(fourier_cantileaver, w, ivar=x, algorithm=\u0026#34;fricas\u0026#34;).expand() w = solution \\begin{equation} _{C_{1}} e^{\\left(\\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{0}} e^{\\left(i \\, \\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{2}} e^{\\left(-i \\, \\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{3}} e^{\\left(-\\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} \\end{equation}\nWe will simplify the repeated, constant top of this expression into a single variable \\(b\\):\nb = var(\u0026#34;b\u0026#34;) top = sqrt(f)*(u/(E*I))**(1/4) w = _c1*e^(b*x) + _c0*e^(i*b*x) + _c2*e^(-i*b*x) + _c3*e^(-b*x) w _C1*e^(b*x) + _C0*e^(I*b*x) + _C2*e^(-I*b*x) + _C3*e^(-b*x) \\begin{equation} _{C_{1}} e^{\\left(b x\\right)} + _{C_{0}} e^{\\left(i \\, b x\\right)} + _{C_{2}} e^{\\left(-i \\, b x\\right)} + _{C_{3}} e^{\\left(-b x\\right)} \\end{equation}\nWe have one equation, four unknowns. However, we are not yet done. We will make one more simplifying assumption\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\nRecall that:\n\\begin{equation} \\begin{cases} \\cosh x = \\frac{e^{x}+e^{-x}}{2} \\\\ \\cos x = \\frac{e^{ix}+e^{-ix}}{2}\\\\ \\sinh x = \\frac{e^{x}-e^{-x}}{2} \\\\ \\sin x = \\frac{e^{ix}-e^{-ix}}{2i}\\\\ \\end{cases} \\end{equation}\nWith a new set of scaling constants \\(d_0\\dots d_3\\), and some rearranging, we can rewrite the above expressions into just a linear combination of those elements. That is, the same expression for \\(w(x)\\) at a specific frequency can be written as:\n\\begin{equation} d_0\\cosh bx +d_1\\sinh bx +d_2\\cos bx +d_3\\sin bx = w \\end{equation}\nNo more imaginaries!!\nSo, let us redefine the expression:\nd0, d1, d2, d3 = var(\u0026#34;d0 d1 d2 d3\u0026#34;) w = d0*cosh(b*x)+d1*sinh(b*x)+d2*cos(b*x)+d3*sin(b*x) w d2*cos(b*x) + d0*cosh(b*x) + d3*sin(b*x) + d1*sinh(b*x) Now, we need to move onto solving when there will be valid solutions to this expression. However, we currently have four unknowns, and only one equation (at \\(x=0\\), \\(w=0\\), because the cantilever is fixed at base); so, to get a system in four elements, we will take some derivatives.\nThe way that we will go about this is by taking three derivatives and supplying the following initial conditions to get four equations:\nwp = diff(w,x,1) wpp = diff(w,x,2) wppp = diff(w,x,3) (wp, wpp, wppp) (b*d3*cos(b*x) + b*d1*cosh(b*x) - b*d2*sin(b*x) + b*d0*sinh(b*x), -b^2*d2*cos(b*x) + b^2*d0*cosh(b*x) - b^2*d3*sin(b*x) + b^2*d1*sinh(b*x), -b^3*d3*cos(b*x) + b^3*d1*cosh(b*x) + b^3*d2*sin(b*x) + b^3*d0*sinh(b*x)) And then, we have a system:\ncond_1 = w.subs(x=0) == 0 cond_2 = wp.subs(x=0) == 0 cond_3 = wpp.subs(x=L) == 0 cond_4 = wppp.subs(x=L) == 0 conds = (cond_1, cond_2, cond_3, cond_4) conds (d0 + d2 == 0, b*d1 + b*d3 == 0, -b^2*d2*cos(L*b) + b^2*d0*cosh(L*b) - b^2*d3*sin(L*b) + b^2*d1*sinh(L*b) == 0, -b^3*d3*cos(L*b) + b^3*d1*cosh(L*b) + b^3*d2*sin(L*b) + b^3*d0*sinh(L*b) == 0) solve(conds, d0, d1, d2, d3).full_simplify() Ok so, we notice that out of all of these boundary expressions the \\(b^{n}\\) term drop out. Therefore, we have the system:\n\\begin{equation} \\begin{cases} d_0 + d_2 = 0 \\\\ d_1 + d_3 = 0 \\\\ -d_2 \\cos Lb + d_0 \\cosh Lb - d_3 \\sin Lb + d_1 \\sinh Lb = 0 \\\\ -d_3 \\cos Lb + d_1 \\cosh Lb + d_2 \\sin Lb + d_0 \\sinh Lb = 0 \\\\ \\end{cases} \\end{equation}\nNow, taking the top expressions, we gather that:\n\\begin{equation} \\begin{cases} d_0 = -d_2 \\\\ d_1 = -d_3 \\end{cases} \\end{equation}\nPerforming these substitutions:\n\\begin{equation} \\begin{cases} d_0 (\\cos Lb + \\cosh Lb) + d_1 (\\sin Lb + \\sinh Lb) = 0 \\\\ d_1 (\\cos Lb + \\cosh Lb) + d_0 (\\sinh Lb- \\sin Lb ) = 0 \\\\ \\end{cases} \\end{equation}\nNow we are going to do some cursed algebra to get rid of all the rest of the \\(d\\). We want to do this because we don\u0026rsquo;t really care much what the constants are; instead, we care about when a solution exists (hopefully, then, telling us what the \\(f\\) baked inside \\(b\\) is). So:\n\\begin{align} \u0026amp;d_0 (\\cos Lb + \\cosh Lb) + d_1 (\\sin Lb + \\sinh Lb) = 0 \\\\ \\Rightarrow\\ \u0026amp; \\frac{-d_0}{d_1} = \\frac{(\\sin Lb + \\sinh Lb)}{(\\cos Lb + \\cosh Lb)} \\end{align}\nand\n\\begin{align} \u0026amp;d_1 (\\cos Lb + \\cosh Lb) + d_0 (\\sinh Lb- \\sin Lb ) = 0 \\\\ \\Rightarrow\\ \u0026amp; \\frac{-d_0}{d_1} = \\frac{(\\cos Lb + \\cosh Lb)}{(\\sinh Lb- \\sin Lb )} \\end{align}\ntherefore, we have:\n\\begin{equation} \\frac{(\\sin Lb + \\sinh Lb)}{(\\cos Lb + \\cosh Lb)} = \\frac{(\\cos Lb + \\cosh Lb)}{(\\sinh Lb- \\sin Lb )} \\end{equation}\nMultiplying each side by the other:\n\\begin{equation} (\\sin Lb + \\sinh Lb)(\\sinh Lb- \\sin Lb ) = (\\cos Lb + \\cosh Lb)^{2} \\end{equation}\nExpanding both sides now:\n\\begin{equation} (\\sinh^{2} Lb-\\sin^{2} Lb) = (\\cos^{2} Lb + 2\\cos Lb\\ \\cosh Lb + \\cosh ^{2}Lb) \\end{equation}\nMoving everything finally to one side:\n\\begin{equation} \\sinh^{2} Lb - \\cosh^{2} Lb -\\sin ^{2} Lb - \\cos ^{2}Lb - 2\\cos Lb \\cosh Lb = 0 \\end{equation}\nOk, this is where the satisfying \u0026ldquo;candy crush\u0026rdquo; begins when things cancel out. Recall pythagoras:\n\\begin{equation} \\begin{cases} \\cosh^{2}x - \\sinh^{2} x = 1 \\\\ \\sin^{2}x + \\cos^{2} x = 1 \\end{cases} \\end{equation}\nTo apply these effectively, multiply both sides by \\(1\\):\n\\begin{equation} -\\sinh^{2} Lb + \\cosh^{2} Lb +\\sin ^{2} Lb + \\cos ^{2}Lb + 2\\cos Lb \\cosh Lb = 0 \\end{equation}\nFinally, we substitute!\n\\begin{align} \u0026amp; -\\sinh^{2} Lb + \\cosh^{2} Lb +\\sin ^{2} Lb + \\cos ^{2}Lb + 2\\cos Lb \\cosh Lb = 0 \\\\ \\Rightarrow\\ \u0026amp; 1 + 1 + 2\\cos Lb \\cosh Lb = 0 \\\\ \\Rightarrow\\ \u0026amp; 2 + 2\\cos Lb \\cosh Lb = 0 \\\\ \\Rightarrow\\ \u0026amp; 1 + \\cos Lb \\cosh Lb = 0 \\end{align}\nOf course, there is oscillating results here. We will numerically locate them. Why did we subject ourselves to tall of this algebra? No idea. As soon as we got rid of all the \\(d\\) we could have just stopped simplifying and just went to the numerical root solving. But here we are.\nWe will try to locate a root for \\(Lb\\) for every \\(\\pi\\) for two rounds around the circle (until \\(4 \\pi\\))\u0026mdash;there is a solution for every \\(\\pi\\), if you don\u0026rsquo;t believe me, plot it or change the bottom to try to find it for every \\(\\frac{\\pi}{2}\\), sage will crash:\nintervals = [jj*pi for jj in range(0, 5)] intervals [0, pi, 2*pi, 3*pi, 4*pi] We will now declare \\(x=Lb\\), and create a nonlinear expression in it:\nx = var(\u0026#34;x\u0026#34;) characteristic_eqn = 1 + cos(x)*cosh(x) == 0 characteristic_eqn cos(x)*cosh(x) + 1 == 0 Root finding time!\ncharacteristic_solutions = [characteristic_eqn.find_root(i,j) for (i,j) in zip(intervals,intervals[1:])] characteristic_solutions [1.8751040687120917, 4.6940911329739246, 7.854757438237603, 10.995540734875457] These are possible \\(Lb\\) candidates.\nThe takeaway here is that:\n(4.6940911329739246/1.8751040687120917)^2 6.266893025769125 (see below\u0026rsquo;s derivation for why frequency changes by a square of this root)\nthe second overtone will be six and a quarter times (\u0026ldquo;much\u0026rdquo;) higher than the fundamental\u0026mdash;so it will be able to dissipate much quicker.\nRecall now that:\n\\begin{equation} b = \\sqrt{f} \\qty(\\frac{\\mu}{EI})^{\\frac{1}{4}} \\end{equation}\nSimplifying some:\n\\begin{align} b \u0026amp;= f^{\\frac{1}{2}} \\qty(\\frac{\\mu}{EI})^{\\frac{1}{4}} \\\\ \u0026amp;= \\qty(f^{2})^{\\frac{1}{4}}\\qty(\\frac{\\mu}{EI})^{\\frac{1}{4}} \\\\ \u0026amp;= \\qty(\\frac{\\mu f^{2}}{EI})^{\\frac{1}{4}} \\end{align}\nTo solve for \\(f\\), give all other expressions and set one of the above characteristic solutions to \\(Lb\\). Then, solve for \\(f\\).\nSolving for frequency to get things to be correct, substituting the fact that \\(bh \\rho = \\mu\\):\n\\begin{align} \u0026amp;Lb = s \\\\ \\Rightarrow\\ \u0026amp; L f^{\\frac{1}{2}} \\qty(\\frac{\\mu}{EI})^{\\frac{1}{4}} = s \\\\ \\Rightarrow\\ \u0026amp; f^{\\frac{1}{2}} = \\frac{s}{L} \\qty(\\frac{EI}{\\mu})^{\\frac{1}{4}} \\\\ \\Rightarrow\\ \u0026amp; f = \\frac{s^{2}}{L^{2}} \\qty(\\frac{EI}{\\mu})^{\\frac{1}{2}} \\\\ \\Rightarrow\\ \u0026amp; f = \\frac{s^{2}}{L^{2}} \\qty(\\frac{E(\\frac{bh^{3}}{12})}{\\mu})^{\\frac{1}{2}} \\\\ \\Rightarrow\\ \u0026amp; f = \\frac{s^{2}}{L^{2}} \\qty(\\frac{E(\\frac{bh^{3}}{12})}{\\rho bh})^{\\frac{1}{2}} \\\\ \\Rightarrow\\ \u0026amp; f = \\frac{s^{2}}{L^{2}} \\qty(\\frac{Eh^{2}}{12\\rho })^{\\frac{1}{2}} \\end{align}\n_E = 70000000000 # pascals _p = 2666 # kg/m^3 _h = 0.0064 # m # target LENGTH = 0.095 # mode to index nth_mode = 0 _s = characteristic_solutions[nth_mode] (_s^2/LENGTH^2)*((_E*(_h^2))/(12*_p))^(1/2) 3688.17772197722 Also, to get the constant for the elastic modulus from our force measurements, see calculating shear\u0026rsquo;s modulus.\nLet us create a code snippet to do that consistently:\n# constants https://www.mit.edu/~6.777/matprops/aluminum.htm _E = 44062894805 # modulus (pascals) _I = 0.0000000001365333333 # second moment (m^4) https://amesweb.info/section/second-moment-of-area-calculator.aspx _u = 3.521355063 # length mass density (kg/m) # target LENGTH = 0.09573 # length of tine (meters) # mode to index nth_mode = 0 # variable declaration # solution eqn solution_eqn = characteristic_solutions[nth_mode] == (LENGTH*(sqrt(f)*(_u/(_E*_I))^(1/4))) # as frequency is squared, we take the SECOND (the non-negative) result, and round it solve(solution_eqn, f)[0].rhs().n() 501.482272272831 ","permalink":"https://www.jemoka.com/posts/kbhnumerical_cantileaver_simulations-1/","tags":null,"title":"Numerical Cantilever Simulations"},{"categories":null,"contents":"Here\u0026rsquo;s the characteristic equation again:\n\\begin{equation} \\pdv[2] x \\qty(EI \\pdv[2]{w}{x}) = -\\mu \\pdv{w}{t}+q(x) \\end{equation}\nAfter Fourier decomposition, we have that:\n\\begin{equation} EI \\dv[4]{\\hat{w}}{x} - \\mu f^{2}\\hat{w} = 0 \\end{equation}\nLet\u0026rsquo;s solve this!\nE,I,u,f = var(\u0026#34;E I u f\u0026#34;) x, L = var(\u0026#34;x L\u0026#34;) w = function(\u0026#39;w\u0026#39;)(x) _c0, _c1, _c2, _c3 = var(\u0026#34;_C0 _C1 _C2 _C3\u0026#34;) fourier_cantileaver = (E*I*diff(w, x, 2) - u*f^2*w == 0) fourier_cantileaver -f^2*u*w(x) + E*I*diff(w(x), x, x) == 0 And now, we can go about solving this result.\nsolution = desolve(fourier_cantileaver, w, ivar=x, algorithm=\u0026#34;fricas\u0026#34;).expand() w = solution \\begin{equation} _{C_{1}} e^{\\left(\\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{0}} e^{\\left(i \\, \\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{2}} e^{\\left(-i \\, \\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} + _{C_{3}} e^{\\left(-\\sqrt{f} x \\left(\\frac{u}{E I}\\right)^{\\frac{1}{4}}\\right)} \\end{equation}\nWe will simplify the repeated, constant top of this expression into a single variable \\(b\\):\nb = var(\u0026#34;b\u0026#34;) top = sqrt(f)*(u/(E*I))**(1/4) w = _c1*e^(b*x) + _c0*e^(i*b*x) + _c2*e^(-i*b*x) + _c3*e^(-b*x) w _C1*e^(b*x) + _C0*e^(I*b*x) + _C2*e^(-I*b*x) + _C3*e^(-b*x) \\begin{equation} _{C_{1}} e^{\\left(b x\\right)} + _{C_{0}} e^{\\left(i \\, b x\\right)} + _{C_{2}} e^{\\left(-i \\, b x\\right)} + _{C_{3}} e^{\\left(-b x\\right)} \\end{equation}\nWe have one equation, four unknowns. However, we are not yet done. We will make one more simplifying assumption\u0026mdash;try to get the \\(e^{x}\\) into sinusoidal form. We know this is supposed to oscillate, and it being in sinusoidal makes the process of solving for periodic solutions easier.\nRecall that:\n\\begin{equation} \\begin{cases} \\cosh x = \\frac{e^{x}+e^{-x}}{2} \\\\ \\cos x = \\frac{e^{ix}+e^{-ix}}{2}\\\\ \\sinh x = \\frac{e^{x}-e^{-x}}{2} \\\\ \\sin x = \\frac{e^{ix}-e^{-ix}}{2i}\\\\ \\end{cases} \\end{equation}\nWith a new set of scaling constants \\(d_0\\dots d_3\\), and some rearranging, we can rewrite the above expressions into just a linear combination of those elements. That is, the same expression for \\(w(x)\\) at a specific frequency can be written as:\n\\begin{equation} d_0\\cosh bx +d_1\\sinh bx +d_2\\cos bx +d_3\\sin bx = w \\end{equation}\nNo more imaginaries!!\nSo, let us redefine the expression:\nd0, d1, d2, d3 = var(\u0026#34;d0 d1 d2 d3\u0026#34;) w = d0*cosh(b*x)+d1*sinh(b*x)+d2*cos(b*x)+d3*sin(b*x) w d2*cos(b*x) + d0*cosh(b*x) + d3*sin(b*x) + d1*sinh(b*x) Now, we need to move onto solving when there will be valid solutions to this expression. However, we currently have four unknowns, and only one equation (at \\(x=0\\), \\(w=0\\), because the cantilever is fixed at base); so, to get a system in four elements, we will take some derivatives.\nThe way that we will go about this is by taking three derivatives and supplying the following initial conditions to get four equations:\nwp = diff(w,x,1) wpp = diff(w,x,2) wppp = diff(w,x,3) (wp, wpp, wppp) (b*d3*cos(b*x) + b*d1*cosh(b*x) - b*d2*sin(b*x) + b*d0*sinh(b*x), -b^2*d2*cos(b*x) + b^2*d0*cosh(b*x) - b^2*d3*sin(b*x) + b^2*d1*sinh(b*x), -b^3*d3*cos(b*x) + b^3*d1*cosh(b*x) + b^3*d2*sin(b*x) + b^3*d0*sinh(b*x)) And then, we have a system:\ncond_1 = w.subs(x=0) == 0 cond_2 = wp.subs(x=0) == 0 cond_3 = wpp.subs(x=L) == 0 cond_4 = wppp.subs(x=L) == 0 conds = (cond_1, cond_2, cond_3, cond_4) conds (d0 + d2 == 0, b*d1 + b*d3 == 0, -b^2*d2*cos(L*b) + b^2*d0*cosh(L*b) - b^2*d3*sin(L*b) + b^2*d1*sinh(L*b) == 0, -b^3*d3*cos(L*b) + b^3*d1*cosh(L*b) + b^3*d2*sin(L*b) + b^3*d0*sinh(L*b) == 0) solve(conds, d0, d1, d2, d3).full_simplify() Ok so, we notice that out of all of these boundary expressions the \\(b^{n}\\) term drop out. Therefore, we have the system:\n\\begin{equation} \\begin{cases} d_0 + d_2 = 0 \\\\ d_1 + d_3 = 0 \\\\ -d_2 \\cos Lb + d_0 \\cosh Lb - d_3 \\sin Lb + d_1 \\sinh Lb = 0 \\\\ -d_3 \\cos Lb + d_1 \\cosh Lb + d_2 \\sin Lb + d_0 \\sinh Lb = 0 \\\\ \\end{cases} \\end{equation}\nNow, taking the top expressions, we gather that:\n\\begin{equation} \\begin{cases} d_0 = -d_2 \\\\ d_1 = -d_3 \\end{cases} \\end{equation}\nPerforming these substitutions:\n\\begin{equation} \\begin{cases} d_0 (\\cos Lb + \\cosh Lb) + d_1 (\\sin Lb + \\sinh Lb) = 0 \\\\ d_1 (\\cos Lb + \\cosh Lb) + d_0 (\\sinh Lb- \\sin Lb ) = 0 \\\\ \\end{cases} \\end{equation}\nNow we are going to do some cursed algebra to get rid of all the rest of the \\(d\\). We want to do this because we don\u0026rsquo;t really care much what the constants are; instead, we care about when a solution exists (hopefully, then, telling us what the \\(f\\) baked inside \\(b\\) is). So:\n\\begin{align} \u0026amp;d_0 (\\cos Lb + \\cosh Lb) + d_1 (\\sin Lb + \\sinh Lb) = 0 \\\\ \\Rightarrow\\ \u0026amp; \\frac{-d_0}{d_1} = \\frac{(\\sin Lb + \\sinh Lb)}{(\\cos Lb + \\cosh Lb)} \\end{align}\nand\n\\begin{align} \u0026amp;d_1 (\\cos Lb + \\cosh Lb) + d_0 (\\sinh Lb- \\sin Lb ) = 0 \\\\ \\Rightarrow\\ \u0026amp; \\frac{-d_0}{d_1} = \\frac{(\\cos Lb + \\cosh Lb)}{(\\sinh Lb- \\sin Lb )} \\end{align}\ntherefore, we have:\n\\begin{equation} \\frac{(\\sin Lb + \\sinh Lb)}{(\\cos Lb + \\cosh Lb)} = \\frac{(\\cos Lb + \\cosh Lb)}{(\\sinh Lb- \\sin Lb )} \\end{equation}\nMultiplying each side by the other:\n\\begin{equation} (\\sin Lb + \\sinh Lb)(\\sinh Lb- \\sin Lb ) = (\\cos Lb + \\cosh Lb)^{2} \\end{equation}\nExpanding both sides now:\n\\begin{equation} (\\sinh^{2} Lb-\\sin^{2} Lb) = (\\cos^{2} Lb + 2\\cos Lb\\ \\cosh Lb + \\cosh ^{2}Lb) \\end{equation}\nMoving everything finally to one side:\n\\begin{equation} \\sinh^{2} Lb - \\cosh^{2} Lb -\\sin ^{2} Lb - \\cos ^{2}Lb - 2\\cos Lb \\cosh Lb = 0 \\end{equation}\nOk, this is where the satisfying \u0026ldquo;candy crush\u0026rdquo; begins when things cancel out. Recall pythagoras:\n\\begin{equation} \\begin{cases} \\cosh^{2}x - \\sinh^{2} x = 1 \\\\ \\sin^{2}x + \\cos^{2} x = 1 \\end{cases} \\end{equation}\nTo apply these effectively, multiply both sides by \\(1\\):\n\\begin{equation} -\\sinh^{2} Lb + \\cosh^{2} Lb +\\sin ^{2} Lb + \\cos ^{2}Lb + 2\\cos Lb \\cosh Lb = 0 \\end{equation}\nFinally, we substitute!\n\\begin{align} \u0026amp; -\\sinh^{2} Lb + \\cosh^{2} Lb +\\sin ^{2} Lb + \\cos ^{2}Lb + 2\\cos Lb \\cosh Lb = 0 \\\\ \\Rightarrow\\ \u0026amp; 1 + 1 + 2\\cos Lb \\cosh Lb = 0 \\\\ \\Rightarrow\\ \u0026amp; 2 + 2\\cos Lb \\cosh Lb = 0 \\\\ \\Rightarrow\\ \u0026amp; 1 + \\cos Lb \\cosh Lb = 0 \\end{align}\nOf course, there is oscillating results here. We will numerically locate them. Why did we subject ourselves to tall of this algebra? No idea. As soon as we got rid of all the \\(d\\) we could have just stopped simplifying and just went to the numerical root solving. But here we are.\nWe will try to locate a root for \\(Lb\\) for every \\(\\pi\\) for two rounds around the circle (until \\(4 \\pi\\))\u0026mdash;there is a solution for every \\(\\pi\\), if you don\u0026rsquo;t believe me, plot it or change the bottom to try to find it for every \\(\\frac{\\pi}{2}\\), sage will crash:\nintervals = [jj*pi for jj in range(0, 5)] intervals [0, pi, 2*pi, 3*pi, 4*pi] We will now declare \\(x=Lb\\), and create a nonlinear expression in it:\nx = var(\u0026#34;x\u0026#34;) characteristic_eqn = 1 + cos(x)*cosh(x) == 0 characteristic_eqn cos(x)*cosh(x) + 1 == 0 Root finding time!\ncharacteristic_solutions = [characteristic_eqn.find_root(i,j) for (i,j) in zip(intervals,intervals[1:])] characteristic_solutions [1.8751040687120917, 4.6940911329739246, 7.854757438237603, 10.995540734875457] These are possible \\(Lb\\) candidates.\nThe takeaway here is that:\n(4.6940911329739246/1.8751040687120917)^2 6.266893025769125 (see below\u0026rsquo;s derivation for why frequency changes by a square of this root)\nthe second overtone will be six and a quarter times (\u0026ldquo;much\u0026rdquo;) higher than the fundamental\u0026mdash;so it will be able to dissipate much quicker.\n\\begin{equation} \\sqrt{f}\\qty(\\frac{u}{EI})^{\\frac{1}{4}} L = s \\end{equation}\n\\begin{equation} \\sqrt{f} = \\frac{s}{L} \\qty(\\frac{EI}{u})^{\\frac{1}{4}} \\end{equation}\n_E = 70000000000 # pascals _p = 2766 # kg/m^3 _h = 0.006 # m _I = 0.0000000001302083333 # m^4 _u = 0.10388 # kg/m, approximate # target LENGTH = 0.095 # mode to index nth_mode = 0 _s = characteristic_solutions[nth_mode] ((3.5160)/(LENGTH^2))*((_E*_I)/_u)^(1/2) 3649.25402142506 _E = 70000000000 # pascals _p = 2766 # kg/m^3 _h = 0.006 # m # target LENGTH = 0.095 # mode to index nth_mode = 0 _s = characteristic_solutions[nth_mode] (_s^2/LENGTH^2)*((_E*(_h^2))/(12*_p))^(1/2) 3394.58823149786 ","permalink":"https://www.jemoka.com/posts/kbhnumerical_cantileaver_simulations/","tags":null,"title":"Numerical Cantilever Simulations"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_capm_problem_set/","tags":null,"title":"NUS-ECON320 CAPM Problem Set"},{"categories":null,"contents":"Let\u0026rsquo;s import some tools.\nimport pandas as pd from scipy.optimize import minimize import numpy as np from datetime import datetime from tqdm import tqdm import torch tqdm.pandas() And load our data:\ndf = pd.read_csv(\u0026#34;./currency_signal.csv\u0026#34;, index_col=0, header=None, parse_dates=[0]) df Let\u0026rsquo;s rename the headers\ndf.index.rename(\u0026#34;date\u0026#34;, True) df.columns = [\u0026#34;value\u0026#34;] Awesome. For the rest of the calculations, we will hide the 2020 data from the model:\ndata = df[df.index \u0026lt; datetime(2020, 1,1)] data value date 2006-03-01 0.000050 2006-03-02 0.001778 2006-03-03 0.000116 2006-03-06 -0.001038 2006-03-07 -0.001197 ... ... 2019-12-25 -0.010659 2019-12-26 -0.000869 2019-12-27 0.000075 2019-12-30 0.000033 2019-12-31 0.000944 [3610 rows x 1 columns] we will add a column of randomness to this, to serve as the seed of our epsilon:\ndata[\u0026#34;epsilon\u0026#34;] = np.random.normal(0,1, data.shape[0]) data value epsilon date 2006-03-01 0.000050 -0.255699 2006-03-02 0.001778 0.157341 2006-03-03 0.000116 0.574378 2006-03-06 -0.001038 -1.319365 2006-03-07 -0.001197 -0.717148 ... ... ... 2019-12-25 -0.010659 0.153559 2019-12-26 -0.000869 -1.066562 2019-12-27 0.000075 0.025730 2019-12-30 0.000033 0.760713 2019-12-31 0.000944 -0.427494 [3610 rows x 2 columns] Awesome, we will now seed three parameter variables. Recall that the GARCH model we are dealing with is:\n\\begin{equation} \\begin{cases} \\eta_t = \\sigma_{t}\\epsilon_{t} \\\\ {\\sigma_{t}}^{2} = \\alpha {\\eta_{t}}^{2} + \\beta {\\sigma_{t-1}}^{2} + \\gamma \\end{cases} \\end{equation}\nSolving for explicit solutions of \\(n_t\\) and \\(\\sigma_t\\), in terms of the others using computer algebra, we have:\n\\begin{equation} \\sigma_{t}^{2} = -\\frac{\\beta \\mathit{\\sigma_{t-1}}^{2} + y}{\\alpha \\epsilon^{2} - 1} \\end{equation}\nThe value of \\(\\eta_t\\) is naturally \\(\\sigma_t \\epsilon_t\\) (i.e. \\(\\eta^{2} = (\\sigma_{t})^{2}(\\epsilon_{t})^{2}\\)).\nSo, to make the squared results, we want to square both value and epsilon:\ndata[\u0026#34;value2\u0026#34;] = data.value**2 data[\u0026#34;epsilon2\u0026#34;] = data.epsilon**2 data value epsilon value2 epsilon2 date 2006-03-01 0.000050 -0.255699 2.450633e-09 0.065382 2006-03-02 0.001778 0.157341 3.162006e-06 0.024756 2006-03-03 0.000116 0.574378 1.334210e-08 0.329910 2006-03-06 -0.001038 -1.319365 1.076978e-06 1.740723 2006-03-07 -0.001197 -0.717148 1.432477e-06 0.514301 ... ... ... ... ... 2019-12-25 -0.010659 0.153559 1.136119e-04 0.023580 2019-12-26 -0.000869 -1.066562 7.549935e-07 1.137555 2019-12-27 0.000075 0.025730 5.670657e-09 0.000662 2019-12-30 0.000033 0.760713 1.083948e-09 0.578684 2019-12-31 0.000944 -0.427494 8.913486e-07 0.182751 [3610 rows x 4 columns] Now, we can now compute a column of these, based on the data we have. To be able to optimize this symbolically, we will leverage PyTorch.\nLet\u0026rsquo;s seed these constants all at \\(1\\), to be optimized later:\na = torch.tensor(1e-10, requires_grad=True) b = torch.tensor(1e-10, requires_grad=True) y = torch.tensor(1e-10, requires_grad=True) (a,b,y) (tensor(1.0000e-10, requires_grad=True), tensor(1.0000e-10, requires_grad=True), tensor(1.0000e-10, requires_grad=True)) We use the complex data type here to make the subtract operation work. We will eventually project it down to real space without much trouble.\nAwesome, let us compute this series of \\(\\sigma\\), and optimize for the loss.\nHere is a gradient descent optimizer:\n# we will use the gradient descent scheme optimizer = torch.optim.SGD([a,b,y], lr=3e-3) optimizer SGD ( Parameter Group 0 dampening: 0 differentiable: False foreach: None lr: 0.003 maximize: False momentum: 0 nesterov: False weight_decay: 0 ) And now, for 1000 steps, we will minimize the difference between the computed \\(n\\) and actual value against \\(\\alpha, \\beta, \\gamma\\). We will run the scheme for 50 steps.\nfor _ in tqdm(range(500)): prev_sigma_2 = 0 # # for each row for i in range(len(data)): # get previous value, or seed at 0 # if it doesn\u0026#39;t exist sigma_2 = (-(b*prev_sigma_2+y)/(a*data[\u0026#34;epsilon2\u0026#34;].iloc[i]-1)) n_2 = sigma_2*data[\u0026#34;epsilon2\u0026#34;].iloc[i] ((n_2-data[\u0026#34;value2\u0026#34;].iloc[i])**2).backward() prev_sigma_2 = sigma_2.detach() optimizer.step() optimizer.zero_grad() Awesome, now, let\u0026rsquo;s see the fitted results:\n(a,b,y) (tensor(611584.9375, requires_grad=True), tensor(37750.6133, requires_grad=True), tensor(-26.5902, requires_grad=True)) We will now work to validate these results in the entire dataset.\ndata_val = df.copy() data_val value date 2006-03-01 0.000050 2006-03-02 0.001778 2006-03-03 0.000116 2006-03-06 -0.001038 2006-03-07 -0.001197 ... ... 2020-05-18 0.000264 2020-05-19 0.001434 2020-05-20 0.000995 2020-05-21 0.000120 2020-05-22 0.000424 [3713 rows x 1 columns] Now, we will use these values to compute the variance and the predicted variance on the data.\nRecall that:\n\\begin{equation} \\sigma_{t}^{2} = -\\frac{\\beta \\mathit{\\sigma_{t-1}}^{2} + y}{\\alpha \\epsilon^{2} - 1} \\end{equation}\nSo:\ndata_val[\u0026#34;epsilon\u0026#34;] = np.random.normal(0,1, data_val.shape[0]) data_val value epsilon date 2006-03-01 0.000050 0.018859 2006-03-02 0.001778 1.943619 2006-03-03 0.000116 0.397312 2006-03-06 -0.001038 1.025379 2006-03-07 -0.001197 0.081920 ... ... ... 2020-05-18 0.000264 -0.976598 2020-05-19 0.001434 -0.357048 2020-05-20 0.000995 -1.230387 2020-05-21 0.000120 0.972614 2020-05-22 0.000424 -0.199802 [3713 rows x 2 columns] Now, we will generate a column of sigma squared\nfor i, date in enumerate(data_val.index): prev_sigma_2 = 0 sigma_2 = (-(b*prev_sigma_2+y)/(a*(data_val[\u0026#34;epsilon\u0026#34;]**2).iloc[i]-1)).detach().numpy() # get previous value, or seed at 0 # if it doesn\u0026#39;t exist data_val.loc[date, \u0026#34;sigma\u0026#34;] = sigma_2**0.5 prev_sigma_2 = sigma_2 data_val value epsilon sigma date 2006-03-01 0.000050 0.018859 0.350442 2006-03-02 0.001778 1.943619 0.003393 2006-03-03 0.000116 0.397312 0.016596 2006-03-06 -0.001038 1.025379 0.006431 2006-03-07 -0.001197 0.081920 0.080500 ... ... ... ... 2020-05-18 0.000264 -0.976598 0.006752 2020-05-19 0.001434 -0.357048 0.018468 2020-05-20 0.000995 -1.230387 0.005359 2020-05-21 0.000120 0.972614 0.006779 2020-05-22 0.000424 -0.199802 0.033002 [3713 rows x 3 columns] And finally, let us generate the eta column:\nRecall that \\(\\eta_t = \\sigma_{t}\\epsilon_{t}\\), so:\ndata_val[\u0026#34;eta\u0026#34;] = data_val.sigma * data_val.epsilon data_val value epsilon sigma eta date 2006-03-01 0.000050 0.018859 0.350442 0.006609 2006-03-02 0.001778 1.943619 0.003393 0.006594 2006-03-03 0.000116 0.397312 0.016596 0.006594 2006-03-06 -0.001038 1.025379 0.006431 0.006594 2006-03-07 -0.001197 0.081920 0.080500 0.006595 ... ... ... ... ... 2020-05-18 0.000264 -0.976598 0.006752 -0.006594 2020-05-19 0.001434 -0.357048 0.018468 -0.006594 2020-05-20 0.000995 -1.230387 0.005359 -0.006594 2020-05-21 0.000120 0.972614 0.006779 0.006594 2020-05-22 0.000424 -0.199802 0.033002 -0.006594 [3713 rows x 4 columns] And finally, let us compute the log loss:\ndata_val[\u0026#34;loss\u0026#34;] = (data_val.eta-data_val.value).abs() data_val value epsilon sigma eta loss date 2006-03-01 0.000050 0.018859 0.350442 0.006609 0.006559 2006-03-02 0.001778 1.943619 0.003393 0.006594 0.004816 2006-03-03 0.000116 0.397312 0.016596 0.006594 0.006478 2006-03-06 -0.001038 1.025379 0.006431 0.006594 0.007632 2006-03-07 -0.001197 0.081920 0.080500 0.006595 0.007791 ... ... ... ... ... ... 2020-05-18 0.000264 -0.976598 0.006752 -0.006594 0.006857 2020-05-19 0.001434 -0.357048 0.018468 -0.006594 0.008028 2020-05-20 0.000995 -1.230387 0.005359 -0.006594 0.007589 2020-05-21 0.000120 0.972614 0.006779 0.006594 0.006474 2020-05-22 0.000424 -0.199802 0.033002 -0.006594 0.007018 [3713 rows x 5 columns] Saving the data:\ndata_val.to_csv(\u0026#34;currency_arbitrage.csv\u0026#34;) ","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_currency_arbitrage/","tags":null,"title":"NUS-ECON320 Currency Arbitrage"},{"categories":null,"contents":"We want to construct a combined agent\n\\begin{equation} (k_1+k_2)x^{*}(k_1+k_2, \\gamma^{*}) = x^{*}(k_1,\\gamma_{1})k_1+x^{*}(k_2, \\gamma_{2})k_2 \\end{equation}\nwhich combines the relative risk of \\(\\gamma_{1}, \\gamma_{2}\\) into some new \\(\\gamma^{*}\\), which produces the same combined consumption of both agents \\(k_1+k_2\\).\nLet us create some CAS tools to solve the inter-temporal choice problem generically for 10 steps in the past.\nWe do this by solving backwards. We will create a variable \\(k\\) to measure asset, and \\(k_{t}\\) the remaining asset at time \\(t\\).\nLet us first declare the function for power utility. \\(k\\) is our asset holding, \\(\\gamma\\) our relative margin of risk, and \\(U\\) the power utility.\n# risk aversion y = var(\u0026#34;y\u0026#34;, latex_name=\u0026#34;\\gamma\u0026#34;, domain=\u0026#39;real\u0026#39;) # discount factor d = var(\u0026#34;d\u0026#34;, latex_name=\u0026#34;\\delta\u0026#34;, domain=\u0026#39;real\u0026#39;) # final value at time t=f k_f = var(\u0026#34;k_f\u0026#34;, latex_name=\u0026#34;k_f\u0026#34;, domain=\u0026#39;real\u0026#39;) # the instrument\u0026#39;s percentage return over a period: i.e. (1+mu)*I_t = k_{t+1} m = var(\u0026#34;m\u0026#34;, latex_name=\u0026#34;\\mu\u0026#34;, domain=\u0026#39;real\u0026#39;) # boundary conditions assume(y\u0026gt;0) assume(y\u0026lt;1) assume(d\u0026gt;0) assume(d\u0026lt;1) # power utility u(c) = ((c^(1-y)-1)/(1-y)) u c |--\u0026gt; -(c^(-y + 1) - 1)/(y - 1) At the final time stamp, we desire to consume all of our assets. Therefore, we will seed our investment amount at \\(I=0\\). We will optimize for eventual global utility, therefore, we will talley our utility; starting this talley at \\(0\\).\n# at the final time, leave nothing for investment I=0; u_total = 0 From every step from here, we will discount this utility by \\(d\\), then solve for the previous step\u0026rsquo;s target consumption that would maximize utility. That is, at every step, we desire:\n\\begin{equation} k_{t-1} = I_{t} + c_{t} \\implies c_{t} = k_{t-1}-I_{t} \\end{equation}\n\u0026ldquo;we want to consume all we have that needed\u0026rsquo;t to be invested\u0026rdquo;\nand\n\\(\\max u(c_{t})\\)\nRecall also that \\((1+\\mu)I_{t} = k_{t+1}\\) (as \\(\\mu\\) is the mean log-return, 1+that times \\(I\\), how much was invested at time \\(t\\), is the expected capital one time period from then.) Therefore, to make sure that \\(k_{f}\\) gets back in the final period, we solve for our seed value for \\(I\\), how much to invest would be:\n\\begin{equation} I_{t-1} = \\frac{k_t}{(1+m)} \\end{equation}\nEnough talk, let\u0026rsquo;s get to it:\n# create an dictionary to keep track of all the capital variables k = {} # we will iterate time stamps 1-10 T = 10 # a variable for captial at that time for i in range(T): k_t = var(f\u0026#34;k_{T-i}\u0026#34;, latex_name=f\u0026#34;k_{T-i}\u0026#34;) # t-i becasue we are solving backwards; i0 = T10 # what can be consumed at every time stamp # is the k of the previous timestamp, minus # what needs to be left over # we multiply here by d because we want to # discount future utility u_total = d*u_total + u(k_t-I) # add the current variable to dictionary k[T-i] = k_t # recall again i0=T10 because backwards # solve for the next investment amount I = k_t/(1+m) u_total -(((((((d*(d*(k_10^(-y + 1) - 1)/(y - 1) + ((k_9 - k_10/(m + 1))^(-y + 1) - 1)/(y - 1)) + ((k_8 - k_9/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_7 - k_8/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_6 - k_7/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_5 - k_6/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_4 - k_5/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_3 - k_4/(m + 1))^(-y + 1) - 1)/(y - 1))*d + ((k_2 - k_3/(m + 1))^(-y + 1) - 1)/(y - 1))*d - ((k_1 - k_2/(m + 1))^(-y + 1) - 1)/(y - 1) We can now use the scipy numerical optimizer to minimize this target. Recall that we can recover the actual value of consumption at each step as \\(c=k-\\frac{k}{m+1}\\).\nWe will set some initial conditions:\n_m = 0.01 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.8 # generally risk averse _d = 0.9 # the future matters slightly less Recall that the scipy optimizer MINIMIZES, so we will make the loss the negative of utility. Before we finally start, we need to make the actual, numerical loss function that performs the substitution:\n# we reverse the k_* variables because it is stored in the dictionary # in reverse, because we knew the reverse condition first optim_variables = list(k.values()) optim_variables.reverse() # this function is also the callback, so it returning # True terminates execution def u_total_loss(x): # the optimizer\u0026#39;s current step # we want to take [1:], because we need to keep k1 the same at _k the # initial value substitution_dict = {key: val for key, val in zip(optim_variables[1:], x)} # initial conditions substitution_dict[m] = _m substitution_dict[y] = _y substitution_dict[d] = _d substitution_dict[d] = _d # we want to keep the initial value k1 the same substitution_dict[k[1]] = _k try: # get value content = (-1*u_total).subs(substitution_dict) # recall we multiply by -1 because we are MINIMIZING, so the loss is # the inverse of the maximization utility target return float(content.n()), False except: return 0, True Finally, we are ready to start. We will now create the other initial conditions k1\u0026hellip;k10 and : we will set the initial value to all be 1000 (i.e. do nothing) and have the optimizer work it out from there:\nfrom scipy.optimize import minimize target = minimize(lambda x:u_total_loss(x)[0], [_k for _ in range(T-1)], callback=lambda x:u_total_loss(x)[1]) target fun: -50.71592850322347 hess_inv: array([[ 9518.97596212, 7617.14636381, 5964.42171873, 4433.87331935, 4253.91810669, 3528.72923763, 2329.61846616, 1769.85078017, 1126.51562458], [ 7617.14636381, 14333.33933517, 11251.71278723, 8073.31207641, 7444.53071922, 6481.03236385, 4347.35353474, 2644.39855553, 1359.86586059], [ 5964.42171873, 11251.71278723, 15011.27497355, 10093.46973099, 9229.06386286, 8371.07459024, 5510.14654004, 3480.74298654, 1639.19265606], [ 4433.87331935, 8073.31207641, 10093.46973099, 12434.28059884, 11689.33288295, 10711.57399875, 7440.7461982 , 4810.57094062, 2255.16306648], [ 4253.91810669, 7444.53071922, 9229.06386286, 11689.33288295, 14840.59602968, 12519.06872583, 8708.9160148 , 5688.83339388, 2598.27394651], [ 3528.72923763, 6481.03236385, 8371.07459024, 10711.57399875, 12519.06872583, 14999.44881857, 10630.30739223, 6512.62254338, 2293.45506703], [ 2329.61846616, 4347.35353474, 5510.14654004, 7440.7461982 , 8708.9160148 , 10630.30739223, 12147.11811342, 7149.37937935, 2657.8129831 ], [ 1769.85078017, 2644.39855553, 3480.74298654, 4810.57094062, 5688.83339388, 6512.62254338, 7149.37937935, 7260.90962516, 2422.66762041], [ 1126.51562458, 1359.86586059, 1639.19265606, 2255.16306648, 2598.27394651, 2293.45506703, 2657.8129831 , 2422.66762041, 2911.30717272]]) jac: array([ 0.00000000e+00, -3.81469727e-06, 2.38418579e-06, 0.00000000e+00, 8.58306885e-06, -5.24520874e-06, 2.86102295e-06, -1.43051147e-06, -5.24520874e-06]) message: \u0026#39;Optimization terminated successfully.\u0026#39; nfev: 1360 nit: 130 njev: 136 status: 0 success: True x: array([841.22097906, 699.82556541, 573.89912346, 461.63474591, 361.51493714, 272.10309839, 192.29084196, 120.94057011, 57.12129925]) Awesome! We now can recover \\(c\\) at each point by a nice helpful function:\nc(k0, k1) = k0 - k1/(_m+1) \u0026ldquo;Consumption is how much we have, minus how much we would be investing\u0026rdquo;\nSo, let us translate our list to the actual values consumed:\ncapital_over_time = [_k]+target.x.tolist() # we need to add the initial condition _k back to the # inventory list consumption_over_time = [c(i,j) for i,j in zip(capital_over_time, capital_over_time[1:])] consumption_over_time [167.107941529699, 148.324379643989, 131.608611479784, 116.835018601197, 103.699164584812, 92.1059288329209, 81.7161261514775, 72.5477032414004, 64.3848282779261] Examples of Output _m = 0.01 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.8 # generally risk averse _d = 0.9 # the future matters slightly less [167.107941529699, 148.324379643989, 131.608611479784, 116.835018601197, 103.699164584812, 92.1059288329209, 81.7161261514775, 72.5477032414004, 64.3848282779261] _m = 0.1 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.8 # generally risk averse _d = 0.9 # the future matters slightly less [154.860597149863, 152.989432556196, 151.010433069881, 149.201249715528, 147.329750167852, 145.539019666462, 143.739371599600, 141.984228587213, 140.243839963791] _m = 0.01 # 1% period-to-period increase _k = 1000 # $1000 capital _y = 0.2 # generally risky _d = 0.9 # the future matters slightly less [388.525041338376, 241.124420093987, 149.632568775223, 92.8644259086613, 57.6330459746870, 35.7667230511026, 22.1970017374152, 13.7754327365677, 8.54930907023498] _m = -0.01 # this is a loosing stock _k = 1000 # $1000 capital _y = 0.9 # very safe _d = 0.9 # the future matters fun: 0 hess_inv: array([[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1]]) jac: array([0., 0., 0., 0., 0., 0., 0., 0., 0.]) message: \u0026#39;Optimization terminated successfully.\u0026#39; nfev: 10 nit: 0 njev: 1 status: 0 success: True x: array([1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.]) Evidently: do nothing if we have a loosing cause.\n_m = 1.00 # this is SUPER winning stock _k = 1000 # $1000 capital _y = 0.9 # very safe _d = 0.9 # the future matters [125.667556437602, 241.474827418105, 460.068836905327, 868.972817783791, 4540.45893314523, 4219.93058738029, 3988.05775624984, 3996.89431939885, 3615.74982832315] We made so much money that we are spending a lot of it and still spending it.\n","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_risk_appetite/","tags":null,"title":"NUS-ECON320 Inter-Temporal Choice"},{"categories":null,"contents":"Let\u0026rsquo;s begin. We want to create test for the linearity of a few assets, for whether or not they follow the CAPM.\nNote that we will be using the Sharpe-Linter version of CAPM:\n\\begin{equation} E[R_{i}-R_{f}] = \\beta_{im} E[(R_{m}-R_{f})] \\end{equation}\n\\begin{equation} \\beta_{im} := \\frac{Cov[(R_{i}-R_{f}),(R_{m}-R_{f})]}{Var[R_{m}-R_{f}]} \\end{equation}\nRecall that we declare \\(R_{f}\\) (the risk-free rate) to be non-stochastic.\nLet us begin. We will create a generic function to analyze some given stock.\nWe will first import our utilities\nimport pandas as pd import numpy as np Let\u0026rsquo;s load the data from our market (NYSE) as well as our 10 year t-bill data.\nt_bill = pd.read_csv(\u0026#34;./linearity_test_data/10yrTBill.csv\u0026#34;) nyse = pd.read_csv(\u0026#34;./linearity_test_data/NYSEComposite.csv\u0026#34;) nyse.head() Date Close 0 11/7/2013 16:00:00 9924.37 1 11/8/2013 16:00:00 10032.14 2 11/11/2013 16:00:00 10042.95 3 11/12/2013 16:00:00 10009.84 4 11/13/2013 16:00:00 10079.89 Excellent. Let\u0026rsquo;s load in the data for that stock.\ndef load_stock(stock): return pd.read_csv(f\u0026#34;./linearity_test_data/{stock}.csv\u0026#34;) load_stock(\u0026#34;LMT\u0026#34;).head() Date Close 0 11/7/2013 16:00:00 136.20 1 11/8/2013 16:00:00 138.11 2 11/11/2013 16:00:00 137.15 3 11/12/2013 16:00:00 137.23 4 11/13/2013 16:00:00 137.26 And now, let\u0026rsquo;s load all three stocks, then concatenate them all into a big-ol DataFrame.\n# load data df = { \u0026#34;Date\u0026#34;: nyse.Date, \u0026#34;NYSE\u0026#34;: nyse.Close, \u0026#34;TBill\u0026#34;: t_bill.Close, \u0026#34;LMT\u0026#34;: load_stock(\u0026#34;LMT\u0026#34;).Close, \u0026#34;TWTR\u0026#34;: load_stock(\u0026#34;TWTR\u0026#34;).Close, \u0026#34;MCD\u0026#34;: load_stock(\u0026#34;MCD\u0026#34;).Close } # convert to dataframe df = pd.DataFrame(df) # drop empty df.dropna(inplace=True) df Date NYSE TBill LMT TWTR MCD 0 11/7/2013 16:00:00 9924.37 26.13 136.20 44.90 97.20 1 11/8/2013 16:00:00 10032.14 27.46 138.11 41.65 97.01 2 11/11/2013 16:00:00 10042.95 27.51 137.15 42.90 97.09 3 11/12/2013 16:00:00 10009.84 27.68 137.23 41.90 97.66 4 11/13/2013 16:00:00 10079.89 27.25 137.26 42.60 98.11 ... ... ... ... ... ... ... 2154 10/24/2022 16:00:00 14226.11 30.44 440.11 39.60 252.21 2155 10/25/2022 16:00:00 14440.69 31.56 439.30 39.30 249.28 2156 10/26/2022 16:00:00 14531.69 33.66 441.11 39.91 250.38 2157 10/27/2022 16:00:00 14569.90 34.83 442.69 40.16 248.36 2158 10/28/2022 16:00:00 14795.63 33.95 443.41 39.56 248.07 [2159 rows x 6 columns] Excellent. Now, let\u0026rsquo;s convert all of these values into daily log-returns (we don\u0026rsquo;t really care about the actual pricing.)\nlog_returns = df[[\u0026#34;NYSE\u0026#34;, \u0026#34;TBill\u0026#34;, \u0026#34;LMT\u0026#34;, \u0026#34;TWTR\u0026#34;, \u0026#34;MCD\u0026#34;]].apply(np.log, inplace=True) df.loc[:, [\u0026#34;NYSE\u0026#34;, \u0026#34;TBill\u0026#34;, \u0026#34;LMT\u0026#34;, \u0026#34;TWTR\u0026#34;, \u0026#34;MCD\u0026#34;]] = log_returns df Date NYSE TBill LMT TWTR MCD 0 11/7/2013 16:00:00 9.202749 3.263084 4.914124 3.804438 4.576771 1 11/8/2013 16:00:00 9.213549 3.312730 4.928050 3.729301 4.574814 2 11/11/2013 16:00:00 9.214626 3.314550 4.921075 3.758872 4.575638 3 11/12/2013 16:00:00 9.211324 3.320710 4.921658 3.735286 4.581492 4 11/13/2013 16:00:00 9.218298 3.305054 4.921877 3.751854 4.586089 ... ... ... ... ... ... ... 2154 10/24/2022 16:00:00 9.562834 3.415758 6.087025 3.678829 5.530262 2155 10/25/2022 16:00:00 9.577805 3.451890 6.085183 3.671225 5.518577 2156 10/26/2022 16:00:00 9.584087 3.516310 6.089294 3.686627 5.522980 2157 10/27/2022 16:00:00 9.586713 3.550479 6.092870 3.692871 5.514879 2158 10/28/2022 16:00:00 9.602087 3.524889 6.094495 3.677819 5.513711 [2159 rows x 6 columns] We will now calculate the log daily returns. But before\u0026mdash;the dates are no longer relavent here so we drop them.\ndf Date NYSE TBill LMT TWTR MCD 0 11/7/2013 16:00:00 9.202749 3.263084 4.914124 3.804438 4.576771 1 11/8/2013 16:00:00 9.213549 3.312730 4.928050 3.729301 4.574814 2 11/11/2013 16:00:00 9.214626 3.314550 4.921075 3.758872 4.575638 3 11/12/2013 16:00:00 9.211324 3.320710 4.921658 3.735286 4.581492 4 11/13/2013 16:00:00 9.218298 3.305054 4.921877 3.751854 4.586089 ... ... ... ... ... ... ... 2154 10/24/2022 16:00:00 9.562834 3.415758 6.087025 3.678829 5.530262 2155 10/25/2022 16:00:00 9.577805 3.451890 6.085183 3.671225 5.518577 2156 10/26/2022 16:00:00 9.584087 3.516310 6.089294 3.686627 5.522980 2157 10/27/2022 16:00:00 9.586713 3.550479 6.092870 3.692871 5.514879 2158 10/28/2022 16:00:00 9.602087 3.524889 6.094495 3.677819 5.513711 [2159 rows x 6 columns] And now, the log returns! We will shift this data by one column and subtract.\nreturns = df.drop(columns=[\u0026#34;Date\u0026#34;]) - df.drop(columns=[\u0026#34;Date\u0026#34;]).shift(1) returns.dropna(inplace=True) returns NYSE TBill LMT TWTR MCD 1 0.010801 0.049646 0.013926 -0.075136 -0.001957 2 0.001077 0.001819 -0.006975 0.029570 0.000824 3 -0.003302 0.006161 0.000583 -0.023586 0.005854 4 0.006974 -0.015657 0.000219 0.016568 0.004597 5 0.005010 -0.008476 0.007476 0.047896 -0.005622 ... ... ... ... ... ... 2154 0.005785 0.004940 -0.023467 -0.014291 0.001349 2155 0.014971 0.036133 -0.001842 -0.007605 -0.011685 2156 0.006282 0.064420 0.004112 0.015402 0.004403 2157 0.002626 0.034169 0.003575 0.006245 -0.008100 2158 0.015374 -0.025590 0.001625 -0.015053 -0.001168 [2158 rows x 5 columns] We are now ready to run the correlation study.\nLet\u0026rsquo;s now subtract everything by the risk-free rate (dropping the rfr itself):\nrisk_free_excess = returns.drop(columns=\u0026#34;TBill\u0026#34;).apply(lambda x: x-returns.TBill) risk_free_excess NYSE LMT TWTR MCD 1 -0.038846 -0.035720 -0.124783 -0.051603 2 -0.000742 -0.008794 0.027751 -0.000995 3 -0.009463 -0.005577 -0.029747 -0.000307 4 0.022630 0.015875 0.032225 0.020254 5 0.013486 0.015952 0.056372 0.002854 ... ... ... ... ... 2154 0.000845 -0.028406 -0.019231 -0.003591 2155 -0.021162 -0.037975 -0.043738 -0.047818 2156 -0.058138 -0.060308 -0.049017 -0.060017 2157 -0.031543 -0.030593 -0.027924 -0.042269 2158 0.040964 0.027215 0.010537 0.024422 [2158 rows x 4 columns] Actual Regression It is now time to perform the actual linear regression!\nimport statsmodels.api as sm Let\u0026rsquo;s work with Lockheed Martin first, fitting an ordinary least squares. Remember that the OLS functions reads the endogenous variable first (for us, the return of the asset.)\n# add a column of ones to our input market excess returns nyse_with_bias = sm.add_constant(risk_free_excess.NYSE) # perform linreg lmt_model = sm.OLS(risk_free_excess.LMT, nyse_with_bias).fit() lmt_model.summary() OLS Regression Results ============================================================================== Dep. Variable: LMT R-squared: 0.859 Model: OLS Adj. R-squared: 0.859 Method: Least Squares F-statistic: 1.312e+04 Date: Mon, 31 Oct 2022 Prob (F-statistic): 0.00 Time: 10:39:24 Log-Likelihood: 6318.9 No. Observations: 2158 AIC: -1.263e+04 Df Residuals: 2156 BIC: -1.262e+04 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0004 0.000 1.311 0.190 -0.000 0.001 NYSE 0.9449 0.008 114.552 0.000 0.929 0.961 ============================================================================== Omnibus: 423.969 Durbin-Watson: 1.965 Prob(Omnibus): 0.000 Jarque-Bera (JB): 11575.074 Skew: -0.160 Prob(JB): 0.00 Kurtosis: 14.341 Cond. No. 29.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Based on the constants row, we can see that\u0026mdash;within \\(95\\%\\) confidence\u0026mdash;the intercept is generally \\(0\\) and CAPM applies. However, we do see a slight positive compared to the market. Furthermore, we can see that the regression has a beta value of \\(0.9449\\) \u0026mdash; according the CAPM model, it being slightly undervarying that the market.\nWe can continue with the other stocks.\n# perform linreg mcd_model = sm.OLS(risk_free_excess.MCD, nyse_with_bias).fit() mcd_model.summary() OLS Regression Results ============================================================================== Dep. Variable: MCD R-squared: 0.887 Model: OLS Adj. R-squared: 0.887 Method: Least Squares F-statistic: 1.697e+04 Date: Mon, 31 Oct 2022 Prob (F-statistic): 0.00 Time: 10:39:24 Log-Likelihood: 6551.1 No. Observations: 2158 AIC: -1.310e+04 Df Residuals: 2156 BIC: -1.309e+04 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0003 0.000 1.004 0.315 -0.000 0.001 NYSE 0.9651 0.007 130.287 0.000 0.951 0.980 ============================================================================== Omnibus: 323.911 Durbin-Watson: 1.988 Prob(Omnibus): 0.000 Jarque-Bera (JB): 3032.550 Skew: 0.395 Prob(JB): 0.00 Kurtosis: 8.753 Cond. No. 29.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Same thing as before, we are within \\(95\\%\\) confidence having a intercept of \\(0\\) (with a slight positive edge), and it looks like MacDonald\u0026rsquo;s vary a little bit more than Lockheed Martin. The food industry is probably a tougher business than that in defense.\nLastly, to analyze the recently delisted Twitter!\n# perform linreg twtr_model = sm.OLS(risk_free_excess.TWTR, nyse_with_bias).fit() twtr_model.summary() OLS Regression Results ============================================================================== Dep. Variable: TWTR R-squared: 0.522 Model: OLS Adj. R-squared: 0.522 Method: Least Squares F-statistic: 2357. Date: Mon, 31 Oct 2022 Prob (F-statistic): 0.00 Time: 10:39:24 Log-Likelihood: 4307.1 No. Observations: 2158 AIC: -8610. Df Residuals: 2156 BIC: -8599. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const -0.0002 0.001 -0.346 0.730 -0.002 0.001 NYSE 1.0173 0.021 48.549 0.000 0.976 1.058 ============================================================================== Omnibus: 661.205 Durbin-Watson: 1.986 Prob(Omnibus): 0.000 Jarque-Bera (JB): 15925.609 Skew: -0.883 Prob(JB): 0.00 Kurtosis: 16.191 Cond. No. 29.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Evidently, Twitter is much more variable. It looks like it has a nontrivial bias (the intercept being -0.001 being within the \\(95\\%\\) confidence band \u0026mdash; that the security is possibly significantly underperforming the CAPM expectation in the market.) Furthermore, we have a positive beta value: that the asset is more variable than the market.\nmanual regression We can also use the betas formula to manually calculate what we expect for the beta values (i.e. as if they were one IID random variable.)\nrisk_free_cov = risk_free_excess.cov() risk_free_cov NYSE LMT TWTR MCD NYSE 0.001143 0.001080 0.001163 0.001103 LMT 0.001080 0.001188 0.001116 0.001083 TWTR 0.001163 0.001116 0.002264 0.001155 MCD 0.001103 0.001083 0.001155 0.001200 Finally, to construct the beta values. Recall that:\n\\begin{equation} \\beta_{im} := \\frac{Cov[(R_{i}-R_{f}),(R_{m}-R_{f})]}{Var[R_{m}-R_{f}]} \\end{equation}\nand that:\n\\begin{equation} Var[X] = Cov[X,X], \\forall X \\end{equation}\n# get the market variance (covariance with itself) market_variation = risk_free_cov.NYSE.NYSE # calculate betas betas = {\u0026#34;LMT\u0026#34;: (risk_free_cov.LMT.NYSE/market_variation), \u0026#34;TWTR\u0026#34;: (risk_free_cov.TWTR.NYSE/market_variation), \u0026#34;MCD\u0026#34;: (risk_free_cov.MCD.NYSE/market_variation)} # and make dataframe betas = pd.Series(betas) betas LMT 0.944899 TWTR 1.017294 MCD 0.965081 dtype: float64 Apparently, all of our assets swing less than the overall NYSE market! Especially Lockheed\u0026mdash;it is only \\(94.4\\%\\) of the market variation. Furthermore, it is interesting to see that Twitter swings much more dramatically compared to the market.\nFund creation We will now create two funds with the three securities, one with equal parts and one which attempts to maximizes the bias (max returns) while minimizing the beta variance value compared to the market.\n\u0026ldquo;Equal-Parts Fund\u0026rdquo; (\u0026ldquo;Fund 1\u0026rdquo;) We will now create a fund in equal parts. Here it is:\nfund_1_returns = returns.LMT + returns.TWTR + returns.MCD fund_1_returns 1 -0.063167 2 0.023420 3 -0.017149 4 0.021384 5 0.049750 ... 2154 -0.036409 2155 -0.021132 2156 0.023917 2157 0.001720 2158 -0.014596 Length: 2158, dtype: float64 We will calculate the excess returns of this fund:\nfund_1_excess = fund_1_returns-returns.TBill fund_1_excess 1 -0.112813 2 0.021600 3 -0.023310 4 0.037041 5 0.058226 ... 2154 -0.041349 2155 -0.057265 2156 -0.040503 2157 -0.032449 2158 0.010994 Length: 2158, dtype: float64 And then perform a regression\n# perform linreg fund_1_model = sm.OLS(fund_1_excess, nyse_with_bias).fit() fund_1_model.summary() OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.473 Model: OLS Adj. R-squared: 0.473 Method: Least Squares F-statistic: 1935. Date: Mon, 31 Oct 2022 Prob (F-statistic): 3.01e-302 Time: 10:39:24 Log-Likelihood: 3869.5 No. Observations: 2158 AIC: -7735. Df Residuals: 2156 BIC: -7724. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0007 0.001 0.841 0.401 -0.001 0.002 NYSE 1.1290 0.026 43.993 0.000 1.079 1.179 ============================================================================== Omnibus: 600.456 Durbin-Watson: 2.022 Prob(Omnibus): 0.000 Jarque-Bera (JB): 8416.514 Skew: -0.914 Prob(JB): 0.00 Kurtosis: 12.501 Cond. No. 29.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Surprisingly, we have now created a significantly riskier investment that, though riskier, generates a much higher probability of reward (\\(+0.001\\) is now within the \\(99\\%\\) band!)\nA Better Fund To me, this is the payoff of this assignment. We will now use CAPM to create the \u0026ldquo;best\u0026rdquo; fund combination\u0026mdash;given some variance, the funds which match CAPM. To do this, let\u0026rsquo;s create a generic linear combination of the assets.\nimport sympy as sym x = sym.Symbol(\u0026#39;x\u0026#39;) y = sym.Symbol(\u0026#39;y\u0026#39;) z = sym.Symbol(\u0026#39;z\u0026#39;) fund_2_returns = x*returns.LMT + y*returns.TWTR + z*returns.MCD fund_2_returns 1 0.0139260753744255*x - 0.0751364261353569*y - ... 2 -0.00697525170622448*x + 0.0295704573211193*y ... 3 0.000583132897928884*x - 0.0235859990058791*y ... 4 0.000218587198947517*x + 0.016568426347233*y +... 5 0.00747599199607762*x + 0.0478955096700351*y -... ... 2154 -0.0234665578621085*x - 0.0142913301107561*y +... 2155 -0.00184214468578059*x - 0.0076045993852194*y ... 2156 0.00411172646842317*x + 0.0154024001854269*y +... 2157 0.00357547337231878*x + 0.0062445563228315*y -... 2158 0.00162509910496933*x - 0.0150529686289622*y -... Length: 2158, dtype: object Excellent. We will also calculate the excess returns of this fund:\nfund_2_excess = fund_2_returns-returns.TBill Y = fund_2_excess.to_numpy() Y [0.0139260753744255*x - 0.0751364261353569*y - 0.00195664549320096*z - 0.0496463208073039 -0.00697525170622448*x + 0.0295704573211193*y + 0.000824317408861575*z - 0.00181917459665826 0.000583132897928884*x - 0.0235859990058791*y + 0.00585367525146019*z - 0.00616055581298536 ... 0.00411172646842317*x + 0.0154024001854269*y + 0.00440300114913317*z - 0.0644196927849867 0.00357547337231878*x + 0.0062445563228315*y - 0.0081004573348249*z - 0.0341688956152497 0.00162509910496933*x - 0.0150529686289622*y - 0.00116834209450634*z + 0.0255902303732043] We cast this type to a numpy array because we are about to perform some matrix operations upon it.\nNow, let us perform the actual linear regression ourselves. Recall that the pseudoinverse linear regression estimator is:\n\\begin{equation} \\beta = (X^{T}X)^{-1}X^{T}Y \\end{equation}\nWe have already our \\(Y\\) as a vector (above), and our \\(X\\) is:\nX = nyse_with_bias.to_numpy() X [[ 1.00000000e+00 -3.88457302e-02] [ 1.00000000e+00 -7.42217926e-04] [ 1.00000000e+00 -9.46284244e-03] ... [ 1.00000000e+00 -5.81378271e-02] [ 1.00000000e+00 -3.15429207e-02] [ 1.00000000e+00 4.09643405e-02]] We now have our matrices, let\u0026rsquo;s perform the linear regression!\nlinear_model = np.linalg.inv((X.transpose()@X))@X.transpose()@Y linear_model [0.000544056413840724*x - 6.62061061591867e-5*y + 0.000429966553373172*z - 0.000178620725465344 0.0457830563134785*x + 0.118178191274045*y + 0.0659651260604729*z + 0.899115719100281] Excellent. So we now have two rows; the top row represents the \u0026ldquo;bias\u0026rdquo;\u0026mdash;how much deviation there is from CAPM, and the bottom row represents the \u0026ldquo;rate\u0026rdquo;\u0026mdash;the \u0026ldquo;beta\u0026rdquo; value which represents how much excess variance there is.\nWe can will solve for a combination of solutions to give us specific values of returns vs risk. We will set the asset to learn exactly as much as the market (i.e. no bias).\ndeviance_expr = linear_model[0] deviance_expr 0.000544056413840724*x - 6.62061061591867e-5*y + 0.000429966553373172*z - 0.000178620725465344 We will now try to make variance exactly as much as that in the market.\nrisk_expr = linear_model[1] - 1 risk_expr 0.0457830563134785*x + 0.118178191274045*y + 0.0659651260604729*z - 0.100884280899719 Let us now calculate the boundary condition of our optimization problem by solving an expression in these two expressions.\nsolution = sym.solvers.solve([deviance_expr, risk_expr], x,y,z) solution {x: 0.412737013327711 - 0.819584899551304*z, y: 0.693765220909132 - 0.24067066980814*z} Excellent. Let us recalculate our optimization objective (\u0026ldquo;deviance\u0026rdquo;\u0026mdash;return) in terms of these new solutions. We aim now to maximize this expression by minimizing (i.e. our optimizer minimizes) the negative thereof\u0026mdash;recalling that scypy works as a minimizer.\noptim_objective = deviance_expr.subs(solution)-1e2 optim_objective -5.04831636563563e-19*z - 100.0 We can now use this value to solve for a \\(z\\) value.\noptim_solution = sym.solvers.solve([optim_objective], z) optim_solution {z: -1.98085842402250e+20} Excellent. We can now solve for the rest of our values.\nz0 = float(optim_solution[z]) x0 = solution[x].subs(z, z0) y0 = solution[y].subs(z, z0) (x0,y0,z0) (1.62348165247784e+16, 4.76734523704593e+15, -1.980858424022502e+16) This would create the following plan:\n# solution fund_2_nobias = x0*returns.LMT + y0*returns.TWTR + z0*returns.MCD fund_2_nobias.mean() 0.009168283711770158 Recall that this is the performance of the balanced portfolio:\nfund_1_returns.mean() 0.0009224705380695683 Finally, let\u0026rsquo;s plot the prices of our various funds:\nimport matplotlib.pyplot as plt import matplotlib.dates as mdates import seaborn as sns from datetime import datetime sns.set() fund_2_price = x0*df.LMT + y0*df.TWTR + z0*df.MCD fund_1_price = df.LMT + df.TWTR fund_l_price = df.LMT fund_t_price = df.TWTR dates = df.Date.apply(lambda x:datetime.strptime(x, \u0026#34;%m/%d/%Y %H:%M:%S\u0026#34;)) sns.lineplot(x=dates, y=fund_2_price.apply(sym.Float).astype(float)) sns.lineplot(x=dates, y=fund_1_price.apply(sym.Float).astype(float)) sns.lineplot(x=dates, y=fund_l_price.apply(sym.Float).astype(float)) sns.lineplot(x=dates, y=fund_t_price.apply(sym.Float).astype(float)) plt.gca().xaxis.set_major_locator(mdates.YearLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%Y\u0026#39;)) plt.gca().set_ylabel(\u0026#34;Price\u0026#34;) plt.show() None Recall that we didn\u0026rsquo;t actually buy any MacDonald\u0026rsquo;s. So, we have\u0026mdash;blue, being our \u0026ldquo;optimal\u0026rdquo; portfolio, yellow, our balanced portfolio, green, being Lockheed, and red, being Twitter.\nOur portfolio works surprisingly well!\n","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_linearity_tests/","tags":null,"title":"NUS-ECON320 Linearity Tests"},{"categories":null,"contents":"The code created for this problem can be found here.\nProblem 1 Let\u0026rsquo;s begin with a normal function:\n\\begin{equation} f(x) = (\\sqrt{x}-1)^{2} \\end{equation}\nTaking just a normal Riemann sum, we see that, as expected, it converges to about \\(0.167\\) by the following values between bounds \\([0,1]\\) at different \\(N\\):\nN Value 10 0.23 100 0.172 1000 0.167 10000 0.167 100000 0.167 Problem 2 First, as we are implementing a discrete random walk, here\u0026rsquo;s a fun example; \\(p=0.51\\), \\(\\epsilon=0.001\\).\nWhat is particularly interesting about this case is that, due the probability of change being slightly above \\(50\\%\\), we can see that the sequence has an overall positive growth pattern; however, as far as daily returns is concerned, there is almost no value from day-to-day gains in the market.\nTo actually analyze the our expected value for the probability distributions in number of steps \\(T\\) to travel from \\(0\\) to \\(1\\), as a function of \\(p, \\epsilon\\), we perform the following computation:\nExpected Value of T We set:\n\\begin{equation} \\Delta = \\begin{cases} +\\epsilon, P=p\\\\ -\\epsilon, P=1-p \\end{cases} \\end{equation}\nTherefore, for \\(T\\) as a function from \\(0\\) to \\(1\\), we have:\n\\begin{align} E(T)\u0026amp;=\\frac{1}{E\\qty(\\Delta) } \\\\ \u0026amp;= \\frac{1}{p\\epsilon-(1-p)\\epsilon } \\\\ \u0026amp;= \\frac{1}{\\epsilon (2p-1)} \\end{align}\nNow we will calculate the Variance in \\(T\\):\n\\begin{align} Var(T) \u0026amp;= \\frac{1}{Var(\\Delta)} \\end{align}\nWhere, \\(Var(\\Delta)\\) is calculated by:\n\\begin{align} Var(\\Delta) \u0026amp;= E(\\Delta^{2})-E^{2}(\\Delta) \\\\ \u0026amp;= \\qty(\\epsilon^{2} (2p-1)) - \\qty(\\epsilon (2p-1))^{2} \\end{align}\nAnd therefore:\n\\begin{equation} Var(T) = \\frac{1}{\\qty(\\epsilon^{2} (2p-1)) - \\qty(\\epsilon (2p-1))^{2}} \\end{equation}\nProblem 3 Yes, as we expect, that as \\(\\epsilon\\) decreases, the actual steps \\(T\\) it takes to travel from \\([0,1]\\) increases by an order of magnitude. Given \\(10\\) trials, with \\(p=0.51\\) and \\(\\epsilon = \\{0.1,0.01\\}\\), we have that:\n\\(\\epsilon\\) Mean \\(T\\) Std. \\(T\\) 0.1 570.8 1051.142 0.01 3848.2 1457.180 We can see this on the expected value calculations as well, that:\n\\begin{equation} \\lim_{\\epsilon \\to 0} E(T) = \\frac{1}{\\epsilon (2p-1)} = \\infty \\end{equation}\nThis is not true for the case of \\(p=0.5\\), where the limit will create an undefined behavior with \\(0\\infty\\), and l\u0026rsquo;hospital\u0026rsquo;s rule upon \\(\\epsilon\\) doesn\u0026rsquo;t apply here.\nProblem 4 Yes, the quadratic variation converges towards \\(0\\). Similarly as before, with \\(p=0.51\\) and \\(\\epsilon = \\{0.1,0.01,0.001\\}\\), our quadratic variations are:\n\\(\\epsilon\\) quadratic variation 0.1 5.02 0.01 0.32 0.001 0.05 It seems like that, as long as the path terminates and epsilon becomes smaller, the sum of squared difference will converge towards \\(0\\).\nThis means that, for all \\(p\u0026gt;0.5\\), the squared differences will be convergent. However, for \\(p\\leq 0.5\\), the squared differences are arguably still convergent but the sequence doesn\u0026rsquo;t terminate.\nProblem 5 To allow negative values, we changed the function to:\n\\begin{equation} f(x) = ({x}-1)^{2} \\end{equation}\nThe results of running the three expressions with \\(p=0.51\\), \\(\\epsilon=\\{0.1, 0.01, 0.001\\}\\), similarly to before, respectively are as follows:\n\\(\\epsilon\\) \\(f(x_{i})\\) \\(f(x_{i+1})\\) \\(f\\qty(\\frac{x_{i+1}-x_{i}}{2})\\) 0.1 3.03 -2.37 -1.85 0.01 1.7 -1 -0.17 0.001 0.359 0.307 0.938 It seems like\u0026mdash;while all three of these results converge\u0026mdash;they converge to distinctly different limits. Of course, this result also depends on \\(p\\), as the probability determines whether the path is even complete in the first place, which will of course affect the convergence here.\n","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_stochastic_integration/","tags":null,"title":"NUS-ECON320 Stochastic Integration"},{"categories":null,"contents":"Let \\(X\\) denote price and \\(Y\\) denote volatility. The two objects obey the following process:\n\\begin{equation} \\begin{cases} \\dd{X} = \\mu X \\dd{t} + XY \\dd{W} \\\\ \\dd{Y} = \\sigma Y \\dd{B} \\end{cases} \\end{equation}\nwhere, \\(W\\) and \\(B\\) are correlated Brownian motions with correlation \\(\\rho\\) \u0026mdash; \\(E[(\\dd{W})(\\dd{B})] = \\rho \\dd{t}\\).\nLet\u0026rsquo;s work with \\(Y\\) first. We understand that \\(Y\\) is some continuous variable \\(e^{a}\\). Therefore, \\(\\dv{Y}{t}=ae^{a}\\). Therefore, \\(dY = ae^{a}dt\\). Finally, then \\(\\frac{\\dd{Y}}{Y} = \\frac{ae^{a}}{e^{a}}\\dd{t} = a\\).\nFinally, then, because we defined \\(Y=e^{a} \\implies \\ln Y = a = \\frac{\\dd{Y}}{Y}\\).\nSo, we have that:\n\\begin{align} \u0026amp;\\dd{Y} = \\sigma Y\\dd{B} \\\\ \\Rightarrow\\ \u0026amp; \\dd{\\log Y} = \\frac{\\sigma Y\\dd{B}}{Y} = \\sigma \\dd{B} \\end{align}\nThis tells that the change in log returns in \\(Y\\) is normal (as \\(B\\) is a Brownian Motion), with a standard deviation of \\(\\sigma\\). Therefore:\n\\begin{equation} \\dd{\\log Y} \\sim \\mathcal{N}(0, \\sigma^{2} \\dd{t}) \\end{equation}\nWe therefore see that the log-returns of \\(Y\\) is a normal with variance \\(\\sigma^{2}\\), making \\(Y\\) itself a Brownian Motion with center \\(0\\) and variance \\(\\sigma^{2}\\).\nSo now, tackling the expression above in \\(X\\), we will do the same exact thing as above and divide by \\(X\\):\n\\begin{equation} \\dd{\\log X} = \\mu \\dd{t} + Y\\dd{W} \\end{equation}\nSo we can see that \\(X\\) is a Geometric Brownian Motion as a sum of two random variables\u0026mdash;its volatility is determined by \\(Y\\) with a time-drift \\(\\mu \\dd{t}\\).\nWe see that we are almost ready to have an analytical solution here, because the top expression is applying some function \\(f=\\log\\) to a stochastic differential equation by time; however, the right side \\(Y\\) here is not quite a constant (it is itself a stochastic process), so we can\u0026rsquo;t simply apply an Itô Intergral and call it a day.\nSo instead, we will proceed to a Monte-Carlo simulation of the results to verify as much as we can.\nWe will begin by setting the sane values for variances\u0026mdash;having \\(0.1\\%\\) drift and \\(1\\%\\) variance in variance, and the two Brownian motions being inverses of each other \\(\\rho = 0.5\\).\nmu = 0.001 sigma = 0.01 rho = 0.5 (mu,sigma,rho) (0.001, 0.01, 0.5) We will seed a standard Brownian motion; as the two random motions are covariate, we can use the value of one to generate another: therefore we will return both at once.\nfrom numpy.random import normal def dbdw(): dB = normal() dW = dB + normal(0, (1-rho)**2) return (dB, dW) dbdw() (-1.0246010237177643, -1.281335746614678) Excellent.\nWe will now simulate the system we were given:\n\\begin{equation} \\begin{cases} \\dd{X} = \\mu X \\dd{t} + XY \\dd{W} \\\\ \\dd{Y} = \\sigma Y \\dd{B} \\end{cases} \\end{equation}\nLet\u0026rsquo;s set the number of trials to \\(10000\\).\nN = 1000 We will measure the convergence of \\(\\bar{\\dd{X}}\\) and \\(\\bar{\\dd{Y}}\\): we will tally each value at each time \\(t\\) as well as compare their expected values over time.\nWe will first seed our systems at \\(1\\%\\) variance and \\(1\\) dollar of price.\nX = 1 Y = 0.01 Now, it\u0026rsquo;s actual simulation time!\n# history of Y and X X_hist = [] Y_hist = [] # history of dx dX_hist = [] dY_hist = [] # current expected value EdX = 0 EdY = 0 # difference in E EdX_diff = 0 EdY_diff = 0 # for n loops, we simulate for _ in range(N): # get a source of randmess dB, dW = dbdw() # get the current dx and dw _dX = mu*X+X*Y*dW _dY = sigma*Y*dB # apply it X += _dX Y += _dY # tally it Y_hist.append(Y) X_hist.append(X) dX_hist.append(_dX) dY_hist.append(_dY) # calculate new expected value # we don\u0026#39;t store it immediately b/c we want to check convergence _EdX = sum(dX_hist)/len(dX_hist) _EdY = sum(dY_hist)/len(dY_hist) EdX_diff = abs(_EdX-EdX) EdY_diff = abs(_EdY-EdY) # store new expected value EdX = _EdX EdY = _EdY Let\u0026rsquo;s observe a few values! For starters, let\u0026rsquo;s measure our new expected values.\nEdX 0.0013333651336800837 EdY -1.225482645599256e-06 And, let\u0026rsquo;s check if we have converged by seeing if the difference is a reasonably small value:\n(EdX_diff, EdY_diff) (2.578663659035343e-05, 1.2183875816528115e-07) Looks like both of our variables have converged. Now, let\u0026rsquo;s plot a few things. Let\u0026rsquo;s first build a table with our data.\nimport pandas as pd data = pd.DataFrame({\u0026#34;price\u0026#34;: X_hist, \u0026#34;variance\u0026#34;: Y_hist}) data[\u0026#34;time\u0026#34;] = data.index data price variance time 0 0.998644 0.009974 0 1 0.980393 0.009796 1 2 0.998355 0.009967 2 3 0.994514 0.009913 3 4 1.001363 0.009961 4 .. ... ... ... 995 2.323640 0.008778 995 996 2.321473 0.008715 996 997 2.343427 0.008818 997 998 2.306271 0.008654 998 999 2.333365 0.008775 999 [1000 rows x 3 columns] We will use this to continue the rest of our analysis. For data augmentation, we will also calculate the natural logs of the change to get the rate of change.\nimport numpy as np data[\u0026#34;price_log\u0026#34;] = np.log(data.price) data[\u0026#34;variance_log\u0026#34;] = np.log(data.variance) data[\u0026#34;price_log_change\u0026#34;] = data.price_log - data.price_log.shift(1) data[\u0026#34;variance_log_change\u0026#34;] = data.variance_log - data.variance_log.shift(1) # drop the first row we have w/o change data = data.dropna() data price variance ... price_log_change variance_log_change 1 0.980393 0.009796 ... -0.018444 -0.018005 2 0.998355 0.009967 ... 0.018155 0.017332 3 0.994514 0.009913 ... -0.003855 -0.005443 4 1.001363 0.009961 ... 0.006863 0.004801 5 0.991306 0.009895 ... -0.010094 -0.006639 .. ... ... ... ... ... 995 2.323640 0.008778 ... 0.002148 0.002124 996 2.321473 0.008715 ... -0.000933 -0.007227 997 2.343427 0.008818 ... 0.009413 0.011765 998 2.306271 0.008654 ... -0.015983 -0.018804 999 2.333365 0.008775 ... 0.011680 0.013827 [999 rows x 7 columns] Let\u0026rsquo;s begin by plotting what we have:\nimport seaborn as sns import matplotlib.pyplot as plt sns.set() We will plot price and variation on two axes.\nplt.gcf().clear() sns.lineplot(x=data.time, y=data.price, color=\u0026#34;g\u0026#34;) ax2 = plt.twinx() sns.lineplot(x=data.time, y=data.variance, color=\u0026#34;b\u0026#34;, ax=ax2) plt.show() Where, the blue line represents the percent variance over time and the green line represents the price. Given the \\(0.1\\%\\) drift we provided, we can see that our simulated market grows steadily in the 1000 data point.\nWe can then plot the log (percent) changes.\nplt.gcf().clear() sns.lineplot(x=data.time, y=data.price_log_change, color=\u0026#34;g\u0026#34;) ax2 = plt.twinx() sns.lineplot(x=data.time, y=data.variance_log_change, color=\u0026#34;b\u0026#34;, ax=ax2) plt.show() As you can see\u0026mdash;we have fairly strong random variables, centered around \\(0\\). Having verified that our drift and variables behave in the way that we expect, we can proceed with analysis.\nWe can use a single-variable \\(t\\) test to figure the \\(99\\%\\) confidence band of the result. To do this, we first need to calculate the mean and standardized deviation of the price percent change (log difference).\nlog_mean, log_std = (data.price_log_change.mean(), data.price_log_change.std()) (log_mean, log_std) (0.0008495184126335735, 0.008471735971085885) And now, we will calculate the\nfrom scipy.stats import t lower_bound, upper_bound = t.interval(0.99, len(data)-1, loc=log_mean, scale=log_std) lower_bound -0.021014037766751738 Therefore, with \\(99\\%\\) confidence, we can say that our asset\u0026mdash;given its current parameters, and an \\(N=1000\\) Monte-Carlo simulation\u0026mdash;will not have a more than \\(2.1\\%\\) drop in value.\nWe will use a hedged option to minimize loss. We will use this value to determine the maximum loss for an European put option, maturing in \\(T\\) time, such that the exercise thereof will be hedged against drops of asset price.\nFirst, we will determine the cost of a correctly hedged European put option.\nWe will define \\(S_{0}\\) as the current price of the asset. We will use \\(P\\) as the price of the put option.\nWe desire the strike price of the option to be:\n\\begin{equation} K = S_{0} + P \\end{equation}\nthat is: the price of the put option we desire here will recuperate the price to trade the option and protect against loss. We will symbolically solve for the price of such an option.\nNote that the codeblocks switches here from standard Python to SageMath.\nWe first define the standard normal cumulative distribution.\nfrom sage.symbolic.integration.integral import definite_integral z = var(\u0026#34;z\u0026#34;) N(x) = 1/sqrt(2*pi)*definite_integral(e^(-z^2/2), z, -infinity, x) We will then leverage the Euopean call Black-Scholes model to calculate the optimal put price. We first instantiate variables \\(T\\), and we will set current time to be \\(0\\).\nWe will use \\(v\\) for \\(\\sigma\\), the volatility of the security. We will use \\(S\\) for current price. Lastly, we define \\(P\\) to be our put price. We will call \\(r\\) our risk-free rate.\nTo determine the discount factor, we first implement symbolically our expression for desired strike price.\nv,T,S,P,r = var(\u0026#34;v T S P r\u0026#34;) K = S+P K P + S Great. Now we will implement our discount factors.\nd1 = 1/v*sqrt(T) * (ln(S/K) + (r+v^2/2)*(T)) d2 = d1-v*T d1, d2 (1/2*((v^2 + 2*r)*T + 2*log(S/(P + S)))*sqrt(T)/v, -T*v + 1/2*((v^2 + 2*r)*T + 2*log(S/(P + S)))*sqrt(T)/v) And lastly, we will implement the Black-Scholes expression for puts as a logical expression.\nexpr = P == N(-d2)*K*e^(-r*T)-N(-d1)*S expr \\begin{equation} P = \\frac{{\\left({\\left(\\operatorname{erf}\\left(\\frac{\\sqrt{2} {\\left(T v^{2} + 2 \\, T r + 2 \\, \\log\\left(\\frac{S}{P + S}\\right)\\right)} \\sqrt{T}}{4 \\, v}\\right) - 1\\right)} e^{\\left(T r\\right)} - \\operatorname{erf}\\left(-\\frac{\\sqrt{2} {\\left(2 \\, T v^{2} - {\\left(T v^{2} + 2 \\, T r + 2 \\, \\log\\left(\\frac{S}{P + S}\\right)\\right)} \\sqrt{T}\\right)}}{4 \\, v}\\right) + 1\\right)} S}{\\operatorname{erf}\\left(-\\frac{\\sqrt{2} {\\left(2 \\, T v^{2} - {\\left(T v^{2} + 2 \\, T r + 2 \\, \\log\\left(\\frac{S}{P + S}\\right)\\right)} \\sqrt{T}\\right)}}{4 \\, v}\\right) + 2 \\, e^{\\left(T r\\right)} - 1} \\end{equation}\nNumerical solutions to this expression\u0026mdash;fitting for each of the values from before\u0026mdash;would then indicate the correct price of the option to generate the hedging effect desired.\n","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_volatility_hedging/","tags":null,"title":"NUS-ECON320 Volatility Hedging"},{"categories":null,"contents":"The doctor-patient ratio in Haiti is 1 out of 67,000; a combination of malnutrition and malpractice results in a fatality rate of 47%.\nIn Breath, Eyes, Memory, the high rate of fatalities from birth is included as a part of a proverb in Sophie’s village. Ife tells that, of “three children” conceived by an old woman, “one dies in her body.” (Danticat 118)\nNext Steps Token: s_6285_15\nFollow this link for the next step. You maybe able to continue to the next phrase of the game; it is also possible that you may have died during birth and would have to restart.\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_1/","tags":null,"title":"NUS-ENG401 Childbirth"},{"categories":null,"contents":"Apart from Russia, Central Africa and the Caribbeans have the highest average death rates of regions on Earth. Death is a pretty common occurrence, and—it appears—through the vicissitudes of the game you have died.\nBetter luck next time!\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_0/","tags":null,"title":"NUS-ENG401 Death"},{"categories":null,"contents":"Despite having some access to education, actual success through it varies significantly as the resources are scarce. For instance, postsecondary education only shares 1% of educational spending in Martinique, so access to it is extremely limited.\nIn Black Shack Alley, José’s quarter scholarship—not enough to support his education—causes his mother to lament that they are “black, poor, and alone in the world” (Zobel 125): their station in Martinican society prevented their access to the already limited resource.\nNext Steps Token: s_5166_12\nResources to advance in education is fickle! To continue to the next step, you must locate Jack L. (\u0026lsquo;23) on the San Mateo campus, and he will provide you with the next steps to complete this track.\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_5/","tags":null,"title":"NUS-ENG401 Endgame of Education"},{"categories":null,"contents":"Literacy rates differ significantly between genders in Central Africa. In rural Nigeria, there is a 24.1% difference in literacy rates between men and women.\nIn Joys of Motherhood, Adankwo’s natural and implicitly differentiated treatment of Nhu Ego’s sons and daughters reflects the androcentrism in Nigerian society’s view of education; in the work, she asks Nhu Ego to not “forget the … twin [girl’s] bride prices will help out … boy’s school fees.” (Emecheta 127)\nNext Steps Token: s_4979_1d\nFollow this link for the next step.\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_2/","tags":null,"title":"NUS-ENG401 Gender and Education"},{"categories":null,"contents":"Welcome! The device of the station of birth plays a large part in all four of the works we read over the semester. In I, Tituba, the author grants Tituba renewed empowerment through her birth; in Black Shack Alley, Jose’s birth in the Alley forces him to leverage the racially unequal devices of the French regime to gain social advancement; Sophie’s trauma in Breath, Eyes, Memory is propagated by her violent conception—which results in her mother’s forced testing upon her; Joys of Motherhood’s Nnu Ego’s family is loving, yet with conservative values which forces a crippling sense of motherly duty that almost drove her to death. Birth, and challenging the station assigned at birth, is a fundamental value pervasive through the texts.\nThis game aims to explore some of the dynamics found in all four of the works, while exploring some aspects of Haitian, Martinican, or Nigerian culture.\nTo play the game, here are what you need to know\u0026ndash;\nThe game works like a CTF: through the game, you are hunting for game tokens that look like this: s_[numbers]_[numbersletters] You can validate whether or not the token is correct with the tool provided below Validate a Token! To check whether or not a token you received through the game is valid, use the utility below:\nValidate\nplease enter a token The Game Please go ahead to this link to get started.\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_utility/","tags":null,"title":"NUS-ENG401 Gift Utility"},{"categories":null,"contents":"General Information Due Date Topic Important Documents 9/29 Lit. Devices I, Tituba Prompt In an interview, Maryse Conde explains, \u0026ldquo;I was attracted to write the particular story of Tituba because this woman was unjustly treated by history. I felt the need to give her a reality that was denied to her because of her color and her gender.\u0026rdquo; Choose one or two literary devices and explain how Conde uses it/them in the novel to give Tituba her subjecthood. Examples could be: narrative voice, allusion, irony, dialogue, etc.\nClaim Synthesis Quotes Bin Birth Determines Capacity That birth determines the capacity for one to do Evil\nThere was one thing, however, that I didn\u0026rsquo;t know: evil is a gift received at birth. There\u0026rsquo;s no acquiring it. Those of us who have not come into this world armed with spurs and fangs are losers in every combat. (73)\nMama Yaya highlights that misfortune lies in the center of life derived from birth\nMisfortune, as you know, is our constant companion. We are born with it, we lie with it, and we squabble with it for the same withered breast. It eats the codfish from our calabash. But we\u0026rsquo;re tough, us n\u0026mdash;! (85)\nBelieves that having choice in birth is what would make it fulfilling\n(Irony between \u0026ldquo;gift\u0026rdquo; and \u0026ldquo;choice\u0026rdquo;)\nI began to doubt seriously Mama Yaya\u0026rsquo;s basic conviction that life is a gift. Life would only be a gift if each of us could choose the womb that carried us. \u0026hellip; If one day I am born again, let it be in the steely army of conquerors! (120)\nTituba believes that she is born as a healer\nThe terror of these people seemed like an injustice to me. They should have greeted me with shouts of joy and welcome and presented me with a list of illnesses that I would have tried my utmost to cure. I was born to heal, not to frighten. (12)\n\u0026ldquo;Births\u0026rdquo; Other People Elizabeth Parris \u0026ldquo;reborn\u0026rdquo; after Tituba\u0026rsquo;s Care\nUp till then I had not called on the supernatural to care for Elizabeth Parris. \u0026hellip; hat night I decided to use my powers. \u0026hellip; In the morning the color returned to Goodwife Parris\u0026rsquo;s cheeks. She asked for a little water. Toward midday she managed to feed herself. And in the evening she went to sleep like a newborn babe. (45)\nThe \u0026ldquo;evil\u0026rdquo; of abortion transferred from Tituba into Betsy\nI made her swear not to tell anyone and at dusk I plunged her up to her neck in a liquid to which I had given all the properties of amniotic fluid. \u0026hellip; Plunging Betsey into this scalding hot bath, it seemed to me that these same hands, that not long ago had dealt death were now giving life, and I was purifying myself of the murder of my child. (63)\nHer upper lip curled up into an ugly pout, revealing her sick gums. \u0026ldquo;You, do good? You\u0026rsquo;re a Negress, Tituba! You can only do evil. You are evil itself.\u0026rdquo; \u0026hellip; \u0026ldquo;That bath you had me take; what was in it? The blood of a newborn baby that died from one of your spells?\u0026rdquo; I was aghast. (77)\nRebirth After Death (like the actual book) Tituba\u0026rsquo;s Freeing from Prison into Benjamin is Described an Rebirth\nHe smiled cynically. \u0026ldquo;A man who hasn\u0026rsquo;t got very much money. You know how much slaves are selling for at the present time? Twenty-five pounds!\u0026rdquo; Our conversation stopped there, but now I knew the fate awaiting me. Another master, another bondage. (120)\nThen with one skillful blow of the mallet he smashed my chains to pieces. He did the same thing with my wrists while I screamed. \u0026hellip; I screamed, and this scream, the terrified cry of a newborn baby, heralded my return to this world. I had to learn how to walk again. \u0026hellip; Few people have the misfortune to be born twice. (122)\nTituba\u0026rsquo;s \u0026ldquo;real\u0026rdquo; story begins only after death\nAnd that is the story of my life. Such a bitter, bitter story. My real story starts where this one leaves off and it has no end. (175)\nSuccessful rebirth only without birth\nI watched her grow up and stumble around on her shaky legs, exploring the pur- gatory of the plantation, finding her delight in the shape of a cloud, the drooping foliage of an ylang-ylang, or the taste of a bitter orange. \u0026hellip; A child I didn\u0026rsquo;t give birth to but whom I chose! What motherhood could be nobler! (177)\nMisc Book opens with the framing of her being born\nI was born from this act of aggression. From this act of hatred and contempt. (3)\nPackage insert praises death as something positive\n\u0026ldquo;Death is a porte whereby we pass to joye; Lyfe is a lake that drowneth all in payne \u0026ndash;John Harrington\u0026rdquo; (Cover Insert)\nPlans for Abortion\nThere is no happiness in motherhood for a slave. It is little more than the expulsion of an innocent baby, who will have no chance to change its fate, into a world of slavery and abjection\u0026hellip;. That night, my baby was carried out of my womb in a flow of black blood. I saw him wave his arms like a tadpole in distress and I burst into tears. (52)\nSubclaim Development Tituba Realizes Birth is Involuntary Birth into live is a deterministic process for which those being \u0026ldquo;born\u0026rdquo; have no agency over. For African folks, Mama Yaya claims that misfortune is one such deterministic factor of their birth. Although Mama Yaya disagrees, Tituba believes that life is not a gift unless it is deterministic (this is of course ironic, because you don\u0026rsquo;t choose your gifts.) Despite the indeterminism, Tituba believes that she is born as a healer She leverages (Re)Birth to change others, to poor results Perhaps as an attempt to help others control (\u0026ldquo;choose\u0026rdquo;) birth, she uses her power to reborn people; like\nElizabeth Parris Betsy Parris But Psych! Both of them turned on her. Especially Betsy Parris.\nAlso her child was aborted.\nThe work literally provides rebirth of Tituba and empowers her to give birth despite her abortion Tituba Herself raised her station against those of Benjamin. Yet, this was not voluntary (see quote in section) and designed by Condé in the story.\nTituba\u0026rsquo;s \u0026ldquo;real\u0026rdquo; story begins only after forcible death/\u0026ldquo;rebirth\u0026rdquo;, and she (author?) considers it nobel to give this new form of birth without giving birth (which didn\u0026rsquo;t happen in real world), but without the request of the born either.\nConclusiony bit So this whole book, beginning at her birth, covered by a celebration of alternative birth illustrates such a process of providing agency.\nThe Claim The motif of birth and rebirth plays an important role in Maryse Condé\u0026rsquo;s work I, Tituba. Despite Tituba\u0026rsquo;s own failed attempt at controlling the (re)birth of herself and others to better their fate in history, Condé offers Tituba a renewed empowerment in birth by both illustrating her \u0026ldquo;rebirth\u0026rdquo; and providing her a chance to elect a descendant she wasn\u0026rsquo;t originally able to bear.\n","permalink":"https://www.jemoka.com/posts/kbhi_tituba_essay_planning/","tags":null,"title":"NUS-ENG401 I, Tituba Essay Planning"},{"categories":null,"contents":"Joys of Motherhood highlights the plurality of duties for the reader women have to undertake in order to succeed in Nigerian society. Women represent 80% of agricultural labor in Nigeria—a dangerous job, yet is significantly underrepresented in knowledge-based work.\nPrior to gaining ownership to her own stall, Nhu Ego has to “spread her wares on the pavement” (Emecheta 113) selling goods in order to make ends meet—despite Nnaife’s money from employment which he often squanders.\nNext Steps Token: s_2827_13\nThe conditions of this duality of work is harsh. This is where this story leaves off. Tap on this link, and one of two things may happen\u0026mdash;you maybe directed back to this page, or you maybe redirected somewhere else. Unfortunately, neither of these paths lead to further advancement.\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_6/","tags":null,"title":"NUS-ENG401 Many Hats"},{"categories":null,"contents":"Even if the education system provides a ticket for its successful students to gain social advancement, it is often difficult or even arbitrary. Access to education is also frequently dependent on race.\nIn Black Shack Alley, Zobel frames the value of schooling as a “gateway … to escape.” (Zobel) Zobel highlights that the main way to escape the oppression in the colonies is by leveraging the itself oppressive systems of education.\nNext Steps Token: s_7776_1b\nThe process of pursuing education takes a lot more effort than the steps before! Please locate the link to the next target by looking under the cabinet from which a puffer-fish hangs in our San Mateo campus.\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_3/","tags":null,"title":"NUS-ENG401 Pursuing Education"},{"categories":null,"contents":"Traditional values in Caribbean and African societies often place womens’ value in the context of other men. When women pursue independent careers such as midwives and healers, they could be called “witches.”\nMaryse Condé demonstrates this bias in the novel I, Tituba. She writes that “Yao’s love had transformed [Tituba]’s mother”, making her a “young woman.” (Condé 7) In the passage, the womanhood of Tituba’s mother is framed as only being granted when she encounters Yao; in contrast, Mama Yaya’s womanhood exists independently, yet she is viewed as a witch.\nNext Steps Token: s_6780_15\nYou now get to make a choice. You may either\u0026hellip;\nPursue independence You may choose to face the consequences of leveraging the harsh education system to attempt to achieve social advancement. To do so, follow this link.\nSeek domestic dependence Or, you may choose to follow domestic roles without continuing to pursue education. If so, follow this link.\n","permalink":"https://www.jemoka.com/posts/kbhnus_eng401_gift_4/","tags":null,"title":"NUS-ENG401 What is a Witch?"},{"categories":null,"contents":"Statement Suppose \\(U_1\\), \\(U_2\\), and \\(W\\) are subspaces of \\(V\\), such that:\n\\begin{equation} \\begin{cases} V = U_1 \\oplus W\\\\ V = U_2 \\oplus W \\end{cases} \\end{equation}\nProve or give a counterexample that \\(U_1=U_2\\)\nIntuition The statement is not true. The definition of direct sums makes it such that, \\(\\forall v \\in V\\), there exists a unique representation of \\(v\\) with \\(u_{1i}+w_{i} = v\\) for \\(u_{1j}\\in U_1, w_{j} \\in W\\) as well as another unique representation \\(u_{2i} + w_{i}=v\\) for \\(u_{2j} \\in U_{2}, w_{j} \\in W\\).\nHowever, the definition of direct sums doesn\u0026rsquo;t guarantee that the distinct unique representations are equivalent; although \\(V\\) can only be represented uniquely by EITHER a sum of \\(U_1+W\\) or \\(U_2+W\\), it does not mean that each \\(v \\in V\\) itself has only one unique representation.\nCounterexample In constructing a counterexample, we turn to the fact that the sums of two variables creates a third free variable; therefore, we can figure two distinct ways of creating a third, final free variable that construct an equivalent space.\nConstructing \\(U_1\\) as a subspace We begin with constructing:\n\\begin{equation} U_1= \\left\\{\\begin{pmatrix} x_1\\\\y_1\\\\2y_1 \\end{pmatrix}, x_1,y_1 \\in \\mathbb{F} \\right\\} \\end{equation}\nBy setting both free variables to \\(0\\), we construct the additive identity. Then:\n\\begin{equation} \\lambda \\begin{pmatrix} x_1 \\\\ y_1 \\\\ 2y_1 \\end{pmatrix} = \\begin{pmatrix} \\lambda x_1 \\\\ \\lambda y_1\\\\ 2(\\lambda y_1) \\end{pmatrix} \\end{equation}\nby multiplication in \\(\\mathbb{F}\\), scalar multiplication, commutativity, and associativity. We can show closure under addition by inheriting the operation in \\(\\mathbb{F}\\) as well as applying distributive to the factor of \\(2\\).\nTherefore, we show that \\(U_1\\) is a subspace of \\(\\mathbb{F}^{3}\\).\nConstructing \\(U_2\\) as a subspace Then, we construct:\n\\begin{equation} U_2=\\left\\{\\begin{pmatrix} x_1 \\\\ y_1 \\\\ 0 \\end{pmatrix}, x_1,y_1\\in \\mathbb{F} \\right\\} \\end{equation}\nWe again have \\(0\\) by setting free variables to create the additive identity. Addition and scalar multiplication is closed by inheriting them from \\(\\mathbb{F}\\) (and the fact that \\(0\\) is the additive inverse and therefore \\(\\lambda 0 = 0\\)).\nTherefore, \\(U_2\\) is a subspace as well in \\(\\mathbb{F}^{3}\\).\nConstructing \\(W\\) as a subspace Finally, we have:\n\\begin{equation} W = \\left\\{\\begin{pmatrix} 0 \\\\ 0 \\\\z_1 \\end{pmatrix}, z_1\\in \\mathbb{F} \\right\\} \\end{equation}\nBy setting \\(z_1=0\\), we have the additive identity. As with above, addition and scalar multiplication is closed through inheritance and that \\(\\lambda 0=0\\).\nConstructing Sum of Subsets Let\u0026rsquo;s construct:\n\\begin{equation} U_1+W = V \\end{equation}\nTake \\(u_1 \\in U_1, w \\in W\\), attempting to construct a \\(v\\in V\\), we have that:\n\\begin{equation} \\begin{pmatrix} x_{1} \\\\ y_1 \\\\ 2y_1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ z_1 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ y_1 \\\\ 2y_1+z_1 \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} \\end{equation}\nConstructing Direct Sum For all vectors in \\(\\mathbb{F}^{3}\\), this is an equivalence with 3 free variables and 3 expressions\u0026mdash;rendering each vector in \\(\\mathbb{F}^{3}\\) to have a representation by \\(U_1+W\\). We can see this also with the unique \\(0\\) test:\nWe see that for:\n\\begin{equation} 0 \\in U_1+W \\end{equation}\nTo solve for some \\(u_1 \\in U, w \\in W : u_1+w = 0\\) we have that:\n\\begin{equation} \\begin{pmatrix} x_{1} \\\\ y_1 \\\\ 2y_1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ z_1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\end{equation}\nwhere the first vector is in \\(U_1\\) and the second is in \\(W\\). The first two expressions tell us that \\(x_1=y_1=0\\); the final equation requires that \\(2y_1+z_1=0+z_1=0\\Rightarrow z_1=0\\) .\nTherefore, the only way to write \\(0\\) is to take each element in the sum to \\(0\\) (i.e. in this case \\(u_1=w=0 \\implies u_1+w = 0\\)), making the above a direct sum.\nTherefore:\n\\begin{equation} U_1 \\oplus W = V \\end{equation}\nIn almost the same manner, we can show that:\n\\begin{equation} U_2\\oplus W = V \\end{equation}\nThat, for some \\(u_2\\in U_2, w \\in W, v \\in V\\):\n\\begin{equation} \\begin{pmatrix} x_1\\\\y_1\\\\0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ z_1 \\end{pmatrix} = \\begin{pmatrix} x_1\\\\y_1\\\\z_1 \\end{pmatrix} \\end{equation}\nfor the first vector in \\(U_2\\), the second in \\(W\\). In fact, this is the statement made in example 1.41.\nCreating the Counterexample Finally, we have that:\n\\begin{equation} \\left\\{\\begin{pmatrix} x_1 \\\\ y_1 \\\\ 2y_1 \\end{pmatrix}: x_1,y_1 \\in \\mathbb{F}\\right\\} \\neq\\left\\{\\begin{pmatrix} x_1 \\\\ y_1 \\\\ 0 \\end{pmatrix}: x_1,y_1 \\in \\mathbb{F}\\right\\} \\end{equation}\n\\(\\forall y_1 \\neq 0\\) in the first expression. Therefore, \\(U_1 \\neq U_2\\), finishing the counterexample. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_1_c_proof_preso/","tags":null,"title":"NUS-MATH530 1.C Problem 23"},{"categories":null,"contents":"Claim Proof or give a counter example for the statement that:\n\\begin{align} \\dim\\qty(U_1+U_2+U_3) = \u0026amp;\\dim U_1+\\dim U_2+\\dim U_3\\\\ \u0026amp;-\\dim(U_1 \\cap U_2) - \\dim(U_1 \\cap U_3) - \\dim(U_2 \\cap U_3) \\\\ \u0026amp;+\\dim(U_1 \\cap U_2 \\cap U_3) \\end{align}\nCounterexample This statement is false.\nTake the following three subspaces of \\(\\mathbb{F}^{2}\\):\n\\begin{align} U_1 = \\qty{\\mqty(a \\\\ 0): a \\in \\mathbb{F}}\\\\ U_2 = \\qty{\\mqty(0 \\\\ b): b \\in \\mathbb{F}}\\\\ U_3 = \\qty{\\mqty(c \\\\ c): c \\in \\mathbb{F}} \\end{align}\nsubspace check All \\(U_1\\), \\(U_2\\), \\(U_3\\) are in \\(\\mathbb{F}^{2}\\).\nzero Zero exists in all by setting free variables to \\(0\\)\naddition For \\(U_1\\) \u0026mdash;\n\\begin{equation} \\mqty(a_1 \\\\ 0) + \\mqty(a_2 \\\\ 0) = \\mqty(a_1+a_2 \\\\ 0) \\in \\qty{\\mqty(a \\\\ 0): a \\in \\mathbb{F}} \\end{equation}\nand, by the same token, addition is closed for \\(U_2\\).\nFor \\(U_3\\) \u0026mdash;\n\\begin{equation} \\mqty(c_1 \\\\ c_1) + \\mqty(c_2 \\\\ c_2) = \\mqty(c_1+c_2 \\\\ c_1+c_2) \\in \\qty{\\mqty(c \\\\ c): c \\in \\mathbb{F}} \\end{equation}\nscalar multiplication For \\(U_1\\) \u0026mdash;\n\\begin{equation} \\lambda \\mqty(a \\\\ 0) = \\mqty(\\lambda a \\\\ 0) \\in \\qty{\\mqty(a \\\\ 0): a \\in \\mathbb{F}} \\end{equation}\nand, by the same token, scalar multiplication is closed for \\(U_2\\).\nFor \\(U_3\\) \u0026mdash;\n\\begin{equation} \\lambda \\mqty(c \\\\ c) = \\mqty(\\lambda c \\\\ \\lambda c) \\in \\qty{\\mqty(c \\\\ c): c \\in \\mathbb{F}} \\end{equation}\nconstructing the counterexample Let us calculate the value of both sides of:\n\\begin{align} \\dim\\qty(U_1+U_2+U_3) = \u0026amp;\\dim U_1+\\dim U_2+\\dim U_3\\\\ \u0026amp;-\\dim(U_1 \\cap U_2) - \\dim(U_1 \\cap U_3) - \\dim(U_2 \\cap U_3) \\\\ \u0026amp;+\\dim(U_1 \\cap U_2 \\cap U_3) \\end{align}\nRecall that:\n\\begin{align} U_1 = \\qty{\\mqty(a \\\\ 0): a \\in \\mathbb{F}}\\\\ U_2 = \\qty{\\mqty(0 \\\\ b): b \\in \\mathbb{F}}\\\\ U_3 = \\qty{\\mqty(c \\\\ c): c \\in \\mathbb{F}} \\end{align}\nleft side Let\u0026rsquo;s first construct:\n\\begin{equation} U_1 + U_2 + U_3 \\end{equation}\nBy definition:\n\\begin{equation} U_1 + U_2 + U_3 = \\qty{u_1+u_2+u_3: u_j\\in U_j} \\end{equation}\nTherefore, taking a sample from each results as:\n\\begin{equation} u_1+u_2+u_3 = \\mqty(a \\\\ 0) + \\mqty(0 \\\\ b) + \\mqty(c \\\\c) = \\mqty(a+c \\\\ b +c) \\end{equation}\nThis creates two free variables for slots, meaning:\n\\begin{equation} U_1+U_2+U_3 = \\mathbb{F}^{2} \\end{equation}\nSo: \\(\\dim \\qty(U_1+U_2+U_3)=2\\)\nright side dimension of the subspaces\nLet us construct a basis for each of these spaces to figure their dimension.\nFor \\(U_1\\), \\(\\qty{\\mqty(1 \\\\ 0)}\\). We see that scaling the one vector in this basis will construct all vectors in \\(\\mathbb{F}^{2}\\) for which the second coordinate will be \\(0\\) \u0026mdash; spanning \\(U_1\\). Being a list with one non-zero vector, it is also linearly independent. So \\(\\dim U_1 = 1\\).\nBy almost the same token, for \\(U_2\\), \\(\\qty{\\mqty(0 \\\\ 1)}\\). This makes also \\(\\dim U_2=1\\).\nFor \\(U_3\\), we have \\(\\qty{\\mqty(1 \\\\ 1)}\\). Scaling this one vector will construct all vectors in \\(\\mathbb{F}^{2}\\) for which both coordinates are the same \u0026mdash; spanning \\(U_3\\). Being a list with one non-zero vector, it is also linearly independent. So \\(\\dim U_3 = 1\\).\nThis renders all three subspaces have dimension \\(1\\).\ndimension of the unions\nThese subspaces were picked because of a surprising convenience. Their unions are all the zero vector!\n\\begin{equation} U_1 \\cap U_2 = \\qty{\\mqty(a \\\\ 0): a \\in \\mathbb{F}} \\cap \\qty{\\mqty(0 \\\\ b): b \\in \\mathbb{F}} = \\qty{\\mqty(0 \\\\ 0)} \\end{equation}\nThis is because \\(a=0\\), \\(b=0\\) respectively in order to satisfy both generators.\nSimilarly\n\\begin{equation} U_1 \\cap U_3 = \\qty{\\mqty(a \\\\ 0): a \\in \\mathbb{F}} \\cap \\qty{\\mqty(c \\\\ c): c \\in \\mathbb{F}} = \\qty{\\mqty(0 \\\\ 0)} \\end{equation}\nTo satisfy both generators, \\(a=c\\) for the top coordinate, \\(c=0\\) for the bottom coordinate, so \\(a=c=0\\).\nBy a similar token:\n\\begin{equation} U_2 \\cap U_3 = \\qty{\\mqty(0 \\\\ 0)} \\end{equation}\nWe established before that the span of \\(\\qty{}\\) (which is declared linearly independent) to be \\(\\qty{0}\\), so we see that the dimensions of all three required unions as \\(0\\) (as an empty list has length \\(0\\).)\nconstructing the expression for the right side\nWe have that:\n\\begin{equation} \\dim U_j = 1, j \\in \\qty{1,2,3} \\end{equation}\nAnd that:\n\\begin{equation} \\dim U_{j} \\cap U_{k} = 0 , j,k \\in \\{1,2,3\\} \\end{equation}\nfrom above.\nThis makes\u0026mdash;\n\\begin{align} \\dim \u0026amp;U_1+\\dim U_2+\\dim U_3\\\\ \u0026amp;-\\dim(U_1 \\cap U_2) - \\dim(U_1 \\cap U_3) - \\dim(U_2 \\cap U_3) \\\\ \u0026amp;+\\dim(U_1 \\cap U_2 \\cap U_3)\\\\ =1\u0026amp;+1+1-0-0-0+0 \\\\ =3 \\end{align}\nshowing the counterexample We have now that:\n\\begin{equation} \\dim(U_1+U_2+U_3) = 2 \\end{equation}\nBut:\n\\begin{align} \\dim \u0026amp;U_1+\\dim U_2+\\dim U_3\\\\ \u0026amp;-\\dim(U_1 \\cap U_2) - \\dim(U_1 \\cap U_3) - \\dim(U_2 \\cap U_3) \\\\ \u0026amp;+\\dim(U_1 \\cap U_2 \\cap U_3)\\\\ =3 \\end{align}\nYet \\(2 \\neq 3\\).\nSo:\n\\begin{align} \\dim(U_1+U_2+U_3) \\neq \\dim \u0026amp;U_1+\\dim U_2+\\dim U_3\\\\ \u0026amp;-\\dim(U_1 \\cap U_2) - \\dim(U_1 \\cap U_3) - \\dim(U_2 \\cap U_3) \\\\ \u0026amp;+\\dim(U_1 \\cap U_2 \\cap U_3)\\\\ \\end{align}\nFinishing the counter example. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_2_c_problem_17/","tags":null,"title":"NUS-MATH530 2.C Problem 17"},{"categories":null,"contents":"Statement Support \\(W\\) is finite-dimensional, and \\(T \\in \\mathcal{L}(V,W)\\). Prove that \\(T\\) is injective IFF \\(\\exists S \\in \\mathcal{L}(W,V)\\) such that \\(ST = I \\in \\mathcal{L}(V,V)\\).\nProof Given injectivity Given an injective \\(T \\in \\mathcal{L}(V,W)\\), we desire that \\(\\exists S \\in \\mathcal{L}(W,V)\\) such that \\(ST = I \\in \\mathcal{L}(V,V)\\).\nWe begin with some statements.\nRecall that, a linear map called injective when \\(Tv=Tu \\implies v=u\\) Recall also that the \u0026ldquo;identity map\u0026rdquo; on \\(V\\) is a map \\(I \\in \\mathcal{L}(V,V)\\) such that \\(Iv = v, \\forall v \\in V\\) Motivating \\(S\\) We show that we can indeed create a function \\(S\\) by the injectivity of \\(T\\). Recall a function is a map has the property that \\(v=u \\implies Fv=Fu\\).\nWLOG consider two vectors \\(a,b \\in V\\).\nCreating \\(S\\) Define a function \\(S:W\\to V\\) in the following manner:\n\\begin{equation} S(v) = a \\mid Ta = v \\end{equation}\nDemonstrating that \\(S\\) is a function\nSo, given \\(v, u \\in W\\) and \\(v=u\\), we have:\n\\(Sv = a \\mid Ta=v\\) \\(Su = b \\mid Tb=u\\) If \\(Sv=Su\\), then \\(a=b\\). To demonstrate that \\(S\\) is a function, we now desire that \\(a=b\\).\nFrom the above, we have that \\(Ta=v\\), \\(Tb=u\\). From prior, we have \\(v=u\\) From the two statements above, we have \\(v=u \\implies Ta=Tb\\) Lastly, from the injectivity of \\(T\\), we have that \\(Ta=Tb \\implies a=b\\) Hence demonstrating\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_3_b_problem_20-1/","tags":null,"title":"NUS-MATH530 3.B Problem 20"},{"categories":null,"contents":"Statement Support \\(W\\) is finite-dimensional, and \\(T \\in \\mathcal{L}(V,W)\\). Prove that \\(T\\) is injective IFF \\(\\exists S \\in \\mathcal{L}(W,V)\\) such that \\(ST = I \\in \\mathcal{L}(V,V)\\).\nProof Given injectivity Given an injective \\(T \\in \\mathcal{L}(V,W)\\), we desire that \\(\\exists S \\in \\mathcal{L}(W,V)\\) such that \\(ST = I \\in \\mathcal{L}(V,V)\\).\nCreating \\(S\\) Define a relation \\(S:range\\ T\\to V\\) in the following manner:\n\\begin{equation} S(v) = a \\mid Ta = v \\end{equation}\nDemonstrating that \\(S\\) is a function We show that there are no two possible choices for \\(a\\), and therefore that \\(S\\) is a function, by the injectivity of \\(T\\). Recall a function is a map has the property that \\(v=u \\implies Fv=Fu\\).\nSo, given \\(v, u \\in W\\) and \\(v=u\\), we have:\n\\(Sv = a \\mid Ta=v\\) \\(Su = b \\mid Tb=u\\) If \\(Sv=Su\\), then \\(a=b\\). To demonstrate that \\(S\\) is a function, we now desire that \\(a=b\\).\nFrom the above, we have that \\(Ta=v\\), \\(Tb=u\\). From prior, we have \\(v=u\\) From the two statements above, we have \\(v=u \\implies Ta=Tb\\) Recall now that a linear map called injective when \\(Tv=Tu \\implies v=u\\).\nTherefore, from the injectivity of \\(T\\), we have that \\(Ta=Tb \\implies a=b\\). Hence demonstrating the desired quality that shows \\(S\\) as a function.\nDemonstrating that \\(S\\) is a linear map The linearity \\(S\\) actually simply inherits the linearity of \\(T\\), which is defined to be a linear map.\nAdditivity:\n\\begin{align} Sv+Su \u0026amp;= (a \\mid Ta =v) + (b \\mid Tb =u) \\\\ \u0026amp;= (a+b) \\mid Ta+Tb = (v+u) \\\\ \u0026amp;= (a+b) \\mid T(a+b) = (v+u) \\\\ \u0026amp;= x \\mid Tx = (v+u) \\\\ \u0026amp;= S(v+u) \\end{align}\nHomogenity is shown in a similar fashion. We can therefore conclude that \\(S \\in \\mathcal{L}(range\\ T, V)\\).\nNote on the codomain of \\(S\\) Note that we desire \\(S \\in \\mathcal{L}(W,V),\\ i.e.\\ S:W\\to V\\), And yet, as it stands, \\(S: range\\ T \\to W\\). Fortunately, as \\(range\\ T\\) is a subspace of \\(W\\) (as ranges are subspaces of the codomain), we can leverage Axler 3.A-E11 (Sasha\u0026rsquo;s Proof, \u0026ldquo;maps to subspaces can be extended to the whole space\u0026rdquo;) to arbitrary extend \\(S\\) to \\(S:W\\to V\\).\nIt turns out that where the \u0026ldquo;extended\u0026rdquo; basis vectors gets mapped doesn\u0026rsquo;t matter. We only care about \\(S\\) insofar as its compositional behavior with \\(T\\).\nDemonstrating that \\(S\\) has the properties we desire We desire that \\(ST = I \\in \\mathcal{L}(V,V)\\).\nRecall that the \u0026ldquo;identity map\u0026rdquo; on \\(V\\) is a map \\(I \\in \\mathcal{L}(V,V)\\) such that \\(Iv = v, \\forall v \\in V\\). We now show that \\(ST\\) acts like the identity map.\nWLOG take \\(v \\in V\\).\nLet \\(Tv=a\\). Let \\(Sa = u\\). Based on the definition of \\(S\\) (that \\(Sx = y \\mid Ty=x\\), \u0026ldquo;\\(S\\) is the inverse map\u0026rdquo;), we have that \\(Tu=a\\). Recall once again that a linear map called injective when \\(Tv=Tu \\implies v=u\\).\nWe now have that \\(Tu=a=Tv\\), therefore, because \\(T\\) is given injective, \\(u=v\\).\nWe have show WLOG that \\((ST)v = S(Tv) =Sa = u=v\\). Therefore \\((ST)v=v\\), making \\(ST\\) an identity map \\(ST:V\\to V\\). Lastly, as the product of linear maps are themselves a linear map, \\(ST=I\\in \\mathcal{L}(V,V)\\)\nConclusion Having constructed the existence of \\(S\\) based on the required properties of \\(T\\), we show that given an injective \\(T \\in \\mathcal{L}(V,W)\\), have an \\(S \\in \\mathcal{L}(W,V)\\) such that \\(ST = I \\in \\mathcal{L}(V,V)\\), as desired.\nGiven \\(S\\) Given some \\(T \\in \\mathcal{L}(V,W)\\) and that \\(\\exists S \\in \\mathcal{L}(W,V): ST=I \\in \\mathcal{L}(V,V)\\), we desire that \\(T\\) is injective. Fortunately, we essentially just reverse the logic of the last section in the last part of the proof.\nRecall that a linear map called injective when \\(Tv=Tu \\implies v=u\\). Suppose for the sake of contradiction that \\(\\exists u,v: Tv=Tu\\) but \\(u\\neq v\\).\nLet \\(Tv=Tu=a\\) Let \\(Sa=b\\) Therefore: \\((ST)v=(ST)u=S(a)=b\\). Just to reiterate, this means that we have:\n\\((ST)v=b\\implies Iv=b\\) \\((ST)u=b \\implies Iu=b\\) Therefore, we have that \\(Iv=Iu\\) for distinct \\(v,u\\), which is absurd. Having reached contradiction, we have that \\(Tu=Tv\\implies u=v\\), reaching the definition of injectivity for \\(T\\). \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_3_b_problem_20/","tags":null,"title":"NUS-MATH530 3.B Problem 20"},{"categories":null,"contents":"Chapter 4 discussion with Lachlan 4.2 False.\nThe union between \\(\\{0\\} \\cup \\{p \\in \\mathcal{P}(\\mathbb{F}): deg\\ p = m\\}\\) is not closed under addition. You can add two \\(m\\) degree polynomials and get something that\u0026rsquo;s not \\(m\\) degrees:\n\\begin{equation} (z^{m} + 1) - z^{m} = 1 \\end{equation}\n4.3 False.\nThe union between \\(\\{0\\} \\cup \\{p \\in \\mathcal{P}(\\mathbb{F}): deg\\ p\\ even\\}\\) is not closed also under addition, for the same reason:\n\\begin{equation} (z^{m} + z^{m-1} + 1) - (z^{m} + 1) = z^{m-1} \\end{equation}\nOne Chapter 5 Exercise 5.A.5 Suppose \\(T \\in \\mathcal{L}(V)\\), prove that the intersection of every collection of \\(V\\) that is invariant under \\(T\\) is invariant under \\(T\\)\nLet \\(U_1 \\dots U_{n}\\) be invariant subspaces under \\(T\\).\nThat is:\n\\begin{equation} T u_{j} \\in U_{j} \\end{equation}\nWe desire that:\n\\begin{align} Tu \\in \\bigcap U_{j}\\ |\\ u \\in \\bigcap U_{j} \\end{align}\nWLOG, treat \\(u \\in \\bigcap U_{j}\\) as \\(u \\in U_{j}\\). Now, \\(Tu \\in U_{j}\\). This holds \\(\\forall U_{j}\\). Therefore, \\(Tu \\in \\forall U_{j}\\). So \\(Tu \\in \\bigcap U_{j}\\).\nHence, the intersection of invariant subspaces are invariant as well.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_5_a_and_discussion/","tags":null,"title":"NUS-MATH530 5.A and Discussion"},{"categories":null,"contents":"Dot product Calculations Let\u0026rsquo;s calculate some dot products!\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 0 \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\2 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\1 \\end{pmatrix} = 4 \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} -1 \\\\1 \\end{pmatrix} = 0 \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\1 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = 4 \\end{equation}\nInterpretation Geometrically, the intepretation of the dot product is the magnitude that comes from scaling the bottom projected value by the top value. This is essentially multiplying the proportion of one vector that\u0026rsquo;s parallel to the other by each other.\nCross Product Calculations Cross products!\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\times \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\0 \\end{pmatrix} \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} \\times \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\0 \\end{pmatrix} \\end{equation}\nThe dot product is the point that is perpendicular to the other two input vectors.\nWhy its not a field We want to check why the multiplication of vectors in \\(\\mathbb{F}^{3}\\) via taking the cross product cannot form a field.\nWe can safely assume that the addition operation of the vectors derive their closure, commutativity, associativity from these properties in \\(\\mathbb{F}\\).\nTherefore, we will verify these properties with multiplication. The only closed multiplication-like operation of vectors is the cross-product. Let\u0026rsquo;s first define the cross-product.\nGiven two vectors in \\(\\mathbb{F}^{3}\\):\n\\begin{equation} \\begin{pmatrix} a \\\\b \\\\ c \\end{pmatrix}, \\begin{pmatrix} d \\\\ e\\\\f \\end{pmatrix} \\end{equation}\nTheir cross product is the vector in \\(\\mathbb{F}^{3}\\) defined by:\n\\begin{equation} \\begin{vmatrix} \\hat{i} \u0026amp; \\hat{j} \u0026amp; \\hat{k} \\\\ a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\end{vmatrix} \\end{equation}\nTaking the actual determinant, we have that:\n\\begin{equation} \\begin{vmatrix} \\hat{i} \u0026amp; \\hat{j} \u0026amp; \\hat{k} \\\\ a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\end{vmatrix} = \\begin{pmatrix} bf-ce \\\\ dc-af \\\\ ac-db \\end{pmatrix} \\end{equation}\nIdentity Let\u0026rsquo;s first figure the identity of this operation. We wish to figure some \\((a,b,c)\\) such that the result of the cross product would be \\((d,e,f)\\).\nGeometrically, the perpendicularity of a vector is the resulting value of the cross product; however, no vector (apart from \\(\\vec{0}\\)) can be perfectly perpendicular to itself exactly. This would indicate that no such identities exist.\nWe can also observe that there is no \\(f\\) term on the bottom of the cross product. This would indicate that no combination of \\((a,b,c)\\) can construct the needed \\(f\\) on the last entry.\nFinally, for a more formal proof.\nProof: there can not exist a field-like identity for a cross product.\nFor the sake of contradiction let\u0026rsquo;s say for some nonzero vector \\(\\vec{v} \\in \\mathbb{F}^{3}\\), there exists some identity named \\(\\vec{e} \\in \\mathbb{F}^{3}\\) that follows the properties of identities in a field.\nWe first have that:\n\\begin{equation} \\vec{e} \\times \\vec{v} = \\vec{v} \\end{equation}\nby the definition of the identity.\nAnd also that:\n\\begin{equation} \\vec{v} \\times \\vec{e}= \\vec{v} \\end{equation}\nby the fact that field-like operations commutes.\nWe have also the property of cross products that:\n\\begin{align} \u0026amp;\\vec{a} \\times \\vec{b} = -(\\vec{b} \\times \\vec{a}) \\\\ \\Rightarrow\\ \u0026amp; \\vec{a} \\times \\vec{b} + \\vec{b} \\times \\vec{a} = 0 \\end{align}\nBy applying the inverse of \\(-(\\vec{b}\\times \\vec{a})\\) to both sides, as cross products are closed and therefore an additive inverse exists.\nTherefore, we have that:\n\\begin{equation} \\vec{v} + \\vec{v} = 0 \\end{equation}\nWe see then \\(\\vec{v}\\) is its own additive inverse. Therefore \\(\\vec{v}\\) itself is also \\(0\\). But we established that \\(\\vec{v}\\) can be non-zero. Reaching contradiction, \\(\\blacksquare\\). (this is iffy)\nCommutativity Because of the fact that two-by-two matricies exists on the diagonals, the cross product is also not commutative. In fact,\nDeterminants The geometric interpretation of the determinants is the change in area inside a vector which it stretches a given vector.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_geometric_intepretations/","tags":null,"title":"NUS-MATH530 Geometric Intepretations"},{"categories":null,"contents":" Date Link \u0026lt;2022-09-09 Fri\u0026gt; NUS-MATH530 Solving Systems \u0026lt;2020-09-09 Wed\u0026gt; NUS-MATH530 Geometric Intepretations \u0026lt;2022-09-13 Tue\u0026gt; NUS-MATH530 Linear Vehicles \u0026lt;2022-09-15 Thu\u0026gt; NUS-MATH530 Plane and 1.B \u0026lt;2022-09-27 Tue\u0026gt; NUS-MATH530 1.C Problem 23 \u0026lt;2022-10-29 Sat\u0026gt; NUS-MATH530 2.C Problem 17 \u0026lt;2022-11-16 Wed\u0026gt; NUS-MATH530 3.B Problem 20 \u0026lt;2023-01-23 Mon\u0026gt; NUS-MATH530 3.E Problem 1 \u0026lt;2023-02-14 Tue\u0026gt; NUS-MATH530 5.A and Discussion ","permalink":"https://www.jemoka.com/posts/kbhnus_math530_homework_index/","tags":null,"title":"NUS-MATH530 Homework Index"},{"categories":null,"contents":"Infinite Plane Two Vehicles Yes. Though the travel of the two vehicles are not entirely independent, the second vehicle can diagonally traverse the plane while the first vehicle cuts across it. Practically, the question asks whether or not a combination of:\n\\begin{equation} \\alpha \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\beta \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{equation}\nCan form all vectors in \\(\\mathbb{R}^2\\). Expanding that expression out, we have, given some point \\((a,b)\\) that:\n\\begin{equation} \\begin{pmatrix} \\beta \\\\ \\alpha + \\beta \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\end{equation}\nTherefor, we have expressions:\n\\begin{equation} \\begin{cases} \\beta = a \\\\ \\alpha +\\beta = b \\end{cases} \\end{equation}\nSubstituting the definition of \\(\\beta\\) then:\n\\begin{align} \u0026amp;\\alpha + a = b \\\\ \\Rightarrow\\ \u0026amp;\\alpha = b - a \\end{align}\nTherefore, we have that, for all desired locales \\((a,b)\\) we have a fully determined solution:\n\\begin{equation} \\begin{cases} \\alpha = b-a \\\\ \\beta = a \\end{cases} \\end{equation}\nThis means that some direction of travel in both vehicles will suffice.\nGoing Home Not necessarily. Graphically, after shifting yourself to some location upper-right, its impossible to move horizontally in the vertical-only vehicle.\nPractically, the question is asking that, if you are at some beginning location:\n\\begin{equation} \\begin{pmatrix} a \\\\b \\end{pmatrix} \\end{equation}\nCan we devise some travel that follows:\n\\begin{equation} \\alpha \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} a \\\\b \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\end{equation}\nExpanding this out, we have expressions:\n\\begin{equation} \\begin{cases} 0 + a = 0 \\\\ \\alpha + b = 0 \\end{cases} \\end{equation}\nNamely, we have that:\n\\begin{equation} \\begin{cases} a = 0 \\\\ \\alpha = -b \\end{cases} \\end{equation}\nWe are therefore under-determined here; there is a solution only \\(\\forall a=0\\), but for no other \\(a\\).\nInfinite Space Pickup and Hoverboard We have:\n\\begin{equation} \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 3 \\\\ -2 \\\\1 \\end{pmatrix} \\end{equation}\nto go to all directions \\((a,b,c)\\). Let\u0026rsquo;s try solving:\n\\begin{equation} \\begin{cases} \\alpha + 3\\beta = a\\\\ \\alpha -2\\beta = b \\\\ \\beta = c \\end{cases} \\end{equation}\nSubstituting the value for \\(\\beta=c\\), to the above equations, we have:\n\\begin{equation} \\begin{cases} \\alpha + 3c = a \\\\ \\alpha - 2c = b \\\\ \\end{cases} \\end{equation}\nAnd therefore, we have results:\n\\begin{equation} \\begin{cases} \\alpha = a-3c \\\\ \\alpha = b+ 2c \\end{cases} \\end{equation}\nThis equation is again over-determined. Therefore, you cannot get anywhere in your space; you can, however, get to all the points \\((a,b,c)\\) where:\n\\begin{equation} a-3c = b+2c \\end{equation}\nPickup, Hoverboard, AND Jetpack (part 1) We now have:\n\\begin{equation} \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 3 \\\\ -2 \\\\ 1 \\end{pmatrix} + \\gamma \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} \\end{equation}\nto go to all points \\((a,b,c)\\), we now try solving:\n\\begin{equation} \\begin{cases} \\alpha + 3\\beta = a \\\\ \\alpha -2\\beta + \\gamma = b\\\\ \\beta +\\gamma = c \\end{cases} \\end{equation}\nAt this point, it is probably easier to use a matrix to solve this expression. Hence, let\u0026rsquo;s solve:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 1 \u0026amp; -2 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\c \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s first subtract the first column from the second column:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; -5 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} a \\\\ b-a \\\\c \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s now rotate rows \\(2\\) and \\(3\\):\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; -5 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a \\\\c\\\\ b-a \\end{pmatrix} \\end{equation}\nGreat. Now let\u0026rsquo;s subtract thrice the second row towards the first row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; -3 \\\\ 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; -5 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c \\\\c\\\\ b-a \\end{pmatrix} \\end{equation}\nAnd add five times the second row to the last row\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; -3 \\\\ 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 6 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c \\\\c\\\\ b-a+5c \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s subtract a sixth of the last row to the second row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; -3 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 6 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c \\\\c-\\frac{b-a+5c}{6}\\\\ b-a+5c \\end{pmatrix} \\end{equation}\nAnd add a half to the top row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 6 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c + \\frac{b-a+5c}{2} \\\\c-\\frac{b-a+5c}{6}\\\\ b-a+5c \\end{pmatrix} \\end{equation}\nAnd finally divide the bottom row by \\(6\\):\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c + \\frac{b-a+5c}{2} \\\\c-\\frac{b-a+5c}{6}\\\\ \\frac{{b-a+5c}}{6} \\end{pmatrix} \\end{equation}\nGreat. So now we have a fully determined solution \\(\\forall \\alpha, \\beta, \\gamma\\). Therefore, given a pair of location which you want to reach \\((a,b,c)\\), we can use the expressions above to solve for the values by which we have to move each vehicle.\nPickup, Hoverboard, AND Jetpack (part 2) We have the same problem, but with new numbers.\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 1 \u0026amp; -2 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\c \\end{pmatrix} \\end{equation}\nAt this point, we really want to be checking of the vectors which form this matrix is a spanning set; that is, after performing Gaussian elimination, do we get back a zero-row? If so, it will restrict some combination of our input \\((a,b,c)\\) to converge to \\(0\\).\nLet\u0026rsquo;s begin by subtracting the first row from second row\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 0 \u0026amp; -5 \u0026amp; -5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\end{equation}\nAnd now, let\u0026rsquo;s divide the middle row by \\(\\frac{-1}{5}\\)\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s then subtract the middle row from the last row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{pmatrix} \\end{equation}\nAlready we see an \\(0\\) row emerging. That means that some combination of the variables have to be \\(0\\) for a solution to exist: these vectors do not span the space and therefore we can\u0026rsquo;t get everywhere in space.\nPickup Breaks Down We now want to know if the following vectors would reach \\((0,0,0)\\) after driving the pickup some distance \\(d\\); that is, if we started at some \\((a,b,c)\\), can:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 1 \u0026amp; -2 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\\\ \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = -\\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} \\end{equation}\nyield a solution? We have already performed the Gaussian Elimination above, therefore, we will skip directly to the solution:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = -\\begin{pmatrix} a-3c + \\frac{b-a+5c}{2} \\\\c-\\frac{b-a+5c}{6}\\\\ \\frac{{b-a+5c}}{6} \\end{pmatrix} \\end{equation}\nObviously the bottom few rows yield a solution, however, the top row places some limitation on our possible location. Namely, that:\n\\begin{equation} -\\frac{a+b-c}{2} = 0 \\end{equation}\nIf the locations you are at do not behave with these rules, a solution will not be yielded.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_linear_vehicles/","tags":null,"title":"NUS-MATH530 Linear Vehicles"},{"categories":null,"contents":"Equation of a Plane We want to determine all points on the plane formed by two vectors.\nLet\u0026rsquo;s take two vectors \\(\\vec{u} \\in V\\) and \\(\\vec{v} \\in V\\). The orthogonal vector to the both of them (i.e. the normal direction of the plane) is:\n\\begin{equation} \\vec{u}\\times \\vec{v} \\end{equation}\nby the definition of the cross product.\nThe points on the plane, therefore, have to be orthogonal themselves to this normal vector. This means that the dot product of the candidate vector against these vectors should be \\(0\\):\n\\begin{equation} (\\vec{u} \\times \\vec{v}) \\cdot \\begin{pmatrix} x_{1} \\\\ \\dots \\\\ x_{n} \\end{pmatrix} = 0 \\end{equation}\nThis forms the final equation for a plane given two vectors in \\(\\mathbb{F}^{n}\\).\nA.B Exercises Double Negative We desire that \\(-(-v)=v \\forall v \\in V\\)\nBy distributivity in vector spaces, and the fact that \\(0v=0\\), we have that:\n\\begin{equation} v+(-1)v = (1-1)v = 0v = 0 \\end{equation}\nTherefore, \\((-1)v=-v\\).\nWe now have:\n\\begin{equation} -(-v) = -((-1)v) \\end{equation}\nThe scalar multiple of \\(v\\), by definition, is also \\(\\in V\\) if \\(v \\in V\\). Therefore, it itself holds that:\n\\begin{equation} (-1)((-1)v) \\end{equation}\nBy associativity:\n\\begin{equation} (-1\\cdot -1)v \\end{equation}\nFinally:\n\\begin{equation} (-1\\cdot -1)v = (1v) = v\\ \\blacksquare \\end{equation}\nOne of it is zero If \\(a \\in \\mathbb{F}\\), \\(v \\in V\\), and \\(av=0\\), we desire that \\(a=0\\) or \\(v=0\\). We perform casework.\nCase 1: \\(a=0\\) \u0026ndash; we are done.\nCase 2: \\(a \\neq 0\\): As \\(a \\in \\mathbb{F}\\), and \\(a \\neq 0\\), \\(\\exists \\frac{1}{a}: a\\cdot \\frac{1}{a}=1\\).\nTherefore:\n\\begin{align} \u0026amp;av = 0 \\\\ \\Rightarrow\\ \u0026amp; \\frac{1}{a}av = \\frac{1}{a} 0 \\\\ \\Rightarrow\\ \u0026amp; 1v = \\frac{1}{a} 0 \\\\ \\Rightarrow\\ \u0026amp; 1v = 0 \\\\ \\Rightarrow\\ \u0026amp; v=0\\ \\blacksquare \\end{align}\nExistence and Uniqueness Given Equation Given \\(v,w \\in V\\), we desire a unique \\(x\\in V: v+3x=w\\).\nLet\u0026rsquo;s first check existence. Take the expression:\n\\begin{equation} n = \\frac{1}{3} (w-v) \\end{equation}\nAs both \\(v,w \\in V\\), subtraction (addition) and scalar multiplication are defined. Therefore, \\(\\forall w,v \\in V\\), we can construct such an \\(n\\).\nSupplying the expression into \\(v+3x\\) for the definition of \\(x\\):\n\\begin{align} v+3x \u0026amp;= v+3\\qty(\\frac{1}{3}(w-v)) \\\\ \u0026amp;= v+(w-v) \\\\ \u0026amp;= v+w-v \\\\ \u0026amp;= v-v+w \\\\ \u0026amp;= 0+w \\\\ \u0026amp;= w \\end{align}\nby distributivity, associativity, and commutativity in vector spaces, yielding \\(w\\) as desired.\nNow let\u0026rsquo;s check uniqueness.\nSuppose \\(\\exists x_1, x_2: v+3x_1=w\\) and \\(v+3x_2=w\\).\nBy transitivity:\n\\begin{equation} v+3x_1=v+3x_2 \\end{equation}\nApplying \\(-v\\) to both sides:\n\\begin{equation} 3x_1=3x_2 \\end{equation}\nFinally, applying \\(\\frac{1}{3}\\) to both sides:\n\\begin{equation} x_{1}= x_2 \\end{equation}\nTherefore, there only exists one unique \\(x\\) which satisfies the expression. \\(\\blacksquare\\)\nEmpty Set is Not a Vector Space The empty set is not a vector space as it doesn\u0026rsquo;t have an additive identity. \\(\\blacksquare\\)\nAdditive Inverse is also Zero Multiplication We first take the additive inverse expression:\n\\begin{equation} \\forall v \\in V, \\exists -v: v+(-v) = 0 \\end{equation}\nTake now:\n\\begin{equation} 0v \\end{equation}\nWe have that:\n\\begin{align} 0v \u0026amp;= (0+0)v \\\\ \u0026amp;= 0v + 0v \\end{align}\nBy distributivity.\nAs \\(0v \\in V\\), \\(\\exists -0v: 0v+(-0v)=0\\).\n\\begin{align} \u0026amp;0v = 0v+0v \\\\ \\Rightarrow\\ \u0026amp; 0v-0v = 0v+0v-0v \\\\ \\Rightarrow\\ \u0026amp; 0 = 0v \\end{align}\nas desired. Now, we will start from this condition and work out way backwards.\nNote that the statement for additive inverse condition is that:\n\\begin{equation} \\forall v \\in V, \\exists -v: v+(-v) = 0 \\end{equation}\nLet us begin with the expression that:\n\\begin{equation} 0=0v \\end{equation}\nWe have that:\n\\begin{equation} 0=(1-1)v \\end{equation}\nThen, we have by distributivity:\n\\begin{equation} 0 = v + (-1)v \\end{equation}\nscalar multiplication is defined on a vector space. Therefore, we have \\(-1v\\) to construct such an additive inverse \\(\\forall v \\in V\\). \\(\\blacksquare\\)\nWeird Vector Space All operations are defined as given.\nTake scalars \\(t_1, t_2 \\in \\mathbb{R}\\).\n\\begin{equation} (t_1-t_2)\\infty = \\infty \\end{equation}\nYet, if we follow the rules of distribution:\n\\begin{equation} (t_1 -t_2)\\infty = \\infty -\\infty =0 \\end{equation}\nTherefore, distribution doesn\u0026rsquo;t hold on this new structure. It is not a vector space. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_plane_and_1_b/","tags":null,"title":"NUS-MATH530 Plane and 1.B"},{"categories":null,"contents":"Two Variables Let\u0026rsquo;s begin with the equations:\n\\begin{equation} \\begin{cases} 2x+y = 3 \\\\ x - y = 0 \\end{cases} \\end{equation}\nWe will first change this into a matrix equation:\n\\begin{equation} \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} \\end{equation}\nWe need to find, then, the inverse of:\n\\begin{equation} \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} \\end{equation}\nNamely, we need the matrix such that:\n\\begin{equation} M \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = I \\end{equation}\nTo do this, we can use row operations on both sides such that the left side becomes the identity, we are essentially inverting the process of reversing a matrix.\n\\begin{equation} \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s begin:\n\\begin{align} \u0026amp; \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 0 \u0026amp; 3 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; -2 \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 0 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 0 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\\\ \\frac{1}{3} \u0026amp; \\frac{1}{3} \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \u0026amp; \\frac{1}{3} \\\\ \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\end{pmatrix} \\end{align}\nFinally, then, we will applying this matrix to the input:\n\\begin{align} \\begin{pmatrix} \\frac{1}{3} \u0026amp; \\frac{1}{3} \\\\ \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{align}\nThree Variables We do this again, but now with a much larger matrix. Namely:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\end{equation}\nI spend a good two hours (yes) trying to invert this. At this point, I know its invertable but I keep making mistakes. However, a solution exists and it is of shape:\n\\begin{equation} \\begin{pmatrix} \\frac{1}{5} \u0026amp; \\frac{1}{5} \u0026amp; \\frac{2}{5} \\\\ \\frac{1}{5} \u0026amp; \\frac{1}{5} \u0026amp; -\\frac{3}{5} \\\\ \\frac{2}{5} \u0026amp; -\\frac{3}{5} \u0026amp; \\frac{4}{5} \\end{pmatrix} \\end{equation}\nAnd, applying the output, we have that:\n\\begin{equation} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} \\end{equation}\nSo complicated of an inverse, for such a simple result\u0026hellip;\nMatrix Multiplication Matrix multiplication is not commutative. While you can, for instance, multiply a \\(2\\times 3\\) by a \\(3\\times 3\\), we cannot do it the other way.\nFor an equation with three variables, you need three equations at a minimum to have at least one solution; you can get at most the number of equations number of solutions with fewer equations. You probably will have no solutions if you have more equations\u0026mdash;the result is likely to be overdetermined; of course, two equations may be the same relation then in which case one is effectively nulled.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_solving_systems/","tags":null,"title":"NUS-MATH530 Solving Systems"},{"categories":null,"contents":"Proof: identity of a group is unique Assume for contradiction that there exists two identities \\(e_1\\) and \\(e_2\\) which are identities of the group \\(A\\). Take also an \\(a \\in A\\).\nGiven both \\(e_1\\) and \\(e_2\\) are identities, we have that:\n\\begin{equation} a * e_1 = a \\end{equation}\nas well as\n\\begin{equation} a * e_2 = a \\end{equation}\nTherefore, we have by the transitive property that:\n\\begin{equation} a * e_1 = a*e_2 \\end{equation}\nBecause we are in a group, there exists a \\(1/a\\) the inverse of \\(a\\). Applying this inverse to the expression, we have that:\n\\begin{equation} 1/a*a * e_1 = 1/a*a*e_2 \\end{equation}\nTherefore, that:\n\\begin{equation} e_1 = e_2\\ \\blacksquare \\end{equation}\nTherefore, there cannot be two unique identities in a group.\nProof: inverse of an element in a group is unique Take group \\(A\\) and element \\(a\\in A\\), assume for contradiction that there exists two inverses of \\(a\\) named here \\(a\u0026rsquo;_1\\) and \\(a\u0026rsquo;_2\\). Given they are both inverses for \\(a\\), we have that:\n\\begin{equation} a * a\u0026rsquo;_1 = 1 \\end{equation}\nas well as\n\\begin{equation} a * a\u0026rsquo;_2 = 1 \\end{equation}\nTherefore, we have by the transitive property that:\n\\begin{equation} a * a\u0026rsquo;_1 = a*a\u0026rsquo;_2 \\end{equation}\nBecause we are in a group, there exists a \\(1/a\\) the inverse of \\(a\\). Applying this inverse to the expression, we have that:\n\\begin{equation} 1/a*a * a\u0026rsquo;_1 = 1/a*a*a\u0026rsquo;_2 \\end{equation}\nTherefore, that:\n\\begin{equation} a\u0026rsquo;_1 = a\u0026rsquo;_2\\ \\blacksquare \\end{equation}\nTherefore, there cannot be two unique inverses for an element in group.\nProof: additive identity in field cannot have multiplicative inverse For some field \\(F\\) take its additive identity \\(0 \\in F\\). Assume for the sake of contradiction there exists a multiplicative inverse for \\(0\\) named \\(0\u0026rsquo; \\in F\\).\nLet\u0026rsquo;s take some \\(a \\in F\\). By definition of the additive identity, we have:\n\\begin{equation} 0 + a = a \\end{equation}\nWe will apply \\(0\u0026rsquo;\\) to both sides, we having that:\n\\begin{equation} 0\u0026rsquo;(0+a) = 0\u0026rsquo;a \\end{equation}\nDistributing \\(0\u0026rsquo;\\) to both sides, we have:\n\\begin{equation} 1 + 0\u0026rsquo;a = 0\u0026rsquo;a \\end{equation}\nGiven \\(a,0\u0026rsquo; \\in F\\), and multiplication is closed in \\(F\\) being a field, \\(0\u0026rsquo;a \\in F\\); applying \\(-0\u0026rsquo;a \\in F\\) the additive inverse of the result of multiplying together to both sides, we have that:\n\\begin{equation} 1 + 0\u0026rsquo;a - 0\u0026rsquo;a = 0\u0026rsquo;a - 0\u0026rsquo;a \\end{equation}\nAnd therefore:\n\\begin{equation} 1 = 0 \\end{equation}\nwhich is absurd, reaching the desired contradiction. \\(\\blacksquare\\)\nSystem \\begin{equation} \\begin{cases} x + 2y + z = 0 \\\\ 2x + 0y - z = 1 \\\\ x - y + z = 2 \\\\ \\end{cases} \\end{equation}\nWe will subtract the top and bottom expressions to have that:\n\\begin{equation} 3y = -2 \\end{equation}\nAnd to get:\n\\begin{equation} y = \\frac{-2}{3} \\end{equation}\nManipulating the second expression, we have that:\n\\begin{equation} 2x -1 = z \\end{equation}\nSubstituting this expression and \\(y\\) into the third expression, we have:\n\\begin{equation} x + \\frac{2}{3} + 2x -1 = 2 \\end{equation}\nperforming algebraic manipulations:\n\\begin{align} \u0026amp;3x + \\frac{2}{3} = 3 \\\\ \\Rightarrow\\ \u0026amp;3x = \\frac{7}{3} \\\\ \\Rightarrow\\ \u0026amp;x = \\frac{7}{9} \\end{align}\nAnd finally:\n\\begin{equation} \\frac{14}{9}-1 = z = \\frac{5}{9} \\end{equation}\nMultiply \\begin{equation} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0\\end{pmatrix} \\begin{pmatrix} x \\\\ y\\\\ z \\end{pmatrix} = \\begin{pmatrix} x+2y+z \\\\ 2x-z \\\\ x-y \\end{pmatrix} \\end{equation}\nThe inner dimensions (column vs. row) of the matricies have to be the same for them to be multiplied; matrix multiplication is not commutative.\nProof: 2x2 Matrices with Real Entries form a Group Under Addition Closure \\begin{equation} \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp;d \\end{pmatrix} + \\begin{pmatrix} e \u0026amp; f \\\\ g \u0026amp; h \\end{pmatrix} = \\begin{pmatrix} a+e \u0026amp; b+f \\\\ c+g \u0026amp; d+h \\end{pmatrix} \\end{equation}\nIdentity \\begin{equation} \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp;d \\end{pmatrix} + \\begin{pmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{pmatrix} = \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{pmatrix} \\end{equation}\nInverse \\begin{equation} \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp;d \\end{pmatrix} + \\begin{pmatrix} -a \u0026amp; -b \\\\ -c \u0026amp; -d \\end{pmatrix} = \\begin{pmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{pmatrix} \\end{equation}\nAssociative \\begin{equation} \\left ( \\begin{pmatrix} x_1 \u0026amp; x_2 \\\\ x_3 \u0026amp; x_4 \\end{pmatrix} + \\begin{pmatrix} y_1 \u0026amp; y_2 \\\\ y_3 \u0026amp; y_4 \\end{pmatrix} \\right) + \\begin{pmatrix} z_1 \u0026amp; z_2 \\\\ z_3 \u0026amp; z_4 \\end{pmatrix} = \\begin{pmatrix} (x_1+y_1)+z_1 \u0026amp; (x_2+y_2)+z_2 \\\\ (x_3+y_3)+z_3 \u0026amp; (x_4+y_4)+z_4 \\end{pmatrix} \\end{equation}\nwhich is equal, by associativity in \\(\\mathbb{F}\\), as:\n\\begin{equation} \\begin{pmatrix} x_1+(y_1+z_1) \u0026amp; x_2+(y_2+z_2) \\\\ x_3+(y_3+z_3) \u0026amp; x_4+(y_4+z_4) \\end{pmatrix} \\end{equation}\nAnd finally, this is equal to:\n\\begin{equation} \\begin{pmatrix} x_1 \u0026amp; x_2 \\\\ x_3 \u0026amp; x_4 \\end{pmatrix} + \\left (\\begin{pmatrix} y_1 \u0026amp; y_2 \\\\ y_3 \u0026amp; y_4 \\end{pmatrix} + \\begin{pmatrix} z_1 \u0026amp; z_2 \\\\ z_3 \u0026amp; z_4 \\end{pmatrix} \\right) \\end{equation}\nWe have therefore shown that 2x2 matricies form a group under addition.\nProof: 2x2 Matrices with Real Entries does not from a Group Under Multiplication Inverse The matrix\n\\begin{equation} \\begin{pmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp;1 \\end{pmatrix} \\end{equation}\nis not invertable. In that, one cannot apply a matrix to this one to result in the multiplicative identity \\(I_2\\).\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_some_matrix_manipulation/","tags":null,"title":"NUS-MATH530 Some Matrix Manipulation"},{"categories":null,"contents":"We declare known battery voltage \\(E(t)\\).\nHere are the \\(y\\) values.\n\\begin{equation} \\begin{cases} \\dv{x_1}{t} = y_{4}\\\\ \\dv{x_2}{t} = y_{3}\\\\ \\dv{x_3}{t} = y_{1}\\\\ \\dv{x_4}{t} = y_{2}\\\\ \\end{cases} \\end{equation}\nAnd here are some of the \\(x\\) values.\n\\begin{equation} \\begin{cases} \\dv{x_4}{t}=-\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R} \\\\ \\dv{y_1}{t}=-\\frac{1}{LC}x_2-\\frac{E(t)}{C} \\\\ \\dv{y_4}{t} = -\\frac{R}{L}y_2-\\frac{2E(t)}{L} \\end{cases} \\end{equation}\nRight off the bat, we can see that we can make one substitution. That, given:\n\\begin{equation} \\begin{cases} \\dv{x_4}{t}=-\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R} \\\\ \\dv{x_4}{t} = y_{2} \\end{cases} \\end{equation}\nwe have that:\n\\begin{equation} y_2 = -\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R} \\end{equation}\nThis renders the last expression:\n\\begin{align} \\dv{y_4}{t} \u0026amp;= -\\frac{R}{L}y_2-\\frac{2E(t)}{L} \\\\ \u0026amp;= -\\frac{R}{L}\\qty(-\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R})-\\frac{2E(t)}{L} \\\\ \u0026amp;= \\qty(\\frac{2}{LC}x_2+\\frac{1}{LC}x_{3}+\\frac{2E(t)}{L})-\\frac{2E(t)}{L} \\\\ \u0026amp;= \\frac{2}{LC}x_2+\\frac{1}{LC}x_{3} \\end{align}\nSo now, we have the final unused expressions:\n\\begin{equation} \\begin{cases} \\dv{x_1}{t} = y_4 \\\\ \\dv{x_2}{t} = y_3 \\\\ \\dv{x_{3}}{t} = y_1 \\\\ \\dv{y_1}{t} = -\\frac{1}{LC}x_2-\\frac{E(t)}{C} \\\\ \\dv{y_4}{t} = \\frac{2}{LC}x_2+\\frac{1}{LC}x_3 \\end{cases} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_circuts/","tags":null,"title":"NUS-MATH570 Circuits"},{"categories":null,"contents":"We need to solve this system:\n\\begin{equation} \\begin{cases} \\dv{I}{t} = -0.73U + 0.0438 + 0.4 \\dv{M}{t} \\\\ \\dv{U}{t} = 0.4I - 0.012 \\\\ \\dv{G}{t} = \\dv{M}{t}- I \\\\ M(t)=0.02\\sin (1.15t + \\phi) \\end{cases} \\end{equation}\nTo be able to work on this, let us create some functions:\n# variable t, dm = var(\u0026#34;t dm\u0026#34;) # functions I = function(\u0026#34;_I\u0026#34;)(t) # _I because i is imaginary U = function(\u0026#34;U\u0026#34;)(t) G = function(\u0026#34;G\u0026#34;)(t) # parameter phi = var(\u0026#34;phi\u0026#34;, latex_name=\u0026#34;\\phi\u0026#34;) # our equations eqns = [ diff(I,t) == -0.73*U + 0.0438 + 0.4*dm, diff(U,t) == 0.4*I - 0.012, diff(G,t) == dm - I ] eqns desolve(eqns, U, ivar=t, algorithm=\u0026#34;fricas\u0026#34;).expand() Great, now, we will run the laplace transform upon these equations:\n# laplace variable s = var(\u0026#34;s\u0026#34;) # laplaced functions Fi = var(\u0026#34;Fi\u0026#34;) Fu = var(\u0026#34;Fu\u0026#34;) Fg = var(\u0026#34;Fg\u0026#34;) Fm = var(\u0026#34;Fm\u0026#34;) # constants I0, U0, G0, M0 = var(\u0026#34;I0 U0 G0 M0\u0026#34;) # substitution dictionary subs = { laplace(I,t,s): Fi, laplace(U,t,s): Fu, laplace(G,t,s): Fg, laplace(M,t,s): Fm, I(0): I0, G(0): G0, U(0): U0, M(0): M0, } # laplace eqns laplace_eqns = [i.laplace(t, s).subs(subs) for i in eqns] laplace_eqns \u0026lt;ipython-input-236-2a2ddfe91635\u0026gt;:20: DeprecationWarning: Substitution using function-call syntax and unnamed arguments is deprecated and will be removed from a future release of Sage; you can use named arguments instead, like EXPR(x=..., y=...) See http://trac.sagemath.org/5930 for details. I(Integer(0)): I0, \u0026lt;ipython-input-236-2a2ddfe91635\u0026gt;:21: DeprecationWarning: Substitution using function-call syntax and unnamed arguments is deprecated and will be removed from a future release of Sage; you can use named arguments instead, like EXPR(x=..., y=...) See http://trac.sagemath.org/5930 for details. G(Integer(0)): G0, \u0026lt;ipython-input-236-2a2ddfe91635\u0026gt;:22: DeprecationWarning: Substitution using function-call syntax and unnamed arguments is deprecated and will be removed from a future release of Sage; you can use named arguments instead, like EXPR(x=..., y=...) See http://trac.sagemath.org/5930 for details. U(Integer(0)): U0, \u0026lt;ipython-input-236-2a2ddfe91635\u0026gt;:23: DeprecationWarning: Substitution using function-call syntax and unnamed arguments is deprecated and will be removed from a future release of Sage; you can use named arguments instead, like EXPR(x=..., y=...) See http://trac.sagemath.org/5930 for details. M(Integer(0)): M0, [Fi*s - I0 == 0.4*Fm*s - 0.73*Fu - 0.4*M0 + 0.0438/s, Fu*s - U0 == 0.4*Fi - 0.012/s, Fg*s - G0 == Fm*s - Fi - M0, Fm == (0.02*s*sin(phi) + 0.023*cos(phi))/(s^2 + 1.3225)] And then, let us solve the Laplace solutions:\n# substitute laplace_solutions = solve(laplace_eqns, Fi, Fu, Fg, Fm, solution_dict=True)[0] laplace_solutions {Fi: 1/100*(80000*(125*I0 - 50*M0 + sin(phi))*s^4 - 2000*(3650*U0 - 46*cos(phi) - 219)*s^3 + 200*(66125*I0 - 26450*M0 + 438)*s^2 - 193085*(50*U0 - 3)*s + 115851)/(100000*s^5 + 161450*s^3 + 38617*s), Fu: 1/50*(5000000*U0*s^4 + 4000*(500*I0 - 200*M0 + 4*sin(phi) - 15)*s^3 + 100*(66125*U0 + 184*cos(phi) + 876)*s^2 + 26450*(100*I0 - 40*M0 - 3)*s + 115851)/(100000*s^5 + 161450*s^3 + 38617*s), Fg: 1/100*(200000*(50*G0 - 50*M0 + sin(phi))*s^5 - 10000*(1000*I0 - 400*M0 - 23*cos(phi) + 8*sin(phi))*s^4 + 200*(80725*G0 - 80725*M0 + 36500*U0 - 460*cos(phi) + 292*sin(phi) - 2190)*s^3 - 40*(330625*I0 - 132250*M0 - 1679*cos(phi) + 2190)*s^2 + 193085*(20*G0 - 20*M0 + 50*U0 - 3)*s - 115851)/(100000*s^6 + 161450*s^4 + 38617*s^2), Fm: 2/5*(20*s*sin(phi) + 23*cos(phi))/(400*s^2 + 529)} Now we inverse Laplace transform:\nI_s(t) = inverse_laplace(laplace_solutions[Fi], s, t) U_s(t) = inverse_laplace(laplace_solutions[Fu], s, t) G_s(t) = inverse_laplace(laplace_solutions[Fg], s, t) M_s(t) = inverse_laplace(laplace_solutions[Fm], s, t) (I_s,U_s,G_s,M_s) (t |--\u0026gt; -1/2061000*sqrt(730)*(103050*U0 + 368*cos(phi) - 6183)*sin(1/50*sqrt(730)*t) + 1/1030500*(1030500*I0 - 412200*M0 - 2336*sin(phi) - 30915)*cos(1/50*sqrt(730)*t) + 529/51525*cos(23/20*t)*sin(phi) + 529/51525*cos(phi)*sin(23/20*t) + 3/100, t |--\u0026gt; 1/37613250*sqrt(730)*(1030500*I0 - 412200*M0 - 2336*sin(phi) - 30915)*sin(1/50*sqrt(730)*t) + 1/103050*(103050*U0 + 368*cos(phi) - 6183)*cos(1/50*sqrt(730)*t) - 184/51525*cos(phi)*cos(23/20*t) + 184/51525*sin(phi)*sin(23/20*t) + 3/50, t |--\u0026gt; -1/15045300*sqrt(730)*(1030500*I0 - 412200*M0 - 2336*sin(phi) - 30915)*sin(1/50*sqrt(730)*t) - 1/41220*(103050*U0 + 368*cos(phi) - 6183)*cos(1/50*sqrt(730)*t) + 1/103050*(920*cos(phi) + 2061*sin(phi))*cos(23/20*t) + 1/103050*(2061*cos(phi) - 920*sin(phi))*sin(23/20*t) + G0 - M0 + 5/2*U0 - 3/100*t - 3/20, t |--\u0026gt; 1/50*cos(23/20*t)*sin(phi) + 1/50*cos(phi)*sin(23/20*t)) Some plots.\nI_specific = I_s.subs(I0=0.024, U0=0.039, M0=0, G0=0, phi=2.35) U_specific = U_s.subs(I0=0.024, U0=0.039, M0=0, G0=0, phi=2.35) G_specific = G_s.subs(I0=0.024, U0=0.039, M0=0, G0=0, phi=2.35) M_specific = M_s.subs(I0=0.024, U0=0.039, M0=0, G0=0, phi=2.35) plot(I_specific, t, 0, 10, color=\u0026#34;blue\u0026#34;) + plot(U_specific, t, 0, 10, color=\u0026#34;orange\u0026#34;) + plot(G_specific, t, 0, 10, color=\u0026#34;green\u0026#34;) + plot(M_specific, t, 0, 10, color=\u0026#34;red\u0026#34;) /Users/houliu/.sage/temp/baboon.jemoka.com/16964/tmp_sei9raar.png ","permalink":"https://www.jemoka.com/posts/kbhnus_math570_finance/","tags":null,"title":"NUS-MATH570 Finance (Laplace)"},{"categories":null,"contents":"We have:\n\\begin{equation} \\frac{2y^{2}}{9-x^{2}} + y \\dv{y}{x} + \\frac{3y}{2-x} = 0 \\end{equation}\nWe want to get rid of things; let\u0026rsquo;s begin by dividing the whole thing by \\(y\\).\n\\begin{equation} \\frac{2y}{9-x^{2}} + \\dv{y}{x} + \\frac{3}{2-x} = 0 \\end{equation}\nFinally, then, moving the right expression to the right, we have:\n\\begin{equation} \\frac{2y}{9-x^{2}} + \\dv{y}{x} = \\frac{-3}{2-x} \\end{equation}\nIn this case, we have functions:\n\\begin{equation} \\begin{cases} P(x) = \\frac{2}{9-x^{2}}\\\\ Q(x) = \\frac{-3}{2-x}\\\\ \\end{cases} \\end{equation}\nTaking first the top integral:\n\\begin{equation} \\int \\frac{2}{9-x^{2}} \\dd{x} = \\frac{1}{3} \\log \\qty(\\frac{x+3}{3-x}) \\end{equation}\nRaising \\(e\\) to that power, we have that:\n\\begin{equation} \\sqrt[3]{e\\frac{x+3}{3-x}} \\end{equation}\nMultiplying \\(Q(x)\\) to that expression, we have that:\n\\begin{equation} \\int \\frac{-3}{2-x}\\sqrt[3]{e\\cdot \\frac{x+3}{3-x}} \\dd{x} \\end{equation}\nTherefore, our entire answer is defined as the integral function that:\n\\begin{equation} y = \\frac{1}{\\sqrt[3]{e\\cdot \\frac{x+3}{3-x}} } \\int \\frac{-3}{2-x}\\sqrt[3]{e\\cdot \\frac{x+3}{3-x}} \\dd{x} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_problem_set_1/","tags":null,"title":"NUS-MATH570 Problem Set 1"},{"categories":null,"contents":"Considering the system:\n\\begin{equation} \\begin{cases} \\dv{x}{t} = -2x+y+(1-\\sigma)z \\\\ \\dv{y}{t} = 3x-y \\\\ \\dv{z}{t} = (3-\\sigma y)x-z\\\\ \\end{cases} \\end{equation}\nwith the initial locations \\((x_0, y_0, z_0)= (-1,1,2)\\).\nWe notice first that the top and bottom expressions as a factor in \\(x\\) multiplied by \\(y\\), which means that our system is not homogenous. Let\u0026rsquo;s expand all the expressions first.\n\\begin{equation} \\begin{cases} \\dv{x}{t} = -2x+y+(1-\\sigma)z \\\\ \\dv{y}{t} = 3x-y \\\\ \\dv{z}{t} = 3x-\\sigma yx-z\\\\ \\end{cases} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_problem_set_2/","tags":null,"title":"NUS-MATH570 Problem Set 2, Problem 1"},{"categories":null,"contents":"Intersects:\n\\begin{equation} f(x) = (x+c)^{2} \\end{equation}\n\\begin{equation} h(x) = c x \\end{equation}\nDoesn\u0026rsquo;t Intersect:\n\\begin{equation} g(x) = c e^{\\frac{x^{4}}{4}}} \\end{equation}\n\\begin{align} \u0026amp;h_1(x)-h_2(x) = c_1x-c_2x \\\\ \\Rightarrow\\ \u0026amp; 0 = c_1x-c_2x \\\\ \\Rightarrow\\ \u0026amp; 0 = x(c_1-c_2) \\end{align}\n\\begin{align} \u0026amp;g_1(x)-g_2(x) = c_1e^{\\frac{x^{4}}{4}} - c_2e^{\\frac{x^{4}}{4}} \\\\ \\Rightarrow\\ \u0026amp; 0 = \\qty(c_1 - c_2)e^{\\frac{x^{4}}{4}} \\\\ \\Rightarrow\\ \u0026amp; 0 = e^{\\frac{x^{4}}{4}}(c_1-c_2) \\end{align}\n\\begin{align} \u0026amp; f_1(x)-f_2(x)=(x+c_1)^{2}-(x+c_2)^{2} \\\\ \\Rightarrow\\ \u0026amp; 0 = (x+c_1)^{2}-(x+c_2)^{2} \\\\ \\Rightarrow\\ \u0026amp; 0 = 2x(c_1-c_2)+{c_1}^{2}+{c_2}^{2} \\end{align}\n\\begin{equation} \\dv{y}{x} + P\u0026rsquo;(x)y = Q\u0026rsquo;(x) \\end{equation}\n\\begin{align} \u0026amp;y = e^{\\int P\u0026rsquo;(x)\\dd{x}} \\int e^{\\int P\u0026rsquo;(x)\\dd{x}} Q\u0026rsquo;(x)\\dd{x} \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)} \\int e^{P(x)}Q\u0026rsquo;(x)\\dd{x} \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)} (\\dots+C) \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)}C + \\dots \\end{align}\n\\begin{equation} h(x) \\in e^{-P(x)}C + \\dots \\end{equation}\n\\begin{equation} g(x) \\in e^{-P(x)}C + \\dots \\end{equation}\n\\begin{align} \u0026amp;0 = (e^{-P(x)}C_1+\\dots)-(e^{-P(x)}C_2 + \\dots) \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)}C_1-e^{-P(x)}C_2 \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)}(C_1-C_2) = 0 \\end{align}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_research_question_1/","tags":null,"title":"NUS-MATH570 Research Question 1"},{"categories":null,"contents":"We are given a set of expressions:\n\\begin{equation} \\begin{cases} \\dv{x}{t} = \\frac{x}{w}\\\\ \\dv{y}{t} = \\frac{xz}{w} \\\\ \\dv{z}{t} = \\beta y \\\\ \\dv{w}{t} = \\beta y \\end{cases} \\end{equation}\nWe are asked to analyze the solutions to this system, its periodicity, etc.\nStability Analysis The immediate thing to do is to shove all of this into a Jacobian matrix\u0026mdash;not for linearnalization, but to check how the slope changes. We will take the eigenvalues of the matrix at the critical points of the function, which will tell us whether or not the functions converge or diverge from those points.\nLet\u0026rsquo;s go about doing that. Let us declare:\n\\begin{equation} \\begin{cases} \\dv{x}{t} = \\frac{x}{w} = f(x,y,z,w)\\\\ \\dv{y}{t} = \\frac{xz}{w} = g(x,y,z,w)\\\\ \\dv{z}{t} = \\beta y = h(x,y,z,w)\\\\ \\dv{w}{t} = \\beta y = j(x,y,z,w) \\end{cases} \\end{equation}\nThen, the Jacobian is (with each cell being \\(\\dv{row}{column}\\)):\ndx dy dz dw df 1/w 0 0 -x/w^2 dg z/w 0 0 -(xz)/w^2 dh 0 beta 0 0 dj 0 beta 0 0 Properly writing that out, this means that:\n\\begin{equation} J = \\mqty(\\frac{1}{w} \u0026amp; 0 \u0026amp; 0 \u0026amp; -\\frac{x}{w^{2}} \\\\ \\frac{z}{w} \u0026amp; 0 \u0026amp; 0 \u0026amp; -\\frac{xz}{w^{2}} \\\\ 0 \u0026amp; \\beta \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\beta \u0026amp; 0 \u0026amp; 0) \\end{equation}\nNow, let us solve for the critical points of this expression. Setting all expressions to \\(0\\):\n\\begin{equation} \\begin{cases} f = 0 \\\\ g = 0 \\\\ h = 0 \\\\ j = 0 \\end{cases} \\end{equation}\nwe have that:\n\\begin{equation} f(\\dots) = \\frac{x}{w} = 0 \\end{equation}\nso \\(x=0\\).\nWe have also:\n\\begin{equation} h(\\dots) = \\beta y= 0 \\end{equation}\ntherefore, \\(y = 0\\).\nEverything else is a free variable.\nSubstituting that into our expressions, we have:\n\\begin{equation} J* = \\mqty(\\frac{1}{w} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\frac{z}{w} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\beta \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\beta \u0026amp; 0 \u0026amp; 0) \\end{equation}\nWe are now ready to eigenvalueize. Using technology:\nw,z,b = var(\u0026#34;w z b\u0026#34;) M = matrix([[1/w, 0,0,0], [z/w,0,0,0], [0,b,0,0], [0,b,0,0]]) M [1/w 0 0 0] [z/w 0 0 0] [ 0 b 0 0] [ 0 b 0 0] So, the moment of truth:\nM.eigenvalues() [1/w, 0, 0, 0] Excellent, so we have two eigenvalues: \\(\\frac{1}{w}\\) and \\(0\\). The \\(0\\) eigenvalue indicates to us that the system has \u0026ldquo;neutral stability\u0026rdquo;: that there will be results for which our system: while not exponentially increasing towards asymptotically, does not settle to a stable point.\nBehavior to Extrema The next natural question is then\u0026mdash;even if our system doesn\u0026rsquo;t settle down, does it become larger over time? For this, we turn to the \\(\\frac{1}{w}\\) term. If our initial conditions render negative \\(w\\) eventually at that point, our system will converge to unstable but containable (i.e. does not go to infinity over time); otherwise, it does becomes unstable AND uncontainable (goes to infinity.)\nTo do this, we need to check two things; regrettably, it seems like we could\u0026rsquo;t end up with a properly described solution to evolve our variables analytically. However, we can leverage the Lipschitz condition and hand-fisted dimensional analysis to clue us in about the behavior of the system.\nContinuity Recall again that our system is:\nWe are given a set of expressions:\n\\begin{equation} \\begin{cases} \\dv{x}{t} = \\frac{x}{w}\\\\ \\dv{y}{t} = \\frac{xz}{w} \\\\ \\dv{z}{t} = \\beta y \\\\ \\dv{w}{t} = \\beta y \\end{cases} \\end{equation}\nTo check the Lipschitz Continuity is actually not super difficult. Research indicates that the Litpschitz condition extends in the expected manner into multiple dimensions, checking continuity with a partial in each direction.\nThe actual partials of the terms on the right, though, are really only discontinuous in this case when we have something under a fraction\u0026mdash;there is fortunately no weird exponential/log/sinusoidal/radical here. Evidently, then, we loose Lipschitz continuity at \\(w=0\\). As long as we don\u0026rsquo;t cross that line, anything to the left or right of it exists and is unique(?) is each dimension.\nHam-fisting Dimensional Analysis The initial conditions asks us for starting with \\(w(0)=5\\sqrt 5\\). Recall that we are interested in the value of \\(w\\) at \\((x,y)=(0,0)\\).\nFurthermore, recall the Lipschitz condition we discussed above. That the function is Lipschitz continuous at two boundary intervals: between \\((-\\infty, 0)\\) and \\((0, \\infty )\\). Starting at the conditions of \\(w(0) = 5\\sqrt{5}\\) indicates that there will be no way for \\(w\\) to cross into \\(\\frac{1}{w} \u0026lt;0\\) territory.\nNote, again, that the eigenvalues of the Jacobian of the system are \\(\\{0, \\frac{1}{w}\\}\\), therefore, a positive \\(\\frac{1}{w}\\) will indicate that the system tends towards infinity as there is one positive eigenvalue.\nHowever, if we started at a negative \\(w\\) in the first place, we will equally be unable to use the same initial conditions to cross into \\(\\frac{1}{w} \u0026gt; 0\\) territory. Because of this, conditions that begin with negative \\(w\\) will be unstable but not asymptotically increasing as there will be no positive eigenvalues of its Jacobian at any given point.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_supply_demand/","tags":null,"title":"NUS-MATH570 Supply Demand"},{"categories":null,"contents":" Instruments Effects Feeling Dynamics Process/Production 01/09/2023 ","permalink":"https://www.jemoka.com/posts/kbhnus_mus150_critical_listening/","tags":null,"title":"NUS-MUS150 Critical Listening"},{"categories":null,"contents":"Vocabulario alejar forzar el fracaso ocultar atemorizarte prescindir exige asegurar Preguntas ¿Hay problemas a largo plazo con el sistema del calificación? ¿Como medimos los éxitos del sistema nueva sin prejuicios las preferencias propias de los estudiantes? ¿Necesita estudiantes motivados para el desarrollo complete del sistema? ¿Hay diferencias sociocultural que puede influir los resultados o el implementación del sistema? ¿Cómo conducta examines del rendimiento de los estudiantes a través escuelas con implementaciones diferencies del sistema? ","permalink":"https://www.jemoka.com/posts/kbhnus_span502_tarea_2/","tags":null,"title":"NUS-SPAN502 Tarea 2"},{"categories":null,"contents":"Vocabularios Nuevos creciente jornada exigir desempeño subir Preguntas ¿En el primer lugar, porqué tenemos semanas de cinco días? ¿Para los ciudades con recursos de educación abundante, hay un necesario real de trabar en jornadas de cuatro días? ¿Tenemos de verdad un sistema responsable para examinar los diferencias de cantidad de educación a través de el palmo entero del proceso de educación de un estudiante? ¿Existen presiones políticas que motivó el propósito? ¿En realidad, existe un problema muy fundamental que causó los problemas que vemos hoy? ","permalink":"https://www.jemoka.com/posts/kbhnus_span502_tarea_4/","tags":null,"title":"NUS-SPAN502 Tarea 4"},{"categories":null,"contents":" Lógico Sentimiento Acuerdo Cancelado/a Cuentas Censurado/a Plataforma Fraces Libertad de Expresión ","permalink":"https://www.jemoka.com/posts/kbhnus_span502_vocab/","tags":null,"title":"NUS-SPAN502 Vocab"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhobjects/","tags":null,"title":"object"},{"categories":null,"contents":"We have an expression:\n\\begin{equation} B = \\frac{FL^{3}}{3EI} = \\frac{N m^{3}}{3 p m^{4}} = \\frac{Nm^{3}}{\\frac{N}{m^{2}}m^{4}} = m \\end{equation}\nWith constants:\n\\(B\\): \\(m\\), deflection at the point of force application \\(F\\): \\(N\\), force applied \\(L\\): \\(m\\), distance between fixed point and point of force application \\(E\\): \\(p=\\frac{N}{m^{2}}\\), elastic modulus \\(I\\): \\(m^{4}\\), second moment of area As per measured:\n\\(B\\): \\(9.15 \\cdot 10^{-4} m\\) \\(F\\): \\(20N\\) \\(L\\): \\(9.373 \\cdot 10^{-2} m\\) \\(I\\): \\(1.37 \\cdot 10^{-10} m^{4}\\) = \\(\\frac{WH^{3}}{12}\\) = \\(\\frac{(6.25 \\cdot 10^{-3})(6.4 \\cdot 10^{-3})^{3}}{12}\\) Theoretical:\n\\(E\\): \\(7 \\cdot 10^{10} P\\) As calculated:\n\\(B\\): \\(5.74 \\cdot 10^{-4} m\\) ","permalink":"https://www.jemoka.com/posts/kbhone_shot_deformation/","tags":null,"title":"One-Shot Deformation"},{"categories":null,"contents":"The Open Voice Brain Model is a audio processing architecture proposed by Laguarta 2021 for audio/biomarker correlation work.\nHere\u0026rsquo;s a fairly self-explanatory figure:\nThe model outputs an AD diagnoses as well as a longitudinal correlation with Memory, Mood, and Respiratory biomarkers.\nThis is then the embedding that they are proposing for use by other tasks.\n","permalink":"https://www.jemoka.com/posts/kbhopen_voice_brain_model/","tags":null,"title":"Open Voice Brain Model"},{"categories":null,"contents":"OpenSMILE is a proprietary audio feature exaction tool.\nSite.\n","permalink":"https://www.jemoka.com/posts/kbhopensmile/","tags":null,"title":"OpenSMILE"},{"categories":null,"contents":" adding multiplying This is object dependent.\n","permalink":"https://www.jemoka.com/posts/kbhoperation/","tags":null,"title":"operation"},{"categories":null,"contents":"Richard Nixon bombs Vietnam for 13 days to beat the VietCong into submission after the Vietnam War.\n","permalink":"https://www.jemoka.com/posts/kbhoperation_linebacker/","tags":null,"title":"Operation Linebacker"},{"categories":null,"contents":"A Linear Map from a vector space to itself is called an operator.\n\\(\\mathcal{L}(V) = \\mathcal{L}(V,V)\\), which is the set of all operators on \\(V\\).\nconstituents a vector space \\(V\\) a Linear Map \\(T \\in \\mathcal{L}(V,V)\\) requirements \\(T\\) is, by the constraints above, an operator additional information injectivity is surjectivity in finite-dimensional operators Suppose \\(V\\) is finite-dimensional and \\(T \\in \\mathcal{L}(V)\\), then, the following statements are equivalent:\n\\(T\\) is invertable \\(T\\) is injective \\(T\\) is surjective THIS IS NOT TRUE IN infinite-demensional vector space OPERATORS! (for instance, backwards shift in \\(\\mathbb{F}^{\\infty}\\) is surjective but not injective.)\nProof:\nFrom the above, \\(1 \\implies 2\\) by definition of invertability.\nThen, we have that \\(T\\) is invertable. We desire that \\(T\\) is surjective. Given invertability, we have that \\(\\null T = \\{0\\}\\). By the rank-nullity theorem, we have that: \\(\\dim V = \\dim range\\ T + \\dim null\\ T = \\dim range\\ T +0= \\dim range\\ T\\). Now, given \\(T\\) is an operator, we have that \\(range\\ T \\subset V\\). Attempting to extend a basis of \\(range\\ T\\) (which, given it is a subspace of \\(V\\), is a linearly independent list in \\(V\\)) to a basis of \\(V\\) will be the trivial extension. So \\(range\\ T = V\\), which is also the codomain of \\(T\\). This makes \\(T\\) surjective, as desired. So \\(2 \\implies 3\\).\nNow, we have that \\(T\\) is surjective, we desire that \\(T\\) is invertable. We essentially reverse-engineer the step before. Given rank-nullity theorem, we have that: \\(\\dim V = \\dim range\\ T + \\dim null\\ T\\). Now, given \\(T\\) is surjective, \\(\\dim range\\ T = \\dim V\\). Therefore, we have that \\(\\dim V = \\dim V + \\dim null\\ T \\implies 0 = \\dim null\\ T\\). This makes the null space of \\(T\\) be \\(\\{0\\}\\). This makes \\(T\\) injective. Having shown \\(T\\) to be both surjective and injective, \\(T\\) is invertable, as desired. So \\(3 \\implies 1\\).\nHaving shown a loop in the statements, all of them are equivalent.\n","permalink":"https://www.jemoka.com/posts/kbhoperator/","tags":null,"title":"operator"},{"categories":null,"contents":"options are derivatives which gives you the permission to make a transaction at a particular date.\nThere are two main types of options:\ncall: gives permission to buy a security on or before the \u0026ldquo;exercise\u0026rdquo; date puts: gives permission to sell a security on or before the \u0026ldquo;exercise\u0026rdquo; date For this article, we will define \\(S_{t}\\) to be the stock price at the time \\(t\\), \\(K\\) as the option\u0026rsquo;s strike price, \\(C_{t}\\) to be the price of the \u0026ldquo;call\u0026rdquo; option, and \\(P_{t}\\) to be the price of the \u0026ldquo;put\u0026rdquo; option at strike price \\(K\\); lastly \\(T\\) we define as the maturity date.\nNaturally, the actual values \\(C_{t}\\) and \\(P_{t}\\) are:\n\\begin{equation} \\begin{cases} C_{t} = Max[0, S_{T}-K] \\\\ P_{t} = Max[0, K-S_{T}] \\\\ \\end{cases} \\end{equation}\nyou either make no money from the option (market price is more optimal), or make some difference between the strike price and the market price.\nThe nice thing here is that little \\(Max\\) term. An option, unlike a futures contract, has no buying obligation: you don\u0026rsquo;t have to exercise it. The payoff is always non-negative!\nNOTE!!! \\(C_{t}\\) at SMALL \\(t\\) is measured at \\(Max[0,S_{*T*}, K}]\\), using \\(S\\) of LARGE \\(T\\). This is because, even when\u0026mdash;currently\u0026mdash;the stock is trading at $60, the right to buy the stock in \\(T\\) months for $70 is not worthless as the price may go up.\nTo analyze options, we usually use the Black-Scholes Formula.\nAmerican vs European Options American options are excercisable at or before the maturity date. European options are exrcercisable only at the maturity date. Analyze Options as Insurance All insurance contracts are actually a form of an option, so why don\u0026rsquo;t we analyze it as such?\nA put option\u0026mdash;-\nAsset insured: stock Current asset value: \\(S_{0}\\) Term of policy: \\(T\\) Maximum coverage: \\(K\\) Deductible: \\(S_0-K\\) Insurance premium: \\(P_{t}\\) A call option is covariant with a put option; so its isomorphic, and so we will deal with it later.\nA few differences:\nAmerican-style early exercise: (you can\u0026rsquo;t, for normal insurance, exercise it without something happening) Marketability: you can\u0026rsquo;t give normal insurance to other people Dividends: holding a stock pays dividends (an option\u0026rsquo;s value goes down as dividends) ","permalink":"https://www.jemoka.com/posts/kbhoptions/","tags":null,"title":"option"},{"categories":null,"contents":"oral lexical retrival is a class of discourse tasks which asks the subject to convert some semantic understanding (\u0026ldquo;concept\u0026rdquo;) into lexical expressions (\u0026ldquo;words\u0026rdquo;)\n\u0026ldquo;ask a patient to describe a thing.\u0026rdquo;\nExamples of oral lexical retrieval:\nSVF BNT Source: CambridgeCore\n","permalink":"https://www.jemoka.com/posts/kbhoral_lexical_retrival/","tags":null,"title":"oral lexical retrieval"},{"categories":null,"contents":"Reading notes conservatives in America make less sense because America is supposed to be liberal/new For most Europeans who came to America, the whole purpose of their difficult and dis- ruptive journey to the New World was not to conserve European institutions but to leave them behind and to create something new, often an entirely new life\nThree splits of conservatism in America those who are most concerned about economic or fiscal issues, that is, pro-business or “free-enterprise” conservatives those most concerned with religious or social issues, that is, pro-church or “traditional-values” conservatives those most concerned with national-security or defense issues, that is, pro-military or “patriotic” conservatives Ronald Reagan unified the three conservatism It was the achievement of Ronald Reagan that he was able in the late 1970s to unite these three different kinds of conservatism into one grand coalition.\nThree-in-one conservatism is a part of American \u0026ldquo;fusionist strategy\u0026rdquo; This was the culmination of a “fusionist strategy” that had been developing amongst American conservatives since the early 1960s.\nBusiness and social conservatism should contradict each other, though However, as we shall see, pro-business conservatism has always included a tendency toward the disruption and even dissolution of religious ideals and social practices.\nExtreme pro-business should also include globalization and erasure of national identities And in recent decades, pro-business conservatism has also included a tendency toward the dismantling of national boundaries and even dissolution of national identities\n\u0026ldquo;conservatism\u0026rdquo; actually conserved American revolutionary force economically this means that the conservative party in America has always sought to conserve a revolutionary force.\nExtreme economic \u0026ldquo;conservatism\u0026rdquo; should destry social and moral arrangements It destroys religious, social, and ultimately moral arrangements as well.\nReligions conservatism founds on the \u0026ldquo;open-market\u0026rdquo; of protestanism This open market in religious matters, so nicely isomorphic with the open market in economic matters, was a powerful factor gen- erating both a reality and an ideology of free choice in the United States.\nBecause the \u0026ldquo;new\u0026rdquo; became new protestanism, religious conservatism is the re-take over of traditional religions Since these churches were continually being left behind, religious conservatism was associated with once-dominant churches that were now dwindling into a minority, and would later dwindle into marginality\nBecause of mass economic benifit, religous conservatism became subordinated to economic conservatism Even ordinary middle-class Protestants benefited from cheaper labor, in the form of domestic servants. And of course it was the businessmen and middle-class Protestants who controlled the political parties, particularly that party which was supposed to be the more conservative one\nBecause there is nothing to conserve about current system, the thing that\u0026rsquo;s conserved is free choice If something were going to be conserved, it would normally be the no-conscription and low-taxation (and free-choice) system.\nEconomic systems propergated the source of American patriotism This meant that people who thought of themselves as American patriots or nationalists, and who sought to conserve the American nation and to promote American national interests\nAmerican conservatism is actually a form of European liberalism we have seen that, from a European perspective, American conservatism was not conservative at all, but actually was a kind of classical lib- eralism.\nwartime strengthened American values and liberalism Moreover, the wartime experience seemed decisively to vindicate and even enhance the strengths of both the traditional American economic system and traditional American moral principles.\n","permalink":"https://www.jemoka.com/posts/kbhrise_of_american_conservatism/","tags":null,"title":"Origins of American Conservatism"},{"categories":null,"contents":"The OTC Markets/pink sheets are an unregulated group of Financial Markets, where many of the Penny stocks are.\n","permalink":"https://www.jemoka.com/posts/kbhotc_markets/","tags":null,"title":"OTC Markets"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhpacific_railroad_act/","tags":null,"title":"Pacific Railroad Act"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2020.605317\nOne-Liner An excercize scheme has had some measured effect on theta/alpha ratio and Brain wave frequency on AD patients; prognosis of AD not controlled for.\nNovelty Leveraged physical training scheme and measured EEG effects by quantifying theta/alpha ratio Notable Methods Used theta/alpha ratio as assay for improvement, and found the exercise scheme did so p\u0026lt;0.05 Only tested patients with AD w/o a control for stage Key Figs Figure 1 This figure tells us th N number of participants through the study\nFigure 2 This figure shows us that the excercize intervention has statistically significant results to both Brain Oscillation frequency and Theta/Alpha ratio. The x-axis shows us the pre-and-post bars for TG (treatment) and CG (control); the y-axis quantifies the value measured in a box plot. The subplots are brain oscelation and theta/alpha ratio respectively.\nNew Concepts theta/alpha ratio Notes ","permalink":"https://www.jemoka.com/posts/kbhparvin_2020/","tags":["ntj"],"title":"Parvin 2020"},{"categories":null,"contents":" No Demo Day TODO Email need statement template Needfinding Not all patients want to be treated the same way Attitudes towards heathcare system Fostering strong interaction; facilitate interaction Problem: patients have attitudes that physicians can\u0026rsquo;t effectively communicate.\nAction item: interview doctors and patients\nNeed two need statement.\n","permalink":"https://www.jemoka.com/posts/kbhpcp_april_checkin/","tags":null,"title":"PCP April Checkin"},{"categories":null,"contents":"permittivity of free space is a constant \\(\\epsilon_{0} \\approx 8.85 \\times 10^{-12} \\frac{C^{2}}{N \\cdot m^{2}}\\).\nredefinition of Coulomb\u0026rsquo;s Constant based on permittivity of free space \\begin{equation} k = \\frac{1}{4\\pi \\epsilon_{0}} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhpermittivity_of_free_space/","tags":null,"title":"permittivity of free space"},{"categories":null,"contents":"We will leverage atoms as qubits. So, how do we isolate a qubit from an atom? We will leverage electrons.\nWe will select the lowest energy state as the base state; as there maybe multiple ground states, we will choose \\(|u\\big\u0026gt;\\) and \\(|d\\big\u0026gt;\\) from two of the states.\n","permalink":"https://www.jemoka.com/posts/kbhphysical_qubits/","tags":null,"title":"physical qubits"},{"categories":null,"contents":"physics is the act of explaining what we see in terms of solving for the \u0026ldquo;unseen\u0026rdquo;. For an explanation to be good, it needs to be testable.\nHow exactly does physics work? \u0026ldquo;classical results\u0026rdquo;\nNewton\u0026rsquo;s laws Maxwell\u0026rsquo;s equations General relativity \u0026ldquo;quantum theory\u0026rdquo;\nA new model that actually allows particle inference.\n","permalink":"https://www.jemoka.com/posts/kbhphysics/","tags":null,"title":"physics"},{"categories":null,"contents":"User Story Sejin is the executive administrative assistant at Nueva, working on scheduling Liza, Lee, Terry, and the other admins against the members of the wider community. Sejin spends most of her day scheduling people, of which, the largest time drawn is spent convincing people to move their schedules \u0026ldquo;in favor\u0026rdquo; of that of another person (i.e. manually). The reason why this is done is because her approach to scheduling is one-shot: emailing everybody for general availability, noting in her mind who the high-priority attendees are (say, Liza), and if no times match asking/convincing those in lower priority to move their schedules. Although she enjoys the process of putting events together, she is particularly frustrated that, due to the busy schedules and often back-and-forth emails needed to get and conform everyone\u0026rsquo;s schedule, response rates to complicated scheduling problems are low.\nSejin, during a main brunt of her job of scheduling inter or intra-admin meetings, need a solution to schedule many executives at once with attention to their priority/authority/importance to a meeting as well as the possible fluidity of their schedules. There is an inherit fluidity to scheduling as a master-planner of a few admin\u0026rsquo;s schedules: in that, if needed, she has authority to move entire meetings as long as they are swapped for equivalent times of availability. Hence, a previously bad time may suddenly become available if enough scheduling conflicts is generated, thereby creating the incentive for swapping another meeting away for the one being scheduled and rescheduling other attendees of lower priority.\nCurrent scheduling software does not account for either types of fluidity. Tools like Doodle/When2Meet can accommodate for inherent \u0026ldquo;priority\u0026rdquo;\u0026mdash;with Sejin choosing the time-slot that would have the most, highest priority individuals scheduled\u0026mdash;but are one-shot planning tools which do not provide space for swapping entire meetings out to make scheduling work better. Other tools like Calendly or simple iCal does not provide any semblance of priority or \u0026ldquo;multi-possibility\u0026rdquo; for meetings, though does indeed provide the time-blocking capability to swap two meetings at will. These problems result in Sejin needing to just create large email chains to resolve scheduling problems. Also, no scheduling tools provide an opportunity to manually \u0026ldquo;convince\u0026rdquo; or request someone to make time due to the constraints that presented during first-round scheduling. Lastly, scheduling software does not space-block. Sometimes there is a physical capacity/de-duplication limit to spaces, which cannot be accounted for.\nOnce the initial scheduling and emailing processes are automated, Sejin can spend more time focusing on what she actually enjoys: thinking about the process of an event and its details. Schedule can now be an afterthought, an event which happens in the background which is eventually reported to her on the online portal as she is planning the details of the event.\nProposal Fundamentally, this is a fractional knapsack problem. \u0026ldquo;How do we maximize the maximum amount of attendance of maximum amounts of important people?\u0026rdquo;\nFrom a target market, I think a good target would be medium organization assistants: Sejin\u0026rsquo;s concerns really only become a problem when you are scheduling for a one (or few) vs. many situation where there is a stable group of people you are scheduling for, who wants to meet with each other or other people outside.\nAs far as UX, this tool should not require log-in except for the master planner (i.e. our user.) Participants in meeting should be able to freely enter their schedules or create evergreen accounts to manage their scheduling. (This is not as well thought out at the moment.)\nLastly, for the tech stack, I don\u0026rsquo;t think I have the ability to finish the entire stack by myself. From a MVP perspective (if we are trying to satisfy all needs), there needs to be a system optimizing a constantly shifting fractional knapsack, a way to put and store availability information, and a way to automate the requesting/convincing of scheduling change (e.g.. \u0026ldquo;MyApp Notification! Liza is not available at the one time you selected, but everyone else is available at this different time. Can you make this time? Y/N\u0026rdquo;) . Ideally, we would also send iCal invites in the end.\n","permalink":"https://www.jemoka.com/posts/kbhpitch_a_project/","tags":null,"title":"Pitch a Project"},{"categories":null,"contents":"A PKM is a tool (like this one!) to help manage your nodes and knowledge.\nstoring AND processing; just storing is just PIM not PKM PKM = PIM (personal info management) + GTD + knowledge management goal: narrowing \u0026ldquo;flood\u0026rdquo; and focus on useful areas move from passive =\u0026gt; active consumption create, not just regurgitate PKM and Context store info based on context Strategy What are you trying to organize what are the inputs? email? lectures?: \u0026ldquo;reference\u0026rdquo; type emails lecture notes tasks? what are you trying to make? test? essays? a time blocking schedule studying for tests (i.e. for linear, etc.) what does your process look like? mishmash of topic based notes and content based notes what parts of it would you like to improve? article capture workflow: how to process random readings topic wise? knowledge capture: how to come across new content and capture them somewhere? todo/notes integration: task management and note taking currently lives separately, how to unify them? zettlekasten\nhow to take smart notes: arens progressive summarization progressive summarization is a technique in note taking that Tiago Forte developed to summarize text:\nlayer 0: reading layer 1: copy/pasted parts from the reading layer 2: bold relevant parts layer 3: highlight bold parts to get crux of ideas layer 4: mini summary layer 5: remix (add links, etc.) capture read rewrite and summarize engage by adding questions, thoughts, opinions, etc. connect old and new ideas think about the context of usage, not the topic how to think better reduce cognitive overload \u0026ldquo;offload info\u0026rdquo;\u0026mdash;put it on paper test yourself often: listening and understanding are not the same keep things simple: one task at a time keep an open mind + \u0026ldquo;collective\u0026rdquo; new perspectives and mental models new cycle capture curate cultivate connect create ","permalink":"https://www.jemoka.com/posts/kbhpkm/","tags":null,"title":"PKM"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhpolio/","tags":null,"title":"Polio"},{"categories":null,"contents":"A polynomial is a polynomial\nconstituents a function \\(p: \\mathbb{F} \\to \\mathbb{F}\\) coefficient \\(a_0, \\dots, a_{m} \\in \\mathbb{F}\\) requirements A polynomial is defined by:\n\\begin{equation} p(z)=a_0+a_1z+a_2z^{2}+\\dots +a_{m}z^{m} \\end{equation}\nfor all \\(z \\in \\mathbb{F}\\)\nadditional information degree of a polynomial \\(\\deg p\\) A polynomial\u0026rsquo;s degree is the value of the highest non-zero exponent. That is, for a polynomial:\n\\begin{equation} p(z) = a_0+a_1z+\\dots +a_{m}z^{m} \\end{equation}\nwith \\(a_{m} \\neq 0\\), the degree of it is \\(m\\). We write \\(\\deg p = m\\).\nA polynomial \\(=0\\) is defined to have degree \\(-\\infty\\)\nOf course, a polynomial with degree \\(n\\), times a polynomial of degree \\(m\\), has degree \\(mn\\). We see that:\n\\begin{equation} x^{n}x^{m} = x^{n+m} \\end{equation}\n\\(\\mathcal{P}(\\mathbb{F})\\) \\(\\mathcal{P}(\\mathbb{F})\\) is the set of all polynomials with coefficients in \\(\\mathbb{F}\\).\n\\(\\mathcal{P}(\\mathbb{F})\\) is a vector space over \\(\\mathbb{F}\\) We first see that polynomials are functions from \\(\\mathbb{F}\\to \\mathbb{F}\\). We have shown previously that F^s is a Vector Space Over F.\nTherefore, we can first say that \\(\\mathcal{P}(\\mathbb{F}) \\subset \\mathbb{F}^{\\mathbb{F}}\\).\nLastly, we simply have to show that \\(\\mathcal{P}(\\mathbb{F})\\) is a subspace.\nzero exists by taking all \\(a_{m} = 0\\) addition is closed by inheriting commutativity and distributivity in \\(\\mathbb{F}\\) scalar multiplication is closed by distributivity Having satisfied the conditions of subspace, \\(\\mathcal{P}(\\mathbb{F})\\) is a vector space. \\(\\blacksquare\\)\n\\(\\mathcal{P}_{m}(\\mathbb{F})\\) For \\(m\\geq 0\\), \\(\\mathcal{P}_{m}(\\mathbb{F})\\) denotes the set of all polynomials with coefficients \\(\\mathbb{F}\\) and degree at most \\(m\\).\n","permalink":"https://www.jemoka.com/posts/kbhpolynomial/","tags":null,"title":"polynomial"},{"categories":null,"contents":"For some \\(a \\in \\mathbb{F}\\), we define \\(a^m\\) to be \\(a\\) multiplied with itself \\(m\\) times.\nadditional information \\((a^m)^n = a^{mn}\\) \\((ab)^m = a^mb^m\\) ","permalink":"https://www.jemoka.com/posts/kbhpower_math/","tags":null,"title":"power (math)"},{"categories":null,"contents":"We can now use power series to also solve differential equations.\n\\begin{equation} \\dv{x}{t} = 0; x(0)=1 \\end{equation}\nWe wish to have a power-series solution of shape:\n\\begin{equation} x(t) = \\sum_{k=0}^{\\infty }a_{k}t^{k} \\end{equation}\nWe want to find the coefficients \\(a_{k}\\). If you can find such a function that fits this form, they both 1) converge and 20 behave the same way as \\(e^{x}\\) does in Simple Differential Equations.\nanalytic functions Functions which can be described with a power series are called analytic functions.\n","permalink":"https://www.jemoka.com/posts/kbhpower_series/","tags":null,"title":"power series"},{"categories":null,"contents":"power utility, or isoelastic utility, is a financial econometric is a utility that results absolute, constant relative risk aversion. i.e.: you tell me how risk averse you are exogenously, I tell you how much utility some consumption is.\nconstituents some relative risk coefficient \\(\\gamma \\in (0,1)\\), higher more risk averse consumption of some asset \\(C\\) requirements Utility \\(U( C)\\) is defined by:\n\\begin{equation} U( C) = \\frac{c^{1-\\gamma}-1}{1-\\gamma} \\end{equation}\nadditional information As you can see, the higher \\(\\gamma\\), the lower utility some consumption brings.\n","permalink":"https://www.jemoka.com/posts/kbhpower_utility/","tags":null,"title":"power utility"},{"categories":null,"contents":"The price\n","permalink":"https://www.jemoka.com/posts/kbhprice/","tags":null,"title":"price"},{"categories":null,"contents":"gravity sucks.\ngeneral relativity claims that our best theory of how gravity work does not work with non-\n","permalink":"https://www.jemoka.com/posts/kbhproblem_with_gravity/","tags":null,"title":"problem with gravity"},{"categories":null,"contents":"Take two linear maps \\(T \\in \\mathcal{L}(U,V)\\) and \\(S \\in \\mathcal{L}(V,W)\\), then \\(ST \\in \\mathcal{L}(U,W)\\) is defined by:\n\\begin{equation} (ST)(u) = S(Tu) \\end{equation}\nIndeed the \u0026ldquo;product\u0026rdquo; of Linear Maps is just function composition. Of course, \\(ST\\) is defined only when \\(T\\) maps to something in the domain of \\(S\\).\nThe following there properties hold on linear-map products (note that commutativity isn\u0026rsquo;t one of them!):\nassociativity \\begin{equation} (T_1T_2)T_3 = T_1(T_2T_3) \\end{equation}\nidentity \\begin{equation} TI = IT = T \\end{equation}\nfor \\(T \\in \\mathcal{L}(V,W)\\) and \\(I \\in \\mathcal{L}(V,V)\\) (OR \\(I \\in \\mathcal{L}(W,W)\\) depending on the order) is the identity map in \\(V\\).\nidentity commutes, as always.\ndistributive in both directions\u0026mdash;\n\\begin{equation} (S_1+S_2)T = S_1T + S_2T \\end{equation}\nand\n\\begin{equation} S(T_1+T_2) = ST_{1}+ST_{2} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhproduct_of_linear_maps/","tags":null,"title":"Product of Linear Maps"},{"categories":null,"contents":"A product of vector spaces is a vector space formed by putting an element from each space into an element of the vector.\nconstituents Suppose \\(V_1 \\dots V_{m}\\) are vector spaces over the same field \\(\\mathbb{F}\\)\nrequirements Product between \\(V_1 \\dots V_{m}\\) is defined:\n\\begin{equation} V_1 \\times \\dots \\times V_{m} = \\{(v_1, \\dots, v_{m}): v_1 \\in V_1 \\dots v_{m} \\in V_{m}\\} \\end{equation}\n\u0026ldquo;chain an element from each space into another vector\u0026rdquo;\nadditional information operations on Product of Vector Spaces The operations on the product of vector spaces are defined in the usual way.\nAddition: \\((u_1, \\dots, u_{m})+(v_1, \\dots, v_{m}) = (u_1+v_1, \\dots, u_{m}+v_{m})\\)\nScalar multiplication: \\(\\lambda (v_1 \\dots v_{m}) = (\\lambda v_1, \\dots, \\lambda v_{m})\\)\nProduct of Vector Spaces is a vector space The operations defined above inherits closure from their respective vector spaces.\nadditive identity: \\((0, \\dots, 0)\\), taking the zero from each vector space additive inverse: \\((-v_1, \\dots, -v_{m})\\), taking the additive inverse from each vector space scalar multiplicative identity: \\(1\\) operations: commutativity, associativity, distributivity \u0026mdash; inheriting from vector spaces \\(\\blacksquare\\)\ndimension of the Product of Vector Spaces is the sum of the spaces\u0026rsquo; dimension Proof:\nTake each \\(V_{j}\\); construct a list such that, for each basis vector in the basis of \\(V_{j}\\), we have an element of the list such that we have that basis vector in the \\(j^{th}\\) slot and \\(0\\) in all others.\nThis list is linearly independent; and, a linear combination thereof span all of \\(V_1 \\times \\dots \\times V_{m}\\). The length of this is the sum of the number of basis vectors of each space, as desired. \\(\\blacksquare\\)\nproduct summation map See: product summation map\n","permalink":"https://www.jemoka.com/posts/kbhproduct_of_vector_spaces/","tags":null,"title":"Product of Vector Space"},{"categories":null,"contents":"Let \\(U_1, \\dots, U_{m}\\) be subspaces of \\(V\\); we define a linear\nWe define \\(\\Gamma\\) to be a map \\(U_1 \\times \\dots U_{m} \\to U_1 + \\dots + U_{m}\\) such that:\n\\begin{equation} \\Gamma (u_1, \\dots, u_{m}) = u_1 + \\dots + u_{m} \\end{equation}\nEssentially, \\(\\Gamma\\) is the sum operation of the elements of the tuple made by the Product of Vector Spaces.\n\\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\Gamma\\) is injective Proof:\nGiven \\(\\Gamma\\) is injective: Given injectivity, we have that injectivity implies that null space is \\(\\{0\\}\\). Now, because the only way to produce \\(0\\) is to have the input product/tuple be 0, \\(u_1 \\dots u_{m} = 0\\). So, given a sum of subsets is a direct sum IFF there is only one way to write \\(0\\), the sum is a direct sum.\nGiven direct sum: Reverse the logic of above directly. Given its a direct sum, the only way to be in the null space of \\(\\Gamma\\) (i.e. have the sum of the elements of tuple by \\(0\\)) is by taking each \\(u_1 \\dots u_{m}\\) to \\(0\\). Now, injectivity implies that null space is \\(\\{0\\}\\), so \\(\\Gamma\\) is injective. \\(\\blacksquare\\)\nAside: \\(\\Gamma\\) is surjective because product of vector-spaces is simply the pre-combined version of the sum.\nSo a corollary of the above result is that: \\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\Gamma\\) is invertable, because injectivity and surjectivity implies invertability.\n\\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\dim (U_1 + \\dots + U_{m}) = \\dim U_1 + \\dots + \\dim U_{m}\\) \\(\\Gamma\\) is surjective for all cases because product of vector-spaces is simply the pre-combined version of the sum.\nSo, by rank-nullity theorem, \\(\\dim (U_1 \\times \\dots U_{m}) = \\dim null\\ \\Gamma + \\dim (U_1 + \\dots + U_{m})\\).\nNow, \\(\\dim null\\ \\Gamma = 0\\) IFF \\(\\dim (U_1 \\times \\dots U_{m}) = 0 + \\dim (U_1 + \\dots + U_{m})\\).\nNow, dimension of the Product of Vector Spaces is the sum of the spaces\u0026rsquo; dimension.\nSo: \\(\\dim null\\ \\Gamma = 0\\) IFF \\(\\dim U_1 + \\dots + \\dim U_{m} = 0 + \\dim (U_1 + \\dots + U_{m})\\).\nNow, \\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\Gamma\\) is injective, and from above \\(\\dim null\\ \\Gamma = 0\\) (that \\(\\Gamma\\) is injective) IFF \\(\\dim U_1 + \\dots + \\dim U_{m} = 0 + \\dim (U_1 + \\dots + U_{m})\\).\nSo, \\(U_1 + \\dots + U_{m}\\) is a direct sum IFF \\(\\dim (U_1 + \\dots + U_{m}) = \\dim U_1 + \\dots + \\dim U_{m}\\), as desired. \\(\\blacksquare\\)\n(Note that this proof is built out of a series of IFFs, so it goes in both directions.)\n","permalink":"https://www.jemoka.com/posts/kbhproduct_summation_map/","tags":null,"title":"product summation map"},{"categories":null,"contents":"This is a work-in-progress page listing all of my production projects.\nYappin: Podcast https://anchor.fm/yappin/\n20MinuteRants: Blog https://20mr.substack.com/\nProject80: Podcast See Project80.\nNorman Stories: Fiction https://hidonipothan.substack.com/\n(left) Director - Hillview Broadcasting: Production Studio https://hillview.tv/\n","permalink":"https://www.jemoka.com/posts/kbhproduction_index/","tags":["index"],"title":"Production Index"},{"categories":null,"contents":"We mentioned this in class, and I figured we should write it down.\nSo, if you think about the Product of Vector Space:\n\\begin{equation} \\mathbb{R} \\times \\mathbb{R} \\end{equation}\nyou are essentially taking the \\(x\\) axis straight line and \u0026ldquo;duplicating\u0026rdquo; it along the \\(y\\) axis.\nNow, the opposite of this is the quotient space:\n\\begin{equation} \\mathbb{R}^{2} / \\left\\{\\mqty(a \\\\ 0): a \\in \\mathbb{R} \\right\\} \\end{equation}\nWhere, we are essentially taking the line in the \\(x\\) axis and squish it down, leaving us only the \\(y\\) component freedom to play with (as each element is \\(v +\\left\\{\\mqty(a \\\\ 0): a \\in \\mathbb{R} \\right\\}\\)).\nThis also gets us the result that two affine subsets parallel to \\(U\\) are either equal or disjoint; specifically the conclusion that \\(v-w \\in U \\implies v+U = w+U\\): for our example, only shifting up and down should do different things; if two shifts\u0026rsquo; up-down shift is \\(0\\) (i.e. it drops us back into \\(\\mqty(a \\\\0)\\) land), well then it will not move us anywhere different.\n","permalink":"https://www.jemoka.com/posts/kbhproducts_and_quotients_the_intuition/","tags":null,"title":"products and quotients, the intuition"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhprof_xin_liu/","tags":null,"title":"Prof. Xin Liu"},{"categories":null,"contents":"Project80 is a podcast hosted by Houjun Liu, Anoushka Krishnan, Micah Brown, Mia Tavares, among others.\nCollege Application w.r.t. Project80 Cheese mission statement: Project80 is a good way of creating a self-propegating set of learning that would serve to benefit and educate future generations in hopes of creating a more equitable planet.\n","permalink":"https://www.jemoka.com/posts/kbhproject80/","tags":null,"title":"Project80"},{"categories":null,"contents":"Natural science education resources traditionally teach only codified theory. While theory education is crucial, much of academic science takes place via scrutinizing contested scientific discourse. Due to such resources’ content complexity, high school students are rarely exposed to current, debatable, and relevant science. In response, we introduce Project80: a systemic, student-run protocol to synthesize the latest primary literature in a sub-field into approachable, produced multimedia educational content. The protocol is run by a team of 7 students over the course of 1 month. Students running the protocol consume complex scientific literature, distill relevant data and findings, and synthesize a culminating product of audiovisual content to supplement existing biology and chemistry pedagogy. The system runs independently with limited faculty involvement. Our analysis indicates that the multimedia content created by this protocol will be relevant to roughly 30 courses locally at our institution and will have further extensions in secondary education beyond.\n","permalink":"https://www.jemoka.com/posts/kbhproject80_abstract/","tags":null,"title":"Project80 Abstract"},{"categories":null,"contents":"Projects Index is a index that contains a list of almost all projects for which I have ever worked on. Major categories are highlighted from chapter titles.\nResearch Projects I have spent the last 6 years or so working as an actively-publishing data science researcher; my research interests are mainly in textual data mining, semantic analysis, L2 learning, and science education. As a part of my work with Professor Brian Macwinney, I have also recently taken up interest in acoustic modeling.\nFor a list of my recent research, please head to the Research Index.\nMedia Production Projects I produce a lot of media (videos, podcasts, blogs, live events/talks) as a part of publicizing my work or for other purposes. For those types of projects, head on over to Production Index.\nLarge-Scale Endeavors Condution An open-source task management app. Website.\nMotivation: I got really tired with most other to-do apps after swapping them out over and over again, until I got fed up and built one with some friends.\nRole: Co-Founder, Lead Developer. Technologies: React, Ionic, Firebase, Typescript, Swift, PostgreSQL Key facts: 10,000+ users, 8-person team, featured in the Bay Area almanac, praised by Asana’s head of developer relations for “open-source advocacy” MODAP A R\u0026amp;D team for fireline safety during emergency fires. Repository.\nMotivation: a friend approached me with an opportunity to help our local community, especially with the increased influx of fires.\nRole: Team Lead Technologies: Rust, Torch, ARM, electronics (i2C, UART, messaging protocols, etc.) Key facts: coordinated 5 engineers in developing new technology, supported by Dr. Robert G. Gann, Deputy Director, Center of Excellence for Advanced Technology Aerial Firefighting at the state of Colorado as well as Captain Mason of CalFire CMU batchalign A pipeline for the automated preparation of annotated CHAT transcripts from raw audio. Repository.\nMotivation: my work over the summer.\nRole: Author Technologies: Torch, Huggingface, NLTK, CLAN, computational linguistics Key facts: work developed with and maintained under Prof. Brian MacWhinney at CMU\u0026rsquo;s psycolinguistics department. AIBridge A bootcamp for non-CS students in data science. Website\nMotivation:\nRole: Co-Founder, Lecturer Technologies: Python, ScyPy, Scikit-learn, Pandas Key facts: worked with Prof. Xin Liu at UC Davis to develop an introductary one-week bootcamp in ML. We piloted the program this summer at Davis to an in-person group of 20 PhD students in food science sponsored by AIFS. Full-Stack Projects tractotato CommonLisp macroset for time tracking. Repo.\nMotivation: I wanted to learn CommonLisp macros syntax after reading the Land of Lisp book.\nRole: author Technologies: CommonLisp Scratchathon Portal Portal to submit projects for a scratch hackathon I hosted. Repo.\nMotivation: my friends McGuy and fuelvin, both content creators on Scratch on YouTube, put together a Scratch hackathon summer of 2020. This is the submission portal.\nRole: author Technologies: React, Vercel, Firebase syzygy Library rethinking to-do list dating to be more flexible and powerful. Repo.\nMotivation: a friend and I wanted to innovate beyond the scope of Condution to see how we can abstract away a to-do list system to its bare minimum.\nRole: co-founder, co-author Technologies: Rust positron Library for building lightweight native apps using web tech. Repo.\nMotivation: I wanted to re-make electron to be more lightweight using Suckless\u0026rsquo; Surf browser concept.\nRole: author Technologies: C++, GTK OS/Driver Development Broadcom Wifi/Bluetooth 4377 Chip Linux Driver A driver patchset to support cutting-edge Broadcom 4377 Wifi/Bluetooth chips. Repo.\nMotivation: I needed to be able to use Wifi on my laptop while running Arch Linux.\nRole: author Technologies: C, (small amounts of) Assembly Key facts: integrated into the t2linux pipeline used to make WiFi possible on Linux for most MacBooks released after 2018 Distributed Algorithms and Parallel Computing coveather An encrypted, anonymized system for protected health information verification. Preprint, Repo, and internal note.\nMotivation: I wanted to be able to make vaccine passports more feasible because the current COVID testing/vaccine verification scheme is really bad.\nRole: author Technologies: Clojure, core.async concurrency, Monte-Carlo simulations, blockchain, PGP Key facts: project won first place at the California STEM Fair, and got special recognition from the Yale Science and Engineering assoc. Total award $3000. multischedule A multiple-asynchronous scheduling and delegation algorithm. Repo.\nMotivation: (didn\u0026rsquo;t even come close to getting there) I wanted to create a way to solve or simplify debugging loop overrun problems in robotics codebases.\nRole: author Technologies: Clojure, core.async concurrency rotifer A work-in-progress distributed algorithm for taproot. Repo.\nMotivation: I wanted to make taproot even more distributed if possible.\nRole: author Technologies: Clojure, XML, UDP, ICE simian Exploring OT/CRDT and collaborative text editing for taproot. Repo.\nMotivation: I wanted to learn about how apps like Google Docs work, and explore Operational Transformation/CRDT, in hopes of putting it into taproot.\nRole: author Technologies: Clojure, OT, CRDT aron A distributed multi-dimensional optimization tool. Repo.\nMotivation: Nueva\u0026rsquo;s course scheduling was quite a mess, and I wanted to help. It is a very complex problem and this project is in the freezer at the moment.\nRole: author Technologies: CommonLisp mitte Easy UDP sockets. Repo, Docs.\nMotivation: a friend and I wanted to explore UDP.\nRole: co-author Technologies: Rust, UDP, ICE (connection) Cryptography and security See also: coveather.\njrainbow An implementation of a MD5 rainbow table. Repo, Crate.\nMotivation: I wanted to understand how Rainbow Tables worked.\nRole: author Technologies: Rust, MD5 Note-taking Systems and \\(\\LaTeX\\) improvements taproot A shared zettlekasten of notes and learning resources put together by some friends and I. there has been a few iterations. Current Repo, Current Site, Legacy Site, Even More Legacy Site.\nMotivation: I started writing nice \\(\\LaTeX\\) PDFs of my homework, and some friends wanted to have access to it. Later when I mentioned it, another friend had a similar need; so we asked many people to pool our notes and work together to share.\nRole: co-founder, co-lead, developer Technologies: Next.JS, XeLaTeX, GNU Make, Firn, Hugo, Emacs Org, Org-Publish, Markdown blag The zettlekasten you are currently in! My currently maintained personal knowledgebase. Repo, Site.\nMotivation: I wanted to experiment with more advanced note-taking techniques after developing taproot, and it ended up superseeding the note-taking abilities of taproot.\nRole: author Technologies: Next.js, Emacs Org, Hugo gdoc.el A utility to enable GNU Emacs to edit Google Doc documents based on the gdrive utility. Repo.\nMotivation: I wanted to edit Google Docs in Emacs!\nRole: author Technologies: GNU Emacs, elisp interesting Things that my friends and I find interesting, chucked on the web and builds itself. Repo, Site. No longer maintained.\nMotivation: many text channels were too clogged with stuff my friend group found interesting, so I wanted to take initiative to collect them.\nRole: co-founder, author Technologies: Next.js, Vercel, remark, CommonMark Markdown Public Configurations borg Automatically configure terminals. Repo.\nMotivation: I needed a way to copy my system terminal config onto a system quickly.\nRole: author Technologies: Bash, Zsh, OhMyZsh .config A group of sane configuration files. Repo.\nMotivation: some Redditors asked for my Config, and I thought I\u0026rsquo;d share it to benefit the community; also for personal backup.\nRole: author, maintainer Technologies: Unix administration, Perl, Ruby, LISP .emacs.d Simple, powerful, and semantic GNU Emacs configuration for personal use. Repo.\nMotivation: I wanted to track my progress in developing a working Emacs config.\nRole: author, maintainer Technologies: GNU Emacs, elisp ","permalink":"https://www.jemoka.com/posts/kbhprojects/","tags":["index"],"title":"Projects Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhproof/","tags":null,"title":"proof"},{"categories":null,"contents":"base case Prove some base case \\(n_0\\)\ninductive step Prove that, given \\(n\\), \\(n_{j} \\implies n_{j+1}\\).\n","permalink":"https://www.jemoka.com/posts/kbhproof_by_induction/","tags":null,"title":"proof by induction"},{"categories":null,"contents":"Based on the wise words of a crab, I will start writing down some Proof Design Patterns I saw over Axler.\ninheriting properties (splitting, doing, merging) \u0026ldquo;complex numbers inherit commutativity via real numbers\u0026rdquo;\nconstruct then generalize for uniqueness and existence\ntry to remember to go backwards\nto prove IFF\nzero is cool, and here too!, also \\(1-1=0\\)\n\\(0v = 0\\) \\(1-1 = 0\\) \\(v-v=0\\) a.k.a. \\(v+(-v)=0\\) \\(v+0 = v\\) distributivity is epic: it is essentially the only tool to connect scalar multiplication and addition in a vector space\n\u0026ldquo;smallest\u0026rdquo; double containement proofs to show set equivalence: prove one way, then prove the converse (\\(a \\subset b, b\\subset a \\Rightarrow a=b\\))\ncouple hints\nstep 1: identify hypothesis (assumptions) desired conclusion (results, trying/to/proof) step 2: define write down precise, mathematical notations ","permalink":"https://www.jemoka.com/posts/kbhproof_design_patterns-1/","tags":null,"title":"Proof Design Patterns"},{"categories":null,"contents":"Based on the wise words of a crab, I will start writing down some Proof Design Patterns I saw over Axler.\ninheriting properties (splitting, doing, merging) \u0026ldquo;complex numbers inherit commutativity via real numbers\u0026rdquo;\nconstruct then generalize for uniqueness and existence\ntry to remember to go backwards\nto prove IFF\nzero is cool, and here too!, also \\(1-1=0\\)\n\\(0v = 0\\) \\(1-1 = 0\\) \\(v-v=0\\) a.k.a. \\(v+(-v)=0\\) \\(v+0 = v\\) distributivity is epic: it is essentially the only tool to connect scalar multiplication and addition in a vector space\n\u0026ldquo;smallest\u0026rdquo; double containement proofs to show set equivalence: prove one way, then prove the converse (\\(a \\subset b, b\\subset a \\Rightarrow a=b\\))\ncouple hints\nstep 1: identify hypothesis (assumptions) desired conclusion (results, trying/to/proof) step 2: define write down precise, mathematical notations proving uniqueness: set up two distinct results, show that they are the same\nproving negation: if the \u0026ldquo;negative\u0026rdquo; is distinct, but the direct case is more nebulous, use proves by contradiction\nproof by induction\nespecially if you are dealing with polynomials, try factoring tools to help includes length of linearly-independent list \\(\\leq\\) length of spanning list Uniqueness by construction: uniqueness part of basis of domain\npick one element that does exist pick arbitrary elements and construct a result if we are trying to prove equivalence, double-containment is a good bet\nsee fundamental theorem of linear maps: but basically wehnever you need to construct basis of things start with an arbiturary basis of the subspace and expand into that of the whole space\na loop in the statements makes them all equivalent\n","permalink":"https://www.jemoka.com/posts/kbhproof_design_patterns/","tags":null,"title":"Proof Design Patterns"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhproof_of_work/","tags":null,"title":"proof of work"},{"categories":null,"contents":"propaganda is a form of advertising which:\npropaganda persuades people into believe in a cause often defies reason to reach into ?? See examples:\nUS WWII Propaganda techniques for propaganda Name calling Generalities Transferring of authority Public testimonial Attachment to plane folks Bandwagoning (FOMO) Fear Bad logic Unwanted extrapolation ","permalink":"https://www.jemoka.com/posts/kbhpropaganda/","tags":null,"title":"propaganda"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhprotons/","tags":null,"title":"proton"},{"categories":null,"contents":"the fast you are willing to prototype, the more willing you are to fail, the faster you will get to a successful partial solution you can refine and repeat.\nhow to prototype faster? In order of decreasing slowness\u0026mdash;-\nbuild out the whole product\u0026hellip; building the minimum viable product\u0026hellip; skeleton prototyping (Figma)\u0026hellip; Pen and paper\u0026hellip; Talking about it The trade-off: each level gives increased fidelity: its closer to what will actually ship, so you can get better+detailed feedback.\n","permalink":"https://www.jemoka.com/posts/kbhprototyping/","tags":null,"title":"Prototyping"},{"categories":null,"contents":"psychoacoustics is the study of sound perception and cognition\nhow does sound work how we perceive it why? and what are its applications? ","permalink":"https://www.jemoka.com/posts/kbhpsycoacoustics/","tags":null,"title":"psychoacoustics"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcorrelation/","tags":null,"title":"quantum correlation"},{"categories":null,"contents":" Viewing computational linguistics from the length across linear algebra and linear structure Quantum algorithms and the necessary infra were being developed; and in the 2010s programmable quantum computers became showing up Quantum is done over the complexes, which makes the normal linguistics done with the reals more powerful.\nwant to infer the probability distribution of words based on their letters\nLinearity breaks down: letter combinations in not commutative; and P(letter C) + P(letter A) != P(letters CA) instead of encoding letters as one-hot vectors; we encode these letters with matrices: adds more dimensions\nimmediate benefits: noncommutivity of matricies is a PLUS words is just the composed results into another 2x2 matricies then, to map into probability distrubtion, we map the matrix into a partial trace things create bounds from the problem: letters\nimprove upon optimization scheme in a quantum rhelm\nimplement this scheme on a quantum computer: https://arxiv.org/pdf/1710.10248.pdf\ntask: NTJ reading; come up with the needed novelty\n","permalink":"https://www.jemoka.com/posts/kbhquantum_group_project/","tags":null,"title":"Quantum Group Project"},{"categories":null,"contents":"The computation model behind quantum theory. It proposes quantum computers, proposed during the 80s. Theoretically, quantum computers have quantum supremacy, which is exciting. It is a theory that works with counterfactual information.\nquantum computer A quantum computer is a computer that uses quantum effects to perform Turing-like computations\nquantum supremacy That a quantum computer outperforms all classical computers\nuniversal computer \u0026ldquo;a programmable system whose repertoire includes all physically possible computations\u0026rdquo; \u0026mdash; Turing.\nYou will realize that modern computers are not actually capable of all computations\u0026mdash;apparently, they can\u0026rsquo;t make itself.\nTherefore, to actually achieve this, we have to make a more general type of computer: a constructor \u0026mdash; a universal quantum constructor.\n","permalink":"https://www.jemoka.com/posts/kbhquantum_information_theory/","tags":null,"title":"quantum information theory"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhquantum_supremecy/","tags":null,"title":"quantum supremecy"},{"categories":null,"contents":"quantum theory allows us to understand physics; it reconciliations the classical world with the quantum world.\nClassical particles, in the double slit experiment, would just straight go through and bounce off Actual particles (quantum) like light, under quantum theory, would actually exhibit interference via wave-like hebahior The measurement of quantum theory is done via quantum information theory.\n","permalink":"https://www.jemoka.com/posts/kbhquantum_theory/","tags":null,"title":"quantum theory"},{"categories":null,"contents":"A qubit is a two-layer quantum theory system.\nA classical bit is something that can be set between two values, a qubit between a much higher dimension.\n","permalink":"https://www.jemoka.com/posts/kbhqubits/","tags":null,"title":"qubit"},{"categories":null,"contents":"a quotient group is a group which is the product of mapping things out.\nsubgroups The set of integers \\(\\mathbb{Z}\\) is obviously a group. You can show it to yourself that multiples of any number in the group is a subgroup of that group.\nFor instance:\n\\(3 \\mathbb{Z}\\), the set \\(\\{\\dots -6, -3, 0, 3, 6, \\dots\\}\\) is a subgroup\nactual quotient groups We can use the subgroup above to mask out a group. The resulting product is NOT a subgroup, but its a new group with individual elements being subsets of our original group.\nFor instance, the \\(\\mod 3\\) quotient group is written as:\n\\begin{equation} \\mathbb{Z}} / 3 \\mathbb{Z} \\end{equation}\nEach element in this new group is a set; for instance, in \\(\\mathbb{Z} / 3\\mathbb{Z}\\), \\(0\\) is actually the set \\(\\{\\dots -6, -3, 0, 3, 6, \\dots\\}\\) (i.e. the subgroup that we were masking by). Other elements in the quotient space (\u0026ldquo;1\u0026rdquo;, a.k.a. \\(\\{ \\dots, -2, 1, 4, 7 \\dots \\}\\), or \u0026ldquo;2\u0026rdquo;, a.k.a. \\(\\{\\dots, -1, 2, 5, 8 \\dots \\}\\)) are called \u0026ldquo;cosets\u0026rdquo; of \\(3 \\mathbb{Z}\\). You will notice they are not a subgroups.\n","permalink":"https://www.jemoka.com/posts/kbhquotient_group/","tags":null,"title":"quotient group"},{"categories":null,"contents":"The quotient map \\(\\pi\\) is the Linear Map \\(V \\to V / U\\) such that:\n\\begin{equation} \\pi(v) = v+U \\end{equation}\nfor \\(v \\in V\\).\nI.e.: the quotient map is affine subsetification map given a vector.\n","permalink":"https://www.jemoka.com/posts/kbhquotient_map/","tags":null,"title":"quotient map"},{"categories":null,"contents":"Suppose \\(T \\in \\mathcal{L}(V)\\), and \\(U \\subset V\\), an invariant subspace under \\(T\\). Then:\n\\begin{equation} (T / U)(v+U) = Tv+U, \\forall v \\in V \\end{equation}\nwhere \\(T / U \\in \\mathcal{L}(V / U)\\)\n\u0026ldquo;if you can operator on \\(V\\), you can operator on \\(V / U\\) in the same way.\u0026rdquo; Yes I just verbed operator.\nquotient operator is well-defined Why is this not possible for any subspace of \\(V\\)? This is because we need \\(T\\) to preserve the exact structure of the subspace we are quotienting out by; otherwise our affine subset maybe squished to various unexpected places. The technical way to show that this is well-defined leverages the property of two affine subsets being equal:\nSuppose \\(v +U = w+U\\), we desire that \\(T / U (v+U) = T / U (w+U)\\). That is, we desire that \\(Tv +U = Tw +U\\).\nIf \\(v+U = w+U\\) , then, \\(v-w \\in U\\). Now, this means that \\(T(v-w) \\in U\\) only because \\(U\\) is invariant under \\(T\\) (otherwise it could be sent to anywhere in \\(V\\) as \\(T \\in \\mathcal{L}(V)\\) not \\(\\mathcal{L}(U)\\)). Therefore, \\(Tv-Tw \\in U\\), and so \\(Tv +U = Tw+U\\), as desired. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhquotient_operator/","tags":null,"title":"quotient operator"},{"categories":null,"contents":"A quotient space is the set of all affine subsets of \\(V\\) parallel to some subspace \\(U\\). This should be reminiscent of quotient groups.\nconstituents vector space \\(V\\) a subspace \\(U \\subset V\\) requirements \\begin{equation} V / U = \\{v+U : v \\in V \\} \\end{equation}\nadditional information operations on quotient space Addition and scalar multiplication on the quotient space is defined in the expected way:\ngiven \\((v+U), (w+U) \\in V / U\\), and \\(\\lambda \\in \\mathbb{F}\\):\n\\begin{equation} \\begin{cases} (v+U) + (w+U) = ((v+w)+U) \\\\ \\lambda (v+U) = ((\\lambda v)+U) \\end{cases} \\end{equation}\nquotient space operations behave uniformly on equivalent affine subsets The tricky thing about quotient space operations is that there are multiple ways of representing a single affine subset parallel to \\(U\\); the one-liner about this is that if you think about shifting a parallel line with a vector: shifting the line along any perpendicular vector to the line with the same magnitude will get you the same shifted line.\nFor the operations above to work, we have to make sure that they behave in the same way on distinct representations of the same affine subset, which we endeavor to proof here:\nSuppose we have \\(v,w \\in V\\), \\(v\u0026rsquo;,w\u0026rsquo; \\in V\\), and that \\(v+U = v\u0026rsquo;+U\\); \\(w+U = w\u0026rsquo;+U\\). We desire that the operations above behave the same way to any addition groupings: that WLOG \\((v+U)+(w+U) = (v\u0026rsquo;+U)+(w\u0026rsquo;+U)\\) \u0026mdash; that is, we have to show that \\((v+w)+U = (v\u0026rsquo;+w\u0026rsquo;)+U\\).\nBy the fact that two affine subsets parallel to \\(U\\) are either equal or disjoint, we have that \\(v-v\u0026rsquo;, w-w\u0026rsquo; \\in U\\). And so, \\((v-v\u0026rsquo;)+(w-w\u0026rsquo;) \\in U\\). Commuting these things under \\(V\\), we now have that \\((v+w)-(v\u0026rsquo;+w\u0026rsquo;) \\in U\\). Therefore, invoking the same result again, \\((v+w)+U = (v\u0026rsquo;+w\u0026rsquo;)+U\\), as desired.\nThe same logic can be used for scalar multiplication. Suppose we have \\(v, v\u0026rsquo; \\in V\\), \\(\\lambda \\in \\mathbb{F}\\), and that \\(v+U = v\u0026rsquo;+U\\). We desire that WLOG \\(\\lambda (v+U) = \\lambda (v\u0026rsquo;+U)\\) \u0026mdash; that is, we have to show that \\((\\lambda v)+U = (\\lambda v\u0026rsquo;)+U\\).\nAgain invoking the two affine subsets parallel to \\(U\\) are either equal or disjoint result, we have that \\(v-v\u0026rsquo; \\in U\\). Now, this means that \\(\\lambda (v-v\u0026rsquo;) = \\lambda v-\\lambda v\u0026rsquo; \\in U\\) because closure of scalar multiplication in \\(U\\). Invoking the result again, we now have that \\(\\lambda v + U = \\lambda v\u0026rsquo; +U\\), as desired.\nHaving shown both operations make sense, we can declare that they make sense indeed. \\(\\blacksquare\\)\nquotient space is a vector space Given the name! (jk)\nBleh I give up just prove it yourself given the above operations and the fact that the additive identity is \\(0+U = U\\), the additive inverse is \\(-v+U\\).\n\u0026ldquo;instead of the elements single vectors, we fuse the whole affine subset together. instead of counting the contents, we count the bucket.\u0026rdquo;\ndimension of a quotient space is the difference between dimensions of its constituents that is,\n\\begin{equation} \\dim V / U = \\dim V - \\dim U \\end{equation}\nfor finite dimensional \\(V\\).\nProof:\nLet \\(\\pi: V \\to V /U\\). By definition, \\(null\\ \\pi =U\\); and, given the input is any \\(v \\in V\\), \\(range\\ \\pi = V / U\\). rank-nullity theorem then tells us that:\n\\begin{equation} \\dim V = \\dim U + \\dim V / U \\end{equation}\nnow subtract and get \\(\\dim V /U\\) by itself. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhquotient_space/","tags":null,"title":"quotient space"},{"categories":null,"contents":"Most high-school science programs have a strong focus on scientific theory and do not train students to conduct independent research. Previous work has demonstrated the efficacy of a mentor-supported, student-driven teaching program to effectively introduce research-specific skills in a classroom context. Despite the effectiveness of such programs, their class-based formats and requirements for multiple full-time faculty mentors limit their throughput, and the finite expertise of full-time mentors requires participants to focus on specific research subjects.\nTo address these limitations, we introduce R@N, an extracurricular, student-led, and student-driven program for the independent acquisition of research-specific skills through the self-guided completion of a series of formative checkpoints (“nodes”) for mastery. Students in the program can choose specific subsets of nodes to be trained in research in subjects of their interest. The program is developed and moderated by a small team of students in consultation with skill-specific faculty mentors through regular meetings. Students meet weekly to create, update, and revise nodes in collaboration with mentors in order to enable and supplement the learning of students participating in the program.\nThe program offers a few key results: it electively allows the student body (approximately 400 in our institution) to asynchronously acquire the skills of independent research and enables a group of around 20 students to develop and codify tools and skills for research pedagogy. The program can be sustained with limited faculty involvement, requiring one dedicated faculty mentor working in conjunction with a larger pool of research mentors who commit around 2 hours per month.\n","permalink":"https://www.jemoka.com/posts/kbhr_n_abstract/","tags":null,"title":"R@N Abstract"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhr_n_meeting_with_angi/","tags":null,"title":"R@N Meeting with Angi"},{"categories":null,"contents":"Let\u0026rsquo;s compute what \\(e^{tA}\\) should look like, where \\(t\\) is some scalar and \\(A\\) is a diagonalizable matrix. This is a supplement to Second-Order Linear Differential Equations.\nLet \\(v_1\\dots v_{m}\\) be the eigenvectors of \\(A\\). Let \\(\\lambda_{1}\\dots\\lambda_{m}\\) be the eigenvalues.\nRecall that we can therefore diagonalize \\(A\\) as:\n\\begin{equation} A = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nread: change of choordinates into the eigenbases, scale by the eigenvalues, then change back to normal choordinates.\nNow, imagine if we are multiplying \\(A\\) by itself manymany times; what will that look like?\n\\begin{equation} A^{n} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1}\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\dots \\end{equation}\nThe middle parts, nicely, cancels out! Its a matrix applied to its inverse! So, we get rid of it\n\\begin{equation} A^{n} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\dots \\end{equation}\nNow, we are multiplying diagonal matricies against itself! If you work out the mechanics of matrix multiplication, you will note that each element simply gets scaled to higher powers (the matricies are diagonal!)! So then, we have:\n\\begin{equation} A^{n} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{{\\lambda_{1}}^{n}, \\dots, {\\lambda_{m}}^{n}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nNice.\nRecall also the Tayler expasion of \\(e^{x}\\); we will apply it to to \\(e^{tA}\\):\n\\begin{equation} e^{tA} = \\sum_{k=0}^{\\infty} \\frac{1}{k!}(tA)^{k} = \\sum_{k=0}^{\\infty} \\frac{t^{k}}{k!}A^{k} \\end{equation}\nOk. We now apply our definition of \\(A^{n}\\) derived above:\n\\begin{equation} e^{tA} = \\sum_{k=0}^{\\infty} \\frac{t^{k}}{k!}\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{{\\lambda_{1}}^{k}, \\dots, {\\lambda_{m}}^{k}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nSee now that \\(\\mqty(v_1 \u0026amp; \\dots \u0026amp;v_{m})\\) and its inverse is both constant in the sum, so we take it out:\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\qty(\\sum_{k=0}^{\\infty}\\frac{t^{k}}{k!} \\mqty(\\dmat{{\\lambda_{1}}^{k}, \\dots, {\\lambda_{m}}^{k}}))\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nAnd now, the actual mechanics of adding a matrix is just adding it elementwise, so we will put the summations into the matrix:\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\sum_{k=0}^{\\infty}\\frac{t^{k}}{k!} {\\lambda_{1}}^{k}, \\dots, \\sum_{k=0}^{\\infty}\\frac{t^{k}}{k!} {\\lambda_{m}}^{k}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nNote now that each value in that matrix is just the Tayler expansion of \\(e^{k_{\\lambda_{j}}}\\) (take a moment to pause if this is not immediately obvious; think about what each element in that diagonal matrix look like and what the Tayler polynomial \\(e^{x}\\) should look like. Perhaps what some arbitrary \\(e^{ab}\\) should looks like.\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhraising_e_to_a_matrix/","tags":null,"title":"raising e to a matrix"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrandom/","tags":null,"title":"random"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrandom_variables/","tags":null,"title":"random variable"},{"categories":null,"contents":"The Random Walk Hypothesis is a financial econometric hypothesis that stocks have the same distribution and independent of each other: that stocks are a random variable and not predictable in a macro space.\nTo set up the random walk hypothesis, let\u0026rsquo;s begin with some time \\(t\\), an asset return \\(r_t\\), some time elapsed \\(k\\), and some future asset return \\(r_{t+k}\\).\nWe will create two random variables \\(f(r_t)\\) and \\(g(r_{t+k})\\), which \\(f\\) and \\(g\\) are arbitrary functions we applied to analyze the return at that time.\nThe Random Walk Hypothesis tells us that, at any two unrelated given time, you cannot use the behavior of \\(r_t\\) to predict anything about \\(r_{t+k}\\), under any kind of analysis \\(f\\) or \\(g\\), that:\n\\begin{equation} Cov[f(r_t), g(r_{t+k})] = 0 \\end{equation}\nSo, all of the Random Walk Hypothesis models would leverage the above result, that the two time info don\u0026rsquo;t evolve together and they are independently, randomly distributed: they are random variables.\nFor the market to be a typical Random Walk, the central limit theorem has to hold on the value of return. This usually possible, but if the variance of the return is not finite, the return will not hold the central limit theorem which means that the return will not be normal. Of course the return does not have to hold central limit theorem, then we use other convergence distributions but still model it in the Random Walk Hypothesis as a random variable.\nreturn (FinMetrics) Importantly: its not the price that follows the random walk; it is the RETURN that follows the walk; if it was the price, then its possible for price to become negative. Return, technically, is defined by:\n\\begin{equation} R_t = \\frac{p_t-p_{t-1}}{p_{t-1}} \\end{equation}\nHowever, we really are interested in the natural log of the prices:\n\\begin{equation} r_t = log(p_t) - log(p_{t-1}) \\approx R_t \\end{equation}\nWe can do this is because, for small \\(x\\), \\(log\\ x \\approx x-1\\).\nWe do this is because, if we were wanting to add the returns over the last \\(n\\) days, in \\(R_t\\) you\u0026rsquo;d have to multiply them:\n\\begin{equation} \\frac{p_{t+1}}{p_t} \\cdot \\frac{p_t}{p_{t-1}} = \\frac{p_{t+1}}{p_{t-1}} \\end{equation}\nThis is bad, because of the central limit theorem. To make a random variable built of normalizing \\(n\\) items, you have to add and not multiply them together over a time range. We want to be able to add.\nTherefore, \\(r_t\\) can achieve the same division by adding (see the log laws).\nBut either way, with enough, we know that \\(r_t\\) is independently, identity distributed.\ntime series analysis Over some days \\(k\\), we have:\n\\begin{equation} Y_{k} = \\sum_{i=1}^{k} x_{i} \\end{equation}\nGiven that \\(x_{i}\\) is distributed randomly: \\(\\{x_{i}\\}_{i=1}^{N}\\). This becomes the foundation of time series analysis. The problem of course becomes harder when the values drift against each other, is nonindependent, etc. We can use the Martingale Model to take generic random walk to a more dependent model.\nCJ test If you have some amount of volacitity measurement, we first know that, by the Random Walk Hypothesis, we have:\n\\begin{equation} X_{k} \\sim N(0,\\sigma^{2}) \\end{equation}\nGiven some future return, you hope that:\n\\begin{equation} Y_{k}=\\sum_{i=1}^{k}X_{k}\\sim N(0,\\sigma^{2}) \\end{equation}\nIf so, if you have like \\(20\\%\\) of log returns, to have a statistically significant return, we have that:\n\\begin{equation} \\sigma =\\frac{0.2}{\\sqrt{12}} \\end{equation}\ngetting a statistically significant difference from it is hard.\n","permalink":"https://www.jemoka.com/posts/kbhrandom_walk/","tags":null,"title":"Random Walk Hypothesis"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrandom_wol/","tags":null,"title":"random wol"},{"categories":null,"contents":"randomized algorithm is a type of algorithm, similar to relaxation.\nMake a hard problem easier by changing the problem What if, instead of guaranteeing we find the best/correct answer, we only provide some chance of finding the best/correct answer? primality testing primality testing is very important for modern crypto systems; we need to be able to find large prime numbers, and be able to generate them quickly.\ntraditional primality testing We can divide every prime number below \\(\\sqrt x\\). In theory, this is pretty fast, but we need to know all the primes we need to test.\nThis would therefore take \\(O(\\sqrt{x})\\) time.\nmiller-rabin primality testing miller-rabin primality testing is a primality testing randomized algorithm.\nConstruct a set of equations, each one requiring an exponentiation and a division If any of them is false, the number is composite If they are all true, the probability that the number is composite is reduced to \\(\\frac{1}{4}\\). If we run miller-rabin 10 times \\(O(10)=O(1)\\), the number is \\(1-\\left(\\frac{1}{4}\\right)^{10}\\) chance of being prime.\nThis is of course much much faster than traditional primality testing.\nModern cryptographic system uses this.\n","permalink":"https://www.jemoka.com/posts/kbhrandomized_algorithum/","tags":null,"title":"randomized algorithm"},{"categories":null,"contents":"The range (image, column space) is the set that some function \\(T\\) maps to.\nconstituents some \\(T: V\\to W\\)\nrequirements The range is just the space the map maps to:\n\\begin{equation} range\\ T = \\{Tv: v \\in V\\} \\end{equation}\nadditional information range is a subspace of the codomain This result is hopefully not super surprising.\nzero \\begin{equation} T0 = 0 \\end{equation}\nas linear maps take \\(0\\) to \\(0\\), so \\(0\\) is definitely in the range.\naddition and scalar multiplication inherits from additivity and homogeneity of Linear Maps.\nGiven \\(T v_1 = w_1,\\ T v_2=w_2\\), we have that \\(w_1, w_2 \\in range\\ T\\).\n\\begin{equation} T(v_1 + v_2) = w_1 + w_2 \\end{equation}\n\\begin{equation} T(\\lambda v_1) = \\lambda w_1 \\end{equation}\nSo closed under addition and scalar multiplication. Having shown the zero and closure, we have that the range is a subspace of the codomain. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhrange/","tags":null,"title":"range"},{"categories":null,"contents":"rational numbers are ratios:\n\\begin{equation} \\mathbb{Q} = \\left\\{\\frac{a}{b} \\middle| a,b\\in \\mathbb{Z}, b\\neq 0\\right\\} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhrational_number/","tags":null,"title":"rational number"},{"categories":null,"contents":"\\(\\mathbb{R}\\) real numbers are numbers generatable by a possibly infinite sum of powers of 10.\n","permalink":"https://www.jemoka.com/posts/kbhreal_number/","tags":null,"title":"real number"},{"categories":null,"contents":"in NSM, reductive paraphrase is the act of reducing all utterances in a language into semantic primes.\nThis is usually done with the application of an inherent, universal grammar: the conceptual grammar of semantic primes.\nproblems with reductive paraphrasing In the experiment conducted by (Labov 1973), Labov (according to (Geeraerts 2009), manuscript not found) showed that the boundaries of cup vs. mug are not clearly delineated.\n","permalink":"https://www.jemoka.com/posts/kbhreductive_paraphrase/","tags":null,"title":"reductive paraphrase"},{"categories":null,"contents":"Thanks for opening Jack\u0026rsquo;s long rambly PDF. Please read all of it; I wanted to get this out there before anything else so I apologize in advance for a letter that\u0026rsquo;s on the longer side and I didn\u0026rsquo;t have time to write a shorter one.\nBefore you begin, please read Michael\u0026rsquo;s AMAZING notes on our pitch to get the context. It\u0026rsquo;s amazing. I will not repeat here anything mentioned there.\nPat yourself on the back Oh god was that a difficult semester. We got through many a challenges and worked together to solve most of them. That\u0026rsquo;s cool. We also built a thing that the XRT team liked; so that\u0026rsquo;s cool too.\nSome of you (in the meeting) will already have known, but we are greenlit to go into phase -1! What does that mean? What changes? How can you help? Will meetings finally end on time? When will Jack finish asking silly questions? Find out more\u0026hellip; below.\nBut not too hard Just to reiterate our master deliverable as a team (like how this pitch is culminating the deliverable assigned to us on 1/6), we have until July 8th, 2022 to pitch, again:\nWhat exactly are we doing, in one line, in laymen\u0026rsquo;s terms? Why is it helpful? Clarify the roles and responsibilities for the \u0026ldquo;master faculty member\u0026rdquo;, what time commitments and value they add, and what they have to drop to support the program How can we derive legitimacy for what we are doing? (see below) For me, he also added the derivable of talking more slowly. Presumably, De wants us to come with a glossy pitch too.\nBe legit Why do we need \u0026ldquo;legitimacy\u0026rdquo;? We need motivation for kids to do this, and Nueva\u0026rsquo;s rubber stamp would be a good way to do so. this is the focus of how we are asking Lisa to greenlight phase 2 (see below)\nA valid answer for \u0026ldquo;legitimacy\u0026rdquo; is \u0026ldquo;adding the list of skills students achieved on their transcript.\u0026rdquo; Is this a good answer? Not at the moment. Its very unmotivated (this response does not pass the \u0026ldquo;why is that helpful?\u0026rdquo; test).\nAnd follow the yellow-brick road There is going to be a three stage roadmap.\nPhase -1: developing answers to PREPARE to pitch to Liza the idea, asking her to give feedback WITHOUT any of the \u0026ldquo;asks\u0026rdquo; (legitimacy, faculty time, etc.) Phase 1: building a down-scaled version of the program somewhere. Ted has mentioned interest in this, so we maybe able to co-opt some or all of his classes. Developing details and proof-of-feasibility to pitch to Liza again, this time WITH the asks to roll out to the whole school Phase 2: roll out to the whole school and prey to the Research Gods But not the leader I can\u0026rsquo;t be around forever. We are in phase -1; I will probably be gone in the middle of phase 1. We will probably have to have a faculty supporting this program unofficially for sometime, which will be a big ask.\nThis means we have to make some program changes in anticipation\u0026mdash;\nSeek a corpus callosotomy \u0026ldquo;R@N\u0026rdquo; is now separated form \u0026ldquo;Nueva Research Program.\u0026rdquo; \u0026ldquo;R@N\u0026rdquo;\u0026rsquo;s purpose is a working group to build the \u0026ldquo;Nueva Research Program.\u0026rdquo;\nWe need to separate the two as soon as possible, so that means soon. As soon as after the 7/8 deadline, I hope to make this happen. This means changes changes to our leadership structure.\nAs node A.2 outlines, \u0026ldquo;Nueva Research Program\u0026rdquo; meetings have three stable positions.\nTeams’ Stable — Responsible for managing the count, content, and quality of active Research at Nueva projects, as well as the proces of matching team members to teams. (2-3 hrs/wk) Content Stable — Responsible for managing the content of the training program and review teams. Responsible for updating nodes. Runs meetings. (1-2 hrs/wk) Participant Stable — Responsible for managing the count and recruitment of new students into the program, and identifying key experts and mentors to help build new nodes or support the program. Responsible for participant sheet (1-2 hrs/wk) As well as three review teams\nHypothesis Sciences (key mentor: TBD) Non-Hypothesis Sciences (key mentor: Ted) Literacy, Soft Skills, and Development (key mentor: TBD) In a meeting (TBD) before 7/8, we will organize ourselves into three pairs again. Each pair will choose one \u0026ldquo;stable\u0026rdquo; role and one \u0026ldquo;review team\u0026rdquo; role\u0026mdash;essentially acting as a joint-power head for the new program and a review team in itself.\nWe will split our meetings from then on in half; the first bit dealing with R@N, which I will run; the second, ACTUALLY DOING Nueva Research Programs\u0026rsquo; work, lead by the \u0026ldquo;content stable\u0026rdquo; team. This also means that we will separate the two work docs.\nOh, yeah, also, if you have gotten this far; the headings of this document forms a pretty bad poem. Please send this poem to me privately on a direct message. Thank you.\nPresumably, much of the early \u0026ldquo;nueva research program\u0026rdquo; meetings will be solely the participant stable thinking about recruiting metrics and content stable voting on new nodes. That\u0026rsquo;s OK. The protocol\u0026rsquo;s there to be changed if needed.\nBut not without your consent Although we want each and every one of you on the team (evidenced by the fact that we will be pretty screwed if anyone leaves), your main academics comes first. Please talk to me privately if you have any concerns, no harm no foul.\nAlrighty. Let\u0026rsquo;s find a time to meet.\nhttps://www.when2meet.com/?15887080-XHXI8\nI kinda want to meet y\u0026rsquo;all physically over coffee if you want; but if not virtual is all good.\nThanks again for everything!\n\u0026mdash;Jack\n","permalink":"https://www.jemoka.com/posts/kbhresearch_at_nueva_notes_06_09_2022/","tags":null,"title":"Regarding R@N"},{"categories":null,"contents":"background info Recall asymtotic analysis. We remember that:\nconstant time \u0026lt; logarithmic time \u0026lt; linear time \u0026lt; polynomial time \u0026lt; exponential time The question? What happens if dynamic programming is too slow/not good enough for the problem? What if dynamic programming is not needed; instead, why don\u0026rsquo;t we just settle for a pretty good solution?\nTake, for instance, Nueva Courses. The optimal solution is \u0026ldquo;most students get their highest possible preferences.\u0026rdquo; However, this is impractical and pretty much impossible. Instead, what if we endeavor to figure a schedule that generally maximize happiness?\nrelaxation methods constraint relaxation constraint relaxation is a relaxation method to remove extra constraints.\nMotivating problem: traveling salesman problem\nVisit all towns in a given location Travel the minimal distance to do so Cannot visit any town more than once Calculating the basic, naive solution to find all roads is \\(O(n!)\\). Best known solution is \\(O(2^nn^2)\\), which is still slow. Its also an \\(NP\\) hard problem.\nHence, to actually solve it in a reasonable time, we are going to make two relaxations.\nThe salesmen can visit a town more than once The salesmen can teleport to visited towns By these two relations, we convert traveling salesmen to the minimum spanning tree problem.\nWe now (how?) that solving MST is no worse than optimal TSP. We will solve MST, then use that problem as the upper bound of solution to TSP.\ncontinuous relaxation continuous relaxation is a relaxation method to convert difficult discrete problems into continuous ones.\nMotivating problem: set cover\nYou are having a party, and you want your friends to get a nice paper invite.\nyou will send invitations to some subsets of your friends tell them to send invitations to all your mutual friends with them What\u0026rsquo;s the minimum number of friends to invite, and who?\nSet-cover is also hard, and also NP hard. The problem is that sending invitation is discrete.\nHence, to solve, we make it possible to solve for fractions of invitations. Hence, we can prove that our solution is guaranteed to be within bounds\nLagrangian relaxation Lagrangian relaxation is a relaxation method to convert hard-limit constrains into flexible penalization (negative values).\nMotivating problem: shortest paths problem with a constraint.\nYou need to drive the shortest number of miles as well as doing it in a hard constraint to complete the solution in a certain time.\nWe can instead relax the problem into overtime driving being a negative value in the solution.\n","permalink":"https://www.jemoka.com/posts/kbhrelaxation_algorithums/","tags":null,"title":"relaxation (algorithms)"},{"categories":null,"contents":"In this experiment, a model was devised, trained, and evaluated to automate psychotherapist/client text conversations through the use of state-of-the-art, Seq2Seq Transformer-based Natural Language Generation (NLG) systems. Through training the model upon a mix of the Cornell Movie Dialogue Corpus for language understanding and an open-source, anonymized, and public licensed psychotherapeutic dataset, the model achieved statistically significant performance in published, standardized qualitative benchmarks against human-written validation data - meeting or exceeding human-written responses\u0026rsquo; performance in 59.7% and 67.1% of the test set for two independent test methods respectively. Although the model cannot replace the work of psychotherapists entirely, its ability to synthesize human-appearing utterances for the majority of the test set serves as a promising step towards communizing and easing stigma at the psychotherapeutic point-of-care.\n","permalink":"https://www.jemoka.com/posts/kbhreplier_abstract/","tags":null,"title":"Replier Abstract"},{"categories":null,"contents":"I have done various published academic research projects in the fields of natural language processing and science education. Specifically, I have an interest in textual data mining, semantic analysis, L2 acquisition, and flipped/alternative learning.\nComputational Linguistics ConDef/Dictembed Wikipedia is a surprisingly good dictionary, and so we can mine it for building context-aware dictionary. Repository, Paper.\nTitle: ConDef: Automated Context-Aware Lexicography Using Large Online Encyclopedias\nCollaborators: Zachary Sayyah - Nueva School\nStatus: Accepted for Oral Presentation and Publication\nVenue: SAI 2022 Computing Conference\nAbstract: ConDef Abstract\nReplier Using a logistic-increase mechanism to slowly blend data to fine-tune a transformer for psychotherapy. Repo, Link.\nTitle: Towards Automated Psychotherapy via Language ModelingTowards Automated Psychotherapy via Language Modeling\nCollaborators: solo project\nStatus: Pre-Print\nVenue: Cornell ArXiV\nAbstract: Replier Abstract\nGregarious Using BPE over a huge convolutional neural network with skip connections for highly-accurate identification of chat-bots on the internet. Repo, Link.\nTitle: Byte-Pair and N-Gram Convolutional Methods of Analysing Automatically Disseminated Content on Social Platforms\nCollaborators: solo project\nStatus: Pre-Print\nVenue: Open Science Foundation Preprints\nAbstract: Gregarious Abstract\nBRANDON/nsm Investigating into the Natural Semantic Metalanguage theory, and how we can use deep-learning methods to deal with prooving/disprooving the Lexicalist Hypothesis. Repo.\nCollaborators: Brandon Cho - Nueva School/Princeton\nWork-in-progress.\npolitisort Sorting and generating politically-motivated utterances. Repo.\nCollaborators: Zachary Sayyah - Nueva School\nWork-in-progress.\ndementia A task similar to ADReSS Challenge, training on acoustic and possibly linguistic features. internal link\nLab: PsychLing - CMU Dietrich\nPI: Prof. Brian MacWhinney - CMU\nWork-in-progress.\nbatchalign Suite of software tools for the automatic transcription, tokenization, alignment, and morphosyntactical analysis of raw audio files. Main goal is to make the process of CHAT transcription entirely automated. Repo, internal Link.\nLab: PsychLing - CMU Dietrich\nPI: Prof. Brian MacWhinney - CMU\nIn Review\nScience Education Project80 A student-driven podcast protocol which trains students to digest scientific research. Link, Internal Link.\nTitle: Project 80: a reproducible, student-driven framework for creating multimedia educational resources from primary literature\nCollaborators: Anoushka Krishnan, Micah Brown - Nueva School\nLab: XRT - Nueva School\nPI: Paul Hauser, Luke De - Nueva School\nStatus: Published\nVenue: EB2022/FASEB Journal\nAbstract: Project80 Abstract\nResearch@Nueva A student-lead, student-taught independent program that trains high-school students as researchers and facilitates publish-quality student research.\nTitle: R@N: an Asynchronous, Student-Led Program to Train Students to Conduct Independent Research\nCollaborators: Michael, Flint, Kian, Vinca, Oliver, Zach, Albert - Nueva School\nLab: XRT - Nueva School\nPI: Paul Hauser - Nueva School\nStatus: Accepted\nVenue: DiscoverBMB2023/Journal of Biological Chemistry\nAbstract: R@N Abstract\nParallel Computing/Blockchain Coveather See also coveather. Link.\nTitle: Encrypted, Anonymized System for Protected Health Information Verification Built via Proof of Stake\nCollaborators: solo project\nStatus: Pre-Print and Oral Presentation at the California STEM Fair\nVenue: Cornell ArXiV\nAbstract: Coveather Abstract\n","permalink":"https://www.jemoka.com/posts/kbhresearch_index/","tags":["index"],"title":"Research Index"},{"categories":null,"contents":"A reticle is a photomask/template for a lithography system (like a negative). KLA was the first company to automatically inspect wafers and reticles.\n","permalink":"https://www.jemoka.com/posts/kbhreticle/","tags":null,"title":"reticle"},{"categories":null,"contents":"Richard Nixon is an American president, but pretty much is the watergate guy.\nServed in House and Senate Eisenhower\u0026rsquo;s VP for 8 years Lost first to JFK Richard Nixon is a pragmatist; he pushes economy out of presession via Keynsian Politics.\nRichard Nixon also realized that the large southern population can be motivated via racist policies, so he shifted the .\npolitical positions of Richard Nixon Richard Nixon\u0026rsquo;s Treatment against the Vietnam War Richard Nixon\u0026rsquo;s Foreign Policy ","permalink":"https://www.jemoka.com/posts/kbhrichard_nixon/","tags":null,"title":"Richard Nixon"},{"categories":null,"contents":"Richard Nixon\u0026rsquo;s foreign policy is marked by the \u0026ldquo;Nixon Doctrine\u0026rdquo;: shifting the burden of military containment to allies.\nSupports China as a means against USSR Negotiate with the USSR to lower tension Shifts focus into building and supporting allies ","permalink":"https://www.jemoka.com/posts/kbhrichard_nixon_s_foreign_policy/","tags":null,"title":"Richard Nixon's Foreign Policy"},{"categories":null,"contents":"Richard Nixon proposed the strategy of vietnamization as a treatment to the Vietnam War. He also expanded to Cambodia. To beat the Viet Cong into submission, he initialized the Operation Linebacker campaign.\n","permalink":"https://www.jemoka.com/posts/kbhrichard_nixon_s_treatment_against_the_vietnam_war/","tags":null,"title":"Richard Nixon's Treatment against the Vietnam War"},{"categories":null,"contents":"Rick Wallace is the CEO of KLA.\n","permalink":"https://www.jemoka.com/posts/kbhrick_wallace/","tags":null,"title":"Rick Wallace"},{"categories":null,"contents":"Ronald Reagan is a president of the United States. He rises a wave of the New Right.\nComes out of Hollywood and was CA governor Reagan was a democrat, but McCarthyism lead him Reagan was an FBI informer for McCarthyism investigations Reagan was the first two-term president since 1961, was able to maintain more power compared to others \u0026ldquo;The Great Communicator\u0026rdquo; Reagan politics \u0026ldquo;Government isn\u0026rsquo;t the solution to the problem, its the problem.\u0026rdquo;\nwished for limited politics states rights condemned welfare and \u0026ldquo;welfare cheats\u0026rdquo; (the undertone of racist appeal) Evangelical undertones, family values, moral majority Against affirmative action Supply-side economics: \u0026ldquo;getting rid of taxes will allow more people to spend\u0026rdquo; Anti-Soviet rhetoric Creates the largest increase in welfare spending, gutting about $1.5Bn.\nReagan policy changes Lowering taxes: 70% of tax to 28% of taxes Increase defense budget: 1 trillion to 3 trillion Rising inequality, 1% controlled 40% of wealth (double from the 1970s) Reagan Foreign Policy Ronald Reagan creates the largest military build-up in history (larger than Korea and Vietnam.)\nReasserted Command-in-Chief abilities Creates the National Security Council (for whom the ) Comitted the US to supporting the anti-Marxist insurrections around the world Credited with falling the USSR Supreme Court Interview Process A new interview process for the supreme court designed by Ronald Reagan, creating an extensive process to vet conservative. Reagan swapped out 50% of the Federal judicial process.\nReagan\u0026rsquo;s Legacy Inflation dropped\nUSSR Collapse\nMilitary complex expanded\nIncomes rose\nInequality widened\nWelfare slashed\nDebt\nConcentrated power in the white house\nCentralized conservative agenda\n","permalink":"https://www.jemoka.com/posts/kbhronald_raegan/","tags":null,"title":"Ronald Reagan"},{"categories":null,"contents":"The Rosa Parks bus incident is the instigator which needed to act on an issue to challenge the civil rights movement.\nShe participated in many civil rights agitations, and became the instigator .\n","permalink":"https://www.jemoka.com/posts/kbhrosa_parks/","tags":null,"title":"Rosa Parks"},{"categories":null,"contents":"On the dynamics of Tuning Forks. (Rossing, Russell, and Brown 1992)\nCharacterizing Tuning Forks Aluminum, tines 10mm apart. Four main groups of vibration:\nSymmetrical In-Plane Antisymmetrical In-Plane Symmetrical Out-Of-Plane Antisymmetrical Out-Of-Plane (a) and (c) are in the first group; (b) is in the second group, where the fork just warps.\nDeriving Tuning Forks\u0026rsquo; Frequency As per before, we can treat tuning forks acting in clang and fundamental modes as a good\u0026rsquo;ol fashioned cantilever beam.\nThe frequency action of a cantilever beam is defined as follows:\nOtherwise, for asymmetric modes, we can use the same exact expression but with uniform rods unfixed at either end:\nNote that density is not uniform at this point (because the bottom handle-y bit.)\n","permalink":"https://www.jemoka.com/posts/kbhrossing_1990/","tags":null,"title":"Rossing 1990"},{"categories":null,"contents":"total kinetic energy \\begin{equation} KE_{rigid} = \\frac{1}{2} M{V_{cm}}^2 + \\frac{1}{2} I_{CM}{\\omega_{CM}}^2 \\end{equation}\ntorque from gravity For even non rigid bodies, the following follows:\n\\begin{equation} \\vec{\\tau}_g = \\vec{R}_{CM} \\times M\\vec{g} \\end{equation}\nActually, this follows for any \\(f\\) (like \\(g\\)) evenly applied across point masses.\npotential energy \\begin{equation} \\Delta PE_g = mg\\Delta h \\end{equation}\nwhere, \\(\\Delta h\\) is the travel of center of mass. Regardless of whether or not its point.\n","permalink":"https://www.jemoka.com/posts/kbhrotational_energy/","tags":null,"title":"rotational energy theorem"},{"categories":null,"contents":"Rural Electrification Administration create electrification throughout cities. Most of American infrastructure still 1930s.\n","permalink":"https://www.jemoka.com/posts/kbhrural_electrification_administration/","tags":null,"title":"Rural Electrification Administration"},{"categories":null,"contents":"Observations from studying the comedian Russel Howard.\nStretching analogies Using language/motion/figure do describe something on the opposite end of the spectrum Take, for instance, age: 5Y/O: \u0026ldquo;cheers mum, wasen\u0026rsquo;t on my to-do list\u0026rdquo; A surprisingly sentimental dog: \u0026ldquo;because when I wake up tomorrow I want to see you, and I want to go for a lovely walk\u0026rdquo; Large motions + deadpan after Endless extrapolations of a normal setup: setup: Russian hackers were controlling people\u0026rsquo;s toys; punchline: \u0026ldquo;5 men were dildo\u0026rsquo;d to death, we don\u0026rsquo;t have a recording but here are their final words \u0026mdash; \u0026lsquo;oh yeaaah\u0026rsquo;, \u0026lsquo;oh fuck yeaaah\u0026rsquo;\u0026rdquo; Setup: gweneth paltro Punchline: \u0026ldquo;put an egg up there, you will feel more femenine. no! you will feel like a chicken\u0026rdquo;\nMultiple use of setups: \u0026ldquo;happy birthday too you\u0026rdquo; Peach ","permalink":"https://www.jemoka.com/posts/kbhrussel_howard/","tags":null,"title":"Russel Howard"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624594\n(Sadeghian, Schaffer, and Zahorian 2021)\nOne-Liner Using a genetic algorithm, picked features to optimize fore; achieved \\(94\\%\\) with just MMSE data alone (ok like duh me too). Developed ASR tool to aid.\nNovelty Developed an ASR methodology for speech, complete with punctuations Used a genetic algorithm to do feature selection; NNs performed worse because \u0026ldquo;space is smaller???\u0026rdquo; Notable Methods Used a GRU to insert punctuations The paper leveraged the nuke that is a bidirectional GRU, ATTENTION,\nKey Figs Fully automated ANN transcript does pretty well in terms of classifier AD/NL.\nNew Concepts fusion genetic algorithm MMSE Notes very confusing (too many things going on at once)\n","permalink":"https://www.jemoka.com/posts/kbhsadeghian_2021/","tags":["ntj"],"title":"Sadeghian 2021"},{"categories":null,"contents":"Demo day No value add for demo-day winner Competition makes you want to prepare more \u0026ldquo;this much budget for an enriching experience\u0026rdquo; Mentor Conversations None yet\nIntegration Integration into soundscape Hiring Need help designing a PCB\n","permalink":"https://www.jemoka.com/posts/kbhsalus_april_checkin/","tags":null,"title":"Salus April Checkin"},{"categories":null,"contents":" adding emails need sequential typing hitting enter should move on to the next page date selection \u0026ldquo;Monday next week\u0026rdquo; doesn\u0026rsquo;t NLP reading calendar output isn\u0026rsquo;t sorted bugs reading other people\u0026rsquo;s calendars isn\u0026rsquo;t working\nneed some information about that the heck is actually happening on the scheduling\nshow other people\u0026rsquo;s overall availibliity in the scheduling page\nthe weight number doesn\u0026rsquo;t make sense\u0026mdash;correct alt-text and make the number work\nhave the idea of a \u0026ldquo;meeting owner\u0026rdquo;, and we only reach out to them to confirm final date + have an ability to add a message; also allow the message owner to change to alternate schedules on that date\nalso scheduling multiple people is broken. ah. ","permalink":"https://www.jemoka.com/posts/kbhscalander_notes-1/","tags":null,"title":"scalander notes"},{"categories":null,"contents":" adding emails need sequential typing hitting enter should move on to the next page date selection \u0026ldquo;Monday next week\u0026rdquo; doesn\u0026rsquo;t NLP reading calendar output isn\u0026rsquo;t sorted bugs reading other people\u0026rsquo;s calendars isn\u0026rsquo;t working need some information about that the heck is actually happening on the scheduling show other people\u0026rsquo;s overall availibliity in the scheduling page idea of \u0026ldquo;budget\u0026rdquo; next actions scheduling multiple people is broken. ah. have the idea of a \u0026ldquo;meeting owner\u0026rdquo;, and we only reach out to them to confirm final date + have an ability to add a message (with templates); also allow the message owner to change to alternate schedules on that date the weight number doesn\u0026rsquo;t make sense\u0026mdash;correct alt-text and make the number work ","permalink":"https://www.jemoka.com/posts/kbhscalander_notes-2/","tags":null,"title":"scalander notes"},{"categories":null,"contents":" adding emails need sequential typing hitting enter should move on to the next page date selection \u0026ldquo;Monday next week\u0026rdquo; doesn\u0026rsquo;t NLP reading calendar output isn\u0026rsquo;t sorted bugs reading other people\u0026rsquo;s calendars isn\u0026rsquo;t working need some information about that the heck is actually happening on the scheduling show other people\u0026rsquo;s overall availibliity in the scheduling page idea of \u0026ldquo;budget\u0026rdquo; next actions scheduling multiple people is broken. ah. have the idea of a \u0026ldquo;meeting owner\u0026rdquo;, and we only reach out to them to confirm final date + have an ability to add a message (with templates); also allow the message owner to change to alternate schedules on that date (have default notifications in the iCal invite) somehow remind people after the fact that the meeting is scheduled tie in evite abilities (this will be nice for your party, etc.) event planning built in? type in a budget and find vendors for the party. age range? the weight number doesn\u0026rsquo;t make sense\u0026mdash;correct alt-text and make the number work home page \u0026ldquo;the front page looks like that for an OB-GYN\u0026rdquo; \u0026mdash; feels like ZocDoc it is also not that fun maybe some kind of memphis design ","permalink":"https://www.jemoka.com/posts/kbhscalander_notes/","tags":null,"title":"scalander notes"},{"categories":null,"contents":"Scalar multiplication is the process of multiplying a scalar to an element in a set.\nconstituents A set \\(V\\) Some \\(\\lambda \\in \\mathbb{F}\\) Each \\(v \\in V\\) requirements scalar multiplication is defined by a function that results in \\(\\lambda v \\in V\\) (maps back to the space!) to each \\(\\lambda \\in \\mathbb{F}\\) and each \\(v \\in V\\).\nadditional information See also scalar multiplication in \\(\\mathbb{F}^n\\).\n","permalink":"https://www.jemoka.com/posts/kbhscalar_multiplication/","tags":null,"title":"scalar multiplication"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\n","permalink":"https://www.jemoka.com/search/","tags":null,"title":"Search Results"},{"categories":null,"contents":"The second moment of area is a value which\u0026mdash;given an origin\u0026mdash;describes how point masses are distributed around that origin. (i.e. a number for how point masses are distributed). It is in units \\(m^{4}\\).\nTake, for instance, the following picture:\nWe have defined an origin at \\((0,0)\\) of the figure above. Furthermore, we have some \\(\\rho_{i}\\) which is the distance from that origin to each of the infinitesimal areas \\(\\dd{A}\\).\nThen, the second moment of area is defined as:\n\\begin{equation} I = \\iint_{R} \\rho^{2} \\dd{A} \\end{equation}\nThis\u0026hellip; would make sense.\n","permalink":"https://www.jemoka.com/posts/kbhsecond_moment_of_area/","tags":null,"title":"second moment of area"},{"categories":null,"contents":"the trick Here is a pretty ubiquitous trick to solve differential equations of the second order differential equations. It is used to change a second order differential equation to a First-Order Differential Equations.\nIf you have a differential equation of the shape:\n\\begin{equation} x^{\u0026rsquo;\u0026rsquo;} = f(x,x\u0026rsquo;) \\end{equation}\nthat, the second derivative is strictly a function between the first derivative value and the current value.\nWe are going to define a notation \\(x\u0026rsquo; = v\\), which makes sense.\nSo, we will describe:\n\\begin{equation} x^{\u0026rsquo;\u0026rsquo;} = \\dv{v}{t} = \\dv{v}{x} \\dv{x}{t} = v\\dv{v}{x} \\end{equation}\nSo therefore, we have:\n\\begin{equation} x^{\u0026rsquo;\u0026rsquo;} = v\\dv{v}{x} = f(x,v) \\end{equation}\nSo turns out, the original input \\(t\\) is, given a specific equation above, we have no need to know it.\nTo actually go about solving it, see solving homogeneous higher-order differential equations.\n","permalink":"https://www.jemoka.com/posts/kbhsecond_order_differential_equations/","tags":null,"title":"second order differential equation"},{"categories":null,"contents":"Here\u0026rsquo;s a general form:\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = f(t) \\end{equation}\nsolving homogeneous higher-order differential equations This problem because easier if the right side is \\(0\\).\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = 0 \\end{equation}\nThe general goal to solve in this case is to make this a system of First-Order Differential Equations.\nTo do this, we begin by making:\n\\begin{equation} y = \\dv{x}{t} \\end{equation}\nTherefore, we can change the first equation:\n\\begin{equation} a \\dv{y}{t} + by + cx = 0 \\end{equation}\nSolving both of these conditions, we form a system of linear equations:\n\\begin{align} \u0026amp;\\dv{x}{t}=y \\\\ \u0026amp;\\dv{y}{t} = \\frac{-c}{a}x-\\frac{b}{a}y \\end{align}\nWe are now first-order, so we can put this into a matrix equation:\n\\begin{equation} \\dv t \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \u0026amp; 1 \\\\ -\\frac{c}{a} \u0026amp; \\frac{-b}{a} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\end{equation}\nNow! We have an equation:\n\\begin{equation} \\dv{t}v = Av \\end{equation}\nThe result above shows that the transformations \\(\\dv{t}\\) and \\(A\\) are isomorphic. Therefore, we now attempt to characterize \\(A\\) to solve this expression.\nLet\u0026rsquo;s begin. We will first shove that \\(v\\) on top of the differential for aesthetics:\n\\begin{equation} \\dv{v}{t} = Av \\end{equation}\nThis expression is actually nicely seperable, so we shall endeavor to separate it:\n\\begin{equation} \\dd{v} = Av\\dd{t} \\end{equation}\nOf course, \\(v\\) is a function of \\(t\\). Therefore, the right side would be woefully complicated. Therefore, we shall do this handwavy thing where we go:\n\\begin{equation} \\frac{1}{v}\\dd{v} = A\\dd{t} \\end{equation}\nNow, \\(A\\) is not a function in \\(t\\) \u0026mdash; its just some constants! So, we can integrate this safely without much trouble:\n\\begin{equation} \\int \\frac{1}{v}\\dd{v} =\\int A\\dd{t} \\end{equation}\nTo get:\n\\begin{equation} \\ln v = t A + C \\end{equation}\nNote the order as \\(t\\) is a constant. Finally, we will invert the natural log and get \\(v\\) back:\n\\begin{equation} v = e^{tA+C} \\end{equation}\nExcellent. We will now apply some log/exponent laws:\n\\begin{equation} v = e^{tA}e^{C} = e^{tA}C \\end{equation}\nthis is so very handwavy. \\(C\\) is technically a vector here\u0026hellip; long story and iffy understanding\nOk, how do we go about solving \\(x\\)?\nNote now that \\(v=(x\\ y)\\), so we will expand that:\n\\begin{equation} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = e^{tA}\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} \\end{equation}\nwhere, as we defined above \\(y=\\dv{x}{t}\\) (each integral needing a different constant.)\nNow. remember that \\(A\\) is diagonalizable; and so will \\(tA\\) (citation needed, but intuition is that scaling eigenvalues do nothing anyways). So, to make this exponentiation easier, we will diagonalize it.\nWe now have that\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\n(how?)\nOk. Finally, we will take the binroller that is \u0026ldquo;constancy\u0026rdquo; and apply it to \\(e^{tA}\\). This took quite a bit of time for me to get, so feel free to take some time to get it too.\nThis all hinges upon the fact that \\(C\\) is a constant, so multiplying any constant to it still makes it \\(C\\).\nSo far, we have that:\n\\begin{equation} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = e^{tA}\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} = \\qty(\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} )\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} \\end{equation}\nRemember, now, that \\(v_1\\dots v_{}\\) and its inverses are nothing but vectors filled with a lot of scalars. And any scalar \\(\\alpha\\) times a constant still results in the (a new) constant: \\(\\alpha C =C\\). So, we will steamroll \\(\\mqty(x_0\u0026amp;y_0)\\) over the right side eigenbases matrix (multiplying a constant vector to any\u0026rsquo;ol matrix will just get a new set of constants back) to get:\n\\begin{align} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \u0026amp;= \\qty(\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} )\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} \\\\ \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}}) \\begin{pmatrix} C_1 \\\\ C_2 \\end{pmatrix} \\end{align}\nNow, the middle thing has \\(t\\) in it! (the input!) So, we can\u0026rsquo;t just steamroll now. We have to preserve the middle part.\n\\begin{align} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}}) \\begin{pmatrix} C_1 \\\\ C_2 \\end{pmatrix} \\\\ \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m}) \\begin{pmatrix} C_1 e^{t\\lambda_{1}} \\\\ C_2 e^{t\\lambda_{2}} \\end{pmatrix} \\end{align}\nAnd finally, we keep steamrolling:\n\\begin{align} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m}) \\begin{pmatrix} C_1 e^{t\\lambda_{1}} \\\\ C_2 e^{t\\lambda_{2}} \\end{pmatrix}\\\\ \u0026amp;= \\mqty({C_{1_{x}} e^{t\\lambda_{1}} + C_{2_{x}} e^{t\\lambda_{2}}} \\\\ {C_{1_{y}} e^{t\\lambda_{1}} + C_{2_{y}} e^{t\\lambda_{2}}}) \\end{align}\nThere is absolutely no difference in nature between \\(C_{j_{x}}\\) and \\(C_{j_{y}}\\) except for the fact that they are different constants (which we got by multiplying \\(v_1 \\dots v_{m}\\)) to it.\nOk so:\n\\begin{equation} \\begin{cases} x = C_{1_{x}} e^{t\\lambda_{1}} + C_{2_{x}} e^{t\\lambda_{2}}\\\\ y = C_{1_{y}} e^{t\\lambda_{1}} + C_{2_{y}} e^{t\\lambda_{2}}\\\\ \\end{cases} \\end{equation}\nconstructing the characteristic equation, as desired.\nmethod of undetermined coefficients Ok. This mechanism hinges upon the fact that linear combinations of differential equation solutions are solutions themselves. You can show this to yourself by illustrating diffeq solutions as subspaces of F^S, which are linear objects.\nTherefore, for a non-homogeneous second-order linear equation, we attempt to find two sets of solutions\u0026mdash;\nnamely, the general solution to the homogeneous case (using method above):\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = 0 \\end{equation}\nas well attempting to fit particular solutions to the general case:\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = f(t) \\end{equation}\nthe linear combination of both solutions would construct the final solution space.\nWe already know how to do step 1\u0026mdash;solve homogeneous higher-order differential equations\u0026mdash;so we won\u0026rsquo;t harp on it here. However, how do we find particular solutions to the general equations?\nWell, we guess! Here\u0026rsquo;s a general table to help illustrate how:\n\\(f(t)\\) \\(x(t)\\) \\(ae^{bt}\\) \\(Ae^{bt}\\) \\(a \\cos (ct) + b\\sin (ct)\\) \\(A\\cos(ct) + B\\sin (ct)\\) \\(kt^{n}\\) \\(A_{n}t^{n} + A_{n-1}x^{n-1} \\dots + A_{0}\\) you can show these to yourself by taking derivatives. \\(a,b,c, k,A,B\\) are distinct constants.\nNow, once you make an educated guess for what \\(x(t)\\) is, perhaps aided by the homogeneous solution, you would take the number of derivatives needed to plug it back to the original expression. Then, equate the left expression and right \\(f(t)\\) and match coefficients of equal-degree terms to solve for the final constants \\(A\\), \\(B\\), etc.\nAfter you finally got the specific solution for \\(A\\) and \\(B\\) , we add the degree of freedom back by adding the homogenous solution in.\nLook for \u0026ldquo;Example 1 (again)\u0026rdquo; on this page (silly, I know, but worth it) to see end-to-end such a solution.\n","permalink":"https://www.jemoka.com/posts/kbhsecond_order_linear_differential_equation/","tags":null,"title":"Second-Order Linear Differential Equations"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhselective_service_system/","tags":null,"title":"Selective Service System"},{"categories":null,"contents":"In NSM, semantic primes are the most fundimental \u0026ldquo;lexical units\u0026rdquo; (so they can be words, or morphemes, etc. the size doesn\u0026rsquo;t matter) across languages.\nThey are the \u0026ldquo;core of a universal mental lexicon\u0026rdquo;.\nThere are\u0026hellip;\nguidelines for identifying semantic primes A semantic prime has to be found in every(ish?) natural language A semantic prime has to be indefinable by other primes proof for the existence of semantic primes Proof: given if the Strong Lexicalization Hypothesis holds, semantic primes must exist.\nAssume for the sake of contradiction no semantic primes exist.\nBecause Strong Lexicalization Hypothesis holds, there does not exist syntactic transformations which can take original single words and transform them into newly lexicalized words to express a different meaning.\nAt the same time, again because of the Strong Lexicalization Hypothesis, one must only leverage syntactic transformation on syntatic constituents when forming ideas.\nTherefore, given a word to lexicalize, it has to be defined by an syntatic transformation on a set of previously lexicalized words.\n(by definition) there are no words lexicalizable from the empty set of words.\nTherefore, there exists some word that needs to be lexicalized by words that are not previously defined, which is absurd. (instead, these words are lexicalized via semantic primes.)\nQED\nproblems with semantic primes the list has grown over time the problem of allolexy: formal restrictions of a language resulting in the same concept needing to be radicalized multiple times (I vs. me) finding semantic primes According to (Geeraerts 2009), (Goddard 2009) provides a \u0026ldquo;practical\u0026rdquo; (though flawed) way of establishing primes. Something to do with large-scale comparisons in \u0026ldquo;whole metalanguage studies\u0026rdquo;, which requires pairwise language comparison\nLocating primes are seen as an enforcement of NSM theories (Vanhatalo, Tissari, and Idström, n.d.). Recent prime locations: in Amharic (Amberber 2008), East Cree (Junker 2008), French (Peeters 1994), Japanese (Onishi 1994), Korean (Yoon 2008), Lao (Enfield 2002), Mandarin (Chappell 2002), Mangaaba-Mbula (Bugenhagen 2002), Malay (Goddard 2002), Polish (Wierzbicka 2002), Russian (Gladkova 2010, for the latest set, see the NSM home page), Spanish (Travis 2002), and Thai (Diller 1994).\n","permalink":"https://www.jemoka.com/posts/kbhsemantic_primes/","tags":null,"title":"semantic prime"},{"categories":null,"contents":"SVF is a standardized Discourse-Completion Task for verbal recall and fluency. It is administered by asking the participant to recall a bunch of words from within a category within 60 seconds.\n","permalink":"https://www.jemoka.com/posts/kbhsemantic_verbal_fluency/","tags":null,"title":"Semantic Verbal Fluency"},{"categories":null,"contents":"The semiconductor industry is a growing industry, the beginning of the semiconductor industry was actually in the silicon valley.\nWe are now taking a look at a reticle.\nalgorithms used in the semiconductor industry Per KLA \u0026mdash;\nClassification Random forest Boosted decision trees MLPs CNNs Reference generation GANs (WAT) VAEs Natural Grouping and Clustering auto-encoders manual feature extractors ","permalink":"https://www.jemoka.com/posts/kbhsemiconductor/","tags":null,"title":"semiconductor"},{"categories":null,"contents":"A set is an unordered collection of objects, which maybe infinitely long. It is generated with \\(\\{, \\}\\). For instance, most numbers are sets.\nconstituents a collection of objects requirements repetition does not matter order does not matter additional information ","permalink":"https://www.jemoka.com/posts/kbhset/","tags":null,"title":"set"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhsets/","tags":null,"title":"sets"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624659\nOne-Liner Multi-feature late fusion of NLP results (by normalizing text and n-gram processing) with OpenSMILE embedding results.\nNovelty NLP transcript normalization (see methods) and OpenSMILE; otherwise similar to Martinc 2021. Same gist but different data-prep.\nNotable Methods N-gram processed the input features Used WordNet to replace words with roots Key Figs New Concepts OpenSMILE ","permalink":"https://www.jemoka.com/posts/kbhshah_2021/","tags":["ntj"],"title":"Shah 2021"},{"categories":null,"contents":"Short selling involves betting against the stock.\nProcess of Short Selling the trader borrows a number of shares from a third party the trader sells them immediately for cash when the security dips, the debt is repaid by repurchasing the same amount of shares of the borrowed security at the lower price traders nets the profit from the negative price differential If the person shorting\nshort squeeze \u0026ldquo;what happened to GameStock\u0026rdquo;\nA short squeeze is a situation in which a bunch of people try to drive the price of the up by buying enough shares such that the short sellers are forced to sell high\u0026mdash;driving up the price.\n","permalink":"https://www.jemoka.com/posts/kbhshort_selling/","tags":null,"title":"short selling"},{"categories":null,"contents":"Here is the most simple Differential Equation one could imagine:\n\\begin{equation} \\dv{x}{t} = f(t,x) \\end{equation}\nOr, perhaps, we have a second order differential equation which is the same thing but in the second degree:\n\\begin{equation} \\dv[2]{x}{t} = f\\qty(t,x,\\dv{x}{t}) \\end{equation}\nThen in which case, we have that the first most simple type of differential equation to be as follows:\n\\begin{equation} \\dv{x}{t} = x(t) \\end{equation}\nIf we can solve this, we can generalize this to most of other First-Order Differential Equations.\nwhere, the function \\(f(t,x)=x(t)\\).\n\\begin{align} \u0026amp; \\dv{x}{t} = x(t) \\\\ \\Rightarrow\\ \u0026amp; \\frac{1}{x(t)}\\dd{x} = \\dd{t} \\end{align}\nAt this point, you may ask yourself, why not construct it such that we have \\(\\dd{x} = x(t)\\dd{t}\\)? Well, its because our \\(x\\) is a variable in \\(t\\), so if we constructed it that way we\u0026rsquo;d have to integrate a function \\(\\dd{t}\\) with usub and the reverse chain rule, etc. etc. If we are instead integrating it on \\(\\dd{x}\\), it becomes much easier because our variable of interest no longer considers the \\(t\\).\nContinuing on, then:\n\\begin{align} \u0026amp;\\frac{1}{x(t)}\\dd{x} = \\dd{t} \\\\ \\Rightarrow\\ \u0026amp;\\int \\frac{1}{x(t)}\\dd{x} = \\int \\dd{t} \\\\ \\Rightarrow\\ \u0026amp; \\ln (x(t)) = t \\\\ \\Rightarrow\\ \u0026amp; x(t) = e^{t} \\end{align}\nAwesome. It should\u0026rsquo;t be hard also to see that, generally:\n\\begin{equation} x(t) = e^{ct} \\end{equation}\nis the solution to all equations \\(\\dv{x}{t} = cx\\).\nTurns out (not proven in the book), this holds for complex valued equations as well. So, we have some:\n\\begin{align} \u0026amp;x(t) = e^{it} \\\\ \\Rightarrow\\ \u0026amp; \\dv{x}{t} = ix \\end{align}\nOf course, from elementary calculus we also learned the fact that \\(e^{x}\\) can be represented as a power series; so check that out for now we connect it.\nThis equation leads us to solve:\n\\begin{equation} \\dv{x}{t} + ax = b(t) \\end{equation}\nIn order to do this, we neeed to find a replacement of the property that:\n\\begin{equation} \\dv t\\qty(e^{at}x) = e^{at}\\qty(\\dv{x}{t} +at) \\end{equation}\nA more general result of the above form is\n\\begin{equation} \\dv{x}{t} + a(t)x = b(t) \\end{equation}\nThis is fine, but now we need to leverage to chain rule to have \\(\\dv t a(t)\\) would be simply changing the above result to \\(a\u0026rsquo;(t)\\).\nBut anyways through this we will end up with the same solution we get from solving differential equations.\n","permalink":"https://www.jemoka.com/posts/kbhsimple_differential_equations/","tags":null,"title":"Simple Differential Equations"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhsingle_party_control/","tags":null,"title":"single party control"},{"categories":null,"contents":"The SIR Model is a model to show how diseases spread.\nSusceptible \u0026ndash; # of susceptible people Infectious \u0026mdash; # of infectious people Removed \u0026mdash; # of removed people Compartmental SIR model S =\u0026gt; I =\u0026gt; R [ =\u0026gt; S]\nSo then, the question is: what is the transfer rate between populations between these compartments?\nParameters:\n\\(R_0\\) \u0026ldquo;reproductive rate\u0026rdquo;: the number of people that one infectious person will infect over the duration of their entire infectious period, if the rest of the population is entirely susceptible (only appropriate for a short duration) \\(D\\) \u0026ldquo;duration\u0026rdquo;: duration of the infectious period \\(N\\) \u0026ldquo;number\u0026rdquo;: population size (fixed) Transition I to R:\n\\begin{equation} \\frac{I}{D} \\end{equation}\n\\(I\\) is the number of infectious people, and \\(\\frac{1}{D}\\) is the number of people that recover/remove per day (i.e. because the duration is \\(D\\).)\nTransition from S to I:\n\\begin{equation} I \\frac{R_0}{D} \\frac{S}{N} \\end{equation}\nSo for \\(\\frac{R_0}{D}\\) is the number of people able to infect per day, \\(\\frac{S}{N}\\) is the percentage of population that\u0026rsquo;s able to infect, and \\(I\\) are the number of people doing the infecting.\nAnd so therefore\u0026mdash;\n\\(\\dv{S}{T} = -\\frac{SIR_{0}}{DN}\\) \\(\\dv{I}{T} = \\frac{SIR_{0}}{DN}\\) \\(\\dv{I}{T} = \\frac{I}{D}\\) Evolutionary Game Theory Suppose that we have two strategies, \\(A\\) and \\(B\\), and they have some payoff matrix:\nA B A (a,a) (b,c) B (c,b) (d,d) and we have some values:\n\\begin{equation} \\mqty(x_{a} \\\\x_{b}) \\end{equation}\nare the relative abundances (i.e. that \\(xa+xb\\)).\nThe finesses (\u0026ldquo;how much are you going to reproduce\u0026rdquo;) of the strategies are determined by\u0026mdash;\n\\(f_{A}(x_{A}, x_{B}) = ax_{A} + bx_{B}\\) \\(f_{B}(x_{A}, x_{B}) = cx_{A} + dx_{B}\\) Except for payoff constants \\((a,b,c,d)\\), everything else is a function of time.\nThe mean fitness, then:\n\\begin{equation} q = x_{A}f_{A} + x_{B}f_{B} \\end{equation}\nLet\u0026rsquo;s have the actual, absolute number of individuals:\n\\begin{equation} \\mqty(N_{A}\\\\ N_{B}) \\end{equation}\nSo, we can talk about the change is individuals using strategy \\(A\\):\n\\begin{equation} \\dv t x_{A} = \\dv t \\frac{N_{A}}{N} = X_{A}(f_{a}) \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhsir_model/","tags":null,"title":"SIR Model"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhslopes/","tags":null,"title":"slope (statistics)"},{"categories":null,"contents":"a function is called smoo\n","permalink":"https://www.jemoka.com/posts/kbhsmooth_function/","tags":null,"title":"smooth function"},{"categories":null,"contents":"Social Security Administration is a welfare program to directly give cash to those who are in need.\n","permalink":"https://www.jemoka.com/posts/kbhsocial_security_administration/","tags":null,"title":"Social Security Administration"},{"categories":null,"contents":"Here\u0026rsquo;s a bit of a guide to start in software development. It is mostly links to other resources that would help.\nIntroductory Remarks Nobody \u0026ldquo;learns\u0026rdquo; software development. Even in job interviews, people expect you to have \u0026ldquo;worked\u0026rdquo; in software development. The industry, as a whole, drives via \u0026ldquo;learn-by-doing\u0026rdquo;, so its best to start thinking about what you want to achieve with software dev in terms of projects, then look specifically for resources to help you achieve those. Once you Google enough, et viola! You will have the skills needed to tackle another project.\nCommon Tooling There are some common tooling that is standard across all of software development.\nGoogle Google it! 99.98% of programming skills center around google-fu. Learn to Google unknown terms and get a better sense of the picture. The same rule applies through this guide as well.\nStackExchange A group of very mean people put together a very helpful family of websites which are essentially vicious forum boards. They are the StackExchange family of boards.\nThe most famous of which, and the one focused on programming, is called StackOverflow. StackOverflow (\u0026ldquo;SO\u0026rdquo;) is an extremely helpful resource for browsing any question you may have. For instance, if your code crashes with a stack trace, Googling the error and set site:stackoverflow.com will get you pretty far.\nIf you ask a question, though, be prepared to get yelled at though, the likely reason is that your question is already answered.\nmacOS For the quick-start type of hardware fitting for this guide: get a macBook. Even the cheapest one.\nDevelopment on Windows is like cooking on campfire. Doable, useful for specific things, but not great overall. If you have a PC, I would (and recommend! its great for advanced users especially) to put Debian/Ubuntu/some easy to use Linux on it. Windows is just terrible.\nI should add that Microsoft started doing Windows Subsystem for Linux: https://docs.microsoft.com/en-us/windows/wsl/install, which apparently have been pretty good. So worth taking a shot if you are stuck on Windows.\n*nix Terminal BSD/UNIX terminal is a tool that essentially skips the fancy user interface (UI) which your operating system draws and directly runs things \u0026ldquo;organically.\u0026rdquo; If you see something in a guide that says like:\n\u0026ldquo;please execute\u0026rdquo;\npython3 test.py or perhaps\nwget https://wandb.ai/jemoka \u0026gt;\u0026gt; test they are probably asking you to type it (\u0026ldquo;execute it\u0026rdquo;) into the Terminal and hit enter.\nRead this guide put together by the Ubuntu people, it\u0026rsquo;s very good. To open the terminal on your macOS device, open an app called Terminal.app. On Ubuntu, I believe its also an app called terminal.\nIDE An \u0026ldquo;IDE\u0026rdquo; is an Integrated Development Environment. It is where code is written. Fortunately, this is an easy one: use VSCode. There is literally no better tool out there for beginners and advanced users; no wonder it has 70% market share.\nSidenote: But Jack? What do you use? I use something called emacs for very specific reasons. Please don\u0026rsquo;t unless you really want misery and to learn a whole language to configure it.\nComputer Language Architecture This is how an idea turns into \u0026ldquo;stuff\u0026rdquo; on your screen.\nHuman programming languages (\u0026ldquo;Python\u0026rdquo;), are a very readable sort of code. No computers can actually read it. Usually, code you write goes through a three-step process before its able to be ran.\nFirst, the language you write gets converted by a \u0026ldquo;compiler\u0026rdquo; or \u0026ldquo;interpreter\u0026rdquo;, specialized pieces of code that takes human programming languages into a more machine-readable form of code named \u0026ldquo;assembly\u0026rdquo; or \u0026ldquo;byte code\u0026rdquo; respectively, called the \u0026ldquo;intermediate\u0026rdquo;.\nFor now, think of the difference between compilers and interpreters as translating code either all-at-once (compilers) or line-by-line (interpreters). Because the former has a grander view of the whole, languages that use a compiler (\u0026ldquo;compiled languages\u0026rdquo;) are faster. Although, many programmers find languages that use in interpreter (\u0026ldquo;interpreted language\u0026rdquo;) easier because they can spot problems line by line.\nBut wait! There\u0026rsquo;s more. Assembly and byte-code (what compilers and interpreters generate) are not actually runnable by a computer. Yet another piece of software called a \u0026ldquo;runtime\u0026rdquo; takes the reasonably-machine-readable code and actually performs the required operations.\nSome runtimes for languages like C++ uses the raw x86 CPU, which is the stereotypical \u0026ldquo;binary\u0026rdquo; zeros-and-ones. Some other languages, say Java, uses horribly complex runtimes that amounts to a whole virtual machine.\nHere\u0026rsquo;s a bit of a table.\nLanguage C/I Compiler/Interpreter Intermediate Runtime Python I python python bytecode python Java C javac java object java VM JavaScript I V8 (literally) js bytecode web browser! C/C++ C gcc/clang x86 asm x86 cpu Wonder what the runtimes for languages like Java are built in? C/C++. Eventually it all becomes x86 cpu instructions but its like a layer cake. This is why Python and friends are called a \u0026ldquo;higher level language\u0026rdquo;.\ngit Git is where all the code is!\nGit is a decentralized \u0026ldquo;version-control\u0026rdquo; system. It is basically a timestamp-backup system of code with messages and branching.\nGitHub is a website where people like to back up their code. Here\u0026rsquo;s my profile on GitHub.\nManaging Git is pretty\u0026hellip; Involved. It, for instance, assumes familiarity with the Terminal as described above. I suggest learning it, though. Here are some good resources:\nMorgan\u0026rsquo;s very very good git tutorial\u0026hellip; On GitHub! And this article on some commands you should know! Industry-Specific Skills What you start with doesn\u0026rsquo;t matter, but start with something Its easiest to learn programming if you have a project in mind. So, find a project in mind\u0026mdash;what it is, though, doesn\u0026rsquo;t matter. The concepts across programming are highly transferable, but the actual skill is easiest to learn if you are learning w.r.t. a project.\nData science, prototyping, and machine learning Python would be your friend for all things of programming where the act of programming is a means to an end. That is: if you are writing code to do something that\u0026rsquo;s not inherently software (data science, machine learning, heck, also manipulating quantum qubits), Python is your friend.\nIts a language that\u0026rsquo;s designed to be easy to write: is a very do-as-I-say language that sacrifices efficiency and elegance for getting crap done. This is how I started programming. This is the book I started with. It teaches Python through programming a series of small projects that are mostly Terminal games.\nTo learn data science, Nueva\u0026rsquo;s very own data science course give very good conceptual framework. A typical first project is to recognize pictures of handwritten digits, for which there is a good guide. I also started something called AIBridge with AIFS, so if we ever publish the recordings I will put them here.\nGoogle also: pip, ipython, Jupyter.\nBackend engineering Backend engineering is the science of dealing with databases and writing API (application programming interfaces). I don\u0026rsquo;t suggest starting with this, but if you are particularly interested in databases, you could!\nTo master backend engineering, first learn a database manipulation language. For 99.98% of the industry, this means mysql. The link directs to a pretty good guide.\nFurthermore, the language with which backend is written is Java. I hate to say it, but despite Java\u0026rsquo;s terribleness (don\u0026rsquo;t worry about it ;)) its very dependable. Here\u0026rsquo;s a book on Java. In general, I really like all of the books from no starch press.\nFrontend and Web engineering Do you like making stuff move? Do you like drawing buttons? Front end maybe for you. The most basic type of front-end engineering is making websites.\nStart by making a \u0026ldquo;vanilla website\u0026rdquo;: HTML (what\u0026rsquo;s on the page), CSS (what colours and sizes), JavaScript (how it moves) is the standard trio of languages to start. freeCodeCamp (a great Medium blog, check their stuff out) has a good guide on the matter.\nHowever, as you progress in your journey, you will find these tools woefully inadequate. Hence, most people writing web front end move on to something called a \u0026ldquo;JavaScript Framework\u0026rdquo;, a tool to generate a \u0026ldquo;vanilla\u0026rdquo; website from some more easily manipulable JS (changing the text on the page moves from a four line operation (indexing, selecting, grabbing, changing) to a one-liner (state.text=new text)).\nA popular JS framework is ReactJS. Check them out.\nFullstack Engineering Frontend + Backend.\nGame development Game development is honestly one of the most horribly complicated and richly science-y part of CS. I am not super experience in game development but learning C++ and mastering Unity, the game engine. Oh, right, game dev is the only, and I repeat only (with invisible footnotes and qualifications) reason why you should be writing code on Windows.\nA friend is good at game dev, I can make an intro if needed.\nGood Luck! Remember: Google-fu and project-based curiosity is your friend. Let me know if you have questions.\n","permalink":"https://www.jemoka.com/posts/kbhsoftware_dev_starter_pack/","tags":null,"title":"software dev starter pack"},{"categories":null,"contents":"H\nWaterfall The waterfall specification gets written before any code written. We hand off spec and code directly to tester, and code should behave like spec.\nCode specification exactly Spec does not update Code happens only after stuff is done\nAgile Agile are designed to work with minimum specification before code. Spec is updated constantly as code changes and get user feedback.\n","permalink":"https://www.jemoka.com/posts/kbhsoftware_development_methodologies/","tags":null,"title":"Software Development Methodologies"},{"categories":null,"contents":"process of Engineering: chronological order User interviews/stories Documentation/Specification Task estimation Design \u0026amp; architecture Testing Project Management Build and Release engineering fundamental trade-off of Software Engineering The MIT vs. New Jersey problem: in Software Engineering, you can only choose one of FAST or ROBUST.\nProblem Fast (\u0026ldquo;Bell Labs/NJ\u0026rdquo;) Robust (\u0026ldquo;MIT\u0026rdquo;) Specs Whatever it looks like screens, states, UI elements documented; transitions Time \u0026ldquo;whenever\u0026rdquo; precise projections, track work and dependencies Testing \u0026ldquo;ran it + didn\u0026rsquo;t crash\u0026rdquo; black, white box, code overage, edge/adv. cases Modular Giant function object/data model, grouped function, abstraction barriers Failure Unpredictable + silent Graceful, noisy, error reporting + logging Language Scripting, high level Low-level, assembly/bare metal, control, can be difficult Proto. Many/Quickly Few/Slowly Being Done Now Later Source: here.\nhow to choose? Which is the better approach? There isn\u0026rsquo;t one. However, here are some critical questions for you to answer:\nDeadline: what happens if you don\u0026rsquo;t finish today? Release cycle: if you ship a bug, how long can you fix it? Consequences: if the software malfunctions, how bad is it? Life-cycle: how long will the software get used? So\u0026mdash;\nAs consequences for deadline gets worse, trend towards fast; as consequences for failure gets worse, trend towards robust.\n","permalink":"https://www.jemoka.com/posts/kbhsoftware_engineering/","tags":null,"title":"Software Engineering"},{"categories":null,"contents":"So let\u0026rsquo;s say given a system:\n\\begin{equation} \\begin{cases} x + 2y + z = 0 \\\\ 2x + 0y - z = 1 \\\\ x - y + 0z = 2 \\end{cases} \\end{equation}\nWe can represent this using a matricies.\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix} \\end{equation}\nWe will use Gaussian elimination. We will begin by multiplying the top row by \\(-2\\).\n\\begin{equation} \\begin{pmatrix} -2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} =\\begin{pmatrix} -2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\2 \\end{pmatrix} \\end{equation}\nAnd then we add row one to row two; we will not write out the transformation matrix:\n\\begin{equation} \\begin{pmatrix} -2 \u0026amp;-4 \u0026amp;-2 \\\\ 2 \u0026amp;-0 \u0026amp;-1 \\\\ 1 \u0026amp;-1 \u0026amp;0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\2 \\end{pmatrix} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhsolving_systems/","tags":null,"title":"solving systems"},{"categories":null,"contents":" \u0026ldquo;laws.als\u0026rdquo;: \u0026ldquo;drumuomup\u0026rdquo; \u0026ldquo;ping.als\u0026rdquo;: \u0026ldquo;walking down the street, eating children\u0026rdquo;\u0026quot;\u0026quot; \u0026ldquo;planets.als\u0026rdquo;: \u0026ldquo;sing a song among the starlight\u0026rdquo; \u0026ldquo;songs.als\u0026rdquo;: \u0026ldquo;thank you klint for your discussion\u0026rdquo; Other things I have to finish \u0026ldquo;Tunel2.als\u0026rdquo; ","permalink":"https://www.jemoka.com/posts/kbhsongs_that_need_lyrics/","tags":null,"title":"Songs that need Lyrics"},{"categories":null,"contents":"sound is the compression of air molecules: high/low pressure air. \u0026ldquo;This is your brain on music.\u0026rdquo;\n\u0026ldquo;Dynamic EQ\u0026rdquo;: to attenuate certain frequencies to preventing things from happening.\nSoothe audio\nhow we hear sound the way that sound is deflected as it enter our ear is important:\nsound bounce around our pinna it echos in the ear canal then it gets processed anechoic chamber an anechoic chamber is a room that blocks all forms of reflection. In the room, people experience hallucinations as the brain is trying to complete information but it can\u0026rsquo;t confirm it using sensory input.\nYou r brain is always trying to inteperate what\u0026rsquo;s going on.\nBasilar Membrane The Basilar Membrane sits after the eardrums; it is a liquid in which a membrane + some hair sits. Depending on the frequency of the sound, the hairs vibrate at different shapes.\n","permalink":"https://www.jemoka.com/posts/kbhsound/","tags":null,"title":"sound"},{"categories":null,"contents":"Reading notes Because feeling for self-endowment, they wish to build socialist society As Communists considered themselves as a vanguard of the revolutionary proletariat – their “aim” was to build socialist society in the whole world.\nSocialist had necesity against capitalist aggression The Soviet approaches towards historical descriptions of the twentieth century showed that with the emergence of the new type of state – socialist one – it became a target for capitalist aggression.\nSocialist revolution requires the creation of socialist society against the world It was first positive move towards realization of the Soviet foreign policy main idea: the world socialist revolution and creation of the socialist society in the whole world.\nThe Soviets believe that the US wants to take over world The US had plans to dominate in the entire world.\nThat the US was intentionally sturggling with socialism All US post-war foreign policy doctrines were aimed on the struggle with socialism\nthat soviets believed that US was exclusivly fighting socialism We can summarize – that on Soviet point of view all American presidents of Cold War period were creating their own doctrines, and all of them were anti-communist and anti-Soviet\nSoviets believes that the US made the first move Soviet concept first vivid steps, which signalized about the start of the confrontation between East and West, were steps made by the West.\nbelieves its a fight against imperialism bipolar confrontation had western roots and the Cold War was the policy of the US and other imperialistic countries against socialist countries.\ncommunism is working towards revolution mankind is a process of revolutionary changes\nthe soviet union believes only it can stop American aggression the Soviet Union was the only power in the world able to stop American ambitions of superpower.\nUSSR believes that itself was the only defender The Soviet Union considered itself as the only defender of the interests of the working class all over the world because it was the first socialist state in history.\nDefinding US and defending imperialism The Imperialistic was the system of capitalist countries: they had a lot of contradictions in their “camp” where each wanted to solve their problems and to defend their own interests by using the others.\nBlack and white view of the world prevailed USSR The entire world was separated into two main categories: friends and enemies. Such black and white world-view was a distinctive feature of Stalin’s way of seeing the world (outside as well as inside the USSR), but even after his death,\n","permalink":"https://www.jemoka.com/posts/kbhsoviet_perspective_on_cold_war/","tags":null,"title":"Soviet Perspective on Cold War"},{"categories":null,"contents":"The span of a bunch of vectors is the set of all linear combinations of that bunch of vectors. We denote it as \\(span(v_1, \\dots v_{m)}\\).\nconstituents for constructing a linear combination a list of vectors \\(v_1,\\dots,v_{m}\\) and scalars \\(a_1, a_2, \\dots, a_{m} \\in \\mathbb{F}\\) requirements \\begin{equation} span(v_{1}..v_{m}) = \\{a_1v_1+\\dots +a_{m}v_{m}:a_1\\dots a_{m} \\in \\mathbb{F}\\} \\end{equation}\nadditional information span is the smallest subspace containing all vectors in the list Part 1: that a span of a list of vectors is a subspace containing those vectors\nBy taking all \\(a_{n}\\) as \\(0\\), we show that the additive identity exists.\nTaking two linear combinations and adding them (i.e. adding two members of the span) is still in the span by commutativity and distributivity (reorganize each constant \\(a_{1}\\) together)\u0026mdash;creating another linear combination and therefore a member of the span.\nScaling a linear combination, by distributivity, just scales the scalars and create yet another linear combination.\nPart 2: a subspace containing the list of vectors contain the span\nsubspaces are closed under scalar multiplication and addition. Therefore, we can just construct every linear combination.\nBy double-containment, a subspace is the smallest subspace containing all vectors. \\(\\blacksquare\\)\nspans If \\(span(v_1, \\dots v_{m})\\) equals \\(V\\), we say that \\(v_1, \\dots, v_{m}\\) spans \\(V\\).\nNOTE! the two things have to be equal\u0026mdash;if the span of a set of vectors is larger than \\(V\\), they do not span \\(V\\).\nlength of linearly-independent list \\(\\leq\\) length of spanning list see here.\n","permalink":"https://www.jemoka.com/posts/kbhspan/","tags":null,"title":"span (linear algebra)"},{"categories":null,"contents":" Órale pues: confirmando No hay pedo: no hay problema Ponte la de puebla: dividirlo Qué padre: sopresa positiva De a grapa: gratis De poca madre: júbilo y aceptación Te vas a dar un ranazo: nos vamos a hacer daño (hurt) Mamá: ¡necesitamos limpiar sus cuartos! Me: órale pues, no hay pedo. Voy a limpiarlo mañana.\nMi plan está simple. Voy a dividir mi cuarto a media, y contrata mi amiga para ayudarme. ¡Ponte la de puebla!\nDos días después\u0026hellip;\nMamá: Oye, ¿su habitación? ¡De poca madre, qué padre! Qué limpia.\nMe: Sí, de a grapa con mi amigo, también.\n","permalink":"https://www.jemoka.com/posts/kbhspanish/","tags":null,"title":"spanish"},{"categories":null,"contents":"A spinal tap is a medical procedure whereby cerebralspinal fluid is collected by puncturing the lumbar; used to diagnose problems where biomakers from the brain are needed.\n","permalink":"https://www.jemoka.com/posts/kbhspinal_tap/","tags":null,"title":"spinal tap"},{"categories":null,"contents":"A stack trace is the output of failing code by the runtime to indicate the location of the fault. For instance, in Python:\n--------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-1-0b766d7d4bc7\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 0+\u0026#34;\u0026#34; TypeError: unsupported operand type(s) for +: \u0026#39;int\u0026#39; and \u0026#39;str\u0026#39; ","permalink":"https://www.jemoka.com/posts/kbhstack_trace/","tags":null,"title":"stack trace"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhstandard_error/","tags":null,"title":"standard error"},{"categories":null,"contents":"Stanford is an university.\n","permalink":"https://www.jemoka.com/posts/kbhstanford/","tags":null,"title":"Stanford"},{"categories":null,"contents":"Staging this file for next year.\nUG Other Duties Here are a list of random indicies which may end up being helpful!\nCLRS Index ","permalink":"https://www.jemoka.com/posts/kbhstanford_courses_index/","tags":["index"],"title":"Stanford UG Courses Index"},{"categories":null,"contents":"Everyone and their dog has a blog at this point. Why not me? You see, I don\u0026rsquo;t really like the idea of blogging, but I do enjoy taking notes. I take a crap tonnes of notes, and sometimes people want to see a copy of them.\nIn order to facilitate this, some friends and I created taproot, a collective note-taking effort which also automatically compiled pretty cool previews and an internet site. I still am one of the primary maintainers of taproot.\nWhile working on the project, however, we noticed that the loop-based architecture (instead of being based on events/triggers), lack of duplicity, and requirement of a central build server made it difficult.\nIn this vein, quantumish (also with his own lovely set of notes, tap on the link!) and I were discussing if the essentials of taproot can be built into a static site generator. Hence, this is an experiment (to hopefully be merged with the taproot group) to facilitate this.\n","permalink":"https://www.jemoka.com/posts/kbhstarting_with_why_the_knowledgebase/","tags":null,"title":"Starting With Why: The Knowledgebase"},{"categories":null,"contents":"The stationary-action principle states that, in a dynamic system, the equations of motion of that system is yielded as the \u0026ldquo;stationary points\u0026rdquo; of the system\u0026rsquo;s action. i.e. the points of \u0026ldquo;least\u0026rdquo; action. (i.e. a ball sliding down a ramp is nice, but you don\u0026rsquo;t expect it\u0026mdash;in that system\u0026mdash;to fly off the ramp, do a turn, and then fly down.\n","permalink":"https://www.jemoka.com/posts/kbhstationary_action_principle/","tags":null,"title":"stationary-action principle"},{"categories":null,"contents":"A statistic is a measure of something\n","permalink":"https://www.jemoka.com/posts/kbhstastistic/","tags":null,"title":"statistic"},{"categories":null,"contents":"This is a theory that come back to CAPM.\n","permalink":"https://www.jemoka.com/posts/kbhstochastic_discount_factor/","tags":null,"title":"Stochastic Discount Factor"},{"categories":null,"contents":"the stock indicies\n","permalink":"https://www.jemoka.com/posts/kbhstock_indicies/","tags":null,"title":"stock indicies"},{"categories":null,"contents":"Stock Issues are policy debate doctrines which divides the debate into 5 subtopical ideas.\nWikipedia\nHarms: what are the problems in the status quo?\nInherency: what are these problems not already being solved? (Or not already being solved in the best way?)\nSignificancy: comparing the advantages and disadvantages of the status quo and your proposed solution, why is the proposed solution more worthy than the status quo?\nThe Ws:\nWhy this? Why is your proposed solution the best (most effective, or most feasible, or fastest, etc.) one?\nWhy now? Why is now the best time to build this solution?\nWhy you? Why are you (and your team) the best builders of this solution?\n","permalink":"https://www.jemoka.com/posts/kbhstock_issues_debate/","tags":null,"title":"Stock Issues (Debate)"},{"categories":null,"contents":" Around 20,000 stocks valued at $47 Trillion Only about 2,000 matter Transaction frequency is high, liquidity is generally low \u0026mdash; grade sizes are small Roughly 59 places to trade stock (exchanges + darkpools) ","permalink":"https://www.jemoka.com/posts/kbhstock_market_survey/","tags":null,"title":"stock market survey"},{"categories":null,"contents":"strain is the proportional deformation of a material given some stress applied\n","permalink":"https://www.jemoka.com/posts/kbhstrain/","tags":null,"title":"strain"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhstress/","tags":null,"title":"stress"},{"categories":null,"contents":"Reading Notes Strong Free Will vs. Weak Free Will \u0026mdash; \u0026ldquo;will\u0026rdquo; and \u0026ldquo;bells inequality\u0026rdquo; is a demonstration of indeterminism/randomness between particles \u0026mdash; but indeterminism and randomness a demonstration of will.\nThat if humans have free will, it should be spawened from the indeterminism of elementary particles It asserts, roughly, that if indeed we humans have free will, then elementary particles already have their own small share of this valuable commodity.\nSPIN Axiom SPIN Axiom: Measurements of the squared (components of) spin of a spin 1 particle in three orthogonal directions always give the answers 1, 0, 1 in some order.\nTWIN Axiom Paired particles will come up with same measurements if measured in the same way\nThe TWIN Axiom: For twinned spin 1 particles, suppose experimenter A performs a triple experiment of measuring the squared spin component of particle a in three orthogonal directions x, y, z, while experimenter B measures the twinned par- ticle b in one direction, w . Then if w happens to be in the same direction as one of x, y, z, experimenter B’s measurement will necessarily yield the same answer as the corresponding measurement by A.\nFree as something that cannot be an uncurried function of previous states To say that A’s choice of x, y, z is free means more precisely that it is not determined by (i.e., is not a function of) what has happened at earlier times (in any inertial frame).\nMIN Axiom Choice of direction of measurement of one twinned qubit does not influence the results of the current qubit (unless they happen to align.)\nThe MIN Axiom: Assume that the experiments performed by A and B are space-like separated. Then experimenter B can freely choose any one of the 33 particular directions w , and a’s response is independent of this choice. Similarly and inde- pendently, A can freely choose any one of the 40 triples x, y, z, and b’s response is independent of that choice.\n","permalink":"https://www.jemoka.com/posts/kbhstrong_free_will/","tags":null,"title":"Strong Free Will"},{"categories":null,"contents":"a\n","permalink":"https://www.jemoka.com/posts/kbhsubgroup/","tags":null,"title":"subgroup"},{"categories":null,"contents":"A subspace is a vector space which is a subset of a vector space, using the same addition and scalar multiplication operations. Intuitively, a subspace of \\(\\mathbb{R}^{2}\\) are all the lines through the origin as well as \\(\\{0\\}\\); a subspace of \\(\\mathbb{R}^{3}\\) are all the planes through the origin as well as \\(\\{0\\}\\), etc. etc.\nconstituents vector space \\(V\\) A subset \\(U \\subset V\\) which is itself a vector space requirements You check if \\(U\\) is a subspace of \\(V\\) by checking IFF the following three conditions:\nadditive identity: \\(0 \\in U\\) closed under the same addition as in \\(V\\): \\(u,w \\in U: u+w \\in U\\) closed under scalar multiplication as in \\(V\\): \\(a \\in \\mathbb{F}\\) and \\(u \\in U\\) means \\(au \\in U\\) Yes, by only checking three you can prove everything else.\nadditional information simplified check for subspace commutativity, associativity, distributivity These properties are inherited from \\(V\\) as they hold for every element in \\(V\\) so they will hold for \\(U \\subset V\\).\nadditive inverse Because scalar multiplication is defined, and we proved in Axler 1.B that \\(-1v=-v\\) (proof: \\(v+(-1)v = (1+(-1))v = 0v = 0\\)).\nmultiplicative identity Its still \\(1\\).\n\\(\\blacksquare\\)\nfinite-dimensional subspaces Every subspace of a finite-dimensional vector space is a finite-dimensional vector space.\nWe prove this result again via induction.\nbase case If \\(U=\\{0\\}\\), we know \\(U\\) is finite-dimensional and are done. If not, take some \\(v_1 \\in U\\) and create a list with only \\(v_1\\) thus far; the invariant here is that the list is linearly independent as we see that a list containing this one element as indeed linearly independent.\ncase \\(j\\) If the linearly independent list we created \\(v_1, \\dots v_{j-1}\\) spans \\(U\\), we are done. We have created a finite list which spans \\(U\\), making \\(U\\) finite-dimensional.\nIf not, that means that we can pick some \\(u \\in U\\) that cannot be written as a linear combination of the invariantly linearly independent vectors \\(v_1, \\dots v_{j-1}\\). We append \\(u\\) to the list, naming it \\(v_{j}\\). As \\(v_{j}\\) cannot be written as a linear combination of the original list, appending it to the list doesn\u0026rsquo;t make the list dependent. This means that the list is still linearly independent.\ninduction Therefore, we have constructed a list of increasing length that is linearly independent. By the fact that length of linearly-independent list \\(\\leq\\) length of spanning list, and the fact that the spanning list of \\(V\\) has finite length (it is given that \\(V\\) is a finite-dimensional vector space), the increasingly longer linearly independent list\u0026mdash;building upwards to eventually span \\(U\\) in finite length.\n","permalink":"https://www.jemoka.com/posts/kbhsubspace/","tags":null,"title":"subspace"},{"categories":null,"contents":"The sum of subsets is the definition of addition upon two subsets.\nApparently, the unions of subsets are almost never subspaces (they don\u0026rsquo;t produce linearity?) Therefore, we like to work with sum of subsets more.\nRemember this has arbitrarily many things!! as a part of the content. When defining, remember to open that possibility.\nconstituents Sub-sets of \\(V\\) named \\(U_1, U_2, \\dots, U_{m}\\)\nrequirements The sum of subsets \\(U_1, \\dots, U_{m}\\) is defined as:\n\\begin{equation} U_1, \\dots, U_{m} = \\{u_1+\\dots+u_{m}: u_1\\in U_1, \\dots, u_{m} \\in U_{m}\\} \\end{equation}\n\u0026ldquo;all elements formed by taking one element from each and add it.\u0026rdquo;\nadditional information sum of subspaces is the smallest subspace with both subspaces Suppose \\(U_1, \\dots U_{m}\\) are subspaces of \\(V\\), then \\(U_1+\\dots +U_{m}\\) is the smallest subspace of \\(V\\) containing \\(U_1, \\dots, U_{m}\\).\nProof:\nIs a subspace\u0026mdash;\nclearly \\(0\\) is in the sum. (taking \\(0\\) from each subspace and adding) addition and scalar multiplication inherits (closed in each subspace, then, reapplying definition of sum of subsets) Smallest containing subspace\u0026mdash;\nBecause a subspace is closed under addition, if a subspace contains \\(U_{1}, \\dots, U_{m}\\) you can always add each of the constituent elements manually to form every \\(U_1+\\dots+U_{m}\\).\nConversely, the subspace \\(U_1+\\dots +U_{m}\\) should contain \\(U_1, \\dots, U_{m}\\) by simply setting the coefficients except for the one you are interested in to \\(0\\).\nTherefore, as both subsets contain each other; they are equivalent.\ndimension of sums Let there be two finite-dimensional subspaces: \\(U_1\\) and \\(U_2\\). Then:\n\\begin{equation} \\dim(U_1+U_2)=\\dim U_1+\\dim U_{2} - \\dim(U_1 \\cap U_2) \\end{equation}\nProof:\nlet us form an basis of \\(U_1 \\cap U_{2}\\): \\(u_1, \\dots u_{m}\\); this indicates to us that \\(\\dim(U_1 \\cap U_{2}) = m\\). Being a basis of \\(U_1 \\cap U_{2}\\), it is linearly independent in \\(U_1\\) (which forms a part of the intersection.\nAs any linearly independent list (in this case, in \\(U_1\\)) can be expanded into a basis of \\(U_1\\). Let\u0026rsquo;s say by some vectors \\(v_1 \\dots v_{j}\\). Therefore, we have that:\nThe new basis is \\(u_1, \\dots u_{m}, v_1, \\dots v_{m}\\), and so:\n\\begin{equation} \\dim U_1 = m+j \\end{equation}\nBy the same token, let\u0026rsquo;s just say some \\(w_1, \\dots w_{k}\\) can be used to extend \\(u_1, \\dots u_{m}\\) into a basis of \\(U_2\\) (as \\(u_1, \\dots u_{m}\\) is also an linearly independent list in \\(U_2\\)). So:\n\\begin{equation} \\dim U_{2} = m+k \\end{equation}\nWe desire that \\(\\dim(U_1+U_2)=\\dim U_1+\\dim U_{2} - \\dim(U_1 \\cap U_2)\\). Having constructed all three of the elements, we desire to find a list that is length \\((m+j)+(m+k)-m = m+j+k\\) that forms a basis of \\(U_1+U_2\\), which will complete the proof.\nConveniently, \\(u_1, \\dots u_{m}, v_1, \\dots v_{j}, w_1, \\dots w_{k}\\) nicely is list of length \\(m+j+k\\). Therefore, we desire that that list forms a basis of \\(U_1+U_{2}\\).\nAs pairwise in this list are the basis of \\(U_1\\) and \\(U_2\\), this list can span both \\(U_1\\) and \\(U_2\\) (just zero out the \u0026ldquo;other\u0026rdquo; sublist\u0026mdash;zero \\(w\\) if desiring a basis of \\(U_1\\), \\(v\\) if \\(U_2\\) \u0026mdash;and you have a basis of each space. As \\(U_1+U_2\\) requires plucking a member from each and adding, as this list spans \\(U_1\\) and \\(U_2\\) separately (again, it forms the basis of the each space), we can just use this list to construct individually each component of \\(U_1+U_2\\) then adding it together. Hence, that long combo list spans \\(U_1+U_2\\).\nThe only thing left is to show that the giant list there is linearly independent. Let\u0026rsquo;s construct:\n\\begin{equation} a_1u_1+ \\dots + a_{m}u_{m} + b_1v_1 + \\dots + b_{j}v_{j} + c_1w_1 + \\dots + c_{k}w_{k} = 0 \\end{equation}\nto demonstrate linearly independence,\nMoving the \\(w\\) to the right, we have that:\n\\begin{equation} a_1u_1+ \\dots + a_{m}u_{m} + b_1v_1 + \\dots + b_{j}v_{j} =-(c_1w_1 + \\dots + c_{k}w_{k}) \\end{equation}\nRecall that \\(u_1 \\dots v_{j}\\) are all vectors in \\(U_1\\). Having written \\(-(c_1w_1 + \\dots + c_{k}w_{k})\\) as a linear combination thereof, we say that \\(-(c_1w_1 + \\dots + c_{k}w_{k}) \\in U_1\\) due to closure. But also, \\(w_1 \\dots w_{k} \\in U_2\\) as they form a basis of \\(U_2\\). Hence, \\(-(c_1w_1 + \\dots + c_{k}w_{k}) \\in U_2\\). So, \\(-(c_1w_1 + \\dots + c_{k}w_{k}) \\in U_1 \\cap U_2\\).\nAnd we said that \\(u_1, \\dots u_{m}\\) are a basis for \\(U_1 \\cap U_{2}\\). Therefore, we can write the \\(c_{i}\\) sums as a linear combination of $u$s:\n\\begin{equation} d_1u_1 \\dots + \\dots + d_{m}u_{m} = (c_1w_1 + \\dots + c_{k}w_{k}) \\end{equation}\nNow, moving the right to the left again:\n\\begin{equation} d_1u_1 \\dots + \\dots + d_{m}u_{m} - (c_1w_1 + \\dots + c_{k}w_{k}) = 0 \\end{equation}\nWe have established before that \\(u_1 \\dots w_{k}\\) is a linearly independent list (it is the basis of \\(U_2\\).) So, to write \\(0\\), \\(d_1 = \\dots = c_{k} = 0\\).\nSubstituting back to the original:\n\\begin{align} a_1u_1+ \\dots + a_{m}u_{m} + b_1v_1 + \\dots + b_{j}v_{j} \u0026amp;=-(c_1w_1 + \\dots + c_{k}w_{k}) \\\\ \u0026amp;= 0 \\end{align}\nrecall \\(u_1 \\dots v_{j}\\) is the basis of \\(U_1\\), meaning they are linearly independent. The above expression makes \\(a_1 = \\dots b_{j} = 0\\). Having shown that, to write \\(0\\) via \\(u, v, \\dots w\\) requires all scalars \\(a,b,c=0\\), the list is linearly independent.\nHaving shown that the list of \\(u_1, \\dots v_1, \\dots w_1 \\dots w_{k}\\) spans \\(U_1+U_2\\) and is linearly independent within it, it is a basis.\nIt does indeed have length \\(m+j+k\\), completing the proof. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhsum_of_subsets/","tags":null,"title":"sum of subsets"},{"categories":null,"contents":"Suppose \\(v \\in V\\), and \\(U \\subset V\\). Then, \\(v+U\\) is the subset (not a subspace, obviously):\n\\begin{equation} v + U = \\{v+u : u \\in U\\} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhsum_of_vector_and_subspace/","tags":null,"title":"sum of vector and subspace"},{"categories":null,"contents":"A function \\(T: V\\to W\\) is surjective if its range equals its codomain \\(W\\). \u0026ldquo;onto\u0026rdquo;\n\u0026ldquo;For any possible output, \\(w \\in W\\) for \\(T \\in \\mathcal{L}(V,W)\\), there is at LEAST one input \\(T\\) that maps \\(Tv \\to w\\). \u0026quot;\n\\begin{equation} \\forall w \\in W, \\exists v \\in V:Tv=W \\end{equation}\nmap to bigger space is not surjective See map to bigger space is not surjective\n","permalink":"https://www.jemoka.com/posts/kbhsurjectivity/","tags":null,"title":"surjectivity"},{"categories":null,"contents":"Suppose \\(T \\in \\mathcal{L}(V,W)\\). Define a \\(\\widetilde{T}: V / (null\\ T) \\to W\\) such that:\n\\begin{align} \\widetilde{T}(v+ null\\ T) = Tv \\end{align}\nso \\(\\widetilde{T}\\) is the map that recovers the mapped result from an affine subset from the null space of the map.\n\\(\\widetilde{T}\\) is well defined Same problem as that with operations on quotient space. We need to make sure that \\(\\widetilde{T}\\) behave the same way on distinct but equivalent representations of the same affine subset.\nSuppose \\(u,v \\in V\\) such that \\(u+null\\ T = v+null\\ T\\). Because two affine subsets parallel to \\(U\\) are either equal or disjoint, we have that \\(u-v \\in null\\ T\\). This means that \\(Tu-Tv = 0 \\implies Tu= Tv\\). So applying \\(\\widetilde{T}\\) on equivalent representations of the same affine subset would yield the same result, as desired. \\(\\blacksquare\\)\nproperties of \\(\\widetilde{T}\\) it is a linear map TBD proof. Basically just like do it inheriting operations from the operations on quotient space.\nit is injective We desire here that \\(null\\ \\widetilde{T} = \\{0\\}\\) which will tell us that \\(\\widetilde{T}\\) is injective.\nSuppose some \\(v + null\\ T\\) is in the null space of \\(\\widetilde{T}\\). So, we have that:\n\\begin{equation} \\widetilde{T}(v+null\\ T) = Tv = 0 \\end{equation}\nSo, we have that \\(v \\in null\\ T\\). Now, this means that \\(v-0 \\in null\\ T\\). Because two affine subsets parallel to \\(U\\) are either equal or disjoint, \\(v + null\\ T = 0 + null\\ T\\) WLOG \\(\\forall v+null\\ T \\in null\\ \\widetilde{T}\\). This means that \\(null\\ \\widetilde{T}=\\{0\\}\\), as desired.\nits range is equal to the map\u0026rsquo;s range \\begin{equation} range\\ \\widetilde{T} = range\\ T \\end{equation}\nby definition of everything.\n\\(V / null\\ T\\) is isomorphic to \\(range\\ T\\) \u0026hellip;.is this the point of this whole thing?\nShown by the two sub-results above, and that injectivity and surjectivity implies invertability.\n","permalink":"https://www.jemoka.com/posts/kbht_twiddle/","tags":null,"title":"T twiddle"},{"categories":null,"contents":"confidence intervals, a review:\n\\begin{equation} statistic \\pm z^*\\sigma_{statistic} \\end{equation}\nFrequently, we don\u0026rsquo;t have access to \\(\\sigma\\) and hence have to guestimate. When we have a sample means and a proportion, we have ways of guestimating it from the standard error (available on the single-sample section of the AP Statistics formula sheet.)\nHowever, for means, the standard error involves! \\(\\sigma\\). How do we figure \\(\\sigma\\) when we don\u0026rsquo;t know it? We could use \\(s\\), sample standard deviation, but then we have to adjust \\(z^*\\) otherwise we will have underestimation. Hence, we have to use a statistic called \\(t^*\\).\nWe can use t-values to perform t-test, a hypothesis test of means.\n","permalink":"https://www.jemoka.com/posts/kbht_statistics/","tags":null,"title":"t-statistics"},{"categories":null,"contents":"A t-test is a hypothesis test for statistical significance between two sample means based on t-statistics. Before it can be conducted, it must meet the conditions for inference.\nconditions for inference (t-test) To use t-statistics, you have to meet three conditions just like the conditions for inference used in z-score.\nrandom sampling normal (sample size larger than 30, or if original distribution is confirmed as roughly symmetric about the mean) Independence use a z-statistic to find a p-value Begin by finding a \\(t\\) statistic. Remember that:\n\\begin{equation} t = \\frac{statistic-parameter}{std\\ err} \\end{equation}\nIn this case, when we are dealing with sample means, then, we have:\n\\begin{equation} t = \\frac{\\bar{x}-\\mu_0}{\\frac{S_x}{\\sqrt{n}}} \\end{equation}\nwhere \\(\\bar{x}\\) is the measured mean, \\(\\mu_0\\) is the null hypothesis mean, and \\(S_x\\) the sample\u0026rsquo;s sample standard deviation.\nQuick note:\n\\(SE = \\frac{S}{\\sqrt{n}}\\) because the central limit theorem states that sample means for their own distribution, whose variance equals the original variance divided by the sample size. Hence, the standard deviation of the means would be the sample standard deviation divided by the square root of the sample size.\nOnce you have a \\(t\\) value, you look at the test and what its asking (above the mean? below the mean? etc.) and add up the tail probabilities.\npaired vs two-sample tests A paired t-test looks at pairs of values as statistic in itself (i.e. substracts directly, etc.) Think about it as a compound statistic, so you are doing a \\(t\\) test on one value, it just happened to be composed/calculated by a pair of values. (for instance, \u0026ldquo;difference between mother-father glucose levels.\u0026rdquo;)\nA two-staple t-test looks at two independent events and compares them. Hence, they are two random variables and should be manipulated as such.\nt-tests for regression lines regression lines can be imbibed with predictive power and confidence intervals:\n\\begin{equation} m \\pm t^* SE_b \\end{equation}\nwhere \\(m\\) is the slope and \\(SE_b\\) is the standard error of the regression line.\nNote that the degrees of freedom used for \\(t^*\\) is the number of data points, minus two.\nconditions for inference (slops) Acronym: LINEAR\nLinear Independent (observations are independent or \\(\u0026lt;10\\%\\)) Normal (for a given \\(x\\), \\(y\\) is normally distributed) Equal variance (for any given \\(x\\), it should have a roughly equal standard deviation in \\(y\\)) Random ","permalink":"https://www.jemoka.com/posts/kbht_test/","tags":null,"title":"t-test"},{"categories":null,"contents":"stuff make Spanish names list name, city, countries corpuses SABSAE: santa barbara english CABNC: British english next ignore any words that goes wrong in the pipeline ~change: noun =\u0026gt; n; verb =\u0026gt; v, etc.~ DET: ignore \u0026ldquo;DEF\u0026rdquo;, or perhaps the entir featureless unbulleted VAD exprimentents errors! line 1492\n*PAR:\tso ‡ anyway I tiptoe to the front door , open the front door and walk in . •1045194_1050644• %mor:\tco|so beg|beg adv|anyway pro:sub|I v|+n|tip+n|toe prep|to det:art|the n|front n|door cm|cm adj|open det:art|the n|front n|door coord|and n|walk adv|in . %gra:\t1|0|BEG 2|1|BEGP 3|5|JCT 4|5|SUBJ 5|0|ROOT 6|5|JCT 7|9|DET 8|9|MOD 9|6|POBJ 10|5|LP 11|14|MOD 12|14|DET 13|14|MOD 14|5|OBJ 15|14|CONJ 16|15|COORD 17|16|NJCT 18|5|PUNCT\nerrors? words without features needs to be correctly handled (done in the middle of meeting) 04111 (me ma SOS) nouns shouldn\u0026rsquo;t mark if it is Com,Neut, should\u0026rsquo;nt mark if its Com fix PASTP =\u0026gt; PAST and does past participles exist? more Move shua to d(e) Include instructions on how to recreate a broken Conda environment Update the package to conda somehow move next steps deal with `n` +\u0026hellip; fix remove bullets results ~ contraction \u0026amp; fused suffix getting rid of punkt in mor , =\u0026gt; cm . =\u0026gt; no PUNKT, stays stuff chocolaty (noadmin, https://docs.chocolatey.org/en-us/choco/setup#non-administrative-install) miniconda setx path \u0026ldquo;%path%;C:\\tools\\miniconda3\\condabin\u0026rdquo; curl env first, the install (Windows can\u0026rsquo;t do it from a URL) readme conda init zsh (close shell, open again) .mp4 mfa model downloading what\u0026rsquo;s the difference between online docker install and manual install NLTK Huggingface transformers tokenizers (versining) /opt/homebrew/Caskroom/miniforge/base/envs/aligner/lib/python3.9/site-packages/montreal_forced_aligner/corpus/text_corpus.py; getattr(self, k).update(error_dict[k]) AttributeError: \u0026rsquo;list\u0026rsquo; object has no attribute \u0026lsquo;update\u0026rsquo; FileArgumentNotFoundError: ; line 139\nDBA See the data on the frequency of haphax legomina vs. COCA ESPNet need to talk to Ji Yang Andrew\u0026rsquo;s Features Collapse two PAR tiers down Checkpoint per file One corpus prompt per run Handle empty tiers I/P selection crashes! contingency preview the LONGEST segment instead of the top one -i kill in the middle fixes \u0026ldquo;my mom\u0026rsquo;s cryin(g)\u0026rdquo; [\u0026lt;] mm [l648] (also themmm after) \u0026ldquo;made her a nice dress\u0026rdquo; [\u0026lt;] mhm [l1086] \u0026ldquo;when I was a kid I\u0026rdquo; \u0026amp;=laughs [l1278] Others chstring (for uh, mm-hmm)\nretrace (asr\u0026amp;fa folder)\nlowcase (caps)\nrep-join.cut (fixes/)\nnumbers \u0026lt;affirmative\u0026gt; \u0026lsquo;mo data! CallFriend/CallHome (ca-data) ISL? SBCSAE Aphasia + MICASE TBI data Providing a Two-Pass Solution Writing Big description of the pipeline Notion of the pipeline Better tokenization? 8/18 Initial segment repetition Extracting studdering Gramatically problematic mar mar has done a thing and its phoneme level We did it, now automated LEAP data next actions Aphasia (-apraxia?): classification Child data (EllisWeismer) Dementia a ~Multiple @Begin/CHECK problem~\n~Placement of @Options~\n~Strange, missing period~\n~Bracket comments should FOLLOW words instead of PRECEEDING them~\n~%xwor: line~\nSTICK TO DASHES WHEN DISTRIBUTING BATCHALIGN\nend the utterance when it ends (incl. inter-utterance pauses)\n\u0026ldquo;I\u0026rdquo; need to be capitalized\n11005 (LT)\nAlign EllisWeismer\nAlso cool to align:\nfluency IISRP/*\nhttps://en.wikipedia.org/wiki/Speaker_diarisation\nhttps://universaldependencies.org/\nAlzheimer\u0026rsquo;s Project https://dementia.talkbank.org/\nhttps://luzs.gitlab.io/adresso-2021/\nSpecifically: https://dementia.talkbank.org/access/English/Pitt.html\nReview Kathleen Fraser: https://drive.google.com/drive/u/1/folders/1lYTIzzXLXw3LlDG9ZQ7k4RayDiP6eLs1\nHere are the review papers: https://drive.google.com/drive/u/1/folders/1pokU75aKt6vNdeSMpc-HfN9fkLvRyutt\nRead this first: https://drive.google.com/drive/u/1/folders/0B3XZtiQwQW4XMnlFN0ZGUndUamM?resourcekey=0-AlOCZb4q9TyG4KpaMQpeoA\nSome PITT data have 3-4 recordings\nThe best way to diagnosing alzhimers\u0026rsquo; is from language.\nWhy this field is needed: to analyze a pre-post test metric.\nDesired output: existence of dementia (a.k.a alzheimer\u0026rsquo;s\u0026rsquo;).\nOther research to read:\nPenn (julia parish something but they don\u0026rsquo;t stare their data but they smile and things with Mark Libermann type of thing) Learning more about speech text https://my.clevelandclinic.org/health/diagnostics/22327-differential-diagnosis python3 ~/mfa_data/batchalign-dist/batchalign.py ~/mfa_data/my_corpus ~/mfa_data/my_corpus_aligned\nchristan marr paper on MFA on child data\n","permalink":"https://www.jemoka.com/posts/kbhtalkbank/","tags":null,"title":"talkbank"},{"categories":null,"contents":"Lit Survey Pipeline Segmentation ","permalink":"https://www.jemoka.com/posts/kbhtalkbank_pipeline_project/","tags":null,"title":"TalkBank Pipeline Project"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhtariffs/","tags":null,"title":"tariffs"},{"categories":null,"contents":"Step 0: know what you are building.\nbreaking tasks The process of breaking tasks down.\nWe need to research tasks to see how complex they are + how to break them down Research takes time! It should be its own task Over the process of research, the task becomes much simpler estimating tasks Requirement: tasks should always be estimated by the person doing the work.\nTask Estimation should be done each time! tasks shift Estimate only in powers of 2: 30 minutes, 1h, 2h, 4h, 8h, etc. If you never done something before, double the time than you estimate If you are teaching someone to do something, quadruple the time than you estimate Add buffer time (*1.5), especially if you think yourself as a procrastinator Focus is draining! You need breaks. Take breaks. Things will go wrong! Plan for it. time iterating If anything is longer than 8 hours, that\u0026rsquo;s a good sign you need to break it down! Likely that you have to break things down MVP You probably don\u0026rsquo;t have time to build your feature list\nMVP: minimum viable product We need the basic set of features; you probably have more features than you have time to build Prioritize what you build based on\u0026hellip; Dependencies: is this required for other stuff to work Viability: can the product exist without this? Time: how long does it take? Be ruthless about what you cut; talk to your user.\n","permalink":"https://www.jemoka.com/posts/kbhtask_estimation/","tags":null,"title":"Task Estimation"},{"categories":null,"contents":"Hostname: baboon.jemoka.com\nTechnology: MacBook Pro A2338\nSerial: C02FX4W2Q05N\nDescription: Space-Grey MacBook Pro 2020\nLost If you have stumbled upon this page due to finding this device in the wild, thank you so much! Reach out to houjun@jemoka.com or +1 650 209-0966 to get in touch. Please do the right thing.\nThank you.\n","permalink":"https://www.jemoka.com/posts/kbhtechnology_baboon_jemoka_com/","tags":null,"title":"Technology: baboon.jemoka.com"},{"categories":null,"contents":"Hostname: balloon.jemoka.com\nTechnology: MacBook Pro A2779\nSerial: Q5491WTGGM\nDescription: Space-Grey MacBook Pro 2023\nLost If you have stumbled upon this page due to finding this device in the wild, thank you so much! Reach out to houjun@jemoka.com or +1 650 209-0966 to get in touch. Please do the right thing.\nThank you.\n","permalink":"https://www.jemoka.com/posts/kbhtechnology_balloon_jemoka_com/","tags":null,"title":"Technology: balloon.jemoka.com"},{"categories":null,"contents":"Hostname: bassoon.jemoka.com\nTechnology: Teenage Engineering OP-Z\nSerial: X3C-KJFBB\nDescription: Gray Portable Synthesizer\nLost If you have stumbled upon this page due to finding this device in the wild, thank you so much! Reach out to houjun@jemoka.com or +1 650 209-0966 to get in touch. Please do the right thing.\nThank you.\n","permalink":"https://www.jemoka.com/posts/kbhtechnology_bassoon_jemoka_com/","tags":null,"title":"Technology: bassoon.jemoka.com"},{"categories":null,"contents":"Hostname: bonbon.jemoka.com\nTechnology: iPhone MT972LL/A\nSerial: C39Z2HS9KPFT\nDescription: iPhone Xs Black\nLost If you have stumbled upon this page due to finding this device in the wild, thank you so much! Reach out to houjun@jemoka.com or +1 650 209-0966 to get in touch. Please do the right thing.\nThank you.\n","permalink":"https://www.jemoka.com/posts/kbhtechnology_bonbon_jemoka_com/","tags":null,"title":"Technology: bonbon.jemoka.com"},{"categories":null,"contents":"Teddy Roosevelt was an American president.\nLarge personality: expanded scope of the Presidency \u0026mdash; \u0026ldquo;if it doesn\u0026rsquo;t explicit say its belong to the congress, it belongs to me\u0026rdquo; Moralist (Support American People), Imperialist (Believes in American Righteousness), Progressive Monroe Doctrine \u0026amp; Roosevelt Corollary: America for Americans The Panama Canal - engineered coup! to build the panama canal ","permalink":"https://www.jemoka.com/posts/kbhteddy_roosevelt/","tags":null,"title":"Teddy Roosevelt"},{"categories":null,"contents":"Given what you claim as a normal distribution, we can test for its normality. Any distribution you claim as normal has to follow that:\n\\begin{equation} np \\geq 10 \u0026amp; n(1-p) \\geq 10 \\end{equation}\nthat number of successes and failures need both be greater than or equal to ten.\n","permalink":"https://www.jemoka.com/posts/kbhtest_for_normality/","tags":null,"title":"test for normality (statistics)"},{"categories":null,"contents":"How many bugs are in 1,000 lines of code?\nTypical code: 1-10 Platform code: 0.1-1 The best\u0026mdash;NASA: 0.01-0.1 Never assume your software doesn\u0026rsquo;t have bugs.\nTest-Driven Development Test before you build!\nSpecs are already written We know what the expected behavior is We can write tests for the expected behavior first All tests fail to start We know we are done writing code when all tests pass \u0026ldquo;NYI\u0026rdquo; (not-yet implemented)\noften, writing test exposes gaps in your specs How NOT! not write tests Random Sampling Pick one or two inputs and show your code works on it Why it doesn\u0026rsquo;t work: there maybe specific inputs that break your code Exhaustive Testing Test for the domain of inputs Why it doesn\u0026rsquo;t work: tests run forever How DO you write tsets Black-Box Testing Pretend the code implementation is a black box All you know is what the specification; and what the input/output produces White-Box Testing You can see the implementation You test for specific edge cases Off-by-one, running time, specific inputs, etc. Malicious Testing What happens if a user is trying to break your system Sometimes, this is known as \u0026ldquo;pen-testing\u0026rdquo; or \u0026ldquo;white-hack hacking\u0026rdquo; Take CS340 Compsec How BIG are your tests Unit Testing Backbone of testing Typically, that means one test per function Tests choose representative inputs Idempotent: the state of the testing system should be a the beginning and end of the test (tests should revert) (setup + teardown tests) Subsystem Testing Exercise multiple functions working together in a system Often takes longer OK to run these less frequently End-to-End Integration Exercise the entire workflow May involve external libraries, hardware, etc. Regression Testing Isolate the cause of the bug to the smallest possible test case Write a test assuming the bug is fixed Fix the bug Add the test to your test suite How MUCH do we run tests Ideally, run tests every time code is committed Ideally\u0026mdash;run tests that address the function Schedule long tests ","permalink":"https://www.jemoka.com/posts/kbhtesting/","tags":null,"title":"Testing"},{"categories":null,"contents":"The Unreasonable Effectiveness of Mathematics in the Natural Sciences is an article by the famous mathematician Eugene Wigner. (Wigner 1990)\nReflection What I found most peculiarly interesting is the focus on many mathematical/physics texts on the idea of the \u0026ldquo;beauty\u0026rdquo; of the expressions; and, it seems, the clear pleasure that Wigner gets from analyzing the systems with the aforementioned \u0026ldquo;beauty.\u0026rdquo;\nSetting aside whether or not this beauty is \u0026ldquo;deserved\u0026rdquo;/appropriate, I love that my attraction to physics is somewhat similar to what Wigner describes. Under the appropriate conditions, with constraints, it is possible to build a solution to physics problems simply through the evolution of mathematics.\nIt is not to say that the models mathematics provides is correct. I like that Winger ended on the note about how \u0026ldquo;false\u0026rdquo; theories, even despite their falseness, provided shockingly accurate estimations of physical phenomena. Perhaps mathematics provides an almost-fully solid foundation to creating physical systems, but then the entire \u0026ldquo;flaw\u0026rdquo; we see with mathematical modeling is in our (in)ability to provide the limitations to scope.\nFor instance, Bohr\u0026rsquo;s model, an example of \u0026ldquo;falsehood\u0026rdquo; modeled, is an over-limitation to scope which\u0026mdash;thought reducing mathematical complexity\u0026mdash;resulted in a \u0026ldquo;wrong\u0026rdquo; theory. However, the mathematics behind the theory remains to be solid despite the scope limitation, making the result work in a reasonable manner (except for the pitfalls).\nThe inherent concern behind this statement, then, is that there is a case where we can build a perfectly reasonable system to model something, but it turns out that the system is correct only in the limited scope which we are used to operating; when suddenly the scope becomes broken, we are so used to the mathematical tools that we have came to rely on that we don\u0026rsquo;t notice their failures.\nI like that this entire point is brought up before our start in DiffEq, perhaps as a \u0026ldquo;with great power comes great responsibility\u0026rdquo; type of caution to us in terms of how our modeling may go awry while at the same time acting as a preview of the usefulness of the principles provided taken as a whole.\nReading notes Maths show up at entirely random places The first point is that mathematical concepts turn up in entirely unexpected connections. Moreover, they often permit an unexpectedly close and accurate description of the phenomena in these connections.\nWondering whether or not the theory is unique due to its applicability He became skeptical concerning the uniqueness of the coordination between keys and doors.\nThat math is really useful, its weird The first point is that the enormous usefulness of mathematics in the natural sciences is something bordering on the mysterious and that there is no rational explanation for it.\nIt also raises the question of how actually unique our theories are given they are all so applicable Second, it is just this uncanny usefulness of mathematical concepts that raises the question of the uniqueness of our physical theories.\nThe goal of mathematics is maximize the space of usefulness The great mathematician fully, almost ruthlessly, exploits the domain of permissible reasoning and skirts the impermissible.\nRegularity is suprising because its\u0026hellip; regularly found, which is unique The second surprising feature is that the regularity which we are discussing is independent of so many conditions which could have an effect on it.\nLaws of Nature are all highly conditional The principal purpose of the preceding discussion is to point out that the laws of nature are all conditional statements and they relate only to a very small part of our knowledge of the world.\nThat maths is just a fallback for \u0026ldquo;beatiful\u0026rdquo; physics happening the connection is that discussed in mathematics simply because he does not know of any other similar connection.\nApart from invarients, we just scope-limit ourselves to get the remaining bits that we need to make stuff work \u0026ldquo;beautifully\u0026rdquo; propose to refer to the observation which these examples illustrate as the empirical law of epistemology. Together with the laws of invariance of physical theories, it is an indispensable foundation of these theories.\n","permalink":"https://www.jemoka.com/posts/kbhthe_unreasonable_effectiveness_of_mathematics_in_the_natural_sciences/","tags":null,"title":"The Unreasonable Effectiveness of Mathematics in the Natural Sciences"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhtherma/","tags":null,"title":"therma"},{"categories":null,"contents":"thermoregulation is the brain\u0026rsquo;s regulation of body temperature to respond to heat, cold events.\nStudies indicate that cold exposure cold exposure can activate AgRP (stimulate food intake) as a means for the brain leveraging CNS regulation to which would lower the glucose level and maintain glucose homeostatis.\nHowever, cold exposure also trigger energy expenditure, and seems contradictory but not really why?.\n","permalink":"https://www.jemoka.com/posts/kbhthermoregulation/","tags":null,"title":"thermoregulation"},{"categories":null,"contents":"The theta/alpha ratio is the ratio between two oscillations measurable by an EEG that is shown to be a possible indicator for AD development.\n","permalink":"https://www.jemoka.com/posts/kbhtheta_alpha_ratio/","tags":null,"title":"theta/alpha ratio"},{"categories":null,"contents":" Because this chapter is not about linear algebra, your instructor may go through it rapidly. You may not be asked to scrutinize all the proofs. Make sure, however, that you at least read and understand the statements of all the results in this chapter—they will be used in later chapters.\nSo we are not going to go through everything very very carefully. Instead, I\u0026rsquo;m just going to go through some interesting results at my own leisure. This also means that this note is not very complete.\nfacts \u0026ldquo;you can factor out every root\u0026rdquo;: \\(p(\\alpha) = 0 \\implies p(z)=(z-\\alpha)q(z)\\)\nfundamental theorem of algebra: \u0026ldquo;if you have an nth-degree polynomial, you can factor it into n factors\u0026rdquo; (over the complex numbers, you have as many roots as the degree of the polynomials)\nthese coefficients are unique barring ordering factoring real polynomials can be treated in two pieces: one piece, the reals, which can be treated usually; then one other piece, the complexes, can be multiplied pairwise together (as over real coeffs they always come in conjugate pairs) into quadratics. This is why all real polynormials can always be factored as \\((x-\\lambda)(x-\\lambda) \\dots (x^{2}+ax+b) (x^{2}+ax+b)\\dots\\) the number of complex polynomials has to be even complex polynomials have \\(deg\\ p\\) factors\nreal polynomials have \\(deg\\ p\\) real/complex factors, but complex factors come in pairs\nwe can squish the complex part of the real polynomials together, and get\u0026mdash;wlog $m$\u0026mdash;first-degree real roots and \\(\\frac{deg\\ p - m}{2}\\) second-degree real roots where \\(b^{2} \u0026lt; 4c\\)\n\\(x^{2} + bx + c\\) has a factor of \\((x-\\lambda_{1})(x-\\lambda_{2})\\) under reals \\(b^{2} \\geq 4c\\)\nkey sequence complex numbers we defined: complex numbers, conjugates, and absolute value 9 properties of complexes (see below) polynomial coefficients polynomial coefficients are unique; namely, if a polynomial is the zero function, all of its coefficients have to be \\(0\\) division, zero, and factoring polynomial division: given two polynomials \\(p,s \\in \\mathcal{P}(\\mathbb{F})\\), with \\(s\\neq 0\\), then \\(\\exists q,r \\in \\mathcal{P}(\\mathbb{F})\\) such that: \\(p = s q +r\\); that is, given two polynomials, you can always divide one by the other with some remainder as long as the \u0026ldquo;other\u0026rdquo; is not \\(0\\) we defined zero (\\(p \\lambda =0\\), then \\(\\lambda\\) is a \u0026ldquo;zero\u0026rdquo;) and factor which is some polynomial \\(s \\in \\mathcal{P}(\\mathbb{F})\\) for another polynomial \\(p\\) such that there exists some \\(q \\in \\mathcal{P}(\\mathbb{F})\\) such that \\(p = s q\\) we show that each zero corresponds to a factor of the shape \\(p(z) = (z-\\lambda)q(z)\\) we show that a polynomial with degree \\(m\\) has at most \\(m\\) distinct zeros FToA and corollaries FToA: every non-constant polynomial under the complexes has a zero and that means every polynomial over the complexes has a unique factorization \\(p(z) = c(z-\\lambda_{1})(z-\\lambda_{2}) \\dots (z-\\lambda_{m})\\) polynomials with zero coefficients have zeros in pairs: if \\(\\lambda \\in \\mathbb{C}\\) is a factor of the polynomial, so is \\(\\bar{\\lambda}\\) Is a real polynomial factorable? A polynomial \\(x^{2}+bx+c\\) is factorable into \\((x-\\lambda_{1})(x-\\lambda_{2})\\) IFF \\(b^{2} \u0026gt; 4c\\). All polynomials over the reals can be factored into at least second degree polynomials \\(p(z) = c(z-\\lambda_{1})(z-\\lambda_{2}) \\dots (z-\\lambda_{m}) \\dots (x^{2}+b_{M}x+c_{M})\\) first, review complex numbers \\(z+\\bar{z} = 2 \\text{Re}\\ z\\) \\(z-\\bar{z} =2(\\text{Im}\\ z)i\\) \\(z\\bar{z} = |z|^{2}\\) \\(\\bar{x+z} = \\bar{w}+\\bar{z}\\), \\(\\bar{wz} = \\bar{w}\\bar{z}\\) \\(\\bar{\\bar{z}} = z\\) \\(| \\text{\\{Re,Im\\}}\\ z| \\leq |z|\\) both real and imaginary components are smaller than the actual absolute value \\(|\\bar{z}| = |z|\\) \\(|wz| = |w| |z|\\) \\(|w+z| \\leq |w| + |z|\\), the triangle inequality triangle inequality For \\(w, z \\in \\mathbb{C}\\), we do route algebra:\npolynomial division Suppose \\(p,s \\in \\mathcal{P}(\\mathbb{F}), s\\neq 0\\), then, \\(\\exists\\) polynomials \\(q,r \\in \\mathcal{P(\\mathbb{F})}\\) such that:\n\\begin{equation} p = s q +r \\end{equation}\nand \\(\\deg r \u0026lt; \\deg s\\).\nProof:\nLet: \\(n = \\deg p\\), and \\(m = \\deg s\\). So, if \\(n \u0026lt; m\\) (i.e. it is not a division), then take \\(q=0\\) and \\(r=p\\).\nNow, let\u0026rsquo;s make ???\nFactoring A polynomial \\(s \\in \\mathcal{P}(\\mathbb{F})\\) is a factor of \\(p \\in \\mathcal{P}(\\mathbb{F})\\) if \\(\\exists\\) \\(q \\in \\mathcal{P}(\\mathbb{F})\\) such that \\(p=s q\\).\nquestions proofs: wut if the FToA holds, isn\u0026rsquo;t the polynomials over the reals a \u0026ldquo;subset\u0026rdquo;(ish) of the polynomials over the complexes? so there is going to be at least complex roots to all polynormials always no? ","permalink":"https://www.jemoka.com/posts/kbhthoughts_on_axler_4/","tags":null,"title":"Thoughts on Axler 4"},{"categories":null,"contents":"A\n","permalink":"https://www.jemoka.com/posts/kbhtiago_forte/","tags":null,"title":"Tiago Forte"},{"categories":null,"contents":"the transformational generative syntax is a linguistical precept proposed by Noam Chomsky which has the interesting conclusion that meaning is supported by structure, rather than the other way around as generative semantics suggests.\nThis means that you can first come up with generic, independent structure to a sentence, then fill in the sentence with meaning.\nFor instance, \u0026ldquo;colorless green ideas sleep furiously\u0026rdquo; is a sentence Noam Chomsky proposes to have perfect structure but failes to be filled with meaning, supporting the transformational generative syntax theory.\nThis supports the Lexicalist Hypothesis, which is the theory that lexicalization transformations are independent of structural transformations. This would therefore support the proof for the existence of semantic primes.\n","permalink":"https://www.jemoka.com/posts/kbhtransformational_generative_syntax/","tags":null,"title":"transformational generative syntax"},{"categories":null,"contents":"Translation Theory\n","permalink":"https://www.jemoka.com/posts/kbhtranslation_studies_index/","tags":null,"title":"Translation Studies Index"},{"categories":null,"contents":"Translation Theory is the theory that studies how translation works.\nSpectrum of Translation domestication and foreignization are processes by which a translator can choose to alter the style of a translation for a purpose.\nforeignization trying to bring the target language closer to the source language\nbring in foreign words use colourful idioms use old words domestication trying to bring he source language closer to the target language\nf1 = var(\u0026#34;f1\u0026#34;) f2 = var(\u0026#34;f2\u0026#34;) df1 = var(\u0026#34;df1\u0026#34;) df2 = var(\u0026#34;df2\u0026#34;) f = ((f1^2)/2 + (f2^2)/2)^(1/2) (((f.diff(f1) * df1)^2 + (f.diff(f2) * df2)^2)^(1/2)).subs(f1=512.5094554, f2=512.5094554, df1=0.5, df2=0.5) 0.353553390593274 ","permalink":"https://www.jemoka.com/posts/kbhtranslation_theory/","tags":null,"title":"Translation Theory"},{"categories":null,"contents":"A load perpendicular to the long end of a rod. Think of a metal rod lying flat on the ground; a transverse\n","permalink":"https://www.jemoka.com/posts/kbhtransverse_loaod/","tags":null,"title":"transverse load"},{"categories":null,"contents":"Tuning Forks (funing torks!) is a Tuning Fork. You smack it and it goes \u0026ldquo;biiing!\u0026rdquo;\nLet\u0026rsquo;s figure out how it works. For us to be one same page, let\u0026rsquo;s define some vocab:\nVocab \u0026ldquo;Tine\u0026rdquo;: one of the two prongs of the fork A Cursory Explanation Source: here and here. Both are not very scientific but a good first step.\nFrom a very basic perspective, hiting a tuning fork creates a transverse wave on the tine you hit, which vibrates and then compresses the air around it in a longitudinal fashion at a set frequency, which we hear as a sound.\nOk but then this raises the question of why there\u0026rsquo;s two tines. The explanation this website gives is essentially that the actual mechanism of the Tuning Fork is in squishing the air immediately around the fork, so\u0026hellip;\nif the tines are push towards together, it creates a void in the space it just was; this creates a low pressure rarefaction area if the tines snap back apart, it compresses the air creating compression by squishing the air around it And therefore, the air around the funing tork is essentially being played like a two-way slingy. To adjust the pitch of the Tuning Fork, you lengthen or shorten it: longer tuning forks have larger tines, which vibrate more slowly.\nOk but now many, many questions why does smacking one side of the Tuning Fork make both sides vibrate presumably the base is not vibrating; hence, how does the downward-bendy vibration cause perpendicular oscillation (does it?) A Detour on Rigid Body Harmonic Motion Let\u0026rsquo;s talk about Bending. How does this relate to springs/slinkies? read this. A Better Detour on Cantilever Beams Cantilever Beams\nA Detour on the Temperature We are really worried about two different things here.\nMetal expands/contracts based on the temperature Temperature affects speed of sound A Detour on Material Science Why are our Tuning Forks out of tune? Fun, Relevant Factoids About the World The range of human hearing from a youngen is about 20Hz to 20,000Hz. Look into Young\u0026rsquo;s Modulus\nDensity Second overtones: six and a quarter; why?\nprove the equations given in Rossing 1990\nwhy do high frequencies die faster?\nWhy are they FORKS? What\u0026rsquo;s wrong with one prong\nLagrangian Mechanics\nexperiments to do in the end measuring in water measuring questions to ask why no free vibrations just standing? do the various tuning fork modes compose what happened to the harmonics of the fundimental? I know the overotens are 6/14, but where did the harmonics go? do they compose? what if we did it in a vaccume? of course the tuning fork is not going to be heard, but will it keep vibrating forever? Nyquist limit (FFT is only accurate to half the sampling rate; 10000 hz sampling (default on logger pro) means max is 5000 Hz) things we can use far field because the wavelength is much mucm much much larger than the seperation between the two tines; what is the wavelength? function of frequency and hertz Questions for Mark cuw tuning forks\u0026rsquo; freq is not the predicted freq of its shortest tine. urg how driven oscellation. how would it actually work? last minute tuning forks easy explanation of FFT \u0026ldquo;wrapping around circle\u0026rdquo; backup slide on octahedral scress explain beta how to get wavelength from sinusoidal equation how does wavelength change with temp; how does our ear compensate? https://en.wikipedia.org/wiki/Residual_stress ","permalink":"https://www.jemoka.com/posts/kbhtuning_forks/","tags":null,"title":"Tuning Fork"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhu1_c/","tags":null,"title":"u1.c"},{"categories":null,"contents":"Questions of Uniqueness and Existance are important elements in Differential Equations.\nHere\u0026rsquo;s a very general form of a differential equations. First, here\u0026rsquo;s the:\nfunction behavior tests Continuity Weakest statement.\nA function is continuous if and only if:\n\\begin{equation} \\lim_{x \\to y} f(x) =f(y) \\end{equation}\nLipschitz Condition Stronger statement.\nThe Lipschitz Condition is a stronger test of Continuity such that:\n\\begin{equation} || F(t,x)-F(t,y)|| \\leq L|| x- y|| \\end{equation}\nfor all \\(t \\in I\\), \\(x,y \\in \\omega\\), with \\(L \\in (0,\\infty)\\) is a Lipschitz Condition in the dependent variable \\(x\\).\nReshaping this into linear one-dimensional function, we have that:\n\\begin{equation} \\left | \\frac{F(t,x)-F(t,y)}{x-y} \\right | \\leq L \u0026lt; \\infty \\end{equation}\nThe important thing here is that its the same \\(L\\) of convergence \\(\\forall t\\). However, \\(L\\) may not be stable\u0026mdash;in can oscillate\nDifferentiable We finally have the strongest statement.\n\\begin{equation} \\lim_{x \\to y} \\frac{f(x)-f(y)}{x-y} = C \\end{equation}\nTo make something Differentiable, it has to not only converge but converge to a constant \\(C\\).\nExistence and Uniqueness Check for differential equation Assume some \\(F:I \\times \\omega \\to \\mathbb{R}^{n}\\) (a function \\(F\\) whose domain is in some space \\(I \\times \\omega\\)) is bounded and continuous and satisfies the Lipschitz Condition, and let \\(x_{0} \\in \\omega\\), then, there exists \\(T_{0} \u0026gt; 0\\) and a unique solution for \\(x(t)\\) that touches \\(x_{0}\\) to the standard First-Order Differential Equation \\(\\dv{x}{t} = F(t,x), x(t_{0}) = t_{0}\\) for some \\(|t-t_{0}| \u0026lt; T_{0}\\).\nTo actually check that \\(F\\) satisfies Lipschitz Condition, we pretty much usually just go and take the partial derivative w.r.t. \\(x\\) (dependent variable, yes its \\(x\\)) of \\(F\\) on \\(x\\), which\u0026mdash;if exists on some bound\u0026mdash;satisfies the Lipschitz condition on that bound.\nProof So we started at:\n\\begin{equation} \\dv{x}{t} = F(t,x), x(t_{0}) = x_{0} \\end{equation}\nWe can separate this expression and integrate:\n\\begin{align} \u0026amp; \\dv{x}{t} = F(t,x) \\\\ \\Rightarrow\\ \u0026amp; \\dd{x} = F(t,x)\\dd{t} \\\\ \\Rightarrow\\ \u0026amp; \\int_{x_{0)}}^{x(t)} \\dd{x} = \\int_{t_{0}}^{t} F(s,x(s)) \\dd{s} \\\\ \\Rightarrow\\ \u0026amp; x(t)-x_{0}= \\int_{t_{0}}^{t} F(s,x(s)) \\dd{s} \\end{align}\nAt this point, if \\(F\\) is seperable, we can then seperate it out by \\(\\dd{t}\\) and taking the right integral. However, we are only interested in existance and uniquness, so we will do something named\u0026hellip;\nPicard Integration Picard Integration is a inductive iteration scheme which leverages the Lipschitz Condition to show that a function integral converges. Begin with the result that all First-Order Differential Equations have shape (after forcibly separating):\n\\begin{equation} x(t)-x_{0}= \\int_{t_{0}}^{t} F(s,x(s)) \\dd{s} \\end{equation}\nWe hope that the inductive sequence:\n\\begin{equation} x_{n+1}(t) = x_{0} + \\int_{t_{0}}^{t} F(s,x_{n}(s)) \\dd{s} \\end{equation}\nconverges to the same result above (that is, the functions \\(x_{n}(s)\\) stop varying and therefore we converge to a solution \\(x(s)\\) to show existance.\nThis is hard!\nHere\u0026rsquo;s a digression/example:\nif we fix a time \\(t=10\\):\nwe hope to say that:\n\\begin{equation} \\lim_{n \\to \\infty } G_{n}(10) = G(10) \\end{equation}\n\\(\\forall \\epsilon \u0026gt; 0\\), \\(\\exists M \u0026lt; \\infty\\), \\(\\forall n\u0026gt;M\\),\n\\begin{equation} |G_{n}(10)-G(10)| \u0026lt; \\epsilon \\end{equation}\nNow, the thing is, for the integral above to converge uniformly, we hope that \\(M\\) stays fixed \\(\\forall t\\) (that all of the domain converges at once after the same under of the iterations.\nTaking the original expression, and applying the following page of algebra to it:\nFinally, we then apply the Lipschitz Condition because our setup is that \\(F\\) satisfies the Lipschitz Condition, we have that:\n\\begin{equation} ||x_{n+1}(t)-x_{n}(t)|| \\leq L\\int_{x_{0}}^{t} ||x_{n}(s)-x_{n-1}(s)||ds \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhuniqueness_and_existance/","tags":null,"title":"Uniqueness and Existance"},{"categories":null,"contents":"A constructor built out of quantum theory which can replicate itself. It is considered a universal computer.\n","permalink":"https://www.jemoka.com/posts/kbhuniversal_quantum_constructor/","tags":null,"title":"universal quantum constructor"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhuniversity_of_georgia/","tags":null,"title":"University of Georgia"},{"categories":null,"contents":" Investment: Paid for 50% of war bonds Production: ships, tanks, airplanes, etc. \u0026mdash; encourages production Conservation: 5% of the world\u0026rsquo;s population production, 50% of the world\u0026rsquo;s manufactured goods \u0026mdash; rationing, grow goods, etc. ","permalink":"https://www.jemoka.com/posts/kbhus_wwii_propaganda/","tags":null,"title":"US WWII Propaganda"},{"categories":null,"contents":"USAYPT or USIYPT is a physics research competition ran by Greg Jacobs.\n2022 My own work doc for the 2022 Tuning Forks problem is here.\nGeneral Tips When in doubt, ask about error prop ANSWER THE RESEARCH QUESTION (elevator) Convey that you understand basics via presentation Have intuition regarding phenomenon Be able to explain every formula from first principles Order of magnitude and dimension analysis Have clear variance in parameters (what did you vary and why) What does the intercepts mean on graphs? \u0026ldquo;Don\u0026rsquo;t be obtuse\u0026rdquo; Connect to simple physics terms Explanations needs to be simple Engage discussion ","permalink":"https://www.jemoka.com/posts/kbhusaypt/","tags":null,"title":"USAYPT"},{"categories":null,"contents":"User Experience is the\n","permalink":"https://www.jemoka.com/posts/kbhuser_experience/","tags":null,"title":"User Experience"},{"categories":null,"contents":"The User Experience design sprung out of WWII\u0026mdash;in Aerospace engineering.\nThe Design Process The \u0026ldquo;Double Diamond\u0026rdquo; Process\nFirst Round of Going Broad Explore the problem space (what are you users trying to do? why? why is it hard?) Decide what to fix (what is the most high impact problem?) Second Round of Going Broad Test potential solutions (does this fix the problem?) Refine final solution (do all users understand this? can they use them?) Usability Heuristics Usability Heuristics is a set of principles used in User Experience design to identify problems and potential solutions.\nVisibility of System Status Keep the users informed about what\u0026rsquo;s actively going on, through appropriate visual feedback placed at an appropriate amount of time.\nMatch Between System and the Real World Use language that\u0026rsquo;s familiar to the user, using words, phrases, concepts familiar to the users rather than internal jargon.\nBalance User Control and Freedom User often perform actions by mistake; mark \u0026ldquo;emergency exits\u0026rdquo; to leave unwanted pathways/actions without causing side effects.\nConsistency and Standards Having consistency between different versions/family of products: putting buttons that do the same thing to the same place across the app, at the same region.\nError Prevention Eliminate error-prone conditions (prevent the users from doing it), or present users with a confirmation before they commit to an erroneous action\nRecognition vs. Recall Users should\u0026rsquo;t need to remember when they are going through an UI; instead they should be able to recognize the intended behavior from the UI\nFlexibility and Efficiency of User Catering functionality to both novice and advanced users. Make advanced actions hidden to novice users, but easily accessible for advanced users.\nMinimalism Keep the UI focused on essential actions and information\u0026mdash;maintaining an aesthetic and minimalist design\nHelp Users Recognize, Diagnoses, and Recover from Errors Errors should\u0026hellip;\ngive context for what the problem is instruct the user for possible next actions Help It maybe necessary to provide documentation to help users understand how to complete their tasks; the documentation should be clear\n","permalink":"https://www.jemoka.com/posts/kbhux_design/","tags":null,"title":"User Experience"},{"categories":null,"contents":"Goal: understand the user.\nFind out\u0026hellip;\nMotivation Context Deeper need? The goal of user interviews is to understand the user even if they know what they want!\nGood User Interviews Make person feel welcome/safe/appreciated\nAsk open-ended \u0026ldquo;questions\u0026rdquo;\nDescribe a time that\u0026hellip; Tell me more about.. Leave space: awkward silences (not too awkward)\nReally listen!; repress the urge to think of what you want to say next\nRepeat statements back to people\nAsk about examples, context, etc.\nA roadmap 1: create a comfortable entry point 2: go wide, deep into more personal and complex questions 3: focus on the problem, not the solution 4: focus on feelings\u0026mdash;feelings matter, how nice matters 5: end with conclusions and statements for what you User Story The user story should contain\u0026hellip;.\nA main character (your user) Character background (motivation) A plot (context) Climax and Resolution Framework describe the user; who are they; what do they like or not like an iStudio classic need statement finish with a description of the emotional impact of using our software ","permalink":"https://www.jemoka.com/posts/kbhuser_interviews/","tags":null,"title":"User Interviews"},{"categories":null,"contents":" Secrets of Silicon Valley - Horowitz Looking for people who have feel for the problem: people need to believe in the problem Team: can people come with execution? people that are good at startups which are usually not good at later stage stuff Buy a startup and kick out the founders This is very typical Team and idea are easy to decouple Vetting problems Lack of market Technically insatiability \u0026ldquo;Unbelievable stupidity\u0026rdquo;: calcium is so cheap Idea goes through many morphs; getting the credit back People wiling to have a meeting? Decoupling value proposition =\u0026gt; iStudio as a service\nRandom Need: Nueva Alumni Network Maybe set up a Nueva alumni network? What could we do to facilitate the Nueva alumni network; extraction of mutual value from the next work.\nNueva alumni as a service.\nInnovation consultants Ideas are no longer valuable, which ideas to peruse is better. \u0026ldquo;helping people along in their relationship with the idea or with each other.\u0026rdquo; Decoupling solution with the customer with the most value.\n","permalink":"https://www.jemoka.com/posts/kbhvc_thing/","tags":null,"title":"vc thing"},{"categories":null,"contents":"A vector is an element of a vector space. They are also called a point.\n","permalink":"https://www.jemoka.com/posts/kbhvector/","tags":null,"title":"vector"},{"categories":null,"contents":"A vector space is an object between a field and a group; it has two ops\u0026mdash;addition and scalar multiplication. Its not quite a field and its more than a group.\nconstituents A set \\(V\\) An addition on \\(V\\) An scalar multiplication on \\(V\\) such that\u0026hellip;\nrequirements commutativity in add.: \\(u+v=v+u\\) associativity in add. and mult.: \\((u+v)+w=u+(v+w)\\); \\((ab)v=a(bv)\\): \\(\\forall u,v,w \\in V\\) and \\(a,b \\in \\mathbb{F}\\) distributivity: goes both ways \\(a(u+v) = au+av\\) AND!! \\((a+b)v=av+bv\\): \\(\\forall a,b \\in \\mathbb{F}\\) and \\(u,v \\in V\\) additive identity: \\(\\exists 0 \\in V: v+0=v \\forall v \\in V\\) additive inverse: \\(\\forall v \\in V, \\exists w \\in V: v+w=0\\) multiplicative identity: \\(1v=v \\forall v \\in V\\) additional information Elements of a vector space are called vectors or points. vector space \u0026ldquo;over\u0026rdquo; fields Scalar multiplication is not in the set \\(V\\); instead, \u0026ldquo;scalars\u0026rdquo; \\(\\lambda\\) come from this magic faraway land called \\(\\mathbb{F}\\). The choice of \\(\\mathbb{F}\\) for each vector space makes it different; so, when precision is needed, we can say that a vector space is \u0026ldquo;over\u0026rdquo; some \\(\\mathbb{F}\\) which contributes its scalars.\nTherefore:\nA vector space over \\(\\mathbb{R}\\) is called a real vector space A vector space over \\(\\mathbb{C}\\) is called a real vector space ","permalink":"https://www.jemoka.com/posts/kbhvector_space/","tags":null,"title":"vector space"},{"categories":null,"contents":"this is worse ","permalink":"https://www.jemoka.com/posts/kbhcraintech/","tags":null,"title":"VFUA"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhvgg/","tags":null,"title":"VGG"},{"categories":null,"contents":"VGGish is VGG, ish. VGGish is a network based on VGG which is pretrained on the audio-feature-extraction task.\n","permalink":"https://www.jemoka.com/posts/kbhvggish/","tags":null,"title":"VGGish"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhvietnam/","tags":null,"title":"Vietnam"},{"categories":null,"contents":"vietnamization is a political position held by Richard Nixon which is characterized by the slow replacement of American troops with Vietnamese ones.\n","permalink":"https://www.jemoka.com/posts/kbhvietnamization/","tags":null,"title":"vietnamization"},{"categories":null,"contents":"The VWAP is a Financial Market metric that stands for \u0026ldquo;volume-weighted average price.\u0026rdquo; It is given by (sumshares brought(shares bought at price*price its at)/(total shares bought in period)).\n\u0026ldquo;the price we care the most about, is the price where the most volume is traded.\u0026rdquo;\nMotivation Its a weighted-by volume trading price. Though the closing price is the price used for accounting, it isn\u0026rsquo;t a good metric for large-volume trades.\nTrading at the VWAP We Trade at the VWAP because a LARGE trade will move the market around, and we don\u0026rsquo;t want that if we are a large trader. So we trade at the VWAP to ensure that we are getting the best possible value.\nBuild a volume a profile Slicing the orders to match Control for volume deviations Volume Profile We use the volume-profile: \u0026ldquo;how much/what percentage of today\u0026rsquo;s volume happened in this chunk of the day\u0026rdquo; to predict today\u0026rsquo;s trading by matching by historical data. This often results in looking like a J curve: lots of trading happen at the beginning of the day, very little towards the middle, and LOTS in the end.\nSlicing Orders Slice your funds needed to trade, volume-wise, according to the Volume Profile. Set limit orders per slice at the best price for the market.\nControl Deviations from Expectation If you were\u0026rsquo;t able to trade by the limit order you posted at that slice, by the end of the slice, cancel your limit order and just send in a market order to ensure your participation with the desired volume at that slice.\n","permalink":"https://www.jemoka.com/posts/kbhvwap/","tags":null,"title":"VWAP"},{"categories":null,"contents":"DOI: 10.21437/Interspeech.2019-2414\n","permalink":"https://www.jemoka.com/posts/kbhwang_2019/","tags":null,"title":"Wang 2019"},{"categories":null,"contents":"Richard Nixon does not like democratic policies. Therefore, he had 5 operatives break into the DNC. Woodward and Berstein reports on the issue. Nixon rebounds and fires his investigator.\nThen, he released the \u0026ldquo;smoking gun\u0026rdquo; tape with the middle missing\n","permalink":"https://www.jemoka.com/posts/kbhwatergate/","tags":null,"title":"watergate"},{"categories":null,"contents":"A study with the goal of identifying semantic primes.\n","permalink":"https://www.jemoka.com/posts/kbhwhole_metalanguage_study/","tags":null,"title":"whole metalanguage study"},{"categories":null,"contents":"WPA is the largest relief program ever in the Great Depression New Deal, to promote public infrastructure and create artistic murals. It helped unskilled men to carry out public works infrastructure.\nThe project started 5/1935 and dissolved 6/1943.\n","permalink":"https://www.jemoka.com/posts/kbhwpa/","tags":null,"title":"Works Progress Administration"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhwriting_index/","tags":null,"title":"Writing Index"},{"categories":null,"contents":" vertibre backbone: 3 points to remember \u0026ldquo;we are in the business of looking for outliers\u0026rdquo; tarpit ides vision with world + good team iStudio Meeting Notes\n","permalink":"https://www.jemoka.com/posts/kbhycomb/","tags":null,"title":"ycomb"},{"categories":null,"contents":"Young\u0026rsquo;s Modulus is a mechanical property that measures the stiffness of a solid material.\nIt measures the ratio between mechanical stress \\(\\sigma\\) and the relative resulting strain \\(\\epsilon\\).\nAnd so, very simply:\n\\begin{equation} E = \\frac{\\sigma }{\\epsilon } \\end{equation}\nThinking about this, silly puddy deforms very easily given a little stress, so it would have low Young\u0026rsquo;s Modulus (\\(\\sigma \\ll \\epsilon\\)); and visa versa. https://aapt.scitation.org/doi/10.1119/1.17116?cookieSet=1\n","permalink":"https://www.jemoka.com/posts/kbhyoung_s_modulus/","tags":null,"title":"Young's Modulus"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2020.624488\nOne-Liner Used an ERNIE trained on transcripts for classification; inclusion of pause encoding made results better.\nNovelty Instead of just looking at actual speech content, look at pauses specific as a feature engineering task \\(89.6\\%\\) on the ADReSS Challenge dataset Notable Methods Applied FA with pause encoding with standard .cha semantics (short pauses, medium pauses, long pauses). Shoved all of this into an ERNIE.\nAssay for performance was LOO\nKey Figs Fig 1 This figure motivates the point that subjects with AD says oh and um more often; which prompted Table 1\nTable 1 Subjects with AD says uh a lot more often; no significance level calculations but ok.\nFigure 5 This figure is the result of a LOO study on the proposed model and presumably others before. X axis is the validation accuracy in question, Y is the density by which the score in X appears in an \\(N=35\\) LOO measurement.\nThis figure tells us that either way the ERNIE model is better than state of the art; furthermore, transcripts with pause encoding did better and did it better more of the time; that\u0026rsquo;s where the 89.6% came from.\nNew Concepts Leave-One-Out cross validation Notes Glorious.\n","permalink":"https://www.jemoka.com/posts/kbhyuan_2021/","tags":["ntj"],"title":"Yuan 2021"},{"categories":null,"contents":"A z-test is a hypothesis test for statistical significance between two sample proportions. Before it can be conducted, it must meet the conditions for inference for a z-test.\nconditions for inference (z-test) has to be random has to be reasonably normal (vis a vi test for normality) each sample has to be independent (or 10% rule) use a z-statistic to find p-value Given a sample proportion, calculate the sample proportion standard deviation (given on the formula sheet) Then, divide the difference between measured and null proportions to figure \\(z\\) that is,\n\\begin{equation} z = \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\end{equation}\nLook up the probability of \\(z\\) taking place on a \\(z\\) table. Then, \\(1-z\\) would yield the \\(p\\) vaule.\n","permalink":"https://www.jemoka.com/posts/kbhz_test/","tags":null,"title":"z-test"},{"categories":null,"contents":"\\(0\\) is a list of length \\(n\\) whose coordinates are all zero\nFormally\u0026mdash;\n\\begin{equation} 0 = (0,\\ldots,0) \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhzero/","tags":null,"title":"zero"},{"categories":null,"contents":"A zettlekasten is an atomic notetaking system.\nSteps:\nLit notes brief: \u0026lt; 3 sentences write it in your own words Reference notes Take reference notes? Fleeting Notes shower notes Permanent nodes Go through each notes from above, think about how it matters to your research Try to explicitly add value to existing ideas Try to find meaningful connection between ideas finding connections How does this fit into what I know? Can this be explained? find keywords not just to store a note, but how to retrieve it \u0026ldquo;in which circumstance will I need this note\u0026rdquo; \u0026ldquo;when and how will I need this idea\u0026rdquo; ","permalink":"https://www.jemoka.com/posts/kbhzettlekasten/","tags":null,"title":"zettlekasten"},{"categories":null,"contents":"a zettlekasten index is an index in a zettlekasten file format; it keeps track of all lists of notes. Head to Index Index for an index of indexes in this particular zettlekasten.\n","permalink":"https://www.jemoka.com/posts/kbhzettlekasten_index/","tags":null,"title":"zettlekasten index"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624683\nOne-Liner late fusion of multimodal signal on the CTP task using transformers, mobilnet, yamnet, and mockingjay\nNovelty Similar to Martinc 2021 and Shah 2021 but actually used the the current Neural-Network state of the art Used late fusion again after the base model training Proposed that inconsistency in the diagnoses of MMSE scores could be a great contributing factor to multi-task learning performance hindrance Notable Methods Proposed base model for transfer learning from text based on MobileNet (image), YAMNet (audio), Mockingjay (speech) and BERT (text) Data all sourced from recording/transcribing/recognizing CTP task Key Figs Figure 3 and 4 This figure tells us the late fusion architecture used\nTable 2 Pre-training with an existing dataset had (not statistically quantified) improvement against a randomly seeded model.\nTable 3 Concat/Add fusion methods between audio and text provided even better results; confirms Martinc 2021 on newer data\n","permalink":"https://www.jemoka.com/posts/kbhzhu_2021/","tags":["ntj"],"title":"Zhu 2021"}]