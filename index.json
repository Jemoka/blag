[{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhistudio_meeting_nodes/","tags":null,"title":""},{"categories":null,"contents":"Separated qubits don\u0026rsquo;t really like to interact. Instead, then, we just make them bigger and control them at the same time. We can implement gates via a sequence of pulses. If you work with interacting qubits a lot, you will end up with the APR Paradox.\nIf you take two qubits, and move them though two gates, you essentially will get entangled results.\nTo make this works, you will need to take some probability. Know correlation, expectation, etc.\n","permalink":"https://www.jemoka.com/posts/kbhmaking_qubits_interact/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhpoint_estimate/","tags":null,"title":""},{"categories":null,"contents":" \\(A\\) does all the asking, \\(B\\) has all the decision making power Population \\(A\\)\u0026rsquo;s match never goes up at best, they stay the same Population \\(B\\)\u0026rsquo;s match can never go down. At worse, they stay the same. Population \\(A\\) always ends up with the highest-preferred person in their realm of possibility Population \\(B\\) always ends up with the lowest-preferred person in their realm of possibility ","permalink":"https://www.jemoka.com/posts/kbhproperties_of_the_stable_matching_algorithm/","tags":null,"title":""},{"categories":null,"contents":" \u0026ldquo;Are the nodes system independent of the class system?\u0026rdquo; Does the model require a set of L2 class? Can we build the model to take advantage of as many 10* things as possible? A preso Demo of a kid moving through MVP vis a vis advantage over just taking all classes Naming skills that would go on the graph Figuring: comparability with flattening like in a L1 system ","permalink":"https://www.jemoka.com/posts/kbhrnn_notes/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrural_hospitals_problem/","tags":null,"title":""},{"categories":null,"contents":"The Stable Matching Problem is Wes Chao\u0026rsquo;s favourite algorithm.\nConsider two populations, \\(A\\) and \\(B\\), who want to form paired relationships between a person \\(A\\) and \\(B\\). \\(A_i\\) has a list of their ranked order matches (I want to be paired with \\(B_1\\) most, \\(B_4\\) second, etc.), and so does \\(B_i\\) (I want to be paired with \\(A_4\\) most \\(A_9\\) second, etc.)\nWe want to discover a stable matching, where pairs are most unwilling to move. We can solve it using the stable matching algorithm.\nNueva Invention Studio speed-dating noises?\napplications of the stable matching problem Dating Applying to college Both of these are high-stress situations, especially if you are doing asking You can mathematically prove that person doing the asking gets the best result Hence, it shows us that the best possible outcomes go to the people who are willing to ask and get rejected.\nextensions to the stable matching problem the stable matching problem can be extended to the rural hospitals problem, which is slightly better.\n","permalink":"https://www.jemoka.com/posts/kbhstable_matching_problem/","tags":null,"title":""},{"categories":null,"contents":"thermoregulation is the brain\u0026rsquo;s regulation of body temperature to respond to heat, cold events.\nStudies indicate that cold exposure cold exposure can activate AgRP (stimulate food intake) as a means for the brain leveraging CNS regulation to which would lower the glucose level and maintain glucose homeostatis.\nHowever, cold exposure also trigger energy expenditure, and seems contradictory but not really why?.\n","permalink":"https://www.jemoka.com/posts/kbhthermoregulation/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhz_score/","tags":null,"title":""},{"categories":null,"contents":"\\begin{align} v+(-1)v \u0026amp;= (1+(-1))v \\\\ \u0026amp;= 0v \\\\ \u0026amp;= 0 \\end{align}\nAs \\((-1)v=0\\), \\((-1)v\\) is the additive identity of \\(v\\) which we defined as \\(-v\\) \\(\\blacksquare\\).\n","permalink":"https://www.jemoka.com/posts/kbh1v_1/","tags":null,"title":"-1v=-v"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhq/","tags":null,"title":":q"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrandom/","tags":null,"title":"$kandom"},{"categories":null,"contents":"\\begin{align} 0v \u0026amp;= (0+0)v \\\\ \u0026amp;= 0v+0v \\end{align}\nGiven scalar multiplication is closed, \\(0v \\in V\\), which means \\(\\exists -0v:0v+(-0v)=0\\). Applying that to both sides:\n\\begin{equation} 0 = 0v\\ \\blacksquare \\end{equation}\nThe opposite proof of \\(\\lambda 0=0\\) but vectors work the same exact way.\n","permalink":"https://www.jemoka.com/posts/kbhzero_times_vector/","tags":null,"title":"0v=0"},{"categories":null,"contents":" New Deal ","permalink":"https://www.jemoka.com/posts/kbh1980s_political_alignment/","tags":null,"title":"1980s Political Alignment"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbh1a/","tags":null,"title":"1a"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhaaa/","tags":null,"title":"AAA"},{"categories":null,"contents":"Welcome to the personal site of Houjun \u0026ldquo;Jack\u0026rdquo; Liu.\nI\u0026rsquo;m on the blaggosphere as @jemokajack and, far more frequently, u/jemoka and @jemoka.\nWho\u0026rsquo;s this guy? I am a human interested in linguistic analysis, L2 learning, and user interfaces. AGI \u0026amp; Emacs are cool. I run Condution, Shabang, and MODAP, do research in NLP and education pedagogy, direct Science Friday streams, captain an entrepreneurship studio, and recently began working for Brian MacWhinney on projects in linguistics.\nNeed to catch me? Email me at houjun at the current domain. Please do email me, I actually check.\nRecent Projects Take a look at my GitHub profile for programming projects. For larger scale things, take a look at the Projects Index on this site.\nNotes This site also contains the vast majority of my course notes. It is a organized in a zettlekasten format. To begin exploring, why don\u0026rsquo;t you check out Nueva Courses Index. Accompanying this index is the collection of assignments found here which a few friends has collected together.\njklsnt Some friends and I started a small collection of fun internets that we made. Check it out!.\nHow do I know you are you? Good question! gpg --locate-keys houjun@jemoka.com. Note that GPG don\u0026rsquo;t actually check fingerprints you received so do that yourself. The same key should also be found at gpg --keyserver pgp.mit.edu --recv-keys 1807A0C6 if my WKD CNAME thing go down.\nBugga Bugga Bontehu? Sometimes I use this domain as a downlink to fastcalculator to friends and coworkers. To achieve this, here are two links you could click on that I don\u0026rsquo;t always promise do anything: oliver and socks.\n","permalink":"https://www.jemoka.com/posts/kbhindex/","tags":null,"title":"About"},{"categories":null,"contents":"Capecitabmine =\u0026gt; 5-Fluoropyrimidine =\u0026gt; Cancer cell death.\n","permalink":"https://www.jemoka.com/posts/kbhaction_of_capecitabmine/","tags":null,"title":"action of Capecitabmine"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhaction_research/","tags":null,"title":"action research"},{"categories":null,"contents":"Comes from doi.org/10.3389/fcomp.2020.00001\nADR is a vectorization/encoding technique whereby time-series data is segmented, clustered via solf-organizing maps, and the centroids of the clusters are used as the encoding\n","permalink":"https://www.jemoka.com/posts/kbhactive_data_representation/","tags":null,"title":"Active Data Representation"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhactive_recall/","tags":null,"title":"active recall"},{"categories":null,"contents":"Operation that adds elements in a set\nconstituents A set \\(V\\) Each non-necessarily-distinct elements \\(u,v \\in V\\) requirements addition on a set \\(V\\) is defined by a function that assigned an element named \\(u+v \\in V\\) (its closed), \\(\\forall u,v\\in V\\)\nadditional information See also addition in \\(\\mathbb{F}^n\\)\n","permalink":"https://www.jemoka.com/posts/kbhadding/","tags":null,"title":"adding"},{"categories":null,"contents":"The additive identity allows another number to retain its identity after adding. That is: there exists an element \\(0\\) such that \\(v+0=v\\) for whatever structure \\(v\\) and addition \\(+\\) you are working with.\n","permalink":"https://www.jemoka.com/posts/kbhadditive_identity/","tags":null,"title":"additive identity"},{"categories":null,"contents":"Assume for the sake of contradiction \\(\\exists\\ 0, 0\u0026rsquo;\\) both being additive identities in vector space \\(V\\).\nTherefore:\n\\begin{equation} 0+0\u0026rsquo; = 0\u0026rsquo; +0 \\end{equation}\ncommutativity.\nTherefore:\n\\begin{equation} 0+0\u0026rsquo; = 0 = 0\u0026rsquo;+0 = 0' \\end{equation}\ndefn. of identity.\nHence: \\(0=0\u0026rsquo;\\), \\(\\blacksquare\\).\n","permalink":"https://www.jemoka.com/posts/kbhadditive_identity_is_unique_in_a_vector_space/","tags":null,"title":"additive identity is unique in a vector space"},{"categories":null,"contents":"Take a vector \\(v \\in V\\) and additive inverses \\(a,b \\in V\\).\n\\begin{equation} a+0 = a \\end{equation}\ndefn. of additive identity\n\\begin{equation} a+(v+b) = a \\end{equation}\ndefn. of additive inverse\n\\begin{equation} (a+v)+b = a \\end{equation}\nassociativity\n\\begin{equation} 0+b = a \\end{equation}\ndefn. of additive inverse\n\\begin{equation} b=a\\ \\blacksquare \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhadditive_inverse_is_unique_in_a_vector_space/","tags":null,"title":"additive inverse is unique in a vector space"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhadhd/","tags":null,"title":"ADHD"},{"categories":null,"contents":"adMe: absorbtion, distribution, metabolism, excretion.\nPharmacology treatment of diseases. The microbiome regulates metabolism.\n","permalink":"https://www.jemoka.com/posts/kbhadme/","tags":null,"title":"adMe"},{"categories":null,"contents":"ADReSS Challenge is a Alzheimer\u0026rsquo;s Dementia Recognition challenge from the data available on DementiaBank.\n","permalink":"https://www.jemoka.com/posts/kbhadress_challenge/","tags":null,"title":"ADReSS Challenge"},{"categories":null,"contents":"The ADReSS Literature Survey is a literature survey for the results published during the ADReSS Challenge.\nAntonsson 2021: disfluency + SVF features trained on SVM: lexical \u0026gt; narrative qual. Chlasta 2021: features extracted from VGGish on SVM; also trained new CNN from .wav. Sadeghian 2021: Used GA for feature sel., achieved 94% w/ MMSE alone; dev\u0026rsquo;d ASR tool. Martinc 2021: CBOW (text) + ADR (sound) late fusion\u0026rsquo;d to a BERT, ablated for features. Meghanani 2021: spontaneous speech transcripts with fastText and CNN; 83.33% acc. Yuan 2021: ERNIE on transcripts with pause encoding; 89.6% acc. Jonell 2021: Developed a kitchen sink of diag. tools and correlated it with biomarkers. Laguarta 2021: multimodel (OVBM) to embed auditory info + biomarkers for clsf. Shah 2021: late fusion of n-gram and OpenSMILE on std. classifiers. Lindsay 2021: Cross-linguistic markers shared for AD patients between English and French. Zhu 2021: late fusion of CTP task for AD clsf. w/ transf., mobilenet, yamnet, mockingjay. Guo 2021: WLS data to augment CTP from ADReSS Challenge and trained it on a BERT. Balagopalan 2021: lexo. and synt. features trained on a BERT and other models. Mahajan 2021: a bimodal model on speech/text with GRU on speech and CNN-LSTM on text. Parvin 2020: excercize scheme effects on theta/alpha ratio and Brain wave frequency. Luz 2021: review paper presenting the ADReSSo challenge and current baselines. From Meghanani 2021, a review:\n","permalink":"https://www.jemoka.com/posts/kbhadress_literature_survey/","tags":["index"],"title":"ADReSS Literature Survey Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhadvertising/","tags":null,"title":"advertising"},{"categories":null,"contents":"Agricultural Adjustment Administration is a part of the New Deal programs to support the agricultural sector and maintain supply. They regulated production of seven different crops to group increase farming income. It is very far-reaching of other parts of the economy.\nIt was ruled unconstitutional in 1936.\n","permalink":"https://www.jemoka.com/posts/kbhagricultural_adjustment_administration/","tags":null,"title":"Agricultural Adjustment Administration"},{"categories":null,"contents":"AgRP is a type of neurons that stimulates food intake.\nInhibit metacortin Activate NPY Release GABA Diet-induced obesity blunts AgRP response, and so, because AgRP plays a part in thermoregulation, diet-inducsed obesity responds less to temperature changes.\n","permalink":"https://www.jemoka.com/posts/kbhagrp/","tags":null,"title":"AgRP"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhai/","tags":null,"title":"AI"},{"categories":null,"contents":"AI Ethics is the Ethics of training AI models.\n","permalink":"https://www.jemoka.com/posts/kbhai_ethics/","tags":null,"title":"AI Ethics"},{"categories":null,"contents":"AIBridge is an introductory AI bootcamp developed and taught by Prof. Xin Liu, yours truly, and Samuel Ren in collaboration with AIFS.\nCourse website: AIBridge Course Website\nAIBridge Lecture Codealongs AIBridgeLab D1Aft AIBridgeLab D2Aft AIBridgeLab D3/D4 AIBridgeLab D4Aft oeansut\\n \\n aosntegu\\n \\n\n","permalink":"https://www.jemoka.com/posts/kbhaibridge/","tags":null,"title":"AIBridge"},{"categories":null,"contents":" Welcome to the AIBridge Course homepage.\nThe purpose of AIBridge is to bridge the gap between computer science and other disciplines. To many, working with AI might seem like an unreachable objective. However, in reality, one week is enough to get started. AIBridge will provide basic programming capability in Python and knowledge of object-oriented programming as well as the concepts behind machine learning and how to implement it using a popular toolbox, Scikit-Learn. Students work to complete a personally-defined project using techniques in AI, with data from their own research or with problems supplied by the Course. This one week course will be hosted in-person at UC Davis and will target mainly undergraduate and non-technical graduate students.\nThe course is taught by Prof. Xin Liu in collaboration with Houjun \u0026ldquo;Jack\u0026rdquo; Liu, Samuel Ren, and Albara Ah Ramli.\nEvergreen Resources Python Tutorial: W3 Schools Python Documentation: Python.org SciKit Documentation: scikit-learn.org Iris Dataset: UCI DB, or, for better user experience, scikit Wine Dataset: UCI DB Class Discord: Invite Data-Loading Cheat-Sheet: Colab When in doubt\u0026hellip;\nGoogle it! Try it! Andrew Ng\u0026rsquo;s Machine Learning Suite of Courses DONE Day 1: Python Basics On Monday, 06/27/2022, we covered the basics of Python so that we are all up to speed to perform basic ML with the Scikit Learn toolkit.\nIntroductory Remarks: Slides Lecture on Python Basics: Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Morning Lecture Notebook, Morning Lab Notebook, Afternoon Lecture Notebook, Afternoon Lab Notebook Day 1 feedback survey: Link\nDONE Day 2: OOP + Linear Models Today, we are going to cover the basic intuition and terminology behind Object Oriented Programming, as well as introduce two simple, linear approaches to Machine Learning tasks: linear regression and logistic regression.\nLecture on OOP and more on functions (morning): Slides Lecture on Linear and Logistic Regression (afternoon): Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Morning Lecture Notebook, Morning Lab Notebook, Afternoon Lab Notebook Day 2 feedback survey: Link\nDONE Day 3: Data + Classifier Today, we are going to cover data cleaning, and three more classifiers!\nLecture on data cleaning and pandas (morning): Slides Lecture on three classification algorithms (afternoon): Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Morning Lab Notebook, Afternoon Lab Notebook Day 3 feedback survey: Link\nDONE Day 4: Operations and Clustering Today, we are going to work on the validation operations tools, and talk about clustering\nLecture on training and data operations (morning): Slides Lecture on clustering and feature operations (afternoon): Slides Lab Exercises: Morning Lab Notes, Afternoon Lab Notes Colab Notebooks: Afternoon Notebook Day 4 feedback survey: Link\nDay 5: Closing Thoughts Today, we are going to tie some loose ends with missing data, error analysis, semi supervised learning, cross validation, and ethics.\nClosing thoughts lecture (morning): Slides Final Project: AIBridge Final Project\nDay 5/Bootcamp feedback survey: Link\nOther Links and Resources Tools we use: AIBridge Packages and Tools Cleaned Wine Dataset (try cleaning it yourself before using!): Google Drive Iris Data with Temperature (don\u0026rsquo;t use without instructions, though!): Google Drive ","permalink":"https://www.jemoka.com/posts/kbhaibridge_course_website/","tags":null,"title":"AIBridge Course Website"},{"categories":null,"contents":"Part 1: ML Training Practice One of the things that makes a very good Sommelier is their ability to figure out as much details about a wine as possible with very little information.\nYou are tasked with making a Sommelier program that is able to figure both the type and quality of wine from available chemical information. Also, you have a \u0026ldquo;flavor-ater\u0026rdquo; machine that makes a linear combination of multiple chemical features together (similar to PCA), which is counted as one chemical feature after combination.\nA good Sommelier uses as little information as possible to deduce the quality and type. So, what is the best model(s) you can build for predicting quality and type of wine based on the least amount of features? What features should you choose?\nGood luck!\nPart 2: ML Project Walk-through Create your own machine learning experiement! Begin with a problem in your field; go through the available/your own data, determine what type of problem it is, and discuss why machine learning could be a good solution for the problem. Research/quantify the baselines in the field for the task (remembering our discussion on ML validation methods), and determine a list of possible features of your data.\nIf we were to help collect data together, how can we best collect a representative sample? How expensive (resources, monetary, or temporal) would it be? What are some ethical issues?\nSelect the features in the data available to you that would be most relavent (this time you are not trying to minimize the features, but select the most appropriate ones), and the model/training mechanism you think would be most appropriate.\nFinally, present your thinking! Share with us a few (1-3) slides on Friday afternoon. If you have additional time, possibly train the model on baseline data!\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_final_project/","tags":null,"title":"AIBridge Final Project"},{"categories":null,"contents":"SPOILER ALERT for future labs!! Don\u0026rsquo;t scroll down!\nWe are going to create a copy of the iris dataset with a random variance.\nimport sklearn from sklearn.datasets import load_iris Let\u0026rsquo;s load the iris dataset:\nx,y = load_iris(return_X_y=True) Because we need to generate a lot of random data, let\u0026rsquo;s import random\nimport random Put this in a df\nimport pandas as pd df = pd.DataFrame(x) df 0 1 2 3 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 .. ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 [150 rows x 4 columns] Let\u0026rsquo;s make 150 random numbers with pretty low variance:\nrandom_ns = [random.uniform(65,65.2) for _ in range(0, 150)] random_series = pd.Series(random_ns) random_series 0 65.127515 1 65.034572 2 65.123271 3 65.043985 4 65.145743 ... 145 65.036410 146 65.157172 147 65.034925 148 65.037373 149 65.042466 Length: 150, dtype: float64 Excellent. Now let\u0026rsquo;s put the two things together!\ndf[\u0026#34;temp\u0026#34;] = random_series df 0 1 2 3 temp 0 5.1 3.5 1.4 0.2 65.127515 1 4.9 3.0 1.4 0.2 65.034572 2 4.7 3.2 1.3 0.2 65.123271 3 4.6 3.1 1.5 0.2 65.043985 4 5.0 3.6 1.4 0.2 65.145743 .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 146 6.3 2.5 5.0 1.9 65.157172 147 6.5 3.0 5.2 2.0 65.034925 148 6.2 3.4 5.4 2.3 65.037373 149 5.9 3.0 5.1 1.8 65.042466 [150 rows x 5 columns] And, while we are at it, let\u0026rsquo;s make new labels\nnames = pd.Series([\u0026#34;sepal length\u0026#34;, \u0026#34;sepal width\u0026#34;, \u0026#34;pedal length\u0026#34;, \u0026#34;pedal width\u0026#34;, \u0026#34;temp\u0026#34;]) df.columns = names df sepal length sepal width pedal length pedal width temp 0 5.1 3.5 1.4 0.2 65.127515 1 4.9 3.0 1.4 0.2 65.034572 2 4.7 3.2 1.3 0.2 65.123271 3 4.6 3.1 1.5 0.2 65.043985 4 5.0 3.6 1.4 0.2 65.145743 .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 146 6.3 2.5 5.0 1.9 65.157172 147 6.5 3.0 5.2 2.0 65.034925 148 6.2 3.4 5.4 2.3 65.037373 149 5.9 3.0 5.1 1.8 65.042466 [150 rows x 5 columns] Excellent. Let\u0026rsquo;s finally get the flower results.\ndf[\u0026#34;species\u0026#34;] = y df sepal length sepal width pedal length pedal width temp species 0 5.1 3.5 1.4 0.2 65.127515 0 1 4.9 3.0 1.4 0.2 65.034572 0 2 4.7 3.2 1.3 0.2 65.123271 0 3 4.6 3.1 1.5 0.2 65.043985 0 4 5.0 3.6 1.4 0.2 65.145743 0 .. ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 2 146 6.3 2.5 5.0 1.9 65.157172 2 147 6.5 3.0 5.2 2.0 65.034925 2 148 6.2 3.4 5.4 2.3 65.037373 2 149 5.9 3.0 5.1 1.8 65.042466 2 [150 rows x 6 columns] And dump it to a CSV.\ndf.to_csv(\u0026#34;./iris_variance.csv\u0026#34;, index=False) Let\u0026rsquo;s select for the input data again:\nX = df.iloc[:,0:5] y = df.iloc[:,5] X sepal length sepal width pedal length pedal width temp 0 5.1 3.5 1.4 0.2 65.127515 1 4.9 3.0 1.4 0.2 65.034572 2 4.7 3.2 1.3 0.2 65.123271 3 4.6 3.1 1.5 0.2 65.043985 4 5.0 3.6 1.4 0.2 65.145743 .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 65.036410 146 6.3 2.5 5.0 1.9 65.157172 147 6.5 3.0 5.2 2.0 65.034925 148 6.2 3.4 5.4 2.3 65.037373 149 5.9 3.0 5.1 1.8 65.042466 [150 rows x 5 columns] And use the variance threshold tool:\nfrom sklearn.feature_selection import VarianceThreshold sel = VarianceThreshold(0.1) sel.fit_transform(X) 5.1 3.5 1.4 0.2 4.9 3 1.4 0.2 4.7 3.2 1.3 0.2 4.6 3.1 1.5 0.2 5 3.6 1.4 0.2 5.4 3.9 1.7 0.4 4.6 3.4 1.4 0.3 \u0026hellip;\nAs we expected.\nAnd let\u0026rsquo;s use the select k best tool:\nfrom sklearn.feature_selection import SelectKBest, chi2 sel = SelectKBest(chi2, k=4) res = sel.fit_transform(X, y) res 5.1 3.5 1.4 0.2 4.9 3 1.4 0.2 4.7 3.2 1.3 0.2 4.6 3.1 1.5 0.2 5 3.6 1.4 0.2 5.4 3.9 1.7 0.4 4.6 3.4 1.4 0.3 5 3.4 1.5 0.2 \u0026hellip;\nAlso, as we expected. Got rid of temp.\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_iris_variance_worksheet/","tags":null,"title":"AIBridge Iris Variance Worksheet"},{"categories":null,"contents":"This is usually not needed if you are using Google Colab. If you are following the instructions provided during our lecture series, please disregard this page.\nHowever, students have expressed interest in working with their own system\u0026rsquo;s copy of Jupyter or local installation. We therefore provide a set of very tenuous instructions for installing the tools used in our session using vanilla C-Python (i.e. not anaconda/conda/miniconda.)\nPython Our tools target Python 3.8+. Use your system\u0026rsquo;s package manager to install Python at least version 3.8, or use Python Foundation\u0026rsquo;s universal installers.\nPackages Python sometimes ships pip, its packaging utility separately. Refer to your own distribution\u0026rsquo;s installation instructions if none of pip or pip3 or python -m pip or python -m pip.\nOnce your copy of pip has been identified, let\u0026rsquo;s move on to\u0026hellip;\nInstalling Packages Here are the packages we will need for our sessions:\nscikit-learn pandas numpy Along with its respective dependencies. Here\u0026rsquo;s a one-liner:\npython3 -m pip install scikit-learn pandas numpy Good luck!\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_packages/","tags":null,"title":"AIBridge Packages and Tools"},{"categories":null,"contents":"Rewa Rai Nitin Lab, Dept. of Food Sci + Tech - Davis\nWine Classification Task Whole data:\nDecision Tree: 98.46% Random Forest: 99.84% Gaussian NB: 97.08% Regression Task Feature selection with 2 best features actually improved.\nTalkthrough Detecting berry infection by leaf classification. Use FTIR spectroscopy as a means of infection classification.\nTana Hernandez PHD Student, Nitin Lab, Dept. of Food Sci + Tech - Davis\nTalkthrough Given input for reaction, predict resulting gell strength from protein+carbo+lactic acid.\nGoal to figure out what features are o predict gell formation. Use feature extraction to reduce the need of doing.\nWet lab task: use high-throughput 96 hole plates to measuring kinetics of absorborance and kinetics. In a single hour, 96 data points can be acquired.\nThen, droplet elements are added to the plates.\nModel: take feature inputs which was selected, classification on gell formation and regression for time for gell.\nJimmy Nguyen PHD Student, Nitin Lab, Dept. of Food Sci + Tech - Davis\nTalk through Need: creating plant-based products which just feels and tastes like actual meet based food.\nTask: given molecular information, classify taste based on like-product and unlike\nLuyao Ma Postdoc Researcher, Nitin Lab, Dept. of Food Sci + Tech - Davis\nTalk thought Problem: lots of antimicrobian resistance in food: on track for 10 million deaths due to antimicrobial resistance. This is caused by antibiotics given to animals, which then is given indirectly to humans. Humans gut bactorials became more more resistant to antibiotics due to antimicrobial bacterial deveolping in animal guts.\nCurrent surveilance systems for antibiotic bacteria: require centralized lab for analysis, data collection is slow, and data integration is very slow (2ish years to publish final results), protocol also changes.\nGoal: rapid in field automatic detection scheme\nExpose wells of bacterial to detect color intensity\n? PHD Student, USDA\nWine Naive bayes (6 RFE features); XE Boost Random Forest + Search with 9 features\nTalkthrough Dietary data Random calls Interested in gut miocrobiome influences. Goal: which factors to predict CAZyme dyvirsetiy?\nRandom forest regression Need for prediction for which features: use Shapley Addadtive for result intepretation.\nYue Wine OH WOWO\nReg:\n99.98 train, 59.788 test.\nBalanced dataset Sequential feature selection PCA -\u0026gt; 3 features Random Forest Something else: ExhaustiveFeatureSelector\nClsf:\nstill 4 features.\nTalkthrough Deep learning, CV applications.\nNutrition product validation so far is entirely manual; current work in bias are mostly political, so finding a ground truth is difficult.\nSupervised is probability difficult; getting the data and cluster.\nSriya Sunil PhD Food Science, Cornell\nWine Decision tree classifier; resulted in 7 features.\n99.97% train, 97.08% test.\nSupport Vector Regression; resulted in 7 features as well.\n39.25% train, 32.79% test.\nTalkthrough Microbial growth on baby spinach. Features: initial counts, prevalence of bacteria, growth of bacteria.\nOutput regression to time to spoilage\n","permalink":"https://www.jemoka.com/posts/kbhaibridge_student_presentations/","tags":null,"title":"AIBridge Student Presentations"},{"categories":null,"contents":"Welcome to the Day-2 Afternoon Lab! We are super excited to work through tasks in linear regression and logistic regression, as well as familiarize you with the Iris dataset.\nIris Dataset Let\u0026rsquo;s load the Iris dataset! Begin by importing the load_iris tool from sklearn. This is an easy loader scheme for the iris dataset.\nfrom sklearn.datasets import load_iris Then, we simply execute the following to load the data.\nx,y = load_iris(return_X_y=True) We use the return_X_y argument here so that, instead of dumping a large CSV, we get the neat-cleaned input and output values.\nLet\u0026rsquo;s inspect this data a little.\nx[0] 5.1 3.5 1.4 0.2 We can see that each sample of the data is a vector in \\(\\mathbb{R}^4\\). They correspond to four attributes:\nseptal length septal width pedal length pedal width What\u0026rsquo;s the output?\ny[0] 0 We can actually see all the possible values of the output by putting it into a set.\nset(y) 0 1 2 There are three different classes of outputs.\nIris Setosa Iris Versicolour Iris Virginica Excellent. So we can see that we have a dataset of four possible inputs and one possible output. Let\u0026rsquo;s see what we can do with it.\nLogistic Regression The simplest thing we can do is a logistic regression. We have a there categories for output and a lot of data for input. Let\u0026rsquo;s figure out if we can predict the output from the input!\nLet\u0026rsquo;s import logistic regression tool first, and instantiate it.\nfrom sklearn.linear_model import LogisticRegression reg = LogisticRegression() We will \u0026ldquo;fit\u0026rdquo; the data to the model: adjusting the model to best represent the data. Our data has 150 samples, so let\u0026rsquo;s fit the data on 140 of them.\ntesting_samples_x = x[-5:] testing_samples_y = y[-5:] x = x[:-5] y = y[:-5] Wonderful. Let\u0026rsquo;s fit the data onto the model.\nreg = reg.fit(x,y) Let\u0026rsquo;s go ahead and run the model on our 10 testing samples!\npredicted_y = reg.predict(testing_samples_x) predicted_y 2 2 2 2 2 And, let\u0026rsquo;s figure out what our actual results say:\ntesting_samples_y 2 2 2 2 2 Woah! That\u0026rsquo;s excellent.\nLinear Regression Instead of predicting the output classes, we can predict some values from the output. How about if we used septal length, width, and pedal length to predict petal width? The output now is a number, not some classes, which calls for linear regression!\nLet\u0026rsquo;s import linear regression tool first, and instantiate it.\nfrom sklearn.linear_model import LinearRegression reg = LinearRegression() We will \u0026ldquo;fit\u0026rdquo; the data to the model again. As we have cleaned out the testing_samples, we simply need to split out the fourth column for the new x and y:\nnew_x = x[:,:3] new_y = x[:,3] new_testing_samples_y = testing_samples_x[:,3] new_testing_samples_x = testing_samples_x[:,:3] Taking now our newly parsed data, let\u0026rsquo;s fit it to a linear model.\nreg = reg.fit(new_x,new_y) Let\u0026rsquo;s go ahead and run the model on our 10 testing samples!\nnew_predicted_y = reg.predict(new_testing_samples_x) new_predicted_y 1.7500734 1.61927061 1.79218767 2.04824364 1.86638164 And, let\u0026rsquo;s figure out what our actual results say:\nnew_testing_samples_y 2.3 1.9 2 2.3 1.8 Close on some samples, not quite there on others. How good does our model actually do? We can use .score() to figure out the \\(r^2\\) value of our line on some data.\nreg.score(new_x, new_y) 0.9405617534915884 Evidently, it seems like about \\(94\\%\\) of the variation in our output data can be explained by the input features. This means that the relationship between septals are not exactly a linear pattern!\nNow you try Download the wine quality dataset Predict the quality of wine given its chemical metrics Predict if its red or white wine given its chemical metrics Vary the amount of data used to .fit the model, how does that influence the results? Vary the amount in each \u0026ldquo;class\u0026rdquo; (red wine, white wine) to fit the model, how much does that influence the results. ","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d1aft/","tags":null,"title":"AIBridgeLab D2Aft"},{"categories":null,"contents":"Woah! We talked about a lot of different ways of doing classification today! Let\u0026rsquo;s see what we can do about this for the Iris dataset!\nIris Dataset Let\u0026rsquo;s load the Iris dataset! Begin by importing the load_iris tool from sklearn. This is an easy loader scheme for the iris dataset.\nfrom sklearn.datasets import load_iris Then, we simply execute the following to load the data.\nx,y = load_iris(return_X_y=True) We use the return_X_y argument here so that, instead of dumping a large CSV, we get the neat-cleaned input and output values.\nA reminder that there is three possible flowers that we can sort by.\nDecision Trees Scikit learn has great facilities for using decision trees for classification! Let\u0026rsquo;s use some of them by fitting to the Iris dataset.\nLet us begin by importing the SciKit learn tree system:\nfrom sklearn.tree import DecisionTreeClassifier We will fit and instantiate this classifier and fit it to the data exactly!\nclf = DecisionTreeClassifier() clf = clf.fit(x,y) One cool thing about decision trees is that we can actually see what its doing! by looking at the series of splits and decisions. This is a function provided by tree too.\n# We first import the plotting utility from matplotlib import matplotlib.pyplot as plt # as well as the tree plotting tool from sklearn.tree import plot_tree # We call the tree plot tool, which puts it on teh matplotlib graph for side effects plot_tree(clf) # And we save the figure plt.savefig(\u0026#34;tree.png\u0026#34;) Cool! As you can see, by the end of the entire graph, the gini impurity of each node has been sorted to 0.\nApparently, if the third feature (pedal length) is smaller that 2.45, it is definitely the first type of flower!\nCan you explain the rest of the divisions?\nThere are some arguments available in .fit of a DecisionTreeClassifier which controls for when splitting ends; for instance, max_depth controls the maximum depth by which the tree can go.\nExtra Addition! Random Forests. If you recall, we make the initial splitting decisions fairly randomly, and simply select the one with the lowest Ginni impurity. Of course, this makes the selection of the initial sets of splits very important.\nWhat if, instead of needing to make a decision about that now, we can just deal with it later? Well, that\u0026rsquo;s where the addition of Random Forests come in.\nAs the name suggests, instead of having one great tree that does a \u0026ldquo;pretty good\u0026rdquo; job, we can have a lot of trees acting in ensemble! We can randomly start a bunch of random trees, and pick the selection that most would correspond with.\nRandom forests come from the ensemble package from sklearn; we can use it fairly simply:\nfrom sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier() Wonderful! I bet you can guess what the syntax is. Instead of fitting on the whole dataset, though, we will fit on the first 145 items.\nclf = clf.fit(x[:-5],y[:-5]) We can go ahead and run predict on some samples, just to see how it does on data it has not already seen before!\nclf.score(x[-5:], y[-5:]) 1.0 As you can see, it still does pretty well!\nSVM Let\u0026rsquo;s put another classification technique we learned today to use! Support Vector Machines. The entire syntax to manipulate support vector machines is very simple; at this point, you can probably guess it in yours sleep :)\nLet\u0026rsquo;s import a SVM:\nfrom sklearn import svm Great. Now, we will instantiate it and fit it onto the data. SVC is the support-vector machine classifier.\nclf = svm.SVC() clf.fit(x,y) Excellent, now, let\u0026rsquo;s score our predictions:\nclf.score(x,y) 0.9733333333333334 As you can see, our data is not entirely linear! Fitting our entire dataset onto a linear SVM didn\u0026rsquo;t score perfectly, which means that the model is not complex enough to support our problem.\nScikit\u0026rsquo;s support vector machine supports lots of nonlinearity function; this is set by the argument kernel. For instance, if we wanted a nonlinear, exponential function kernel (where nonlinear function \\(f(x,x\u0026rsquo;)= e^{-\\gamma||\\big\u0026lt;x,x\u0026rsquo;\\big\u0026gt;||^2}\\)), we can say:\nclf = svm.SVC(kernel=\u0026#34;rbf\u0026#34;) clf.fit(x,y) clf.score(x,y) 0.9733333333333334 Looks like our results are fairly similar, though.\nNaive Bayes One last one! Its Bayes time. Let\u0026rsquo;s first take a look at how an Naive Bayes implementation can be done via Scikit learn.\nOne of the things that the Scikit Learn Naive Bayes estimator does differently than the one that we learned via probabilities is that it assumes that\u0026mdash;instead of a uniform distribution (and therefore \u0026ldquo;chance of occurrence\u0026rdquo; is just occurrence divided by count), our samples are normally distributed. Therefore, we have that\n\\begin{equation} P(x_i | y) = \\frac{1}{\\sqrt{2\\pi{\\sigma^2}_y}}e^{\\left(-\\frac{(x_i-\\mu_y)^2}{2{\\sigma^2}_y}\\right)} \\end{equation}\nWe can instantiate such a model with the same exact syntax.\nfrom sklearn.naive_bayes import GaussianNB clf = GaussianNB() clf = clf.fit(x,y) Let\u0026rsquo;s see how it does!\nclf.score(x,y) 0.96 Same thing as before, it seems simple probabilities can\u0026rsquo;t model our relationship super well. However, this is still a fairly accurate and powerful classifier.\nNow you try! Try all three classifiers on the Wine dataset for red-white divide! Which one does better on generalizing to data you haven\u0026rsquo;t seen before? Explain the results of the decision trees trained on the Wine data by plotting it. Is there anything interesting that the tree used as a heuristic that came up? The probabilistic, uniform Naive-Bayes is fairly simple to implement write if we are using the traditional version of the Bayes theorem. Can you use Pandas to implement one yourself? ","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d3_d4/","tags":null,"title":"AIBridgeLab D3/D4"},{"categories":null,"contents":"Welcome to the Day-3 Morning Lab! We are glad for you to join us. Today, we are learning about how Pandas, a data manipulation tool, works, and working on cleaning some data of your own!\nIris Dataset We are going to lead the Iris dataset from sklearn again. This time, however, we will load the full dataset and parse it ourselves (instead of using return_X_y.)\nLet\u0026rsquo;s begin by importing the Iris dataset, as we expect.\nfrom sklearn.datasets import load_iris And, load the dataset to see what it looks like.\niris = load_iris() iris.keys() dict_keys([\u0026#39;data\u0026#39;, \u0026#39;target\u0026#39;, \u0026#39;frame\u0026#39;, \u0026#39;target_names\u0026#39;, \u0026#39;DESCR\u0026#39;, \u0026#39;feature_names\u0026#39;, \u0026#39;filename\u0026#39;, \u0026#39;data_module\u0026#39;]) We have a pretty large dictionary full of information! Let\u0026rsquo;s pull out data (our input data), target (our output data), and feature_names, the names of our feature.\niris_in = iris[\u0026#34;data\u0026#34;] iris_out = iris[\u0026#34;target\u0026#34;] iris_names = iris[\u0026#34;feature_names\u0026#34;] Data Manipulation pandas is a very helpful utility that allow us to see into data more conveniently. The object that we are usually working with, when using pandas, is called a DataFrame. We can actually create a DataFrame pretty easily. Let\u0026rsquo;s first import pandas\nimport pandas as pd Loading Data We have aliased it as pd so that its easier to type. Awesome! Let\u0026rsquo;s make a DataFrame.\ndf = pd.DataFrame(iris_in) df 0 1 2 3 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 .. ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 [150 rows x 4 columns] Nice! We have our input data contained in a data frame and nicely printed in a table; cool! However, the column names 1, 2, 3, 4 aren\u0026rsquo;t exactly the most useful labels for us. Instead, then, let\u0026rsquo;s change the column headers to:\niris_names sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) How? We can both get and set the columns via df.columns:\ndf.columns = iris_names Let\u0026rsquo;s look at the DataFrame again!\ndf sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 .. ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 [150 rows x 4 columns] Excellent! Now our data frame looks much more reasonable.\nWranging Data How do we manipulate the data around? Well, we can index this data by both columns and rows.\nIndexing by columns first is very easy. Pandas tables are, by default, \u0026ldquo;column-major\u0026rdquo;. This means that we can just index the columns just like a list!\ndf[\u0026#34;petal width (cm)\u0026#34;] 0 0.2 1 0.2 2 0.2 3 0.2 4 0.2 ... 145 2.3 146 1.9 147 2.0 148 2.3 149 1.8 Name: petal width (cm), Length: 150, dtype: float64 Nice! I want to know introduce the idea of a \u0026ldquo;cursor\u0026rdquo;. A \u0026ldquo;cursor\u0026rdquo; is used to index this high-dimensional data; think about it as the way to turn this table into something like an indexable 1-D list.\nThe simplest cursor is .loc (\u0026ldquo;locator.\u0026rdquo;)\nUnlike list indexing directly, .loc is \u0026ldquo;row-major:\u0026rdquo; the first index selects rows instead of columns.\ndf.loc[0] sepal length (cm) 5.1 sepal width (cm) 3.5 petal length (cm) 1.4 petal width (cm) 0.2 Name: 0, dtype: float64 Nice! You can see that .loc turned our table into a list, with each \u0026ldquo;sample\u0026rdquo; of the data more clearly represented by indexing it like a list.\nWhat if, then, we want to select the \u0026ldquo;pedal width\u0026rdquo; value inside this sample? We just select the first index, a comma, then select the second index.\ndf.loc[0, \u0026#34;petal width (cm)\u0026#34;] 0.2 Excellent! We can see, because we changed the header columns to be strings, we have to index them like strings.\nWhat if, instead of the first row, we want to get\u0026hellip; say, the first, fifth, and sixth rows? Unlike traditional lists, Pandas\u0026rsquo; cursors can be indexed by a list.\nSo this:\ndf.loc[0] sepal length (cm) 5.1 sepal width (cm) 3.5 petal length (cm) 1.4 petal width (cm) 0.2 Name: 0, dtype: float64 turns into\ndf.loc[[0,2,8,9]] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 2 4.7 3.2 1.3 0.2 8 4.4 2.9 1.4 0.2 9 4.9 3.1 1.5 0.1 This would give us the 0th, 2nd, 8th, and 9th row!\nThis is all good, but, it\u0026rsquo;s kind of annoying to type the column names (like \u0026ldquo;petal width (cm)\u0026rdquo;) every time! No worries, we can address this.\niloc is a variant of loc which uses integer indexes. For row indexing, the syntax remains exactly the same; iloc, however, converts all column indexes to integers sequentially. Therefore:\ndf.loc[0, \u0026#34;petal width (cm)\u0026#34;] becomes\ndf.iloc[0, 3] 0.2 Nice! Isn\u0026rsquo;t that convenient.\nSome statistics The main gist of the lab here is to manipulate the input data a little. Pandas provides many helpful utilities to help us with that. For instance, let\u0026rsquo;s take a single feature in the data, say, the pedal with:\npwidth = df[\u0026#34;petal width (cm)\u0026#34;] # same pwidth = df.iloc[:,3], where : returns everything in the row dimention pwidth 0 0.2 1 0.2 2 0.2 3 0.2 4 0.2 ... 145 2.3 146 1.9 147 2.0 148 2.3 149 1.8 Name: petal width (cm), Length: 150, dtype: float64 We can now find out how distributed this data is, to glean some info about normalization! The most basic is for us to find the mean width of the petals:\npwidth.mean() 1.1993333333333336 Awesome! We can calculate the standard by applying this constant to that entire row. The syntax works just like how you expect\u0026mdash;subtracting a scalar from the whole column just subtracts that constant from every element\u0026mdash;without any fuss:\n(((pwidth-pwidth.mean())**2).sum()/len(pwidth))**0.5 0.7596926279021594 Cool! In the scheme of things, that\u0026rsquo;s actually a pretty good. However, if it was not, we could normalize the data!\nLet\u0026rsquo;s first get the norm of the vector\npwidth_norm = sum(pwidth**2)**0.5 pwidth_norm 17.38763928772391 And, let\u0026rsquo;s normalize our vector by this norm!\npwidth_normd = pwidth/pwidth_norm pwidth_normd 0 0.011502 1 0.011502 2 0.011502 3 0.011502 4 0.011502 ... 145 0.132278 146 0.109273 147 0.115024 148 0.132278 149 0.103522 Name: petal width (cm), Length: 150, dtype: float64 Excellent. Let\u0026rsquo;s find out its standard deviation again! This time we will use .std() instead.\npwidth_normd.std() 0.04383790440709825 Much better.\nNow you try Load the wine dataset into a DataFrame and manipulate it. Feed slices back into our functions yesterday! Can you make the subsets of the data you made yesterday via the .iloc notation to make slicing easier? Can you quantify the accuracy, precision, and recall on a shuffled version of the wine dataset and logistic regression? seed=0 Is there any columns that need normalisation? Any outliers (2 std. dev away)? Why/why not? Create a balanced version of the wine dataset between red and white classes. Does fitting this normalized version into our model makes training results better? ","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d2aft/","tags":null,"title":"AIBridgeLab D3Morning"},{"categories":null,"contents":"Let\u0026rsquo;s run some clustering algorithms! We are still going to use the Iris data, because we are super familiar with it already. Loading it works the exactly in the same way; I will not repeat the notes but just copy the code and description from before here for your reference\nIris Dataset Let\u0026rsquo;s load the Iris dataset! Begin by importing the load_iris tool from sklearn. This is an easy loader scheme for the iris dataset.\nfrom sklearn.datasets import load_iris Then, we simply execute the following to load the data.\nx,y = load_iris(return_X_y=True) We use the return_X_y argument here so that, instead of dumping a large CSV, we get the neat-cleaned input and output values.\nk-means clustering The basics of k-means clustering works exactly the same as before, except this time we have to specify and get a few more parameters. Let\u0026rsquo;s begin by importing k-means and getting some clusters together!\nfrom sklearn.cluster import KMeans Let\u0026rsquo;s instantiate the KMeans cluster with 3 clusters, which is the number of classes there is.\nkmeans = KMeans(n_clusters=3) kmeans = kmeans.fit(x) Great! Let\u0026rsquo;s take a look at how it sorted all of our samples\nkmeans.labels_ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2 2 0 Let\u0026rsquo;s plot our results.\nimport matplotlib.pyplot as plt We then need to define some colours.\ncolors=[\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;blue\u0026#34;] Recall from yesterday that we realized that inner Septal/Pedal differences are not as variable as intra Septal/Pedal differences. So, we will plot the first and third columns next to each other, and use labels_ for coloring.\n# for each element for indx, element in enumerate(x): # add a scatter point plt.scatter(element[0], element[1], color=colors[kmeans.labels_[indx]]) # save our figure plt.savefig(\u0026#34;scatter.png\u0026#34;) Nice. These look like the main groups are captured!\nLet\u0026rsquo;s compare that to intended classes\ny 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 There are obviously some clustering mistakes. Woah! Without prompting with answers, our model was able to figure out much of the general clusters at which our data exists. Nice.\nWe can also see the \u0026ldquo;average\u0026rdquo;/\u0026ldquo;center\u0026rdquo; for each of the clusters:\nkmeans.cluster_centers_ 5.9016129 2.7483871 4.39354839 1.43387097 5.006 3.428 1.462 0.246 6.85 3.07368421 5.74210526 2.07105263 Nice! These are what our model thinks are the centers of each group.\nPrinciple Component Analysis Let\u0026rsquo;s try reducing the dimentionality of our data by one, so that we only have three dimensions. We do this, by, again, begin importing PCA.\nfrom sklearn.decomposition import PCA When we are instantiating, we need to create a PCA instance with a keyword n_components, which is the number of dimensions (\u0026ldquo;component vectors\u0026rdquo;) we want to keep.\npca = PCA(n_components=3) Great, let\u0026rsquo;s fit our data to this PCA.\npca.fit(x) Wonderful. singular_values_ is how we can get out of the PCA\u0026rsquo;d change of basis results:\ncob = pca.components_ cob 0.36138659 -0.08452251 0.85667061 0.3582892 0.65658877 0.73016143 -0.17337266 -0.07548102 -0.58202985 0.59791083 0.07623608 0.54583143 So, we can then take a change of basis matrix and apply it to some samples!\ncob@(x[0]) 2.81823951 5.64634982 -0.65976754 What\u0026rsquo;s @? Well\u0026hellip; Unfortunately, Python has different operator for matrix-operations (\u0026ldquo;dot\u0026rdquo;); otherwise, it will perform element-wise operations.\nWe can actually also see the \\(R^2\\) values on each of the axis: the variance explained by each of the dimensions.\npca.explained_variance_ 4.22824171 0.24267075 0.0782095 Nice! As you can see, much of the variance is contained in our first dimension here.\n","permalink":"https://www.jemoka.com/posts/kbhaibridgelab_d4aft/","tags":null,"title":"AIBridgeLab D4Aft"},{"categories":null,"contents":"AIFS is a food systems institute at UC Davis.\n","permalink":"https://www.jemoka.com/posts/kbhaifs/","tags":null,"title":"AIFS"},{"categories":null,"contents":"I am honestly not entirely sure why or what state of mind I was in circa 2017 to write, edit, and act! in this video, but I did.\nThis is an adaption of a Greek-Style story which someone else wrote, I don\u0026rsquo;t know who.\nVideo produced mostly by myself in front of a green screen, with help from my lovely mother as well as a very nice teacher named Joseph O\u0026rsquo;Brian.\nhttps://youtu.be/b1YxOkcwtgw\nBe prepared. 前方高能\n","permalink":"https://www.jemoka.com/posts/kbhair_a_greek_style_myth/","tags":null,"title":"Air: A Greek Style Myth"},{"categories":null,"contents":"algebra is the study of\u0026hellip;\nsymbols/variables transformations/operations: \u0026ldquo;add\u0026rdquo;, \u0026ldquo;multiply\u0026rdquo; simple functions abstraction substitution ","permalink":"https://www.jemoka.com/posts/kbhalgebra/","tags":null,"title":"algebra"},{"categories":null,"contents":"Begin with a new installation of MFA, and head to the directory. First run validate with the original dictionary.\nmfa validate ~/Downloads/tb/my_corpus english_us_arpa english_us_arpa We see that there is in deed an section of corpus that is out-of-vocab.\nINFO - 11 OOV word types INFO - 18 total OOV tokens Therefore, we will generate a new dictionary based on the existing dictionary of english_us_arpa.\nFirst download the english_us_arpa model\nmfa model download g2p english_us_arpa Then, perform the actual dictionary generation:\nmfa g2p english_us_arpa ~/Downloads/tb/my_corpus ~/Downloads/tb/my_corpus/new_dict.txt There is a chance this command fails with\nThere was an issue importing Pynini, please ensure that it is installed. If you are on Windows, please use the Windows Subsystem for Linux to use g2p functionality. If so, install pynini\nconda add pynini Finally, run the mfa g2p command above to generate pronunciations.\nYou should end up with a file named new_dict.txt, which should include missing words.\nFinally, perform alignment with this new dictionary.\nmfa align ~/Downloads/tb/my_corpus ~/Downloads/tb/my_corpus/new_dict.txt english_us_arpa ~/Downloads/tb/my_corpus_output Notice here the second argument of mfa align is no longer english_us_arpa, our base dictionary. Instead, it is our custom dictionary.\n","permalink":"https://www.jemoka.com/posts/kbhalign_with_new_vocab/","tags":null,"title":"Align with New Vocab"},{"categories":null,"contents":" Want to interview more severe ashma Want to find someone younger Difference between marketing and purchaser.\nTaking to people Spoke with Matt. Talked with more details with prototyping and how they can build a unique product.\nHave not gotten back to him yet.\n","permalink":"https://www.jemoka.com/posts/kbhalivio_april_checkin/","tags":null,"title":"Alivio April Checkin"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhanatomy/","tags":null,"title":"anatomy"},{"categories":null,"contents":"anatomy learning is the learning of anatomy.\nAnatomy information acquired prior to medical school has a positive correlation in medical school outcomes. Also leveraging anatomy information.\n","permalink":"https://www.jemoka.com/posts/kbhanatomy_learning/","tags":null,"title":"anatomy learning"},{"categories":null,"contents":"Angelman Syndrome is a syndrome is ~1 in 15000, clinically recognizable, developmental delay syndrome.\ncause of Angelman Syndrome Angelman Syndrome is primarily caused by the UBE3A and the ubiquitin proteasome system. Poly-ubiquitin chain asks to discard cells.\n","permalink":"https://www.jemoka.com/posts/kbhangelman_syndrome/","tags":null,"title":"Angelman Syndrome"},{"categories":null,"contents":"Need-finding conversation Main idea: testing?\u0026mdash;pregnancy testing and COVID testing\ntalking to longer-scope challenges in visually impaired community Navigation; transportation Cannot see markers on smaller steps; trying to find an uber drive and cannot reorient ","permalink":"https://www.jemoka.com/posts/kbhanna_s_team_checkin/","tags":null,"title":"Anna's Team Checkin"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhanotehuaoeu/","tags":null,"title":"anotehuaoeu"},{"categories":null,"contents":"Anoushka is a student at Nueva, also the host of Project80, among other things.\n","permalink":"https://www.jemoka.com/posts/kbhanoushka_krishnan/","tags":null,"title":"Anoushka Krishnan"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhanthony_badger/","tags":null,"title":"Anthony Badger"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2020.607449\nOne-Liner oral lexical retrieval works better than qualitative narrative analysis to classify dementia; and semantic fluency + Disfluency features chucked on an SVM returns pretty good results.\nNovelty Tried two different assays of measuring linguistic ability: oral lexical retrieval metrics, and qualitative discourse features analysis of speech.\nNotable Methods Subjects divided into three groups\nGreat cog. decline Impaired but stable Healthy controls Administered BNT and SVF tests as baseline\nKey Figs Table 3 This figure tells us that the percentages of unrelated utterances was a statistically significant metric to figure differences between the three experimental groups.\n(CD, CS, HC: cognitive decline, cognitively stable (but declining normally), healthy control)\n(no other items are bolded)\nTable 4 This figure tells us the disfluency features analyzed. None of them were independently statistically significant.\nTable 5 This figure tells us that analyzing Semantic Verbal Fluency, plus the information of disfluency, trained on an SVM, actually shows \u0026gt;90% recall value?\nNew Concepts Discourse-Completion Task oral lexical retrieval discourse features modalization Semantic Verbal Fluency Boston Naming Test ","permalink":"https://www.jemoka.com/posts/kbhantonsson_2021/","tags":["ntj"],"title":"Antonsson 2021"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhany_name_here/","tags":null,"title":"any name here"},{"categories":null,"contents":"eansoetuhaosneu\n","permalink":"https://www.jemoka.com/posts/kbhaosneuhasoneuh/","tags":null,"title":"aosneuhasoneuh"},{"categories":null,"contents":"AP Phys C Mech is an examination held by the CollegeBoard in mechanics.\nThings to Study Permittivity of free space Impulse Springs! In general. Perhaps review old notes. How to be faster? Kepler\u0026rsquo;s Laws of Planetary Motion\n","permalink":"https://www.jemoka.com/posts/kbhap_phys_c_mech_index/","tags":["index"],"title":"AP Phys C Mech Index"},{"categories":null,"contents":"AP Statistics is an examination by the CollegeBoard.\nSee also crap to remember for AP Stats\nNon-Focus Mistakes file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf Interpretation of regression outputs Backlog Chi-square file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf file:///Users/houliu/Documents/School Work/The Bible/APStats/APStats5Steps.pdf Notes confidence interval hypothesis testing t-statistics chi-square data inference binomial distribution ","permalink":"https://www.jemoka.com/posts/kbhapstats/","tags":["index"],"title":"AP Statistics Index"},{"categories":null,"contents":"Show that:\n\\begin{equation} \\dv t e^{tA} = e^{tA}A \\end{equation}\nWe can apply the result we shown in eigenvalue:\n\\begin{equation} \\dv t \\qty(e^{tA}) = \\dv t \\qty(I + \\sum_{k=1}^{\\infty} \\frac{t^{k}}{k!}A^{k}) = \\qty(\\sum_{k=1}^{\\infty }\\frac{1}{k!}kt^{k-1}A^{k-1})A \\end{equation}\nWe do this separation because \\(k=0\\) would\u0026rsquo;t make sense to raise \\(A\\) (\\(k-1=-1\\)) to as we are unsure about the invertability of \\(A\\). Obviously \\(\\frac{1}{k!}k = \\frac{1}{(k-1)!}\\). Therefore, we can shift our index back yet again:\n\\begin{equation} \\qty(\\sum_{k=1}^{\\infty }\\frac{1}{k!}kt^{k-1}A^{k-1})A = \\qty(\\sum_{j=0}^{\\infty }\\frac{1}{j!}t^{j}A^{j})A \\end{equation}\nAwesome. So now we have the taylor series in \\(e^{tA}\\) back, times \\(A\\).\nSo therefore:\n\\begin{equation} \\qty(\\sum_{j=0}^{\\infty }\\frac{1}{j!}t^{j}A^{j})A = e^{tA}A \\end{equation}\nBe forewarned:\n\\begin{equation} e^{A}e^{B} \\neq e^{A+B} \\end{equation}\nmostly because matrix multiplication is not commutative..\n","permalink":"https://www.jemoka.com/posts/kbhapplying_eigenspace/","tags":null,"title":"applying eigenspace"},{"categories":null,"contents":"If we take entangled qubits, and separate them real far away, their behavior would be the same even despite it will take longer for light to travel.\n","permalink":"https://www.jemoka.com/posts/kbhapr_paradox/","tags":null,"title":"APR Paradox"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhaps/","tags":null,"title":"APS"},{"categories":null,"contents":"Background In the 60s, economists that the pricing of options were independent of pricing of underlying assets. Nowadays, we can see that, if the underlying assets were obeying of a Brownian Motion, there is no additional degree of freedom that options can bring: that knowing the stocks will tell you exactly through a DiffEQ how the option will evolve.\nThe idea, then, is that you can replicate options: by dynamically buying and selling pairs of securities in the same way as the option, your new portfolio can track the option exactly.\nOf course, there is a certain amount of volatility associated with Brownian Motion markets.\nUnfortunately, there is no one fixed volatility which can be used to model all options; you can fit a volatility given all strike prices\u0026mdash;creating an implied volatility surface.\nOtherwise, you can also model volatility as a random variable, a stochastic process modeled by stochastic volatility.\nReading pg 350-352: diffusion are described by stochastic differential equations Option Pricing A Vanilla Call Given some current price \\(S\\), option price \\(K\\), time to maturity \\(T\\); the payoff increases linearly after the option matures. How much should the option be changed for the right to buy the option after \\(T\\) days?\nWe can use the option info to calculate the implied volatility.\n","permalink":"https://www.jemoka.com/posts/kbharbitrage_pricing/","tags":null,"title":"Arbitrage Pricing"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbharthur_m_schlesinger/","tags":null,"title":"Arthur M. Schlesinger"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhasbmb/","tags":null,"title":"ASBMB"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhasee_prism/","tags":null,"title":"ASEE Prism"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhasip/","tags":null,"title":"ASIP"},{"categories":null,"contents":"ASR are tech that helps make transcripts from speech\n","permalink":"https://www.jemoka.com/posts/kbhasr/","tags":null,"title":"ASR"},{"categories":null,"contents":"associative means that operations can be grouped in any way as long as order is preserved.\nThat is:\n\\begin{equation} (AB)C = A(BC) \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhassociative/","tags":null,"title":"associative"},{"categories":null,"contents":"~ Given functions \\(f(n)\\) and \\(g(n)\\), if:\n\\begin{equation} \\lim_{n\\to \\infty} \\left(\\frac{f(n)}{g(n)}\\right) = 1 \\end{equation}\nwe say that \\(f \\sim g\\).\nThat \u0026ndash; the relationship between \\(f\\) and \\(g\\) grows in a similar fashion as \\(n\\) increases. For instance:\n\\(f(n) = n+1\\) \\(g(n) = n+2\\) Therefore:\n\\begin{equation} f\\sim g = \\lim_{n\\to \\infty} \\frac{f(n)}{g(n)} = \\lim_{n\\to \\infty} \\frac{n+1}{n+2} = 1 \\end{equation}\nThe \\(\\sim\\) operator is commutative (\\(f \\sim g \\Rightarrow g\\sim f\\)) and transitive (\\(f\\sim g, g\\sim h \\Rightarrow f \\sim h\\)).\no(n) Given two functions \\(f(n)\\), \\(g(n)\\), if their relationship shows:\n\\begin{equation} \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0 \\end{equation}\nwe can write it as\n\\begin{equation} f = o(g) \\end{equation}\nThis tells us that if \\(n\\) becomes very large, \\(g\\) becomes much larger than \\(f\\). \\(f\\) does not grow nearly as fast as \\(g\\).\nThe operation is not commutative, but is transitive (\\(f = o(g), g = o(h) \\Rightarrow f = o(h)\\))\nO(n) Given two functions \\(f(n)\\), \\(g(n)\\).\n\\begin{equation} \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} \u0026lt; \\infty \\end{equation}\nthat the relationship between \\(f(n)\\) and \\(g(n)\\) is countable as \\(n\\) trends to infinity.\nWe can also say that, given \\(n\\), \\(n_0\\), and some \\(c\\) which \\(\\forall n, n \u0026gt; n_0\\), there is:\n\\begin{equation} |f(n)| \u0026lt; |cg(n)| \\end{equation}\nThis tells us that \\(f(n)\\) does not grow much much faster than \\(g(n)\\).\nTherefore:\nIf \\(f \\sim g\\), \\(f = O(g)\\) (as they grow together, \\(f\\) is not much faster) If \\(f = o(g)\\), \\(f=O(g)\\) (as \\(f\\) does not grow at all, \\(f\\) is not faster) \\(\\theta\\)(n) \\(f=\\theta(g)\\) IFF \\(f=O(g)\\) and \\(g=O(f)\\), its essentially \\(\\sim\\) but without the strict requirement of a 1:1 ratio.\n\\(\\omega\\)(n) and \\(\\Omega\\)(n) The inverses of \\(O\\) and \\(o\\):\n\\(f(n) = O(g(n)) \\Rightarrow g(n) = \\omega(f(n))\\) \\(f(n) = o(g(n)) \\Rightarrow g(n) = \\Omega(f(n))\\) ","permalink":"https://www.jemoka.com/posts/kbhasymtotic_analysis/","tags":null,"title":"asymtotic analysis"},{"categories":null,"contents":"You can use atoms as many different types of qubits.\nmanipulating physical qubits To make physical qubits go to different states, we will again use something in the ancillary states. Rotating it to \\(z\\) \u0026mdash; leverage one lazer to make it fall; \\(rx\\), \\(ry\\), we leverage combinations of two light.\nvarious qubit implementations Implementations of physical qubits\nType Superconductor Ions Atoms Company Google, IBM, Rigetti IonQ, Honeywell Atom Computing, QuEra Nature Artifical Natural Natural Calibration Individual calibration Naturally calibrated Naturally calibrated Coherence Time Short Long Long Connectivity Adjacent connectivity All-to-all More than adjacent Scalability Compatible with existing tech Not easily scalable Potentially scalable Speed Fast gates Kinda fast Untested possible uses for qubits Here are some possible uses for physical qubits\nTraveling salesman Research + simulations Cryptography ","permalink":"https://www.jemoka.com/posts/kbhatoms_as_qubits/","tags":null,"title":"atoms as qubits"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhauthoritarianism/","tags":null,"title":"Authoritarianism"},{"categories":null,"contents":"autism is a spectrum disorder that are caused by both environmental and genetic factors.\nKey Question: how can different chromatin regulators lead to the same set of symptoms named \u0026ldquo;autism\u0026rdquo;.\nautism gene signature The gene signature of autism can be measured in clean and quantitative assays.\n","permalink":"https://www.jemoka.com/posts/kbhautism/","tags":null,"title":"autism"},{"categories":null,"contents":"Key sequence In this chapter, we defined complex numbers, their definition, their closeness under addition and multiplication, and their properties These properties make them a field: namely, they have, associativity, commutativity, identities, inverses, and distribution. notably, they are different from a group by having 1) two operations 2) additionally, commutativity and distributivity. We then defined \\(\\mathbb{F}^n\\), defined addition, additive inverse, and zero. These combined (with some algebra) shows that \\(\\mathbb{F}^n\\) under addition is a commutative group. Lastly, we show that there is this magical thing called scalar multiplication in \\(\\mathbb{F}^n\\) and that its associative, distributive, and has an identity. Technically scalar multiplication in \\(\\mathbb{F}^n\\) commutes too but extremely wonkily so we don\u0026rsquo;t really think about it. New Definitions complex number addition and multiplication of complex numbers subtraction and division of complex numbers field: \\(\\mathbb{F}\\) is \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) power list \\(\\mathbb{F}^n\\): F^n coordinate addition in \\(\\mathbb{F}^n\\) additive inverse of \\(\\mathbb{F}^n\\) \\(0\\): zero scalar multiplication in \\(\\mathbb{F}^n\\) Results and Their Proofs properties of complex arithmetic commutativity associativity identities additive inverse multiplicative inverse distributive property properties of \\(\\mathbb{F}^n\\) addition in \\(\\mathbb{F}^n\\) is associative addition in \\(\\mathbb{F}^n\\) is commutative addition in \\(\\mathbb{F}^n\\) has an identity (zero) addition in \\(\\mathbb{F}^n\\) has an inverse scalar multiplication in \\(\\mathbb{F}^n\\) is associative scalar multiplication in \\(\\mathbb{F}^n\\) has an identity (one) scalar multiplication in \\(\\mathbb{F}^n\\) is distributive Question for Jana No demonstration in exercises or book that scalar multiplication is commutative, why? Interesting Factoids You can take a field, look at an operation, and take that (minus the other op\u0026rsquo;s identity), and call it a group (groups (vector spaces (fields ))) ","permalink":"https://www.jemoka.com/posts/kbhaxler_a/","tags":null,"title":"Axler 1.A"},{"categories":null,"contents":"Key Sequence \\(\\mathbb{F}^{n}\\) not being a field kinda sucks, so we made an object called a \u0026ldquo;vector space\u0026rdquo; which essentially does everything a field does except without necessitating a multiplicative inverse Formally, a vector space is closed over addition and have a scalar multiplication. Its addition is commutative, both addition and scalar multiplication is associative, and distributivity holds. There is an additive identity, additive inverse, and multiplicative identity. We defined something called \\(\\mathbb{F}^{S}\\), which is the set of functions from a set \\(S\\) to \\(\\mathbb{F}\\). Turns out, \\(\\mathbb{F}^{S}\\) is a Vector Space Over \\(\\mathbb{F}\\) and we can secretly treat \\(\\mathbb{F}^{n}\\) and \\(\\mathbb{F}^{\\infty}\\) as special cases of \\(\\mathbb{F}^{s}\\). We established that identity and inverse are unique additively in vector spaces. Lastly, we proved some expressions we already know: \\(0v=0\\), \\(-1v=-v\\). New Definitions addition and scalar multiplication vector space and vectors vector space \u0026ldquo;over\u0026rdquo; fields \\(V\\) denotes a vector space over \\(\\mathbb{F}\\) \\(-v\\) is defined as the additive inverse of \\(v \\in V\\) Results and Their Proofs \\(\\mathbb{F}^{\\infty}\\) is a Vector Space over \\(\\mathbb{F}\\) \\(\\mathbb{F}^{S}\\) is a Vector Space Over \\(\\mathbb{F}\\) All vector spaces \\(\\mathbb{F}^{n}\\) and \\(\\mathbb{F}^{\\infty}\\) are just special cases \\(\\mathbb{F}^{S}\\): you can think about those as a mapping from coordinates \\((1,2,3, \\dots )\\) to their actual values in the \u0026ldquo;vector\u0026rdquo; additive identity is unique in a vector space additive inverse is unique in a vector space \\(0v=0\\), both ways (for zero scalars and vectors) \\(-1v=-v\\) Questions for Jana The way Axler presented the idea of \u0026ldquo;over\u0026rdquo; is a tad weird; is it really only scalar multiplication which hinders vector spaces without \\(\\mathbb{F}\\)? In other words, do the sets that form vector spaces, apart from the \\(\\lambda\\) used for scalar multiplication, need anything to do with the \\(\\mathbb{F}\\) they are \u0026ldquo;over\u0026rdquo;? The name of the field and what its over do not have to be the same\u0026mdash;\u0026ldquo;vector space \\(\\mathbb{C}^2\\) over \\(\\{0,1\\}\\)\u0026rdquo; is a perfectly valid statement If lists have finite length \\(n\\), then what are the elements of \\(\\mathbb{F}^{\\infty}\\) called? \u0026ldquo;we could think about \\(\\mathbb{F}^{\\infty}\\), but we aren\u0026rsquo;t gonna.\u0026rdquo; Why is \\(1v=v\\) an axiom, whereas we say that some \\(0\\) exists? because we know 1 already, and you can follow the behavor of scalar multiplication what\u0026rsquo;s that thing called again in proofs where you just steal the property of a constituent element?: inherits Interesting Factoids The simplest vector space is \\(\\{0\\}\\) ","permalink":"https://www.jemoka.com/posts/kbhaxler_1_b/","tags":null,"title":"Axler 1.B"},{"categories":null,"contents":"Key Sequence we defined subspace and how to check for them we want to operate on subsets, so we defined the sum of subsets we saw that the sum of subspaces are the smallest containing subspace and finally, we defined direct sums and how to prove them New Definitions subspace sum of subsets direct sum Results and Their Proofs checking for subspaces simplified check for subspace sum of subspaces is the smallest subspace with both subspaces creating direct sums a sum of subsets is a direct sum IFF there is only one way to write \\(0\\) a sum of subsets is only a direct sum IFF their intersection is the set containing \\(0\\) Questions for Jana Does the additive identity have be the same between different subspaces of the same vector space? yes, otherwise the larger vector space has two additive identities. Does the addition and multiplication operations in a subspace have to be the same as its constituent vector space? by definition Why are direct sums defined on sub-spaces and not sum of subsets? because the union is usually not a subspace so we use sums and keep it in subspaces ","permalink":"https://www.jemoka.com/posts/kbhaxler_1_c/","tags":null,"title":"Axler 1.C"},{"categories":null,"contents":"3: Show that the set of differential real-valued functions \\(f\\) on the interval \\((-4,4)\\) such that \\(f\u0026rsquo;(-1)=3f(2)\\) is a subspace of \\(\\mathbb{R}^{(-4,4)}\\)\n4: Suppose \\(b \\in R\\). Show that the set of continuous real-valued functions \\(f\\) on the interval \\([0,1]\\) such that \\(\\int_{0}^{1}f=b\\) is a subspace of \\(\\mathbb{R}^{[0,1]}\\) IFF \\(b=0\\)\nAdditive Identity:\nassume \\(\\int_{0}^{1}f=b\\) is a subspace\n","permalink":"https://www.jemoka.com/posts/kbhaxler_1_c_excercises/","tags":null,"title":"Axler 1.C Exercises"},{"categories":null,"contents":"Key Sequence we defined the combination of a list of vectors as a linear combination and defined set of all linear combination of vectors to be called a span we defined the idea of a finite-dimensional vector space vis a vi spanning we took a god-forsaken divergence into polynomials that will surely not come back and bite us in chapter 4 we defined linear independence + linear dependence and, from those definition, proved the actual usecase of these concepts which is the Linear Dependence Lemma we apply the Linear Dependence Lemma to show that length of linearly-independent list \\(\\leq\\) length of spanning list as well as that finite-dimensional vector spaces make finite subspaces. Both of these proofs work by making linearly independent lists\u0026mdash;the former by taking a spanning list and making it smaller and smaller, and the latter by taking a linearly independent list and making it bigger and bigger New Definitions linear combination span + \u0026ldquo;spans\u0026rdquo; finite-dimensional vector space infinite-demensional vector space finite-dimensional subspaces polynomial \\(\\mathcal{P}(\\mathbb{F})\\) \\(\\mathcal{P}_{m}(\\mathbb{F})\\) degree of a polynomial \\(\\deg p\\) linear independence and linear dependence Linear Dependence Lemma Results and Their Proofs span is the smallest subspace containing all vectors in the list \\(\\mathcal{P}(\\mathbb{F})\\) is a vector space over \\(\\mathbb{F}\\) the world famous Linear Dependence Lemma and its fun issue length of linearly-independent list \\(\\leq\\) length of spanning list subspaces of inite-dimensional vector spaces is finite dimensional Questions for Jana obviously polynomials are non-linear structures; under what conditions make them nice to work with in linear algebra? what is the \u0026ldquo;obvious way\u0026rdquo; to change Linear Dependence Lemma\u0026rsquo;s part \\(b\\) to make \\(v_1=0\\) work? for the finite-dimensional subspaces proof, though we know that the process terminates, how do we know that it terminates at a spanning list of \\(U\\) and not just a linearly independent list in \\(U\\)? direct sum and linear independence related; how exactly? Interesting Factoids I just ate an entire Chinese new-year worth of food while typing this up. That\u0026rsquo;s worth something right\n","permalink":"https://www.jemoka.com/posts/kbhaxler_2_a/","tags":null,"title":"Axler 2.A"},{"categories":null,"contents":"Key Sequence we defined basis of a vector space\u0026mdash;a linearly independent spanning list of that vector space\u0026mdash;and shown that to be a basis one has to be able to write a write an unique spanning list we show that you can chop a spanning list of a space down to a basis or build a linearly independent list up to a basis because of this, you can make a spanning list of finite-dimensional vector spaces and chop it down to a basis: so every finite-dimensional vector space has a basis lastly, we can use the fact that you can grow list to basis to show that every subspace of \\(V\\) is a part of a direct sum equaling to \\(V\\) New Definitions basis and criteria for basis\nI mean its a chapter on bases not sure what you are expecting.\nResults and Their Proofs a list is a basis if you can write every memeber of their span uniquely every finite-dimensional vector space has a basis dualing basis constructions all spanning lists contains a basis of which you are spanning a linearly independent list expends to a basis every subspace of \\(V\\) is a part of a direct sum equaling to \\(V\\) Questions for Jana Is the subspace direct sum proof a unique relationship? That is, is every complement \\(W\\) for each \\(U \\subset V\\) unique? ","permalink":"https://www.jemoka.com/posts/kbhaxler_2_b/","tags":null,"title":"Axler 2.B"},{"categories":null,"contents":"Key Sequence Because Length of Basis Doesn\u0026rsquo;t Depend on Basis, we defined dimension as the same, shared length of basis in a vector space We shown that lists of the right length (i.e. dim that space) that is either spanning or linearly independent must be a basis\u0026mdash;\u0026ldquo;half is good enough\u0026rdquo; theorems we also shown that \\(dim(U_1+U_2) = dim(U_1)+dim(U_2) - dim(U_1 \\cap U_2)\\): dimension of sums New Definitions dimension Results and Their Proofs Length of Basis Doesn\u0026rsquo;t Depend on Basis lists of right length are basis linearly independent list of length dim V are a basis of V spanning list of length of dim V are a basis of V dimension of sums Questions for Jana Example 2.41: why is it that \\(\\dim U \\neq 4\\)? We only know that \\(\\dim \\mathcal{P}_{3}(\\mathbb{R}) = 4\\), and \\(\\dim U \\leq 4\\). Is it because \\(U\\) (i.e. basis of \\(U\\) doesn\u0026rsquo;t span the polynomial) is strictly a subset of \\(\\mathcal{P}_{3}(\\mathbb{R})\\), so there must be some extension needed? Interesting Factoids ","permalink":"https://www.jemoka.com/posts/kbhaxler_2_c/","tags":null,"title":"Axler 2.C"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.635945\nOne-Liner extracted lexicographic and syntactical features from ADReSS Challenge data and trained it on various models, with BERT performing the best.\nNovelty ???????\nSeems like results here are a strict subset of Zhu 2021. Same sets of dataprep of Antonsson 2021 but trained on a BERT now. Seem to do worse than Antonsson 2021 too.\nNotable Methods Essentially Antonsson 2021\nAlso performed MMSE score regression. Key Figs Table 7 training result This figure shows us that the results attained by training on extracted feature is past the state-of-the-art at the time.\nTable 4 These tables tells us the feature extracted\n","permalink":"https://www.jemoka.com/posts/kbhbalagopalan_2021/","tags":["ntj"],"title":"Balagopalan 2021"},{"categories":null,"contents":"A basis is a list of vectors in \\(V\\) that spans \\(V\\) and is linearly independent\nconstituents a LIST! of vectors in vector space \\(V\\) requirements the list is\u0026hellip; linear independent spans \\(V\\) additional information criteria for basis A list \\(v_1, \\dots v_{n}\\) of vectors in \\(V\\) is a basis of \\(V\\) IFF every \\(v \\in V\\) can be written uniquely as:\n\\begin{equation} v = a_1v_1+ \\dots + a_{n}v_{n} \\end{equation}\nwhere \\(a_1, \\dots, a_{n} \\in \\mathbb{F}\\).\nforward direction Suppose we have \\(v_1, \\dots, v_{n}\\) as the basis in \\(V\\). We desire that \\(v_1, \\dots v_{n}\\) uniquely constructs each \\(v \\in V\\).\nBy definition, they span \\(V\\) and are linear independent in \\(V\\).\nBecause of the spanning quality, there exists at least one set of \\(a_1, \\dots, a_{n} \\in \\mathbb{F}\\) such that we can write:\n\\begin{equation} v \\in V = a_1v_1+ \\dots + a_{n}v_{n} \\end{equation}\nSuppose now that we have another representation of \\(v\\) via scalars \\(c_1, \\dots, c_{n}\\) and our same list of vectors:\n\\begin{equation} v \\in V =^{?} c_1v_1+ \\dots + c_{n}v_{n} \\end{equation}\nSubtracting the two expressions, we have that:\n\\begin{equation} 0 = (a_1-c_1)v_1 + \\dots +(a_{n}-c_{n}) v_{n} \\end{equation}\nBy definition that \\(v_1 \\dots v_{n}\\) is linearly independent, we have that \\(a_j-c_j=0 \\implies a_{j}=c_{j}\\). Therefore, there is only one unique representation for \\(v\\) as a linear combination of vectors \\(v_1, \\dots v_{n}\\).\n(to be honest, we could have just applied that as the definition of linear independence that the scalars in a linear combo of linearly independent list is unique but this is the more careful definition.)\nbackward direction Suppose we have a list \\(v_1, \\dots v_{n}\\) which uniquely constructs each \\(v \\in V\\). We desire that \\(v_1, \\dots v_{n}\\) is a basis in \\(V\\). Given a linear combination thereof can construct all \\(v \\in V\\), we can say that \\(v_1, \\dots v_{n}\\) spans \\(V\\).\nAs \\(V\\) is a vector space, we have \\(0 \\in V\\). Therefore, there exists some scalars \\(a_1, \\dots a_{n}\\) for which:\n\\begin{equation} 0 = a_1v_1 + \\dots +a_{n}v_{n} \\end{equation}\n(as we already established \\(v_1, \\dots, v_{n}\\) spans \\(V\\) and \\(0 \\in V\\))\nOf course, we are given that \\(v_1, \\dots v_{n}\\) uniquely constructs each \\(v \\in V\\). As the trivial solution does exist: that \\(a_1 = \\dots = a_{n} = 0\\), it is the only solution.\nBy definition of linear independence, then, \\(v_1, \\dots v_{n}\\) is linearly independent. Having constructed that \\(v_1, \\dots v_{n}\\) is both a spanning set in \\(V\\) and are linearly independent, we have that they are a basis of \\(V\\). \\(\\blacksquare\\)\nDualing Basis Construction These are two results that says: \u0026ldquo;you can build up a linearly independent list to a basis or you can pluck away a spanning list to a basis\u0026rdquo;.\nall spanning lists contains a basis of which you are spanning Every spanning list in \\(V\\) contains the basis (and possibly some more) in \\(V\\).\nRead: \u0026ldquo;apply Linear Dependence Lemma your way to success\u0026rdquo;.\nBegin with a spanning list \\(v_1, \\dots v_{m}\\) of \\(V\\). We run a for loop for the list.\nStep 0:\nIf \\(v_1=0\\) (i.e. \\(v_1 \\in span(\\{\\})\\)), delete \\(v_1\\). Otherwise, do nothing.\nStep \\(j\\):\nIf \\(v_{j}\\) is in \\(span(v_1, \\dots v_{j-1})\\), \\(v_{j}\\) satisfies the Linear Dependence Lemma\u0026rsquo;s first condition, and therefore naturally satisfies the second condition (removal from list keeps the same span because \\(v_{j}\\) can just be rewritten from \\(v_1, \\dots v_{j-1}\\)).\nSo we remove \\(v_{j}\\) if it is indeed in the span of the previous vectors. By the Linear Dependence Lemma, the new list spans the same space the old list.\nConclusion\nBy the end of this process, no vectors left in the list will satisfy the Linear Dependence Lemma (read: we got rid of all of them.) Therefore, the list is linearly independent. However, every step of the way the Linear Dependence Lemma ensures that the new list spans the same space; therefore, the new list still spans \\(V\\). Having constructed a linearly independent list that spans \\(V\\), we declare the new list as a basis of \\(V\\).\nAs all we did was pluck vectors out of the old list, the new list is a sublist of the old list. This means that the spanning list (old list) contains the new list, which is a basis. \\(\\blacksquare\\)\na linearly independent list expends to a basis Every linearly independent list of vectors in finite-dimensional vector spaces can be extended to a basis.\nRecall first that every finite-dimensional vector space has a basis.\nLet\u0026rsquo;s begin with a linearly independent list in \\(V\\) \\(u_1, \\dots u_{m}\\). Let\u0026rsquo;s recruit also a basis of \\(V\\): \\(w_{1}, \\dots w_{m}\\).\nNaturally: \\(u_1, \\dots u_{m}, w_1, \\dots w_{m}\\) spans \\(V\\) (as the \\(w\\) vectors already span \\(V\\)). We will now apply the fact that all spanning lists contains a basis of which you are spanning (the order of \\(u\\) vectors first and \\(w\\) vectors second ensuring that you try to remove the \\(w\\), and, as \\(u\\) are linearly independent, none of them will be removed) to get back a basis in \\(V\\) consisting of all \\(u\\) and some \\(w\\). \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhbasis/","tags":null,"title":"basis"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhbatchalign/","tags":null,"title":"batchalign"},{"categories":null,"contents":"Things to include Rev How to handle interspersed results Utterance segmentation Why --prealigned and the overall performance of MFA Beginning/End Bullet and why we throw away Rev\u0026rsquo;s output fixbullets and manual utterance segmentation \u0026amp;*INV= interspersed comments ","permalink":"https://www.jemoka.com/posts/kbhbatchalign_paper_outline/","tags":null,"title":"Batchalign Paper Outline"},{"categories":null,"contents":"Bending is what happens when you apply a transverse load to an object and it goes wooosh.\nThat\u0026rsquo;s cool. Now how does it work? see Euler-Bernoulli Theory\n","permalink":"https://www.jemoka.com/posts/kbhbending/","tags":null,"title":"bending"},{"categories":null,"contents":"A binary operation means that you are taking two things in and you are getting one thing out; for instance:\n\\begin{equation} f: (\\mathbb{F},\\mathbb{F}) \\to \\mathbb{F} \\end{equation}\nThis is also closed, but binary operations dons\u0026rsquo;t have to be.\n","permalink":"https://www.jemoka.com/posts/kbhbinary_operation/","tags":null,"title":"binary operation"},{"categories":null,"contents":"A binomial distribution is a typo of distribution whose contents are:\nBinary Independent Fixed number Same probability The expected value of \\(X\\) following a binomial distribution is \\(np\\), and the standard deviation of \\(X\\) would be \\(\\sqrt{np(1-p)}\\).\n","permalink":"https://www.jemoka.com/posts/kbhbinomial_distribution/","tags":null,"title":"binomial distribution"},{"categories":null,"contents":"bioinformatics is a field of biology that deals with biology information. Blending CS, Data, Strategies and of course biology into one thing.\nFirst, let\u0026rsquo;s review genetic information\npossible use for bioinformatics Find the start/stop codons of known gene, and determine the gene and protein length ","permalink":"https://www.jemoka.com/posts/kbhbioinformatics/","tags":null,"title":"bioinformatics"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhblack_thursday/","tags":null,"title":"Black Thursday"},{"categories":null,"contents":"People have been trading options for a very long time, but there wasn\u0026rsquo;t a good way of quantify the value of an option.\nThere are two main types of uses for Black-Scholes Formula\nyou can use all variables and determine the value of options you can get the price of options being traded, then compute the $σ$\u0026mdash;the market\u0026rsquo;s estimation of volatility (how much they want the insurance policy that is the options) constituents \\(S_0\\): stock price \\(X\\): exercise price \\(r\\): risk-free interest rate \\(T\\): maturity time \\(\\sigma\\): standard-deviation of log returns\u0026mdash;\u0026ldquo;volatility\u0026rdquo; Black-Scholes Formula for an European \u0026ldquo;Call\u0026rdquo; Option Here is the scary formula:\n\\begin{equation} C_0 = S_0 \\mathcal{N}(d_{1})-Xe^{-rT}\\mathcal{N}(d_{2}) \\end{equation}\nwhere, the variables are defined above, and:\n\\begin{equation} \\begin{cases} d_1 = \\frac{\\ln\\qty(\\frac{S_0}{X})+\\qty(r+\\frac{\\sigma^{2}}{2})T}{\\sigma \\sqrt{t}}\\\\ d_2 = \\frac{\\ln\\qty(\\frac{S_0}{X})+\\qty(r-\\frac{\\sigma^{2}}{2})T}{\\sigma \\sqrt{t}} \\end{cases} \\end{equation}\nand \\(\\mathcal{N}\\) is the area at point under the standard normal distribution.\noh god So let\u0026rsquo;s dissect this a little.\nThe first term:\n\\begin{equation} S_0\\mathcal{N}(d_{1}) \\end{equation}\nis the \u0026ldquo;current\u0026rdquo; stock price, weighted by the probability of you being willing to exercise it.\nand the second term:\n\\begin{equation} Xe^{-rT}\\mathcal{N}(d_{2}) \\end{equation}\nis the \u0026ldquo;price\u0026rdquo; of the exercise (what you need to pay, if exercising the option, to get the stock.)\nThis strike price \\(X\\) is discounted by \\(e^{-rT}\\), which is like a time machine that rolls that strike price back to what it would be today (so that it\u0026rsquo;s comparable to \\(S_0\\).) As \\(r\\) is the risk free interest rate, we are essentially saying: \u0026ldquo;in a perfectly functional market, over the next \\(T\\) days, how will our asset grow?\u0026rdquo;\nThis is again weighted by the probability of you being willing to exercise it\u0026mdash;through modified slightly differently.\nTherefore, subtracting the two terms, we get the actual value of the option\u0026mdash;the money you would gain by exercising it, then immediately selling the stock, weighted by how willing you are actually to excercise it.\nLet\u0026rsquo;s now take a look at those \u0026ldquo;probabilities\u0026rdquo; \\(d_{\\{1,2\\}}\\). These factors essentially provide quantification of the statement that: \u0026ldquo;the higher our current price is ABOVE the excrecise price\u0026mdash;accounting for volatility\u0026mdash;the more willing we are to excercise the option.\u0026rdquo;\nNote then, \\(\\ln\\qty(\\frac{S_{0}}{X})\\) form the top of both expressions. That essentially measures how high the current price \\(S_0\\) deviates from the strike price \\(X\\).\nNow, as volatility \\(\\sigma\\) increases, \\(d_1\\) increases and \\(d_2\\) decreases (as \\(\\frac{\\sigma^{2}}{2}\\) is being added in \\(d_1\\) and subtracted in \\(d_2\\)). This is because, as volatility increase, you are less certain about what the actual \u0026ldquo;pay\u0026rdquo; (price) is, but your option\u0026mdash;given its constant strike price\u0026mdash;provides the certainty in gain.\n","permalink":"https://www.jemoka.com/posts/kbhblack_scholes_formula/","tags":null,"title":"Black-Scholes Formula"},{"categories":null,"contents":"The bloch sphere is a sphere encoding all possible probabilities of a qubit shared between two axis, \\(|u\\big\u0026gt;\\) and \\(|d\\big\u0026gt;\\).\nYou will notice that its a unit sphere, in which any magnitude has size \\(1\\). Hence, probabilities would result as projected onto each of the directions.\n","permalink":"https://www.jemoka.com/posts/kbhbloch_sphere/","tags":null,"title":"bloch sphere"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhbluest_eye/","tags":null,"title":"Bluest Eye"},{"categories":null,"contents":"General Information Due Date Topic Important Documents \u0026lt;2022-05-06 Fri\u0026gt; Bluest Eye Essay Bluest Eye Prompt Beauty: discuss Morrison’s treatment of the idea of beauty. From what, where, or whom does this notion come? What effect does it have on the way one perceives the world? On the way others perceive an individual?\nHow does beauty (the acquisition of it, the lack of it, or the presence of it) determine one’s fate in America? Is beauty a necessarily fixed entity or does it fluctuate at the whim of society? How much or to what extent does one’s perception of beauty contribute to one’s sense of self-worth?\nQuotes Bin Beauty Claudia: I had only one desire: to dismember it. To see of what it was made, to discover the dearness, to find the beauty, the desirability that had escaped me, but apparently only me. Pecola: Thrown, in this way, into the binding conviction that only a miracle could relieve her, she would never know her beauty. She would see only what there was to see: the eyes of other people. Maureen: Maureen agreed. \u0026ldquo;Ooooo yes. My mother told me that a girl named Audrey, she went to the beauty parlor where we lived before, and asked the lady to fix her hair like Hedy Lamarr’s, and the lady said, \u0026lsquo;Yeah, when you grow some hair like Hedy Lamarr’s.\u0026rsquo;\u0026rdquo; She laughed long and sweet. (post pecola beat-up) Pauline (Polly): Along with the idea of romantic love, she was introduced to another—physical beauty. In equating physical beauty with virtue, she stripped her mind, bound it, and collected self-contempt by the heap. Pauline (Polly) cont\u0026rsquo;d: She was never able, after her education in the movies, to look at a face and not assign it some category in the scale of absolute beauty, and the scale was one she absorbed in full from the silver screen. Pauline (Polly): More and more she neglected her house, her children, her man\u0026mdash;\u0026hellip;the dark edges that made the daily life with the Fishers lighter, more delicate, more lovely \u0026hellip; Here she found beauty, order, cleanliness, and praise. Pauline (Polly): Pauline kept this order, this beauty, for herself, a private world, and never introduced it into her storefront, or to her children. Cholly after Aunt Death: The funeral banquet was a peal of joy after the thunderous beauty of the funeral. It was like a street tragedy with spontaneity tucked softly into the corners of a highly formal structure. Soaphead Church: He thought it was at once the most fantastic and the most logical petition he had ever received. Here was an ugly little girl asking for beauty. A surge of love and understanding swept through him, but was quickly replaced by anger. Claudia (reflecting on Pecola): All of our waste which we dumped on her and which she absorbed. And all of our beauty, which was hers first and which she gave to us. Eyes a: Her eyes are full of sorrow. She sings to me: \u0026ldquo;When the deep purple falls over sleepy garden walls, someone thinks of me\u0026hellip;.\u0026rdquo; ** Sub-Claim Synthesis There\u0026rsquo;s always the UCLA Writing Lab.\n","permalink":"https://www.jemoka.com/posts/kbhenglish_bluest_eye/","tags":null,"title":"Bluest Eye Essay Planning"},{"categories":null,"contents":"A secondary source comparison activity for the Bluest Eye\nTony Morrison\u0026rsquo;s Rootedness That, if an action were to be done as in a community, its regarded as safer It is a very personal grief and a personal statement done among people you trust. Done within the context of the community, therefore safe.\nPublic (white-washed) and private image, by necessesity, is separated it\u0026rsquo;s just important that it be private. And then, whatever I do that is public can be done seriously.\nthat people are only defined by the uniqueness they have out of the tribe My single solitary and individual Jifejs like the lives of the tribe; it differs in these specific ways, but it is a balanced life because it is both solitary and representative\nPurpose of the novel is enlightening as well as an art form It should have something in it that enlightens; something in it that opens the door arid points the way. Something in it that suggests what the conflicts are, what the problems are.\nThe Novel is a middle class art form The history of the novel as a form began when there was a new class, a middle class, to read it; it was an art form that they needed.\nThat there is already a form of artistry for the lower class, but not middle class The lower classes didn\u0026rsquo;t need novels at that time because they had an art form already they had songs and dances, and ceremony, and gossip, and celebrations.\nnovels of manners tell people of a different world we call 1t the novel of manners, an art form designed to tell peole something they didn\u0026rsquo;t know.\nPortrays quintessential forms of connection How to get married. What a good living was.\nThe African Americans became unexclusive For a long time, the art form that was healing for Black people was music. That music is no longer exclusively ours; we don\u0026rsquo;t have exclusive rights to it.\nThat the story of the novel is told where the reader constructs the story together To construct the dialogue so that it is heard. So that there are no adverbs attached to them: \u0026ldquo;loudly,\u0026rdquo; \u0026ldquo;softly,\u0026rdquo; \u0026ldquo;he said menacingly.'\nThat the artistry is not described as Black but inherently black Black, because it uses the characteristics of Black art\n","permalink":"https://www.jemoka.com/posts/kbhsecondary_source_comparison_activity/","tags":null,"title":"Bluest Eye: secondary source comparison activity"},{"categories":null,"contents":"BNT is a discourse task where subjects are shown 60 pictures decreasing frequency and asked to recall the word.\n","permalink":"https://www.jemoka.com/posts/kbhboston_naming_test/","tags":null,"title":"Boston Naming Test"},{"categories":null,"contents":"Way of performing action research developed by Victoria Clarke and Virginia Braun in 2006\n","permalink":"https://www.jemoka.com/posts/kbhbraun_and_clarke_thematic_analysis/","tags":null,"title":"Braun and Clarke thematic analysis"},{"categories":null,"contents":"Professor Brian Macwinney is a professor of psychology, modern languages, and language technology at CMU.\n","permalink":"https://www.jemoka.com/posts/kbhbrian_macwinney/","tags":null,"title":"Brian Macwinney"},{"categories":null,"contents":"Brown v. Board of Education is a landmark case in the US. This lead for schools to be integrated, and many children were taken out of school out of protest due to the subsequent integration movement between schools.\n","permalink":"https://www.jemoka.com/posts/kbhbrown_v_board_of_education/","tags":null,"title":"Brown v. Board of Education"},{"categories":null,"contents":"Brownian Motion is the pattern for measuring the convergence of random walk through continuous timing.\ndiscrete random walk discrete random walk is a tool used to construct Brownian Motion. It is a random walk which only takes on two discrete values at any given time: \\(\\Delta\\) and its additive inverse \\(-\\Delta\\). These two cases take place at probabilities \\(\\pi\\) and \\(1-\\pi\\).\nTherefore, the expected return over each time \\(k\\) is:\n\\begin{equation} \\epsilon_{k} = \\begin{cases} \\Delta, p(\\pi) \\\\ -\\Delta, p(1-\\pi) \\end{cases} \\end{equation}\n(that, at any given time, the expectation of return is either\u0026mdash;with probability $π$\u0026mdash;\\(\\Delta\\), or\u0026ndash;with probability $1-π$\u0026mdash;\\(-\\Delta\\).\nThis makes \\(\\epsilon_{k}\\) independently and identically distributed. The price, then, is formed by:\n\\begin{equation} p_{k} = p_{k-1}+\\epsilon_{k} \\end{equation}\nand therefore the price follows a random walk.\nSuch a discrete random walk can look like this:\nWe can split this time from \\([0,T]\\) into \\(n\\) pieces; making each segment with length \\(h=\\frac{T}{n}\\). Then, we can parcel out:\n\\begin{equation} p_{n}(t) = p_{[\\frac{t}{h}]} = p_{[\\frac{nt}{T}]} \\end{equation}\nDescretized at integer intervals.\nAt this current, discrete moments have expected value \\(E[p_{n}(T)] = n(\\pi -(1-\\pi))\\Delta\\) and variance \\(Var[p_{n}(T)]=4n\\pi (1-\\pi)\\Delta^{2}\\). #why\nNow, if we want to have a continuous version of the descretized interval above, we will maintain the finiteness of \\(p_{n}(T)\\) but take \\(n\\) to \\(\\infty\\). To get a continuous random walk needed for Brownian Motion, we adjust \\(\\Delta\\), \\(\\pi\\), and \\(1-\\pi\\) such that the expected value and variance tends towards the normal (as we expect for a random walk); that is, we hope to see that:\n\\begin{equation} \\begin{cases} n(\\pi -(1-\\pi))\\Delta \\to \\mu T \\\\ 4n\\pi (1-\\pi )\\Delta ^{2} \\to \\sigma^{2} T \\end{cases} \\end{equation}\nTo solve for these desired convergences into the normal, we have probabilities \\(\\pi, (1-\\pi), \\Delta\\) such that:\n\\begin{equation} \\begin{cases} \\pi = \\frac{1}{2}\\qty(1+\\frac{\\mu \\sqrt{h}}{\\sigma})\\\\ (1-\\pi) = \\frac{1}{2}\\qty(1-\\frac{\\mu \\sqrt{h}}{\\sigma})\\\\ \\Delta = \\sigma \\sqrt{h} \\end{cases} \\end{equation}\nwhere, \\(h = \\frac{1}{n}\\).\nSo looking at the expression for \\(\\Delta\\), we can see that as \\(n\\) in increases, \\(h =\\frac{1}{n}\\) decreases and therefore \\(\\Delta\\) decreases. In fact, we can see that the change in all three variables track the change in the rate of \\(\\sqrt{h}\\); namely, they vary with O(h).\n\\begin{equation} \\pi = (1-\\pi) = \\frac{1}{2}+\\frac{\\mu \\sqrt{h}}{2\\sigma} = \\frac{1}{2}+O\\qty(\\sqrt{h}) \\end{equation}\nOf course:\n\\begin{equation} \\Delta = O\\qty(\\sqrt{h}) \\end{equation}\nSo, finally, we have the conclusion that:\nas \\(n\\) (number of subdivision pieces of the time domain \\(T\\)) increases, \\(\\frac{1}{n}\\) decreases, \\(O\\qty(\\sqrt{h})\\) decreases with the same proportion. Therefore, as \\(\\lim_{n \\to \\infty}\\) in the continuous-time case, the probability of either positive or negative delta (\\(\\pi\\) and \\(-\\pi\\) trends towards each to \\(\\frac{1}{2}\\)) by the same vein, as \\(\\lim_{n \\to \\infty}\\), \\(\\Delta \\to 0\\) Therefore, this is a cool result: in a continuous-time case of a discrete random walk, the returns (NOT! just the expect value, but literal \\(\\Delta\\)) trend towards \\(+0\\) and \\(-0\\) each with \\(\\frac{1}{2}\\) probability.\nactual Brownian motion Given the final results above for the limits of discrete random walk, we can see that the price moment traced from the returns (i.e. \\(p_{k} = p_{k-1}+\\epsilon_{k}\\)) have the properties of normality (\\(p_{n}(T) \\to \\mathcal{N}(\\mu T, \\sigma^{2}T)\\))\nTrue Brownian Motion follows, therefore, three basic properties:\n\\(B_{t}\\) is normally distributed by a mean of \\(0\\), and variance of \\(t\\) For some \\(s\u0026lt;t\\), \\(B_{t}-B_{s}\\) is normally distributed by a mean of \\(0\\), and variance of \\(t-s\\) Distributions \\(B_{j}\\) and \\(B_{t}-B_{s}\\) is independent Standard Brownian Motion Brownian motion that starts at \\(B_0=0\\) is called Standard Brownian Motion\nquadratic variation The quadratic variation of a sequence of values is the expression that:\n\\begin{equation} \\sum_{i=0}^{N-1} (x_{i+1}-x_i)^{2} \\end{equation}\nOn any sequence of values \\(x_0=0,\\dots,x_{N}=1\\) (with defined bounds), the quadratic variation becomes bounded.\n","permalink":"https://www.jemoka.com/posts/kbhbrownian_motion/","tags":null,"title":"Brownian Motion"},{"categories":null,"contents":"duplicate article creation. see Cantilever Beams\n","permalink":"https://www.jemoka.com/posts/kbhcantilever_beam/","tags":null,"title":"cantilever beam"},{"categories":null,"contents":"A Cantilever beam is a rigid structure which is extended horizontally and supported on one end.\nWorking with Cantilever Beams curvature Let\u0026rsquo;s first define a function:\n\\begin{equation} w(x) \\end{equation}\nthis represents the deflection of the beam at point \\(x\\). We will begin by taking its derivative by location:\n\\begin{equation} \\Delta w = \\pdv{w}{x} \\end{equation}\nis the change in deflection over location. \u0026ldquo;How much deviation of the beam from the resting axi is there as you run along it?\u0026rdquo;\nWe now take another derivative:\n\\begin{equation} k = \\pdv[2]{w}{x} \\end{equation}\n\\(k\\) is defined as the \u0026ldquo;Curvature\u0026rdquo; of the beam: the \u0026ldquo;change in the change of bentness.\u0026rdquo; The intuition is essentially this:\na straight, flat beam fixed an one end has \\(\\Delta w=0\\), \\(k=0\\). It does not change from its resting axis, and its rate of change from resting does not change a straight, slanted beam fixed at one end has \\(\\Delta w=C, k=0\\). It changes from its resting axis with a linear rate, and its rate of change from resting does not change. a curved, slanted beam fixed at one end has \\(\\Delta \\omega = f(x), k=C\\). It changes from its resting axis non-linearly (hence curving at a function of \\(x\\)), and its rate of change from resting is changing at a constant \\(c\\). flexural rigidity Flexural Rigidity is the \u0026ldquo;force couple\u0026rdquo; (\u0026ldquo;rate\u0026rdquo;) which relates the Curvature of an non-rigid body and how much torque it actually generates given the object\u0026rsquo;s properties.\nRecall first our Elastic Modulus \\(E\\): it is a fraction of \\(\\frac{stress}{strain}\\) measured in Pascals (force per unit area, i.e. \\(\\frac{N}{m^{2}} = \\frac{kg}{ms^{2}}\\)).\nFind also second moment of area \\(I\\): a value in units \\(m^{4}\\) which is the sum (by area) of the squared displacement of each infinitesimal area to the axis of origin.\nAnd we bam! we multiply the two things together, creating a value \\(EI\\) in units \\(Nm^{2}\\).\nbending moment bending moment is the torque from bending. It is expressed usually in \\(M\\). As mentioned in the section about Flexural Rigidity, we can use that value to relate \\(M\\) with the actual Curvature of your object.\nSpecifically, that:\n\\begin{equation} M = -(EI)k = -EI\\pdv[2]{w}{x} \\end{equation}\n\u0026ldquo;bending moment is flexural rigidity times curvature\u0026rdquo; =\u0026gt; \u0026ldquo;[how much force per distance you exert] is the result of [how bendy your thing is] times [how much you bent it].\u0026rdquo;\nThere is a negative in front because if you pull out your lovely little right hand, point your thumb forward (+y), start curling your nice fingers around your nice hand (-z), you will notice that you are wrapping them downwards (the - part of the z) which is rather not positive. If we want \\(\\pdv[2]{w}{x}\\) to be positive (bend up), we will need to chuck a negative in front of it to make both things positive.\nThis relation, while intuitive, is not from first-principles. In order to get such a derivation, you read Wikipedia.\nmagic We can take two derivatives by location\u0026mdash;\n\\begin{equation} \\pdv[2] x \\qty(EI \\pdv[2]{w}{x}) = -\\mu \\pdv{w}{t}+q(x) \\end{equation}\nwhere \\(\\mu\\) is the mass density, \\(q(x)\\) is the force applied (in Newtons) by area. this is magic. Will come back to it.\n","permalink":"https://www.jemoka.com/posts/kbhcantilever_beams/","tags":null,"title":"Cantilever Beams"},{"categories":null,"contents":"A cancer drug to synthesize Fluoropyrimidine.\n","permalink":"https://www.jemoka.com/posts/kbhcapecitabmine/","tags":null,"title":"Capecitabmine"},{"categories":null,"contents":"CAPM is a method of portfolio selection analysis which focuses on maximizing return given some fixed variance.\nSharpe Ratio The Sharpe Ratio is a measure of the risk-adjusted performance of an asset\u0026mdash;given the rate of return of some risk-free asset.\nIt is defined as:\n\\begin{equation} S_{a} = \\frac{E[R_{a}-R_{b}]}{\\sigma_{a}} \\end{equation}\nwhere, \\(R_{a}\\) is the raw return of the asset, \\(R_{b}\\) is the risk-free rate of return, and \\(\\sigma_{a}\\) is the standard deviation of the asset \u0026ldquo;excess\u0026rdquo; return (i.e. standard deviation actual return - expected return\u0026mdash;how much extra there is).\nCapital Market Line The Capital Market Line is a line that uses the Sharpe Ratio of a market as a whole (how the market is performing against the risk-free rate) to analyze the performance of portfolio. It plots the performance of an \u0026ldquo;optimal portfolio\u0026rdquo; in a given market.\nLet\u0026rsquo;s construct first the Sharpe Ratio of a hypothetical market:\n\\begin{equation} \\frac{R_{t}-r_{f}}{\\sigma_{t}} \\end{equation}\nwhere \\(R_{T}\\) is the market return, \\(r_{f}\\) is the risk-free rate, and \\(\\sigma_{t}\\) is standard-deviation of the market returns.\nWe will multiply this value by the standard-deviation of your portfolio to calculate what the market claims should be your expected return. Then, we shift the line by the risk-free rate (as you are expected also to get that rate back in your return.\nSo an \u0026ldquo;effecient\u0026rdquo; portfolio should behave like:\n\\begin{equation} R_{p} = r_{f}+\\frac{R_{T}-r_{f}}{\\sigma_{T}}\\sigma_{p} \\end{equation}\n(the risk-free rate, plus how much you are expected to get minus the r\nTangency Portfolio ","permalink":"https://www.jemoka.com/posts/kbhcapm/","tags":null,"title":"Capital-Asset Pricing Model"},{"categories":null,"contents":"A category is defined as:\ncollection of objects, where if \\(X\\) is an object of \\(C\\) we write \\(X \\in C\\) ","permalink":"https://www.jemoka.com/posts/kbhcategory/","tags":null,"title":"category"},{"categories":null,"contents":"An abstract study of mathematics based on categories, functors, and natural transformations.\n","permalink":"https://www.jemoka.com/posts/kbhcategory_theory/","tags":null,"title":"category theory"},{"categories":null,"contents":"stock market crash of 1929 At October 24th, 1929, Black Thursday took place, and the stock market crashed. During this time, a record of 13 million shares traded, over $3b of losses. This began a 4 year slide of the global economy.\nCrash theories:\ndemand-driven theory Monetarist theory bank failures of 1929 Banks became irrelevant. Lots of risky loans given out, farmers are taken out huge loans and the banks can\u0026rsquo;t deal.\nother factors economy of credit tariffs ","permalink":"https://www.jemoka.com/posts/kbhcauses_of_the_great_depression/","tags":null,"title":"causes of the Great Depression"},{"categories":null,"contents":"\u0026ldquo;If sample size is large, the sampling distribution is normal. The larger \\(N\\) is, the more normal the resulting shape is.\u0026rdquo;\nIt is technically written as:\ngiven some random variable \\(Y\\), the normalized collection of a random variable \\(X\\) with samples \\(x_j\\),\n\\begin{equation} Y = \\frac{1}{\\sigma \\sqrt{N}} \\sum_{i=1}^N (x_1 - \\mu) \\end{equation}\nWe have that:\n\\begin{equation} \\lim_{N\\to \\infty} Y_n \\sim N(0,1) \\end{equation}\nThat, as long as you normalize a random variable and have enough of it, you get the normal distribution.\nNotably, for the central limit theorem to hold, the variance has to be finite (that the results vary in a certain finite value \\(\\sigma\\). With that \\(\\sigma\\) value, we can see above that the central limit theorem will eventually converge to the normal. THis is useful for the Random Walk Hypothesis.\n","permalink":"https://www.jemoka.com/posts/kbhcentral_limit_theorem/","tags":null,"title":"central limit theorem"},{"categories":null,"contents":" 80% of the human genome is actually transcribed very little \u0026ldquo;junk DNA\u0026rdquo; 40% IncRNA are gene specific ","permalink":"https://www.jemoka.com/posts/kbhchanges_to_central_dogma/","tags":null,"title":"changes to central dogma"},{"categories":null,"contents":"\\(\\chi^2\\) is a test statistic for hypothesis testing.\nmotivation for chi-square The motivation for chi-square is because t-test (means, \u0026ldquo;is the value significantly different\u0026rdquo;) and z-test (proportion, \u0026ldquo;is the incidence percentage significantly different\u0026rdquo;) all don\u0026rsquo;t really cover categorical data samples: \u0026ldquo;the categories are distributed in this way.\u0026rdquo;\nTake, for instance, if we want to test the following null hypothesis:\nCategory Expected Actual A 25 20 B 25 20 C 25 25 D 25 25 \\(\\alpha = 0.05\\). What do we use to test this??\n(hint: we can\u0026rsquo;t, unless\u0026hellip;)\nEnter chi-square.\nchi-square test chi-square test is a hypothesis test for categorical data. It is responsible to translate differences in distributions into p-values for significance.\nBegin by calculating chi-square after you confirmed that your experiment meets conditions for inference (chi-square test).\nOnce you have that, look it up at a chi-square table to figure the appropriate p-value. Then, proceed with normal hypothesis testing.\nBecause of this categorical nature, chi-square test can also be used as a homogeneity test.\nconditions for inference (chi-square test) random sampling expected value for data must be \\(\\geq 5\\) sampling should be \\(\u0026lt;10\\%\\) or independent chi-square test for homogeneity The chi-square test for homogeneity is a test for homogeneity via the chi-square statistic.\nTo do this, we take the probability of a certain outcome happening\u0026mdash;if distributed equally\u0026mdash;and apply it to the samples to compare.\nTake, for instance:\nSubject Right Hand Left Hand Total STEM 30 10 40 Humanities 15 25 40 Equal 15 5 20 Total 60 40 100 We will then figure the expected outcomes:\nRight Left 24 16 24 16 12 8 Awesome! Now, calculate chi-square with each cell of measured outcomes. Calculate degrees of freedom by (num_row-1)*(num_col-1).\nchi-square test for independence The chi-square test for independence is a test designed to accept-reject the null hypothesis of \u0026ldquo;no association between two variables.\u0026rdquo;\nEssentially, you leverage the fact that \u0026ldquo;AND\u0026rdquo; relationships are multiplicative probabilities. Therefore, the expected outcomes are simply the multiplied/fraction of sums:\ncalculating chi-square \\begin{equation} \\chi^2 = \\frac{(\\hat{x}_0-x_0)^2}{x_0} +\\frac{(\\hat{x}_1-x_1)^2}{x_1} + \\cdots + \\frac{(\\hat{x}_n-x_n)^2}{x_n} \\end{equation}\nWhere, \\(\\hat{x}_i\\) is the measured value and \\(x_i\\) is the expected value.\n","permalink":"https://www.jemoka.com/posts/kbhchi_square/","tags":null,"title":"chi-square"},{"categories":null,"contents":"Chiara Marletto is an physicist working on Quantum mechanics working in D. of Physics, Wolfson College, University of Oxford.\nSubfield: constructor theory. She studies quantum theory.\n","permalink":"https://www.jemoka.com/posts/kbhchiara_marletto/","tags":null,"title":"Chiara Marletto"},{"categories":null,"contents":"I was digging through my OneDrive recently for work, and found this piece of writing.\nThere is naught but a small, dirt-filled puddle in front of this lawn. Yet only here – by the puddle – can Gary find a small, much-needed respite from the neverending work. Of course, without the hours he has committed to the sweatshop, his mother would have died ages ago from colora.\nBut how does it matter now? Rarely now \u0026ndash; once every year \u0026ndash; does he even earn the privilege to exit the heavily-guarded area to visit his mother; and how little time he has during such visits: each visit seems to just be a long walk, a knock, a kiss on the cheek \u0026ndash; then back to the workhouse he goes.\nNo, he must push on. Focusing his tired mind back to the concrete structure in front of him, he sees the supervisor hollering the same old phrase. Back to work! Back to work! Move! Move! Break is over!\nWhat is this break, even? The notable lack of timepieces around the lawn means the important task of timekeeping falls to the supervisors \u0026ndash; who, notably, have an obvious interest in shortening the break. And ‘lo, the breaks are shortened: Gary has always remembered the session as until the bottom of the clock, yet doubtless he will find himself staring at a clock hand pointing to the horizontal upon walking into the building.\nHe can do nothing now: there is one \u0026ndash; and the ultimate \u0026ndash; sanction for not listening to the supervisor, and he wants nothing to do with it: beating. Beating that gets harder, faster, as time progresses is the one, the only, and the final answer to all cases of disobedience. Heck, if the supervisor demands time run backwards during breaks, Cronus will listen and obey \u0026ndash; for even he, a god, is probably as scared of these “correctional sessions” as anyone else.\nThere is, then, no time to be wasted. Up towards the factory he walks \u0026ndash; joined by hundreds of others suffering a similar fate, doing the same tedious, repetitive tasks as him. If he hadn\u0026rsquo;t been made dependent \u0026ndash; addicted! in fact \u0026ndash; to the meager wages he received, he could have achieved greatness the world has yet to see.\nBut, alas, towards the factory he walks, steps. Timidly, slowly, shuffling his feet quickly enough so as to not anger the increasingly stressed supervisor. Stressed understandably, perhaps, due to the increasing external talk of organizing such congregations as the “Child Labour Committee”, which Gary himself isn’t sure to what extent he should trust.\nThe quarter-bell strikes. Indeed, his suspicions were correct \u0026ndash; yet superfluous. When all was thought and done, he couldn\u0026rsquo;t possibly have even produced the thought of defying the wishes of the supervisor, let along execute it. But now, he has not even the physical capacity to escape \u0026ndash; the door was locked, and locked means work eternal \u0026ndash; at least until the next meager halt seemingly few decades later.\nSuddenly, a clicking occurs. A machine screeching to a halt, perhaps due to the same overwork and misuse. In walks the supervisor: nevermind that: the work must go on!\nIt is now down to the same routine \u0026ndash; picking the smallest, nimblist of the bunch \u0026ndash; Gary, of course \u0026ndash; to, through great persuasion and threatenings of beatings, climb under the mechanical beast and undo the mess. It’s a dance of oil and gear that Gary has rehearsed many times before, each time dreading the next. Yet he still brought himself to perform the task time and time again for it, although dreadful, seems to be heavenly compared to the alternate: getting the “correctional session.”\nDown the cover he goes: a little pulling there, a little dabbing there, and Hark! The machine jumped to a start with a splash of brilliant pink hue, announcing \u0026ndash; celebrating, it seems \u0026ndash; itself as Gary’s final quarters.\nNevermind that: the work must go on!\n","permalink":"https://www.jemoka.com/posts/kbhchild_labour/","tags":null,"title":"Child Labour: A Short Story"},{"categories":null,"contents":"DOI: 10.3389/fpsyg.2020.623237\nOne-Liner (thrice) Used features extracted by VGGish from raw acoustic audio against a SVM, Perceptron, 1NN; got \\(59.1\\%\\) classif. accuracy for dementia Then, trained a CNN on raw wave-forms and got \\(63.6\\%\\) accuracy Then, they fine-tuned a VGGish on the raw wave-forms and didn\u0026rsquo;t report their results and just said \u0026ldquo;we discovered that audio transfer learning with a pretrained VGGish feature extractor performs better\u0026rdquo; Gah! Novelty Threw the kitchen sink to process only raw acoustic input, most of it missed; wanted 0 human involvement. It seems like last method is promising.\nNotable Methods fine-tuning VGGish against raw acoustic waveforms to build a classifier via a CNN.\nKey Figs Their fancy network Its just a CNN afaik with much maxpooling; could have used some skipped connections. I wonder if it overfit?\nTheir actual training results Looks generally pretty bad, but a run of their DemCNN seem to have gotten state-of-the-art results. Not sure where transfer training data went.\nNew Concepts VGGish Notes Accuracy question According to this the state of the art at the time from pure audio was 56.6%? For a binary classifier isn\u0026rsquo;t that just doing nothing?\nSo somebody did get better before?\n","permalink":"https://www.jemoka.com/posts/kbhchlasta_2021/","tags":["ntj"],"title":"Chlasta 2021"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhchromatin/","tags":null,"title":"chromatin"},{"categories":null,"contents":"civil rights movement starting civil rights moment was kicked off by the Rosa Parks incident, which caused the Montomery Bus Boycott.\nMartin Luther King capitalized the incident to kick start civil rights movement. He employed the method of nonviolence movement.\neducational integration in the civil rights movement K-12 disintegration: Brown v. Board of Education University of Georgia was the first disintegrated university in the south service integration in the civil rights movement Lunch counter boycotts. Nashville became the first desegregated lunch counter.\nSNICK SNICK is a student organization founded by Ella Baker in the civil rights movement that sent students into the most dangerous areas of segregation and leading protests.\nMotown Records Motown Records is an African-American owned Detroit record business\nMalcom X A civil rights movement activist, calling for more violent forms of protest and prosecuting specific white actions. Malcom X and Martin Luther King contradicted each other in methods of active persecution vs. nonviolent integration.\nBloody Sunday Bloody Sunday was a voting rights march from Selma to Montgomery. Peaceful protesters were attacked with nightsticks and tear gas. The event was widely televised: transforming the movement as a televised morality play.\nNonviolence helps getting the clergy leaders as a form of leveraging religion in a show of unity.\nBlack Power Movement A new chapter in the civil rights movement which incorporated less of the elements of integration but instead in wanted more sense of self-determination. nonviolence movement, which the Black Power Movement overrided, had ran its course when Martin Luther King was assassinated.\n","permalink":"https://www.jemoka.com/posts/kbhcivil_rights/","tags":null,"title":"civil rights movement"},{"categories":null,"contents":"A part of the New Deal programs for unmarried men to go and build American infrastructure outdoors under reasonably harsh conditions. \u0026ldquo;Kind of like boy scouts for adults.\u0026rdquo; It is structured like the military; Black men were segregated and not given leadership roles.\n1933-1942.\n","permalink":"https://www.jemoka.com/posts/kbhcivillian_conservation_corps/","tags":null,"title":"Civillian Conservation Corps"},{"categories":null,"contents":"to be closed means that the operation of a group applied to an element of a group would produce another element of the group.\n","permalink":"https://www.jemoka.com/posts/kbhclosed/","tags":null,"title":"closed"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcmu/","tags":null,"title":"CMU"},{"categories":null,"contents":"A brain signal to help maintain glucose homeostatis\nBrain takes glucose product + glucose uptake to control energy balance in food intake and energy expenditure.\nThe brain takes:\nNeural Behavioral Hormonal responses to maintain glucode uptake.\n","permalink":"https://www.jemoka.com/posts/kbhcns_regulation/","tags":null,"title":"CNS regulation"},{"categories":null,"contents":"The time it takes for a qubit to oscillate between two states between damping down.\n","permalink":"https://www.jemoka.com/posts/kbhcoherence_time/","tags":null,"title":"coherence time"},{"categories":null,"contents":"The cold war is a period of time in which there is blocks of conflict. This is after WWII.\nSee also:\ncold war in vietnam ","permalink":"https://www.jemoka.com/posts/kbhcold_war/","tags":null,"title":"cold war"},{"categories":null,"contents":"A fact sheet on the progress of the cold war in Vietnam.\nprogression of US escalation in the war, an overview Reading: encyclopedia Britannica\n1959-1960: VCs initiated a group of ambushes which the exiled government led by Ngô Đình Diệm can no longer fend off 1961: Kennedy takes office, institutes a plan to put American advisors at all levels of Vietnam leadership 1963: Buddest monks rebelled, Ngô family rule became bizarre and leveraged their Roman Catholic views to persecute Buddhists in the region 1963: Ngô Đình Diệm is assassinated after the support of the US (Kennedy) via Cable 243 seeking a regime change 1963: Kennedy assisinated 1964: Vietnam situation worsens 1965: American government fully went in, and Ky was eased out of power when Neuyen Van Thieu ad full control 1965: US fighting was effective though unpresistent; viet cong just went in after US leaves 1967: Protests in the US lead to a growing anti-war sentiment, which the VietCong picked up on 1968: the Tet Offensive, a VietCong operation, tried to pillage South Vietnam. Though it failed, strong anti-war sentiments were stirred. 1969: Anti-War protests pick up force 1970: Ohio National Guard opens fire on unarmed protesters 1973: Peace Pact Signed after the US giving up, essentially 1975: Saigon falls, US evacuates anti-war protest motivation in Vietnam Reading: Protest against the War in Vietnam\nThe first protests rose in the 1950 and picked up force by the late 1960s when LBJ decided not to seek re-election.\nForeign policy is usually hard to change, but the strength of domestic dissent in Vietnam represents an usual shift which drove foreign policy changes.\nRight-wing sentiment: seeing the war as a means of future-proofing the American government from Communistic influences. Left-wing protest More organized than the spontaneous of the right-wing protest Split between moralistic + legalistic interests vs. national interest domestic political influence of the Vietnam War Reading: The War that Killed Trust, Karl Marlantes, 2017\n\u0026ldquo;Of course presidents lie\u0026rdquo;\u0026mdash;that the Vietnam War represented the shift away from genuine truthfulness as a part of American politics Killed 58,000 service-members, and made Americans cynical and distrustful of governmental institutions Systemic Cynicism Johnson\u0026rsquo;s \u0026ldquo;credibility gap\u0026rdquo;: that the president maybe lying. Nowadays this is commonplace, but back then it was quite unusual.\nCLAIM: engendered Cynicism threatened inaction.\nRacial Integration The cold war promised higher degrees of racial integration because of collective service.\nRepeated Touring That, post-draft, the American working class became much more likely to serve \u0026ldquo;voluntarily\u0026rdquo; by being recruited. Unlike the draft, which is some ways is universal service, the volunteer system is much more reliant upon th emiddle class.\nsocial impacts of the Vietnam War Reading: The Social Impact of War, Modell and Haggerty, 1991\nWars\u0026rsquo; effects can be treated with a lens of social manifestation The Vietnam war had an impact on the last 20 years of primary war literature draft The draft is the principle mechanism by which people into the war. The system facilitating the draft in the United States, the Selective Service System, is a good case study for such a system in the Vietnam War.\nBy its design, the draft is supposed to be an equitable process (baring gender and age.) However, the Vietnam War reveals that the military services was not straightforwardly distributed: often drafting children of lower socioeconomic status.\nexperience of servicemen in Vietnam Soldiers in the Vietnam War have shown some negative psychological side effects. Solders are shown to be \u0026ldquo;working through\u0026rdquo; the ideas to process, creating a larger effects.\neffects on the economy War veterans generally had higher incomes than non-vets, mostly because they have more income per level of educational attanment.\nhistoriographical school of Vietnam War Reading: James McLeroy, Small Wars Journal, (Army Special Forces Officer in I Corps, Vietnam, in 1968)\nOrthodox treatment Vietnam War as an extension/afterthought of late-20th century cold war history\nVietnam War escalated only because of United States involvement \u0026ldquo;anti-war\u0026rdquo; is not opposition against communistic conquest but opposition against war in itself Revisionist treatment Vietnam War as a calculable implementation of escalator revolutionary strategy modeled after Mao.\nVietnam War is not an insurgency or a civil war, but instead a part of the three-step guerrilla warfare Provocation of the United States is a part of the strategy\u0026mdash;to force them to move out of Vietnam and to encourage the communist bloc to provide more support ","permalink":"https://www.jemoka.com/posts/kbhcold_war_in_vietnam/","tags":null,"title":"cold war in vietnam"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcollectivist_economy/","tags":null,"title":"Collectivist Economy"},{"categories":null,"contents":"College application is the process of applying to an American college.\n","permalink":"https://www.jemoka.com/posts/kbhcollege_application/","tags":null,"title":"college application"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcollegeboard/","tags":null,"title":"CollegeBoard"},{"categories":null,"contents":"commutativity means that the same operation can be ran in any order.\nThat is:\n\\begin{equation} ABC = ACB \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhcommutivity/","tags":null,"title":"commutativity"},{"categories":null,"contents":"A complex number is a type of number. They are usually written as \\(a+bi\\).\nFormally\u0026mdash;\n\\begin{equation} \\mathbb{C} = \\left\\{a+bi\\ \\middle |\\ a,b \\in \\mathbb{R} \\right\\} \\end{equation}\nconstituents an order pair of two elements \\((a,b)\\) where \\(a,b\\in \\mathbb{R}\\).\nrequirements there are 6. For all statements below, we assume \\(\\alpha = a+bi\\) and \\(\\beta=c+di\\), \\(\\lambda = e+fi\\), where \\(a,b,c,d,e,f \\in \\mathbb{R}\\) and therefore \\(\\alpha, \\beta,\\lambda \\in \\mathbb{C}\\).\ncommutativity \\(\\alpha + \\beta = \\beta + \\alpha\\) and \\(\\alpha\\beta = \\beta\\alpha\\) for all \\(\\alpha,\\beta \\in \\mathbb{C}\\).\nProof of complex number commutativity We desire \\(\\alpha + \\beta = \\beta + \\alpha\\).\n\\begin{align} \\alpha + \\beta \u0026amp;= (a+bi)+(c+di) \\\\ \u0026amp;=(a+c)+(b+d)i \\\\ \u0026amp;=(c+a)+(d+b)i \\\\ \u0026amp;=(c+di) + (a+bi) \\\\ \u0026amp;=\\beta+\\alpha\\ \\blacksquare \\end{align}\nleveraging the commutativity inside real numbers.\nInsights: combining and splitting\nThis proof has the feature of combining, operating (commuting, here), the splitting.\nassociativity \\((\\alpha +\\beta) + \\lambda = \\alpha + (\\beta +\\lambda)\\) and \\((\\alpha\\beta) \\lambda = (\\alpha \\beta) \\lambda\\)\nProven via the same trick from last time\nidentities \\(\\lambda + 0 = \\lambda\\), \\(\\lambda 1 = \\lambda\\)\nProof of complex number additive identity We desire that \\(\\lambda + 0 = 0\\).\n\\begin{align} \\lambda + 0 \u0026amp;= (e+fi) + (0+0i) \\\\ \u0026amp;= (e+0) + (f+0)i \\\\ \u0026amp;= e+fi\\ \\blacksquare \\end{align}\nmultiplicative identity is proven in the same way\nadditive inverse \\(\\forall \\alpha \\in \\mathbb{C}, \\exists !\\ \\beta \\in \\mathbb{C}: \\alpha + \\beta = 0\\)\nProof of complex number additive inverse We desire to claim that \\(\\forall \\alpha \\in \\mathbb{C}, \\exists !\\ \\beta \\in \\mathbb{C}: \\alpha + \\beta = 0\\), specifically that there is a unique \\(\\beta\\) which is the additive inverse of every \\(\\alpha\\).\nTake a number \\(\\alpha \\in \\mathbb{C}\\). We have that \\(\\alpha\\) would then by definition be some \\((a+bi)\\) where \\(a,b \\in \\mathbb{R}\\).\nTake some \\(\\beta\\) for which \\(\\alpha + \\beta = 0\\); by definition we again have \\(\\beta\\) equals some \\((c+di)\\) where \\(c,d \\in \\mathbb{R}\\).\n\\(\\because \\alpha + \\beta =0\\), \\(\\therefore (a+bi) + (c+di) = 0\\). \\(\\therefore (a+c) + (b+d)i = 0\\) \\(\\therefore a+c = 0, b+d = 0\\) \\(\\therefore c = -a, d = -b\\) We have created a unique definition of \\(c,d\\) and therefore \\(\\beta\\) given any \\(\\alpha\\), implying both uniqueness and existence.\nInsights: construct then generalize\nIn this case, the cool insight is the construct and generalize pattern. We are taking a single case \\(\\alpha\\), manipulating it, and wrote the result we want in terms of the constituents of \\(\\alpha\\). This creates both an existence and uniqueness proof.\nmultiplicative inverse \\(\\forall \\alpha \\in \\mathbb{C}, \\alpha \\neq 0, \\exists!\\ \\beta \\in \\mathbb{C} : \\alpha\\beta =1\\)\nThis is proven exactly in the same way as before.\ndistributive property \\(\\lambda(\\alpha+\\beta) = \\lambda \\alpha + \\lambda \\beta\\ \\forall\\ \\lambda, \\alpha, \\beta \\in \\mathbb{C}\\)\nProof of complex number distributive property We desire to claim that \\(\\lambda(\\alpha+\\beta) = \\lambda \\alpha + \\lambda \\beta\\).\n\\begin{align} \\lambda(\\alpha+\\beta) \u0026amp;= (e+fi)((a+bi)+(c+di))\\\\ \u0026amp;=(e+fi)((a+c)+(b+d)i)\\\\ \u0026amp;=((ea+ec)-(fb+fd))+((eb+ed)+(fa+fc))i\\\\ \u0026amp;=ea+ec-fb-fd+(eb+ed+fa+fc)i\\\\ \u0026amp;=ea-fb+ec-fd+(eb+fa+ed+fc)i\\\\ \u0026amp;=(ea-fb)+(ec-fd)+((eb+fa)+(ed+fc))i\\\\ \u0026amp;=((ea-fb)+(eb+fa)i) + ((ec-fd)+(ed+fc)i)\\\\ \u0026amp;=(e+fi)(a+bi) + (e+fi)(c+di)\\\\ \u0026amp;=\\lambda \\alpha + \\lambda \\beta\\ \\blacksquare \\end{align}\nInsights: try to remember to go backwards\nAt some point in this proof I had to reverse complex addition then multiplication, which actually tripped me up for a bit (\u0026ldquo;how does i distribute!!!\u0026rdquo;, etc.) Turns out, there was already a definition for addition and multiplication of complex numbers so we just needed to use that.\nadditional information addition and multiplication of complex numbers \\begin{align} (a+bi) + (c+di) \u0026amp;= (a+c)+(b+d)i \\\\ (a+bi)(c+di) \u0026amp;= (ac-bd)+(ad+bc)i \\end{align}\nwhere, \\(a,b,c,d\\in\\mathbb{R}\\).\nsubtraction and division of complex numbers Let \\(\\alpha, \\beta \\in \\mathbb{C}\\), and \\(-a\\) be the additive inverse of \\(\\alpha\\) and \\(\\frac{1}{\\alpha}\\) be the multiplicative inverse of \\(\\alpha\\).\nsubtraction: \\(\\beta-\\alpha = \\beta + (-\\alpha)\\) division: \\(\\frac{\\beta}{\\alpha} = \\beta\\frac{1}{\\alpha}\\) Simple enough, subtraction and division of complex numbers is just defined by applying the inverses of a number to a different number.\ncomplex numbers form a field See properties of complex arithmetic, how we proved that it satisfies a field.\n","permalink":"https://www.jemoka.com/posts/kbhcomplex_number/","tags":null,"title":"complex number"},{"categories":null,"contents":"complexity theory is a theory in algorithms to analyze time classes.\nWe know that \\(O(n\\ log\\ n)\\) is between \\(O(n)\\) and \\(O(n^2)\\) \u0026mdash; so we can roughly call it \u0026ldquo;polynomial time.\u0026rdquo;\nSince the optimal comparison cannot be faster than polynomial time, we say that comparison-based sorting is a polynomial-time algorithm.\nFrom this information, we can come up with two main time classes: \\(P\\) for solutions with known polynomial time, \\(NP\\) for non-deterministic polynomial time.\nThink of it as \\(P\\) is solvable with polynomial time and \\(NP\\) is verifiable with polynomial time.\nThe cool thing about \\(NP\\) problems is that solving a subset of them (\u0026quot;\\(NP\\) hard\u0026quot; problems) solves all \\(NP\\) problems.\nreduction (algorithms) reduction is how you can use \\(NP-hard\\) problems to solve all \\(NP\\) problems in complexity theory.\nSay, multiplication:\nsay you have a basic algorithm to add we can perform multiplication by asking our black box addition algorithm to add \\(n\\) times in complexity theory terms, this means addition is \u0026ldquo;at least as hard\u0026rdquo; as multiplication. Because, if we can solve any addition problem, we can solve any multiplication problem. \u0026ldquo;Given this, do that.\u0026rdquo;\nproblem classes (see above)\n\u0026ldquo;Polynomial time\u0026rdquo; \\(P\\) \u0026mdash; problems solvable with polynomial time \u0026ldquo;Non-deterministic polynomial time\u0026rdquo; \\(NP\\) \u0026mdash; problem verifiable with polynomial time \u0026ldquo;Exponential time\u0026rdquo; \\(EXPTIME\\) \u0026mdash; problems that can only be solved in exponential time \u0026ldquo;2 Exponential time\u0026rdquo; \\(2EXPTIME\\) \u0026mdash; class of problems that takes \\(2^{2^n}\\) time to solve Space complexity works in a similar way.\n\\(P\\) and \\(NP\\) are deterministic and non-deterministic in context to a Turing machine.\nFundamentally, \\(P\\) and \\(NP\\) only apply to decision problems\u0026mdash;given a problem, output \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no.\u0026rdquo; However, this definition can be stretched: sorting is a decision problem, because it can be stated as \u0026ldquo;given an unsorted array, can you verify whether or not an array is sorted\u0026rdquo;\n","permalink":"https://www.jemoka.com/posts/kbhcomplexity_theory/","tags":null,"title":"complexity theory"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcomposite_system/","tags":null,"title":"composite system"},{"categories":null,"contents":"conceptual grammar is the proposed universal grammar which connects semantic primes. In theory, this grammar is universal across languages.\nThere are three main categories of conceptual grammars:\nCombinatorics (connecting one idea to another) Account of valancies? #what Propositional complementation (location \u0026ldquo;something that happen in this place\u0026rdquo; ","permalink":"https://www.jemoka.com/posts/kbhconceptual_grammar/","tags":null,"title":"conceptual grammar"},{"categories":null,"contents":"Current automated lexicography (term definition) techniques cannot include contextual or new term information as a part of its synthesis. We propose a novel data harvesting scheme leveraging lead paragraphs in Wikipedia to train automated context-aware lexicographical models. Furthermore, we present ConDef, a fine-tuned BART trained on the harvested data that defines vocabulary terms from a short context. ConDef is determined to be highly accurate in context-dependent lexicography as validated on ROUGE-1 and ROUGE-L measures in an 1000-item withheld test set, achieving scores of 46.40% and 43.26% respectively. Furthermore, we demonstrate that ConDef\u0026rsquo;s synthesis serve as good proxies for term definitions by achieving ROUGE-1 measure of 27.79% directly against gold-standard WordNet definitions.Accepted to the 2022 SAI Computing Conference, to be published on Springer Nature\u0026rsquo;s Lecture Notes on Networks and Systems Current automated lexicography (term definition) techniques cannot include contextual or new term information as a part of its synthesis. We propose a novel data harvesting scheme leveraging lead paragraphs in Wikipedia to train automated context-aware lexicographical models. Furthermore, we present ConDef, a fine-tuned BART trained on the harvested data that defines vocabulary terms from a short context. ConDef is determined to be highly accurate in context-dependent lexicography as validated on ROUGE-1 and ROUGE-L measures in an 1000-item withheld test set, achieving scores of 46.40% and 43.26% respectively. Furthermore, we demonstrate that ConDef\u0026rsquo;s synthesis serve as good proxies for term definitions by achieving ROUGE-1 measure of 27.79% directly against gold-standard WordNet definitions.\n","permalink":"https://www.jemoka.com/posts/kbhcondef_abstract/","tags":null,"title":"ConDef Abstract"},{"categories":null,"contents":"There are many condition in the Great Depression caused\nby 1932, 1/4 had no work emigration exceeded immigration decrease in American birth increase of mental illness and suicide people create Hooverviles movies and radio became much more popular ","permalink":"https://www.jemoka.com/posts/kbhconditions_in_the_great_depression/","tags":null,"title":"conditions in the Great Depression"},{"categories":null,"contents":"proportional confidence intervals We will measure a single stastistic from a large population, and call it the point estimate. This is usually denoted as \\(\\hat{p}\\).\nGiven a proportion \\(\\hat{p}\\) (\u0026ldquo;95% of sample), the range which would possibly contain it as part of its \\(2\\sigma\\) range is the \\(95\\%\\) confidence interval.\nTherefore, given a \\(\\hat{p}\\) the plausible interval for its confidence is:\n\\begin{equation} \\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{equation}\nwhere, \\(n\\) is the sample size, \\(\\hat{p}\\) is the point estimate, and \\(z*=1.96\\) is the critical value, the z-score denoting \\(95\\%\\) confidence (or any other desired confidence level).\nconditions for proportional confidence interval There are the conditions that make a proportional confidence interval work\ndistribution is normal \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\) are both \\(\u0026gt;10\\) we are sampling with replacement, or otherwise sampling \\(\u0026lt;10\\%\\) of population (otherwise, we need to apply a finite population correction value confidence intervals The expression is:\n\\begin{equation} \\bar{x} \\pm t^* \\frac{s}{\\sqrt{n}} \\end{equation}\nwhere \\(t*\\) is the \\(t\\) score of the desired power level with the correct degrees of freedom; \\(s\\) the sample standard deviation, \\(n\\) the sample size, and \\(\\har{x}\\) the mean.\n","permalink":"https://www.jemoka.com/posts/kbhconfidence_interval/","tags":null,"title":"confidence interval"},{"categories":null,"contents":"constructor theory deals with \u0026ldquo;constructors\u0026rdquo;, a general type of computer.\nconstructor theory can give us a theory of the universal quantum constructor by expanding upon quantum information theory. It allows us to unify quantum and classical information by simply defining operations in terms of counterfactuals exclusively: that a space is entirely defined by what\u0026rsquo;s possible and what\u0026rsquo;s not possible.\nAccording to constructor theory, fundamental laws are not dynamical laws instead are boundary conditions. We can take the boundary conditions to form the most general set of initial conditions.\nyou can conjecture a set of laws is fully complete at some point, you will find something that hits the bounds then you revise the theory ","permalink":"https://www.jemoka.com/posts/kbhconstructor_theory/","tags":null,"title":"constructor"},{"categories":null,"contents":"Cookie Theft is a Discourse-Completion Task that involves describing the following picture:\n","permalink":"https://www.jemoka.com/posts/kbhctp/","tags":null,"title":"Cookie Theft Picture Description Task"},{"categories":null,"contents":"quantum information theory requires manipulating counterfactual information\u0026mdash;not what the current known states are, but what are the next possible states.\nInside physics, there is already a few principles which are counterfactual.\nConservation of energy: a perpetual machine is *impossible Second law: its impossible to convert all heat into useful work Heisenberg\u0026rsquo;s uncertainty: its impossible to copy reliable all states of a qubit With the impossibles, we can make the possible.\n","permalink":"https://www.jemoka.com/posts/kbhcounterfactual/","tags":null,"title":"counterfactual"},{"categories":null,"contents":"\\begin{equation} cov(x,y) = E[XY]-E[X]E[Y] \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhcovariance/","tags":null,"title":"covariance"},{"categories":null,"contents":"coveather is a novel consensus algorithm based on the proof of work mechanism.\nSee also minimum user base requirements for coveather and Coveather Abstract.\n","permalink":"https://www.jemoka.com/posts/kbhcoveather/","tags":null,"title":"coveather"},{"categories":null,"contents":"Digital Health Passes (DHP), systems of digitally validating quarantine and vaccination status such as the New York IBM Excelsior Pass, demonstrate a lawful means to approach some benefits offered by \u0026ldquo;true elimination\u0026rdquo; treatment strategies-which focus on the complete elimination of cases instead of investing more in controlling the progression of the disease-of COVID-19. Current implementations of DHPs require region-based control and central storage of Protected Health Information (PHI)-creating a challenge to widespread use across different jurisdictions with incompatible data management systems and a lack of standardized patient privacy controls. In this work, a mechanism for decentralized PHI storage and validation is proposed through a novel two-stage handshaking mechanism update to blockchain proof-of-stake consensus. The proposed mechanism, when used to support a DHP, allows individuals to validate their quarantine and testing universally with any jurisdiction while allowing their right of independent movement and the protection of their PHI. Implementational details on the protocol are given, and the protocol is shown to withstand a 1% disturbance attack at only 923 participants via a Monte-Carlo simulation: further validating its stability.\n","permalink":"https://www.jemoka.com/posts/kbhcoveather_abstract/","tags":null,"title":"Coveather Abstract"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcram/","tags":null,"title":"cram"},{"categories":null,"contents":"crap to remember for AP Stats is a cram sheet for the AP Statistics exam.\n95% confidence: \\(z^*=1.96\\)\n\\(r=1\\): perfect positive correlation \\(r=-1\\): perfect negative correlation \\(r=0\\): no correlation S: standard deviation of residuals R-sq: how much of varience in dep. var can be explained by indp. var SE: estimate of standard deviation of the random var. that is slope.\nFor lines:\nNote that p value from regression outputs are two-tailed. So divide by 2 if you want a one-tail result.\nMultiplication changes mean as well as well as standard deviation. Adding changes mean but not standard deviation.\nExpected value of the sum and differences of random variables are just the sums and differences of their expected value. \\(S = X+Y, \\bar{S} = \\bar{X}+\\bar{Y}\\).\nVariance of random variables are just the sum and differences of their variance. \\(S=X+Y,{\\sigma^2}_S = {\\sigma^2}_X+{\\sigma^2}_Y\\).\n#WHAPS\nwhat test what hypothesis and what significance level assumptions and conditions; state! random independent: \\(\\le 10\\%\\) of population. t and z special: normal (z tests: \\(np, n(1-p) \\geq 10\\), t tests: \\(n\u0026gt;30\\) or given) chi-square special: \\(\\forall\\ EV \u0026gt; 5\\) p: z-statistic that would XD: Control (control for confounding and bias, placebo, etc.), Randomization (spread uncontrolled variability), Replication (need to have adequate units and ability to be repeated)\n=\u0026gt; Describing a distribution\nCenter: Mean, Median, or Mode? figure by skew Shape: Symmetric vs Skewed? Unimodal vs Bimodal Spread: Range and Inter-Quartile Range Outlier: anything more than 1.5*IQR away Context: what the distribution shows \u0026ldquo;Experimental Unit\u0026rdquo;: a physic entity that\u0026rsquo;s the primary unit of interest in a research objective.\nConditions for binomial distribution:\nBinary Independent Fixed number of trials All trials with same probability Conditions for geometric distrubiton\nBinary Independent Fixed number of successes All trials with same probability state the thing, state the conditions: \u0026ldquo;normal distribution with n= s=\u0026rdquo;, binomial distribution with n= p= etc.\n","permalink":"https://www.jemoka.com/posts/kbhcrap_to_remember_for_ap_stats/","tags":null,"title":"crap to remember for AP Stats"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcredit/","tags":null,"title":"credit"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcritical_value/","tags":null,"title":"critical value"},{"categories":null,"contents":"criticized the New Deal from all sides. Senator Huy P. Long claimed to \u0026ldquo;show our wealth.\u0026rdquo; nullification from conservative supreme court, FDR threatened to restructure + hurts his coalition.\nFDR ordered cuts in spending 1938 midterms: Republicans can block programs \u0026mdash; gained control of congress + created ability to gain control ","permalink":"https://www.jemoka.com/posts/kbhcriticism_of_the_new_deal/","tags":null,"title":"criticism of the New Deal (See file KBhnew_deal.org)"},{"categories":null,"contents":"constituents additional information lack of inverse of cross product The cross product doesn\u0026rsquo;t have an inverse\ngeometric interpretation of cross product \\begin{equation} a \\times b = |\\vec{a}| |\\vec{b}| \\sin \\theta n \\end{equation}\nwhere, \\(n\\) is the unit vector in some direction.\nThe length of the resulting vector in the cross product is the area of the parallelogram formed by the two vectors.\n","permalink":"https://www.jemoka.com/posts/kbhcross_product/","tags":null,"title":"cross product"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcultural_revolution/","tags":null,"title":"Cultural Revolution"},{"categories":null,"contents":"For data inference tasks, categorical data\n","permalink":"https://www.jemoka.com/posts/kbhdata_inference/","tags":null,"title":"data inference"},{"categories":null,"contents":"a student approach to learning where learning outcomes are driven by student\u0026rsquo;s own experience to deeply drive educational results independenlty\n","permalink":"https://www.jemoka.com/posts/kbhdeep_approach/","tags":null,"title":"deep approach"},{"categories":null,"contents":"Facts Everybody writes bugs Debugging sucks Defensive Programming Tools + Techniques Use language features Specs, documentations, Test-Driven Development, unit testing Fail fast and loudly Systematic debugging Investing in tools Use Language Features Descriptors: static, final, pub./priv. Type checking: prevent type errors Automatic array bounds checking Memory management Compiler optimization Key idea: know what language features are available, why/when to use them. don\u0026rsquo;t work against the language in circumventing them\nSpecs, Docs., TDD, Unit Tests How should it work: specs How does it work: docs How will I know it works: TDD How do I know it still works: unit tests These all force you to think about your code before!! you write it so then you can correct them as soon as possible.\nFailing Fast and Failing Loudly The earlier you recognize there is a problem, the easier it is to fix it Problems not fixed can be lost, covered up, or even relied upon Learn from every failure How do we put this into practice Use asserts, exceptions, logging Fix/diagnose/track every bug, even if you choose not to fix it Add regression tests for every bug + run them regularly Systematic Debugging Systematic Debugging is a framework for debugging software.\nReproduce the bug Reduce the bug to the smallest possible, repeatable test case Faster test cases mean faster iterations in debugging Smaller test cases help eliminate possible causes for error Find the root cause Study data (logs, behavior, etc.), hypothesis, experiment, repeat Change code and data to get more information FIXING SYMPTOM IS NOT ENOUGH Fix the bug Add a regression test, and run all tests Reducing Test Case Start with the data that uncovered the bug Remove pieces of data until the bug no longer occurs Bracketing: create both a test case that fails and similar test cases that pass Binary search: remove/add back half of the data at a time Can work from either end: start with everything and reduce until disappearance, or start with only one line and build until bug Finding the Cause Trace through the program View intermediate results Every iteration of a for loop Input and output of a given function Tools to use assert() printing/logging a debugger binary search Tooling! Linter Fuzzer Sanitizer Valgrind DTrace ","permalink":"https://www.jemoka.com/posts/kbhdefensive_programming/","tags":null,"title":"Defensive Programming"},{"categories":null,"contents":"demand-driven theory hypothesis that the reason why the Great Depression took place was because people were not buying stocks, etc, and there was no demand.\nSee also: Monetarist theory.\n","permalink":"https://www.jemoka.com/posts/kbhdemand_driven_theory/","tags":null,"title":"demand-driven theory"},{"categories":null,"contents":"DementiaBank is a shared database of multimedia interactions for the study of communication in dementia. There are a few projects being explored for DementiaBank.\nSee also: ADReSS Literature Survey\n","permalink":"https://www.jemoka.com/posts/kbhdementiabank/","tags":null,"title":"DementiaBank"},{"categories":null,"contents":"Ideas Can we correlate any longitudinal data with NACC?\nData dementia/English/Lanzi: Alyssa Lanzi\u0026rsquo;s new data\ndementia/English/Delaware\nWhat are the standard for acoustic features?\nMotor cortex/frontal control may also be impacted\nVocal tremer\nWhat are the predictors? How automatic can we make it?\n","permalink":"https://www.jemoka.com/posts/kbhdementiabank_acoustics_brainstoming/","tags":null,"title":"DementiaBank Acoustics Brainstoming"},{"categories":null,"contents":"The DementiaBank Acoustics Project is a working title for an acoustic-only challenge for AD detection. This document serves as the lab notebook for this project.\nThis project will attempt to replicate some of the results of Wang 2019 and Martinc 2021, but focusing on minimizing human involvement; we will first work on raw transcript classification with ERNIE (cutting all CHAT annotations), then introduce pause-encoding in a manner similar to Yuan 2021 which is automated by MFA. Goal is to replicate the results of Yuan 2021/or even Martinc 2021 in a completely automated manner.\nBackground Reading I first began by doing a literature survey on the ADReSS Challenge results published in the Frontiers AD special interest group issue.\nProposal And then, we wrote a proposal: DementiaBank Acoustics Project Proposal\nBrainstoming More notes from the meeting: DementiaBank Acoustics Brainstoming\nProtocol Notes July 1st Began by moving a subsample of Pitt\u0026rsquo;s Cookie Theft to pitt-7-1 in the raw data folder Ran flo on all collected samples. Arguments used are the same as that for batchalign, except we filter out the INV tier as we are detecting AD on patient and not investigator: so flo +d +ca +t* -tINV Moved all collected samples (and changed extension to .txt) to the same sub-folder, but in transcripts_nodisfluency July 2nd Created a dataprep script dataprep.py which dumps a pickled copy of cleaned data to transcripts_nodisfluency/pitt-7-1.dat. Created sliding windows of 5 pieces of dialogue concatenated, stored it in transcripts_nodisfluency/pitt-7-1-windowed.dat Used tencent/HuYong\u0026rsquo;s nghuyong/ernie-2.0-en Ernie 2.0 model, the continuous language model from Baidu (Layer:12, Hidden:768, Heads:12) July 4th Finalized training code. Selected base hyperparameters {bs: 8, epochs: 2, lr: 3e-3, length: 60}. Again, we are using Baidu\u0026rsquo;s nghuyong/ernie-2.0-en. Started training fastcalculator on 24bc812 train: faithful-frog-3 {bs: 8, epochs: 2, lr: 3e-3, length: 60, pitt-7-1-windowed.dat }\nCommentary: LR could be too high, looking at the divergent loss behavior. Decision: dropping bs to 4 and lr to 1e-5, similar to previous transformers. Also training for 3 epochs. train: revived-disco-5 {bs: 4, epochs: 3, lr: 1e-5, length: 60, pitt-7-1-windowed.dat }\nCommentary: quintessential overfitting Decision: Made the corpus bigger cleaned the entire Pitt corpus (pitt-7-4 in the raw folder) to become training data. Similar to pitt-7-1, ran flo on all collected samples; arguments used are the same as that for batchalign, except we filter out the INV tier as we are detecting AD on patient and not investigator: so flo +d +ca +t* -tINV; the flo\u0026rsquo;d results are in transcripts_nodisfluency. the notable difference between the previous dataset 7-1 and the current one 7-4 is that the 7-4 are prepended numbered by the task (cookie/100-01.cha \u0026gt; =cookie-100-01.txt) New (full) Pitt data as prepared above is ran though the dataprep script as of b325514cfad79da82d7a519ed29ea19ed87b2be4 (difference is that empty/dummy files are ignored), and pickled at transcripts_nodisfluency/pitt-7-4.dat and transcripts_nodisfluency/pitt-7-4-windowed.dat respectively. For new data, window size is still 5, splitting 10 cases out for testing now instead of 5. train: vocal-oath-6 {bs: 4, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed.dat}\nCommentary: high recall, low precision. Perhaps classes aren\u0026rsquo;t balanced? Spoiler alert: they are not. An inspection of data reveals that there is 3211 rows of dementia, 2397 rows of control Decision: Created pitt-7-4-bal and pitt-7-4-windowed-bal series of data based on dataprep.py on 703f79248a20fd7a13a5033ca2bf7f691f42c941. This version force-crops to make sure that the dementia and control indicies have the exact same length for each class. train: helpful-leaf-7 {bs: 4, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat}\nBeautiful. Question now is whether or not there is data leakage/external heuristics. It is a good time to do some LOOCV. Getting this result without any disfluency calculations seems unlikely.\nBut anyways, going to discuss these results as they seem to meet results we see in Yuan 2021, even without top-N ensemble; though this is one trial, LOOCV may still show that we actually need it.\nJuly 5th Began the day with creating the script k-fold validation; I originally hoped to exactly replicate the procedure of Yuan 2021 for comparability, but, not sure how they got the actual result of a min/max range with LOOCV on binary; therefore, we will instead create a 95% confidence interval analysis via a single-variable t test on standard k-fold validation. K=50 During one-off testing, another set of hyperparameters seems to work too: {bs: 72, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat}. As we have not begun tuning for hyperparameters, we are just going to use this set, K=50, for the first k-fold trial. k-fold: F4ZVbGfdBAQvtvXemWZCZD code: 55f77ff1dea03c3ed66967864dc52fd2c0062f23\n{bs: 72, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat} K = 50\nIt seems like the results we got is consistent and validates in a manner which we expect.\nJuly 7th Yesterday was a day filled with working on batchalign, but we are back now. Today, I aim to look into the heuristic that I identified yesterday by playing with the model, which is that it seems like the model prefers the use of long-focused sentences about cookies, so the heruistic its picking up is probably on-topicness.\nI am going to first leverage the lovely cdpierse/transformers-interpret tool to help build some explainability by adding it to validate.py. Upon some human validation with random sampling, the model seem to do less well than I\u0026rsquo;d hoped. Running a train cycle with the new results/params seen above to see if it does better.\ntrain: brisk-oath-10 {bs: 72, epochs: 3, lr: 1e-5, length: 60, pitt-7-4-windowed-bal.dat}\nCommentary: It seems like the model is doing overall worse from validation data, but it does fairly well during test data. Decision: I can fairly confidently claim that the model is just fitting on topic. As in, if the topic is about cookies (theft/taking/cookie/mother/etc.), it will be classified as control. One thing that we can do is to claim this task as directly task-controlled: that is, include no data except cookie and control for that difference Then, the model would\u0026rsquo;t be able to predict the result b/c the variation in topic won\u0026rsquo;t have influence. This is going to be prepared in the cookiepitt-7-7-bal* based on dataprep.py in commit 518dec82bb961c0a8ad02e3080289b56102aa1a2 train: super-durian-11 {bs: 72, epochs: 3, lr: 1e-5, length: 60, cookiepitt-7-7-windowed-bal.dat}\nCommentary: the model is no where near convergence Decision: multiplying the LR by 10 train: floral-sunset-12 {bs: 72, epochs: 3, lr: 1e-4, length: 60, cookiepitt-7-7-windowed-bal.dat}\nCommentary: There we go. This seem to be more in line with what we see in Yuan 2021 Decision: ok, let\u0026rsquo;s elongate the actual content. Perhaps we can try a 7-element search instead? This is written as cookiepitt-7-7-*-long. Code based on 9e31f4bc13c4bfe193dcc049059c3d9bda46c8d0 train: sweet-plasma-13 {bs: 72, epochs: 3, lr: 1e-4, length: 60, cookiepitt-7-7-windowed-long-bal.dat}\nCommentary: underfitting Dropping batch size down to 64 to add more steps train: smart-river-14 {bs: 64, epochs: 3, lr: 1e-4, length: 60, cookiepitt-7-7-windowed-long-bal.dat}\nCommentary: this finally fits to the specifications which Yuan 2021 have revealed Decision: running k-fold on this architecture k-fold: XgsP4FVS6ScFxCZKFJoVQ5. Code: 3870651ba71da8ddb3f481a7c3e046397a09d8b2\nJuly 8th Began the day with aligning the entirety of cookie for both control and dementia, named the dataset alignedpitt-7-8 in the RAW folder\nPer what we discussed, will add [pause] as a token to the model. Then, transcript the text such that it would contain normalized values to the pauses for pauses \u0026gt; 0.250 seconds. Therefore, the data would look like\n\u0026ldquo;hello my name is [pause] 262 [pause] bob\u0026rdquo;\nJuly 9th Created transcript.py, which coverts the data in raw to transcripts_pauses, which contains pause values \u0026gt; 250 msc and prepends them with [pause] tokens The code from above is taken from check.py in batchalign, used transcript.py from 7e19a4912cf0ad5d269c139da5ce018615495ebb to clean out the dataset; placed it in similar txt format to alignedpitt-7-8 Ran dataprep with window size of 5, created alignedpitt-7-8.bat and alignedpitt-7-8-windowed.bat as the dataprep file starting a new training run, with [pause] added as a new token, code 06846c6c95e6b1ccf17f0660c5da76aa50231567 train: golden-tree-16 {bs: 64, epochs: 3, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}\nSo realistically, we have the same F1 between the two, but pause encoding increased the accuracy of prediction yet dropped recall dramatically.\nAs a random check, let\u0026rsquo;s find out if simple fine-tuning (only training on classifier) would work, so:\ntrain: jumping-blaze-17 {bs: 64, epochs: 3, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}. This time with only training classifier.\nCommentary: we did not like. start coverging Bumping LR by a factor of 10 train: vital-water-18 {bs: 64, epochs: 3, lr: 1e-3, length: 60, alignedpitt-7-8-windowed.dat}. This time with only training classifier.\nCommentary: barely started converging, seem to be a local Training for 2 more epochs train: fiery-smoke-19 {bs: 64, epochs: 5, lr: 1e-3, length: 60, alignedpitt-7-8-windowed.dat}. This time with only training classifier.\nCommentary: classic overfitting At this point, unlocking the model would probably be a good bet\ntrain: leafy-deluge-20 {bs: 64, epochs: 5, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}.\nTraining once again with code without locking, and bump LR down\nCommentary: classic the recall is slowly creeping up Decision: let\u0026rsquo;s go for 8 epochs train: royal-pond-21 {bs: 64, epochs: 8, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}.\nCommentary: let\u0026rsquo;s run k-fold now, with these settings.\nk-fold: QskZWfEsML52ofcQgGujE2. {bs: 64, epochs: 8, lr: 1e-4, length: 60, alignedpitt-7-8-windowed.dat}.\nOk, the base hypothesis from Yuan 2021 is very much confirmed here. The same training, same content, but pause encoding is very beneficial to the quality of the results. The results that they reported contained an ensemble data, which is in the high 80s; we can now continue doing something new as Yuan 2021\u0026rsquo;s conclusion is fairly achieved.\nWe can probably call the replication stage done, with no dramatically better effect.\nJuly 10th FluCalc! Leonid\u0026rsquo;s lovely new program can be an uberuseful feature extraction tool Let\u0026rsquo;s try using to build a new dataset, and network. FluCalc + Pause Encoding + Textual Data late fusion This is becoming alignedpitt-7-8-flucalc. As the program is currently under heavy development to include results from batchalign, we will specify version V 09-Jul-2022 11:00 for now. Done, the new data has the same i/o shape, but then has a bunch of features filtered for nulls which contains outputs from flucalc. Again, alignedpitt-7-8-flucalc from 4346fc07c4707343c507e32786b6769b6bd6fb49 does not take into account results from the %wor tier! July 11th ab19abd6486884141c9ab4e4e185255a77ae833e is the final-ish version of the late fusion model We are going to use alignedpitt-7-8-flucalc to start training train: royal-pond-21 {bs: 64, epochs: 8, lr: 1e-4, length: 60, alignedpitt-7-8-flucalc-windowed.dat}.\nCommentary: overfitting Decision, droping lr by a factor of 10, also increasing length to 70 train: fallen-dust-25 {bs: 64, epochs: 8, lr: 1e-5, length: 70, alignedpitt-7-8-flucalc-windowed.dat}.\nCommentary: overfitting Decision, droping lr by a factor of 10, dropping batch size to 32, training more to 10 train: dainty-meadow-26 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}.\nah\nAt this point, I think it\u0026rsquo;d be good to do some feature selection Let\u0026rsquo;s do a chi^2 correlation, and select 3 best features import pandas as pd DATA = \u0026#34;/Users/houliu/Documents/Projects/DBC/data/transcripts_pauses/alignedpitt-7-8-flucalc-windowed.bat\u0026#34; # read pickle df = pd.read_pickle(DATA) # test test_data = df[df.split==\u0026#34;test\u0026#34;] # also, get only train data df = df[df.split==\u0026#34;train\u0026#34;] df target mor_Utts ... split utterance trial sample ... 120-2 1049 1 -0.179084 ... train well the boy is getting some cookies handing o... 336-1 2492 0 -0.481740 ... train +oh okay, the the little girl askin(g) for the... 076-4 786 1 -0.179084 ... train well the little boy was looking at that cookie... 279-0 2250 1 1.980274 ... train kid\u0026#39;s stool turnin(g) [pause]540[pause] over s... 014-2 151 1 0.746355 ... train he\u0026#39;s fallin(g) off the chair down here or try... ... ... ... ... ... ... 208-0 1655 0 -0.481740 ... train the boy [pause]920[pause] is going after [paus... 492-0 2696 1 -0.179084 ... train oh yes quite a_lot the kid\u0026#39;s tryin(g) to get t... 497-1 2727 1 0.129396 ... train what else ? \u0026amp;uh the see the [pause]2400[pause]... 175-2 1535 0 0.863668 ... train the window is open you can see out the curtain... 279-0 2261 1 1.980274 ... train the other kid with [pause]610[pause] the stool... [2848 rows x 44 columns] Let\u0026rsquo;s slice out the bits which is labels, etc.\nin_data = df.drop(columns=[\u0026#34;utterance\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;split\u0026#34;]) in_data.columns Index([\u0026#39;mor_Utts\u0026#39;, \u0026#39;mor_Words\u0026#39;, \u0026#39;mor_syllables\u0026#39;, \u0026#39;#_Prolongation\u0026#39;, \u0026#39;%_Prolongation\u0026#39;, \u0026#39;#_Broken_word\u0026#39;, \u0026#39;%_Broken_word\u0026#39;, \u0026#39;#_Block\u0026#39;, \u0026#39;%_Block\u0026#39;, \u0026#39;#_PWR\u0026#39;, \u0026#39;%_PWR\u0026#39;, \u0026#39;#_PWR-RU\u0026#39;, \u0026#39;%_PWR-RU\u0026#39;, \u0026#39;#_WWR\u0026#39;, \u0026#39;%_WWR\u0026#39;, \u0026#39;#_mono-WWR\u0026#39;, \u0026#39;%_mono-WWR\u0026#39;, \u0026#39;#_WWR-RU\u0026#39;, \u0026#39;%_WWR-RU\u0026#39;, \u0026#39;#_mono-WWR-RU\u0026#39;, \u0026#39;%_mono-WWR-RU\u0026#39;, \u0026#39;Mean_RU\u0026#39;, \u0026#39;#_Phonological_fragment\u0026#39;, \u0026#39;%_Phonological_fragment\u0026#39;, \u0026#39;#_Phrase_repetitions\u0026#39;, \u0026#39;%_Phrase_repetitions\u0026#39;, \u0026#39;#_Word_revisions\u0026#39;, \u0026#39;%_Word_revisions\u0026#39;, \u0026#39;#_Phrase_revisions\u0026#39;, \u0026#39;%_Phrase_revisions\u0026#39;, \u0026#39;#_Pauses\u0026#39;, \u0026#39;%_Pauses\u0026#39;, \u0026#39;#_Filled_pauses\u0026#39;, \u0026#39;%_Filled_pauses\u0026#39;, \u0026#39;#_TD\u0026#39;, \u0026#39;%_TD\u0026#39;, \u0026#39;#_SLD\u0026#39;, \u0026#39;%_SLD\u0026#39;, \u0026#39;#_Total_(SLD+TD)\u0026#39;, \u0026#39;%_Total_(SLD+TD)\u0026#39;, \u0026#39;Weighted_SLD\u0026#39;], dtype=\u0026#39;object\u0026#39;) And the labels:\nout_data = df[\u0026#34;target\u0026#34;] out_data trial sample 120-2 1049 1 336-1 2492 0 076-4 786 1 279-0 2250 1 014-2 151 1 .. 208-0 1655 0 492-0 2696 1 497-1 2727 1 175-2 1535 0 279-0 2261 1 Name: target, Length: 2848, dtype: int64 And now, let\u0026rsquo;s select 3 best features.\nfrom sklearn.feature_selection import SelectKBest, f_classif k_best_tool = SelectKBest(f_classif, k=3) k_best_tool.fit(in_data, out_data) best_features = k_best_tool.get_feature_names_out() best_features %_WWR %_mono-WWR %Total(SLD+TD) OD = other disfluencies; SLD = stuttering-like disfluencies; TD = total disfluencies; WWR = whole-word-repetition\nok, let\u0026rsquo;s select those features\ntrain: visionary-plasma-27 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}. Also with feature selection.\nhmmm.\nI am curious if we just ran something like a decision tree, what happens.\nin_features = df.drop(columns=[\u0026#34;utterance\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;split\u0026#34;]) test_features = test_data.drop(columns=[\u0026#34;utterance\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;split\u0026#34;]) in_targets = df[\u0026#34;target\u0026#34;] test_targets = test_data[\u0026#34;target\u0026#34;] seed the classifier, and fit.\nfrom sklearn.ensemble import RandomForestClassifier clsf = RandomForestClassifier() clsf.fit(in_features, in_targets) clsf.score(test_features, test_targets) 0.5932203389830508 OK nevermind. What about SVC?\nfrom sklearn.svm import SVC clsf = SVC() clsf.fit(in_features, in_targets) clsf.score(test_features, test_targets) 0.5932203389830508 Turns out, deep learning still does better. I\u0026rsquo;m thinking maybe the output is being faulty, say, for something like the loss function.\nDecision: switching activation to sigmoid.\ntrain: sunny-bush-31 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nOk let\u0026rsquo;s think about this. Decision: added batch normalization.\ntrain: autumn-jazz-32 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nThe model maybe overfitting on some simple heuristic; some basic statistics revealed that these variables are actually quite differently distributed.\nPerhaps we should increase the complexity of the model?\ntrain: fallen-microwave-33 {bs: 32, epochs: 10, lr: 1e-6, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nJust to test, I am bumping the LR to 1e-5, just to see what happens. I am very confused.\ntrain: upbeat-flower-35 {bs: 32, epochs: 10, lr: 1e-5, length: 70, alignedpitt-7-8-flucalc-windowed.dat}, selected features\nThe more we work on this, the more overfit it gets. (I FORGOT A RELUCTIFIER)\na note {bs: 32, epochs: 10, lr: 1e-5, length: 70, alignedpitt-7-11-flucalc-windowed.dat}, selected features\nPauses, no meta:\nPauses, meta:\nso effectively cointoss\nConcerns and Questions July 2nd pitt7-1/dementia/493-0 PAR tier \u0026ldquo;tell me everything you see going on in that picture\u0026rdquo; doesn\u0026rsquo;t seem to be labeled correctly; I am guessing that\u0026rsquo;s supposed to be INV? Has anyone tried to include investigator/participant cross-dialogue? July 4th Is the model overfitting on antiquated language? Is the model overfitting on cooke-theft on-topic-ness? July 11th LSTM only on pauses? ","permalink":"https://www.jemoka.com/posts/kbhdementiabank_acoustics_project/","tags":null,"title":"DementiaBank Acoustics Project"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdepression/","tags":null,"title":"depression"},{"categories":null,"contents":"Derivat\n","permalink":"https://www.jemoka.com/posts/kbhderivational_words/","tags":null,"title":"derivational words"},{"categories":null,"contents":"a\n","permalink":"https://www.jemoka.com/posts/kbhderivatives/","tags":null,"title":"derivative (finance)"},{"categories":null,"contents":"We will take \\(G(P(t),t)\\) to figure the price of an option, with \\(t\\) being time, strike price \\(X\\) (not introduced yet), and expiration date \\(T \u0026gt; t\\) on a stock with price \\(P(t)\\) at time \\(t\\).\nThis representation does something really important: it expresses \\(G\\) as a function of only the current stock price \\(P(t)\\).\n","permalink":"https://www.jemoka.com/posts/kbhderivative_pricing/","tags":null,"title":"Derivative Pricing"},{"categories":null,"contents":"A derived variable is a mapping between states to a set, usually the natural numbers. Remember, if we can, given a state and match it to a number and show a relation which would iterate the state and decrease the states\u0026rsquo; number. We can show that the algorithm terminates.\n","permalink":"https://www.jemoka.com/posts/kbhderived_variable/","tags":null,"title":"derived variable"},{"categories":null,"contents":"A health concern relating to glucose and obesity.\n","permalink":"https://www.jemoka.com/posts/kbhdiabetes/","tags":null,"title":"diabetes"},{"categories":null,"contents":"We have a function:\n\\begin{equation} |x|+|y|\\frac{dy}{dx} = \\sin \\left(\\frac{x}{n}\\right) \\end{equation}\nWe are to attempt to express the solution analytically and also approximate them.\nTo develop a basic approximate solution, we will leverage a recursive simulation approach.\nWe first set a constant \\(N\\) which in the \\(N\\) value which we will eventually vary.\nN = 0.5 We can get some values by stepping through \\(x\\) and \\(y\\) through which we can then figure \\(\\frac{dy}{dx}\\), namely, how the function evolves.\n# cache res = [] # number of steps steps = 1000 # seed values x = -5 y = 5 # step size step = 1/100 # for number of setps for _ in range(steps): # get the current equation and slope solution dydx = (sin(x/N)-abs(x))/abs(y) # append result res.append((x,y,dydx)) # apply the slope solution to iterate next y # step size is defined by `step` x += step y += dydx*step We have now a set of analytic solutions \\((x,y,\\frac{dy}{dx})\\). Let\u0026rsquo;s plot them!\nscatter_plot([i[0:2] for i in res]) Great, now we have a fairly non-specific but \u0026ldquo;correct\u0026rdquo; solution. We are now going to try to derive an analytic solution.\nWait\u0026hellip; That\u0026rsquo;s not the solution we got! But\u0026hellip; its close: the blue line simply need to be reflected across the \\(x\\) axis.\nIts actually fairly apparent why we will need this negative. We just declared that \\(y\\) was negative for that portion of the solution; the output of a square root could never be negative, so of course to achieve \\(y\\) being negative we have to take into account that square roots have a possible negative output as well.\nNice; now our analytical results agree with out numerical results.\n\\begin{equation} \\begin{cases} y\u0026gt;0 \u0026amp; y=\\sqrt{-2n\\cos\\left(\\frac{x}{n}\\right)-x\\vert x\\vert} +C \\\\ y\u0026lt;0 \u0026amp; y=-\\sqrt{2n\\cos\\left(\\frac{x}{n}\\right)+x\\vert x\\vert}+C \\end{cases} \\end{equation}\nMoving on to the result of the questions.\nSolution behavior The solution are unbounded and mostly decreasing. As \\(n\\in [-1,1]\\), the solution becomes unstable; a solution does not exist at \\(n=0\\).\nAt \\(n=0.5\\), a solution passes through \\((0,-1)\\).\n","permalink":"https://www.jemoka.com/posts/kbhchallenge_1/","tags":null,"title":"DiffEq: Challenge #1"},{"categories":null,"contents":"Textbooks The textbook that we will be using is (Taylor 2011)\nContent Simple Differential Equations solving differential equations Uniqueness and Existance eigenvalue applying eigenspace Second-Order Linear Differential Equation Assignments Challenge #1 The Unreasonable Effectiveness of Mathematics in the Natural Sciences ","permalink":"https://www.jemoka.com/posts/kbhdiffeq_intro/","tags":["Index"],"title":"Differential Equations"},{"categories":null,"contents":"The dimension of a vector space is the length of any basis in the vector space. It is denoted as \\(\\dim V\\).\nadditional information See also finite-dimensional vector space and infinite-demensional vector space\ndimension of subspace is smaller or equal to that of its parent If we have a finite-dimensional \\(V\\) and a subspace thereof \\(U\\), then \\(\\dim U \\leq \\dim V\\).\nFirstly, the every subspace of a finite-dimensional vector space is a finite-dimensional vector space is itself a finite-dimensional vector space. Therefore, it has a finite dimension.\nThen, we will simply think of the basis of \\(U\\) as an linearly independent list in \\(V\\); and of course, the basis of \\(V\\) spans \\(V\\). As length of linearly-independent list \\(\\leq\\) length of spanning list, we have that length of basis of \\(U \\leq\\) length of basis of \\(V\\).\nThis makes \\(\\dim U \\leq \\dim V\\), as desired. \\(\\blacksquare\\)\nlists of right length are a basis These are two results that tell us if you are given a list of list of right length, one condition (spanning or linear independence) can tell you that they are a basis. It\u0026rsquo;s also known (as a John McHugh special:tm:) as the Half Is Good Enough theorems.\nlinearly independent list of length dim V are a basis of V Begin with an linearly independent list in \\(V\\) of length \\(\\dim V\\). We aim to extend this list into a basis of \\(V\\).\nAs we know all basis in \\(V\\) must have length \\(\\dim V\\), and the list is already length \\(\\dim V\\), no extension is needed to form a basis.\nAs every linearly independent list expends to a basis, we conclude that the list is already a basis of \\(V\\), as desired \\(\\blacksquare\\).\nspanning list of length of dim V are a basis of V Begin with a spanning list in \\(V\\) of length \\(\\dim V\\). We aim to reduce this list into a basis of \\(V\\).\nAs we know all basis in \\(V\\) must have length \\(\\dim V\\), and the list is already length \\(\\dim V\\), no reduction is needed to form a basis.\nAs all spanning lists contains a basis of which you are spanning, we conclude that the list is a basis of \\(V\\), as desired \\(\\blacksquare\\).\ndimension of sums See dimension of sums\n","permalink":"https://www.jemoka.com/posts/kbhdimension/","tags":null,"title":"dimension"},{"categories":null,"contents":"A direct sum is a sum of subspaces (not just subsets!!) where there\u0026rsquo;s only one way to represent each element.\nconstituents subspaces of \\(V\\) named \\(U_1, \\dots, U_{m}\\)\nrequirements The sum of subsets of \\(U_1+\\dots+U_{m}\\) is called a direct sum IFF:\neach element in \\(U_1+\\dots +U_{m}\\) can only be written in one way as a sum \\(u_1 +\\dots +u_{m}\\) (as in, they are linearly independent?)\nWe use \\(\\oplus\\) to represent direct sum.\nadditional information why is it called a direct sum? Something is not a direct sum if any of its components can be described using the others. Its kind of line linear independence but! on entire spaces.\na sum of subsets is a direct sum IFF there is only one way to write \\(0\\) Given \\(U_1, \\dots, U_{m}\\) are subspaces of \\(V\\), then \\(U_1+\\dots +U_{m}\\) is a direct sum IFF the only way to write \\(0\\) as a sum \\(u_1 +\\dots +u_{m}\\) is by taking each element to \\(0\\).\nProof:\nif\u0026mdash; If some \\(U_1 + \\dots +U_{m}\\) is a direct sum, definitionally there is only one way to write \\(0\\). And you can always write \\(0\\) by taking all the constituents to \\(0\\) as they are subspaces, so the additive identity exists.\nonly if\u0026mdash; We are given that there is only one way to write \\(0\\), that:\n\\begin{equation} 0 = u_1+ u_2+ \\dots+ u_{m}: u_j \\in U_{j} \\end{equation}\nas \\(U_{j}\\) are all subspaces, and the additive identity exists, we can say that \\(u_1=u_2=\\dots =0\\).\nAssume for the sake of contradiction that \\(U_1 + \\dots +U_{m}\\) is not a direct sum. Therefore:\n\\begin{equation} \\exists\\ v_1 = u_1+u_2+\\dots + u_{m}: u_{j} \\in U_{j} \\end{equation}\nand\n\\begin{equation} \\exists\\ v_1 = w_1+w_2+\\dots + w_{m}: w_{j} \\in U_{j} \\end{equation}\n\u0026ldquo;there are two unique representations of a vector given the sum of subsets\u0026rdquo;\nSubtracting these representations, then:\n\\begin{equation} (v_1-v_1) = (u_1-w_1) + \\dots +(u_{m}-w_{m}): u_{j}, w_{j} \\in U_{j} \\end{equation}\nFinally, then:\n\\begin{equation} 0 = (u_1-w_1) + \\dots +(u_{m}-w_{m}): u_{j}, w_{j} \\in U_{j} \\end{equation}\nWe have established that each slot that makes up this particular sum \\(=0\\). Therefore, \\(u_{i}-w_{i} = 0\\). This means $ui=wi$\u0026mdash;there are no two unique representations of \\(v_{1}\\). Reaching contradiction. \\(\\blacksquare\\)\na sum of subsets is only a direct sum IFF their intersection is set containing \\(0\\) Take \\(U\\) and \\(W\\), two subspaces of \\(V\\). \\(U+V\\) is a direct sum IFF \\(U \\cap W = \\{0\\}\\).\nProof:\nif\u0026mdash; Suppose \\(U+V\\) is a direct sum. \\(\\forall v \\in U \\cap V\\), as \\(v\\) is equal to itself, we have that:\n\\begin{equation} 0 = v+(-v) \\end{equation}\nwhere, \\(v\\) is in \\(U\\) and \\(-v\\) is in \\(V\\) (as both \\(U\\) and \\(V\\) are vector spaces, both would contain \\(-1v=-v\\) as we are given \\(v \\in U \\cap V\\) and scalar multiplication is closed on both.)\nBy the unique representation in the definition of direct sums, you have only one way to construct this expression: namely, that \\(v=0\\) as both are vector spaces so the additive identity exists on both.\nHence:\n\\begin{equation} \\{0\\} = U \\cap V \\end{equation}\nonly if\u0026mdash; Suppose \\(U \\cap W = \\{0\\}\\). Take also \\(u \\in U\\) and \\(w \\in W\\); we can construct an expression:\n\\begin{equation} u + w = 0 \\end{equation}\nIf we can show that there is only one unique combination of \\(u\\) and \\(w\\) to write \\(0\\), we satisfy the previous proof and therefore \\(U+W\\) is a direct sum.\nThe expression above implies that \\(w\\) is the additive inverse of \\(u\\); therefore; \\(u = -w\\). As both \\(U\\) and \\(W\\) are vector spaces, their elements all have inverses. As \\(u\\) is the inverse of \\(w\\), and given the definition of sum of subsets that \\(u \\in U\\) and \\(w \\in W\\), \\(u\\) and \\(w\\) are both in both \\(U\\) and \\(W\\).\nAs the intersection of \\(U\\) and \\(V\\) is \\(0\\), \\(u=w=0\\). Therefore, there is only one unique representation of \\(0\\), namely with \\(u=0,w=0\\), making \\(U+W\\) a direct sum. \\(\\blacksquare\\)\ndirect sum proofs are not pairwise! Those two proofs above only deal with pairs of sum of subsets. If you have multiple subsets, they don\u0026rsquo;t apply!\nevery subspace of \\(V\\) is a part of a direct sum equaling to \\(V\\) For every subspace \\(U\\) of a finite-dimensional \\(V\\), there is a subspace \\(W\\) of \\(V\\) for which \\(V = U \\oplus W\\).\nBecause \\(V\\) is defined to be finite-dimensional, and the fact that a finite-dimensional subspace is finite-dimensional, \\(U\\) is finite-dimensional.\nTherefore, because every finite-dimensional vector space has a basis, \\(U\\) has a basis \\(u_1, \\dots u_{m}\\).\nBecause bases are linearly independent, and \\(U \\subset V\\), \\(u_1, \\dots u_{m}\\) is a linearly independent list in \\(V\\).\nBecause a linearly independent list expends to a basis, we can construct \\(u_1, \\dots u_{m}, w_{1}, \\dots w_{n}\\) as the basis of \\(V\\). We will construct a \\(W = span(w_1, \\dots w_{n})\\) \u0026mdash; the space formed as the span of the \u0026ldquo;extension\u0026rdquo; vectors to make the basis in \\(V\\).\nBecause the list \\(u_{j}\\dots w_{k}\\) we made is a basis in \\(V\\), \\(U+W=V\\).\nYou can see this because every element \\(v \\in V\\) can be constructed with a linear combination \\(u_1, \\dots u_{m}, w_{1}, \\dots w_{n}\\) (again, because this list shown to be a basis of \\(V\\) therefore it spans \\(V\\).) Then, to show that \\(U+W=V\\), we can collapse \\(a_{1}u_1\\dots + a_{m}u_{m}=u \\in U\\), and \\(c_{1}w_1 \\dots +c_{m}w_{m} = w \\in W\\). Hence, every element \\(v \\in V\\) can be constructed by some \\(u \\in U + w \\in W\\), making \\(U+W=V\\).\nNow, we have to show that the combination is a direct sum. There is a few ways of going about this, the one presented by Axler is leveraging the fact that a sum of subsets is only a direct sum IFF their intersection is set containing \\(0\\)\u0026mdash;that \\(U \\cap W = \\{0\\}\\).\nGiven some element \\(v\\) that lives in the intersection between \\(U\\) and \\(W\\), it must be formed as a linear combination of two linearly independent lists (as \\(u_j, \\dots w_{j}\\) is a basis, they are linearly independent.)\nIntuition: if an non-zero element lives in the intersection between two linearly independent lists which together is still linearly independent, it must be able to be written by a linear combination of other elements of that linearly independent list to live in the intersection of the two lists\u0026mdash;which is absurd (violates the definition of linearly dependent). The only element for which this is an exception is \\(0\\).\nActual proof:\nsuppose \\(v \\in U \\cap W\\), so \\(v = a_1u_1\\dots +a_{m}v_{m}\\) as well as \\(v=b_1w_{1} + \\dots b_{n}w_{n}\\). Subtracting the two lists results in:\n\\begin{equation} 0 = a_1u_1+ \\dots a_{m} u_{m} - b_1w_1+ \\dots +b_{n}w_{n} \\end{equation}\nhaving already declared this list linearly independent, we see that each scalar \\(a_1, \\dots -b_{n}\\) must equal to \\(0\\) for this expression. Therefore, the intersection \\(v\\) must be \\(\\{0\\}\\) as \\(0u_1 + \\dots +0u_{m}=0\\).\n","permalink":"https://www.jemoka.com/posts/kbhdirect_sum/","tags":null,"title":"direct sum"},{"categories":null,"contents":"discourse features are marks of fluency/etc. which mark one\u0026rsquo;s speech.\n","permalink":"https://www.jemoka.com/posts/kbhdiscourse_features/","tags":null,"title":"discourse features"},{"categories":null,"contents":"A Discourse-Completion Task is a tool used to elicit speech acts, such as showing an image, etc. For instance,\ntypes of Discourse-Completion Tasks oral lexical retrival Cookie Theft ","permalink":"https://www.jemoka.com/posts/kbhdiscourse_completion_task/","tags":null,"title":"Discourse-Completion Task"},{"categories":null,"contents":"distributed algorithm is a type of algorithm that can be distributed across many modules.\nThere are a few core areas of research:\nfailure-proofing nodes is a distributed algorithm What if one processor fails? communication in a distributed algorithm What if communication between processors fails? What if timing fails? atomicity atomicity is a property of distributed algorithm where, for a set of steps, a processor can only do one or all of the steps. i.e.: if you are asking a node to do something, it can either do all of the thing or be able to roll back as if the entire thing didn\u0026rsquo;t happen.\nleader election (algorithms) leader election is the process by which a distributed algorithm elects the driving node among similar nodes.\nconsensus (algorithms) consensus is a mechanism in a distributed algorithm where the solution requires multiple processes to do the same calculation to confirm.\nalgorithms designed to be distributed MapReduce ","permalink":"https://www.jemoka.com/posts/kbhdistributed_algorithum/","tags":null,"title":"distributed algorithm"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdistributed_morphology/","tags":null,"title":"distributed morphology"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdistributivity/","tags":null,"title":"distributivity"},{"categories":null,"contents":"Dopamine optical sensor. When dopamine is bound, it floreses and can detect micromolar changes and dopamine concentration.\n","permalink":"https://www.jemoka.com/posts/kbhdlight_1/","tags":null,"title":"dLight 1"},{"categories":null,"contents":"See also Software Development Methodologies\ndocumentation Comments Readme Wiki specification UX UI High-Level Architecture (libraries, external APIs) Low-Level Architecture (modules, functions, internal APIs) commenting Almost anything hardcoded (constants, strings, etc.) Anything confusing, tricky, nonstandard Historical notes: if something is added/removed, write it down TODO for bugs or hacks README Files Best used as a quick-start guide What are key pieces of info they will pieces of info they will need? What is your code supposed to do? How does someone run your code? How does a new engineer get set up? General overview of how things are laid out, with links to wiki pages with details Wiki In-depth explanation of subsystems and modules Separate pages for each subsystem Include decisions of their design decisions Discussions of why systems are not designed differently UI/UX Spec How do we know what the software is supposed to do? Varying levels of resolution User stories All the way up to granular details of UI elements Don\u0026rsquo;t forgot to document defaults!\n","permalink":"https://www.jemoka.com/posts/kbhdocumentation_and_specification/","tags":null,"title":"Documentation and Specification"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdopamine/","tags":null,"title":"dopamine"},{"categories":null,"contents":"The dopamine circuitry in NF1.\nGenetically encoded \u0026ldquo;sensors\u0026rdquo; to measure circuits.\n","permalink":"https://www.jemoka.com/posts/kbhdopamine_circuitry_in_nf1/","tags":null,"title":"dopamine circuitry in NF1"},{"categories":null,"contents":"orthogonality test The dot product is an orthogonality test. If the dot product between the two vectors is \\(0\\), they are definitely orthogonal.\ngeometric interpretation of the dot product Well, we have some shape between two vectors; then, we can first write out the law of cosines. Then, we can see that, for two vectors from the same origin, we can say that the projection of vector \\(\\vec{A}\\) onto \\(\\vec{B}\\) is written as:\n\\begin{equation} |\\vec{A}||\\vec{B}|\\cos \\theta \\end{equation}\nwhere, \\(\\theta\\) is the angle between the two vectors.\n","permalink":"https://www.jemoka.com/posts/kbhdot_product/","tags":null,"title":"dot product"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdouble_slit_experiment/","tags":null,"title":"double slit experiment"},{"categories":null,"contents":"A human gene similar to the gene PreTA found in E. Coli, a bacterial found in microbiome. See effects of PreTA on Fluoropyrimidine, and by proxy Capecitabmine for implications on cancer treatment.\n","permalink":"https://www.jemoka.com/posts/kbhdpyd/","tags":null,"title":"DPYD"},{"categories":null,"contents":"Gah I have to do this. Not for public consumption. California laws 2022 DL600 R7 2022.\nConsequences Not licensed If unlicensed person is drivnig your car, it maybe impounded for 30 days Hired to drive interstate commercially need to be older than 21, also need to be older than 21 to transport hazardous materials Class C License Driving #knw Two axle vehicle with a GVWL of 26,000 lbs or less Three axle vehicle weighing 6,000 lbs or less House car \u0026lt; 40 feet or less Three wheel motocycles Vanpool vehicle designed to carry between 10 and no more than 15 people Towing #knw Single vehicle of 10,000 or less Vehicle weighing 4000 lbs or more unladen Trailer coach under 10,000 lbs Fifth wheel trailer exceeding 10,000 lbs but under 15,000 lbs, with endorsement Mor ethings Class C drivers can\u0026rsquo;t tow more than one Motor vehile weigning under 4000 lbs cannot tow more than 6000 lbs Getting in trouble Get a traffic ticket and fail to show up to court: suspend driving One at fault collision or one at fault traffic violation: may take action? Two of either at fault collision or violation conviction: no driving for 30 days unless accompanied by 25 year old adult Three of \u0026ldquo;\u0026rdquo;: no driving for 6 months, on probation for a year. Drugs or alcohol between 13-21: suspension for a year Minor driving Not sure if this applies\nPractice for 50 hours, 10 hours at night #knw\nPass knowledge test\nPass driving test\nCannot drive between 11P and 5A during the first year #knw\nCannot drive with under 20 Y/O unless 25 Y/O licensed accompanied #knw\nUnless\u0026mdash;\nMedical need with doctor\u0026rsquo;s note and end date School and dean\u0026rsquo;s note Work and employer\u0026rsquo;s note and employment status Family need and parent\u0026rsquo;s note Minors can\u0026rsquo;t use a phone while driving.\nSafe car #knw Working driver\u0026rsquo;s window, brake lights, horn, parking brake, turn signals Safe tire (1/32 inch tread) Full windshield Two rear view mirrors, incl. one on left side Working seatbelts Check: clean windows and mirrors, adjust seat and mirrors, check tires.\nSafe personage Vision Hearing Not tired Not medicated Health: no Lapses of conciseness AD \u0026ldquo;related disorders\u0026rdquo; \u0026mdash; anything the doctor reports to DMV Steering Hand to Hand hands 9/3 or 8/4 oclock Push and pull, hands stay where they are Hand over hand Start 9/3 or 8/4 Turn, but leave wheel sliding under Sliding under hand reach over, pull the wheel up One-hand Turning or backing up to turn back Hand at 12 oclock Limeted use Signaling Arm signals when lights are hard to see because of bright sun\nMotorcyclists use these signals, and bikers point their hand straight up to turning direction\nWhen to signal #knw Signal when: turn, change lanes, slow down, stop.\n100 feet before turning Before every lane change: look over and check blind spot 5 seconds before lane change on highway Pulling next to or away curb Signal even if no cars around you Horning \u0026ldquo;It is safer to slow down or stop instead of honking your horn.\u0026rdquo;\nWhen to horn #knw Avoid collisions Alert hazard Alert oncoming traffic on narrow mountain roads when you cannot see at least 200 feet in front of vehicle Don\u0026rsquo;t use horn to move people along, or \u0026ldquo;express anger.\u0026rdquo; The more ya know.\nHeadlights They are bright.\nWhen to headlight #knw When its too dark to see: if you can\u0026rsquo;t see a person 1000 feet away Beginning 30 minutes after sunset until 30 minutes before sunrise Adverse weather: windshield wipers on = low-beam headlights on Clouds dust smoke or fog prevent seeing other cars On sunny days on country or mountain roads When a white regulatory sign says so To help others see your car, when sun is low on horizon When not to high-beam headlight Dim when 500 feet of car coming towards you or 300 feet of a car you are following Emergency flashers If you can see a collision ahead, do:\nTurn on flashers #knw Lightly tap brake pedal three/four times Use hand signals How to stop in a middle of the road during an emergency #knw Start breaking early.\nGive drivers warning\nTurn on emergency flashers if you earn\u0026rsquo;t moving, or use turn signals\nPull off the road\nStop not on the road or, if isn\u0026rsquo;t possible, stop where people can see\nDon\u0026rsquo;t stop just over a hill\nLift the hook to signal an emergency\nPlace emergency triangles 200-300 feet behind vehicle; use flares if needed but be careful b/c they may cause fire\nCall for roadside assistance\n63, 92\nLanes! Reading \u0026rsquo;em Yellow: different directions Single yellow is the center of the road; cannot cross into oncoming traffic Double solid yellow line: not to be crossed \u0026hellip;except hov entrace lane which has a left entrance Instructed to cross because the road is blocked Entering or exiting a driveway, private road, or making a u-turn 2 double yellow line groups spaced 2 feet or more apart are considered a barrier; under no circumstance is to cross Broken yellow line: you may pass if the broken line is next to you White: same directions Single solid white line: traffic lanes in the same direction Double solid white lines: not to be crossed, regular use vs. preferential use lanes (carpool, etc.) Broken white lines: separate roads with two or more lines in the same direction White triangles: yield lines A line where you should yield. Triangles point to the direction of oncoming traffic (\u0026ldquo;towards you.\u0026rdquo;).\nChoosing \u0026rsquo;em Leftmost lane is lane 1, rightmost is lane n\nUse the left lane to pass or turn left Use the right lane to enter or exit traffic Change lanes when Moving from one lane to another Entering freeway Exiting freeway Entering the road from curb or shoulder Protocol for lane change #knw Signal Look in all mirrors Check traffic beside and behind you Look over solder in direction of desired lane change Check blind spots for other vehicles, motorcyclists, and bicycilsts Ensure room Tips stay in one lane don\u0026rsquo;t weave if you start a change, finish it Types of them Lane closest to the center divider is the \u0026ldquo;passing lane\u0026rdquo; HOV lanes is for high occupancy Center left turn lanes The center of some two-way streets has a left turn lane; marked on both sides by two painted lines. Inner line is broken and outer line is solid.\nYou may only drive 200 feet in the center left turn lane #knw\nProtocol for using this lane\nLook for other vehicles coming towards you in the center left turn lane Signal Look over shoulder Merge completely into the center left turn lane Turn when its safe. Turnouts Areas or lanes for turning that are marked? Use when:\nDriving slowly on a two-lane road where passing is unsafe, AND There are 5 or more vehicles following #knw Bike lanes Bike lanes\nBuffered bike lanes: uses chevrons or diagonals to buffer the bikes\nBike route: shared road markings to designate a preferred route\nBike boulevard: bike travel on streets with cars\nSeperated bikeways: completely different\nBikes share the road \u0026ldquo;sharrows!\u0026rdquo;\nCannot drive in bike lane unless\u0026hellip;.\nParking Entering or leaving road Turning (within 200 feet of intersection) Turning Right Drive close to the edge Drive in a bike lane, wait until about 200 feet to make turn #knw Watch for everybody Signal about 100 feet before #knw Look over sholder Stop behind limit line (/before entering crosswalk or intersection) Look both ways and turn when its safe; don\u0026rsquo;t turn into another lane Complete turn Details:\nCan\u0026rsquo;t turn when red arrow, but you can turn against red light You could cross a bus lane to make a right turn, but you can\u0026rsquo;t drive in it There could be designated right turn lanes which let you make a \u0026ldquo;free right turn\u0026rdquo; Left Drive close to the center divider or left turn lane Signal about 100 feed Look over sholder Stop behind limit line (/before entering crosswalk or intersection) Look left, right, then left Turn Details\nonly can turn against light when single-lane-to-single-lane U Conditions Across double-yellow line In a residential district No cars for 200 feet Whenever a sign or light protects against approachng cars At an intersection On a divided driveway, if opening provided Anticonditions WHen \u0026ldquo;no u-turn\u0026rdquo; is posted\nAt a railroad crossing\nOn a divided highway if needed to cross things\nCannot see 200 feet in each direction\nWhen other cars may hit you\nOn a one-way street\nIn front of a fire station\nooo. scary\nIn business districts, including churches apartments and buildings (except for schools); turn only at an intersection or opening if allowed. Merging Highways Enter at or near traffic speed Merge onto highway when safe to do so, don\u0026rsquo;t stop unless needed Merge into a space large enough for your car to join the lane Use mirrors and turn signals Watch for cars Leave three seconds of space (\u0026ldquo;three second rule\u0026rdquo;) between you and the car in front of you Exiting Know the exist Signal, look over sholder, etc. Change lanes Signal intention for 5 seconds Leave Space for entering You will need about a half a block on city streets Or, a full block on the highway Passing If anybody wants to pass, let them pass\nSpace for passing Don\u0026rsquo;t pass if\u0026hellip;\nYou are approaching a hill and cannot see oncoming traffic Within 100 feet of an intersection #knw At crossroads or driveways Condition of Passing You pass on the left, unless\u0026hellip;\nOpen highway with two or more lanes going in your direction Driver ahead of you is turning left, and you don\u0026rsquo;t have to drive off the road to pass You are on a one-way street Never drive off the road to pass.\nProtocol for passing Signal Shoulder Turn Speed up and pass Retturn Parking Find a space #knw three feet longer that your vehicle Turn on turn signal Pull up alongside the vehicle in front; leave about two feet between you and the car to your right. Stop when you rear bumper is aligned with the front of the space Check rearview mirror, look over sholder, keep foot on break and reverse Back up, 45% When rear view is within 18 inches from the curb, straighten out Set parking break. Leave when safe. Parking on a hill \u0026ldquo;Your car may roll when you breaks fail.\u0026rdquo;\nDownhill: wheels towards the curb Uphill: wheels away from curb No curb: turn towards the sholder of the road \u0026ldquo;towards the sholder, except when uphill with curb\u0026rdquo;\nColors White curb: stop for picking up or dropping off passengers or mails Green curb: park for limited time Yellow: load and unload, staying in the vehicle Red: no stopping Blue: disabled\u0026mdash;fine of $1,000, 6 months in county jail #knw Can\u0026rsquo;t park when No marking Unmarked or marked crosswalk Sidewalk, partially blocking sidewalk, or in front of driveway Within 3 feet of disabled sidewalk ramp #knw On diagnal lines next to disabled space Within 15 feet of a fire hydrant #knw Double parking On the wrong side of the street or freeway, except: 1) emergency 2) law enforcement officer 3) specificaly permitted stop.\nTo stop and park then, park off the pavement, stay with the car and lock the doors until help arrives; visibility is 200 feet in each direction required. #knw\nLights Flashing red: stop sign\u0026ndash;stop and go when its safe Flashing yellow: yield sign\u0026mdash;proceed with caution Flashing yellow arrow: unprotected turn Broken traffic lights become a four way stop sign.\nSigns Stop sign is stop; there should be a limit line; if no limit line, stop before intesection Yield sign is to yield; slow down Right of Way Without stop/yield signs Whomever gets to the intersection first has right of way T intersection without stop/yield signs The through road have right of way Stop signs Stop first, then follow right of way rules as if no intersection Turning left Right of way to anyone approaching that\u0026rsquo;s \u0026ldquo;close enough to be dangerous\u0026rdquo; Turning right Check for pedestrians crossing the street, and bikes and motors next to you Green light Pedestrians Divided highways Vehicles coming in the lane you are about to enter Entering traffic The traffic you are entering Roundabouts The logistics of using a roundabout\nSlow down Yield to traffic Watch for signs Travel in counter-clockwise direction, don\u0026rsquo;t stop or pass Signal when you change lanes or exit If you miss your exit, try again Choosing lane Rightmost for turning right Either lane (\u0026ldquo;middle\u0026rdquo;, if exists) for straight Innermost for left turn or u turn Pedestrians Pedestrians have right-of-way Pedestrian crossing need to cross first, you yield or slow to them Which means\u0026hellip;\nDo not pass a stopped vehicle Don\u0026rsquo;t drive on a sidewalk except to cross it or enter/exit it Don\u0026rsquo;t stop in a crosswalk If people make eyecontact, they are crossing the street Obey pedestrian\u0026rsquo;s signs Watch for seniors, people with disabilities, young children.\nCrosswalks Crosswalks are marked (but not all) School crossings have yellow lines Pedestrians have right of wall in all crosswalks Flashing light crosswalks exists to, just be prepared to stop regardless Blind White canes and guide dogs have absolute right of way Stop at all stop walks Don\u0026rsquo;t stop in the middle of stop walk Don\u0026rsquo;t give verbal directions to blind pedestrian Don\u0026rsquo;t turn right w/o looking for pedestrians Don\u0026rsquo;t honk at a blind person Don\u0026rsquo;t block sidewalk Pulling in cane + stepping away: you may go Mountain roads Uphill car has right of way Downhill car has more control backing up the hill Roadsharing Large cars Average passenter car at 55mph has 400 feet before stopping Large car takes 800 feet Don\u0026rsquo;t move in front of a large car and suddenly stop.\nLook at turn signals: large vehicles may swing their back, say, left in order to turn right.\nDon\u0026rsquo;t\nChange lanes in front of them to reach an exit or turn (tight spaces around large vehicles is dangerous) Drive next to them (unless passing); after you pass, move ahead of it Follow too closely: that\u0026rsquo;s tailgating. Give more space Underestimate the size and speed of the vehicle \u0026ldquo;If you can\u0026rsquo;t see a truck\u0026rsquo;s side mirrors, it can\u0026rsquo;t see you.\u0026rdquo;\nalways pass it on the left side\nBuses and rails when loading is happening without a safety zone, stop behind the nearest door Stopped busses can only be passed at 10mph Don\u0026rsquo;t pass on the left side, unless\u0026hellip; you are on a one-way street tracks are so close to the right you can\u0026rsquo;t pass on the right traffic officer directs you to Never turn in front of a light rail vehicle Check for traffic lights (light rails can interrupt them) Motocycles 4 second following distance Given a motocycle a full lane; its legal to share but its unsafe Don\u0026rsquo;t try to pass a motorcycle in the same lane When possible, move to one side of your lane Check for motocyclists Emergency vehicles Give them right of way: drive to the edge until they\u0026rsquo;ve passed \u0026hellip;except in intersections: never stop in an intersection (continue through and stop) Obey loudspeaker orders Illegal to follow 300 feet of any emergency vehicles with flashing siren Slow cars Slow down for them NEV LSV Like gold carts\nThey have max speed 25mph They can\u0026rsquo;t drive in roads with speed limit larger than 35 mph Bikes Front lamp with white light visible for 300 feet Rear red reflector (visible from 500 feet) White or yellow reflector on each pedal (visible for 200 feet) Travel lanes Must ride to the curb if slow, unless\nPassing in the same direction Preparing to turn left Avoiding a hazard/road condition Approaching right turn On a one way road with two or more lanes (if so, bikers may right next to left curb) Passing bikers 3 feet clearance\nSchool buses Yellow lights flashing is to slow Red lights flashing is to stop If you fail to stop, you can be fined up to $1,000 and driving maybe suspended for a year Workzone fines Traffic violations have fines of $1,000 or more Assulting a worker has a fine of $2,000 plus imprisonment for up to on year Some regions are double-fine zones Speed Limit \u0026ldquo;Basic speed law\u0026rdquo;: you may never drive faster than its safe.\n10mph to pas a roadcar\n15 mph in blind intersections (cannot see 100 in both directions when within 100 feet)\nif your view is blocked in a blind intersection, inch forward until you can see 15 mph also in some school, alleys (roads no wider 25 feet), 100 feet of railroad tracks if visiblity less then 400 feet\n25 mph when you are 500-1000 feet of a school, when crossing the street, residential\n55 mph on two lane undivided highway\nYou cannot block traffic flow\nDrive far-right lane of you are towing\nRailroad Look in both directions Except train anytime Don\u0026rsquo;t stop in traintracks Watch for other cars Stop between 15-50 feet from the neearest tracks Fines and Stuff Smoking with a minor: $100\nDumping animals: $1,000, six months in jail\nEvading law enforcement:\nstate prison up to 7 years, county jail for 1 year Fine between $2,000 and $10,000 Or both Evading law enforcement and commiting manslauter\nImprisonment for 4-10 years Speed content and reckless driving: fine and imprsionment\nTexting\nWear earplugs in bot hyears\nCarry anything that extends beyond the fenders on the left side, or more then 6 inches on the right side\nCargo more the 4 feet must display a 1 feet red or flourencesnt flag\nTransport animals unless secured\nAllow a person to be in a back of a pickup truck unless secured\nDrive a car with a video monitor except when it doesn\u0026rsquo;t face driver\nThrow a cig from the car\nCut signs that block the windshiled\nDon\u0026rsquo;t hang objects on the mirror\nDon\u0026rsquo;t sticker, unless\n7 inch square on lower corner of passengers or rear window 5 inch square on the lower corner of the driver window Side windows behind driver 5 inch located in the center uppermost portion Funeral pocessions have right of way\nPoints 36 month record Suspension when: 4 points in 12 months, 6 in 24, or 8 in 36 Once 18 months to earn back points via traffic school Best Practices Scan road 10-15 seconds ahead of you Don\u0026rsquo;t stare Don\u0026rsquo;t tailgate: 3 seconds between you and the car ahead passes Allow extra space when\u0026hellip; If you have a tailgator, (and move! if you can) The driver behind you wants to pass Slippery Following on icy or wet Towing a trailer Followiing a car that blocks you ahead Merging onto freeway Following Don\u0026rsquo;t stay in the blind spot Don\u0026rsquo;t driving alongside cars Make space when possible Keep space between you and parked cars Be careful when nearing motorcyclists and bicyclists At intersections Look both ways Look left first (vehicles coming from the left are closer) Look right Take one more look to the left 5-10mph on wet road, reduce speed by half on snow, tiny very slow on ice Don\u0026rsquo;t use breaks if starting to hydroplone If you can\u0026rsquo;t see farther than 100 feet, its unsafe to drive faster than 30mph Seat belts Click it or ticket Under 16 years old, you may also get ticket Child safety Under 2 years old: secure in a real facing child restraight system (unless child weighs more than 40 pounds or is more that 3 ft 4 inches taller)\nChilden under 8 years old, less than 4 feet 9 inches tall: secure in a front-facing restraight system\nCould use front seat if there\u0026rsquo;s no rear seat or if they are side facing jump seat\n8 years old or older, or 4 feet 9 inches tall: use seat belts\n6 y/o or younger unattended illegal to leave in car; supervision could be 12 year old.\nHot vehicle can kill\nEmergencies Skids Slippery surface Slowly remove foot from gas pedal Don\u0026rsquo;t use breaks Turn the steering wheel in the direction of the skid If your breaches get wet, dry them by pressing gas and brake at the same time.\nLock wheel Breaking too hard when going to fast: skid no matter steering wheel\nRemove foot from break Straighten front wheel If ABS not working, step on brake gradually until safe speed. If the brake petal sinks to the floor, bump the brakes.\nDriving off pavement Grip wheel slowly Remove your foot from gas Brake gently Check for traffic Steer back Accelerator mallfunction Shift to neutral Apply breakes Look for traffic Honk horn and emergency flashers Drive car off the road Turn of ignition Collision If collision causes more than $1000 in property damage, you msut report to DMV Driving is suspended for 4 years of no insurance Disabled Vehicle Safely pull over Exit on the right side Find assistance Return no vehicle Stay inside with your seat belt Uuse flashers Railroad If a train is coming, get out and run in a 45 degree away from the train and tracks. Dial 911 If train not coming, exit vehicle, dial emergency number on the railroad crossing box, and then call 911 DUI Don\u0026rsquo;t drink and drive Don\u0026rsquo;t take drugs Use any combination of drugs Illegal to drink alcohol or smoke or eat cannabis products while in a car, whether self or passenger. If you are carrying it, it must be full and unopened. If its open, keep it in the trunk.\nLimits 0.08% over 21 0.01% under 21 0.01% under DUI probation 0.04% if commercial 0.04% if driving for hire DUI Arrests Hold license for 30 days Hearing from 10 days DUI Convictions Completion of DUI program Install Ignition Interlock Device 6 months in jail $390-$1000 May inpound vehicle Carrying under 21 May not carry unless someone older Fine up to $1000 and impound for 30 days, suspencion for 1 year 0.01% or higher you have to complete program, 0.05% suspension ","permalink":"https://www.jemoka.com/posts/kbhdriving/","tags":null,"title":"Driving"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhdriving_practice/","tags":null,"title":"Driving Practice"},{"categories":null,"contents":"Dup15q Syndrome is an autistic syndrome associated with a gain of variant function in UBE3A. It is the opposite of Angelman Syndrome, which is a loss of function result on UBE3A.\n","permalink":"https://www.jemoka.com/posts/kbhdup15q/","tags":null,"title":"Dup15q Syndrome"},{"categories":null,"contents":"dynamic programming is a three-step algorithm to tackle large, multi-step problems; high level idea: guessing + caching + recursion.\ndynamic programming can sometimes not be good enough, and it doesn\u0026rsquo;t really give us fast enough to get what we need to use. That\u0026rsquo;s when we need to deal with relaxation, or possibly greedy programming.\nmain steps of dynamic programming Break a hard problem into sub-problems Guess what sub-problem to solve Solve the sub-problem and store the solution Repeat #2 and #3 Combine sub-problem solutions to solve the hard problem analyzing runtime of dynamic programming To analyze runtime of dynamic programming problems, you ask:\nHow many sub-problems are there? How long does it take to solve each sub-problem? How long does it take to combine sub-problems? fibonacchi numbers: dynamic programming here\u0026rsquo;s an example top-down dynamic programming problem:\nThere are \\(n\\) sub-problems: \\(F_1, F_2, \\ldots, F_{n-1}\\). Solve a sub-problem, then store the solution \\(F_{n-1} = F_{n-2}+F_{n-3}\\) Continue until \\(F_1 =1\\). Now, we can recurs back up (popping the call stack) and cache all calculated results So then we can just look up any \\(F_k\\). shortest path: dynamic programming here\u0026rsquo;s a graph! how do we get to node \\(6\\)?\nGuess that the shortest path goes through 10 Go recursively until you get to root, cache the solution Do it again until you got to all subproblems Look up cached result ","permalink":"https://www.jemoka.com/posts/kbhdynamic_programming/","tags":null,"title":"dynamic programming"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhe_coli/","tags":null,"title":"E. Coli"},{"categories":null,"contents":"Presented Project80 Talks Person Society Keywords Email Chhavi Chauhuan, PhD, ELS ASIP AI Ethics, Pathology cchauhan@asip.org J. Elliott Robinson, PhD, MD ASBMB NF1, Dopamine, ADHD elliott.robinson@cchmc.org Jason Yi, PhD ASBMB UBE3A, Recklinghaus, Dup15q domain Erica Korb, PhD ASBMB Autism, Chromatin ekorb@pennmedicine.upenn.edu Catherine Wang AAA student approach to learning ??? Megan Fagalde, PhD Candidate AAA anatomy learning mfagalde@iu.edu Michelle A. Sveistrup AAA haptic abilities, HAT msveistr@uwo.ca AAA anatomy learning Alam Boyd AAA partner vs. individual work Magnus ??? AAA Orna Issler ASBMB IncRNA, LINC00473, FEDORA orna.issler@mssm.edu Kaushik Ragunathan ASBMB whimsical adaptations ragunath@med.umich.edu Tracy l. Bale ASBMB i think like P80 scary tbale@som.umaryland.edu Gregory Morton APS thermoregulation, glucose gjmorton@uw.edu Peter Turnbaugh ASBMB Fluoropyrimidine, PreTA, DPYD peter.turnbaugh@ucsf.edu Ralph DeBernandis ASBMB metabolic alterations, LIPT1 People Meeters Person Place Email Job Followup Jay Pieczynski Rollins jpieczynski@rollings.edu Assist. Prof. P80, College Apps Sebastian Hernandez Rollins shernandez1@rollings.edu Undergrad \u0026quot;\u0026quot; Bryson Arnett U of Kentucky Undegrad Jennifer Pousont Pingry Eric P. Chang Pace U echang@pace.edu Assist. Prof P80 ","permalink":"https://www.jemoka.com/posts/kbheb_emails/","tags":["index"],"title":"EB2022 Index"},{"categories":null,"contents":"Slightly nontraditional Ted class, which is that it is in complete modular architecture: no large group lectures, work is done in 2-3 week sprints.\nFirst two days, we will be doing intro together. There are 12 modules, and you do 6. There will be core modules and branches.\nThere are 3 symposiums which the groups share out. This class is very hard; we are using a graduate school textbook. We will be sidestepping some depth: main idea is to show the big area.\nTracks 1 =\u0026gt; {2,4,5} 4 =\u0026gt; 5 7 =\u0026gt; 8 8 =\u0026gt; {11,12} 3 =\u0026gt; 6 6 =\u0026gt; 9 9 =\u0026gt; 10 Good to learn MatLab.\nLogistics Create a portfolio journal; supply one entry a week.\nIntroductory Reading How Did Economists Get It So Wrong?\n","permalink":"https://www.jemoka.com/posts/kbhecon320_architecture/","tags":null,"title":"ECON320 Architecture"},{"categories":null,"contents":"The economy of credit is an effect where credit is being traded liberally, and people are buying stocks on large margins and unable to pay back.\n","permalink":"https://www.jemoka.com/posts/kbheconomy_of_credit/","tags":null,"title":"economy of credit"},{"categories":null,"contents":" Many Mexican-Americans worked as migratory laborers + outside programs Indian Reorganization Act of 1934 Woman were paied less Environmental cost of damns and public projects commentary on the effects of the New Deal Incorporating aspects of Arthur M. Schlesinger\u0026rsquo;s Appraisal of the New Deal, William E. Leuchtenburg\u0026rsquo;s Appraisal of the New Deal, Anthony Badger\u0026rsquo;s Appraisal of the New Deal.\nThrough the analysis of the New Deal programs, what was particularly salient was Anthony Badger\u0026rsquo;s framing of the event as not one that is ultimately \u0026ldquo;successful\u0026rdquo; or \u0026ldquo;failed\u0026rdquo; but instead one which focuses on its long-term effects in context with the future policies. The equivocal labeling allows nuance that places the Deal properly in its historical content. According to Badger, helping the poor, a significant policy goal of the deals, were left as \u0026ldquo;unfinished business\u0026rdquo; when going to war. This idea contrasts with William E. Leuchtenburg\u0026rsquo;s framing of the same event\u0026mdash;that it was never the true intention of the deal to assist in subsidies on a humane level, but that which supported the economy and incidentally those that reaped benefits on it.\nThis new frame is much more useful when analyzing the deal. In fact, Leuchtenburg took this a step further and claimed that the New Deal didn\u0026rsquo;t work largely because it was impossible for it to have repaired the damage by the Hoover administration. Furthermore, according to Schlesinger, programs like the NRA were created with already the clear assumption that there were not enough policy tools in place to actually achieve it to the fullest extent. Under this mind frame, then, it is not difficult to see the New Deal as one that intentionally brought a failing US economy\u0026mdash;and those participating in it\u0026mdash;to full swing whilst ignoring those that didn\u0026rsquo;t have an economic influence. It was, therefore, never about helping \u0026ldquo;people\u0026rdquo;: it is a policy and economic tool like any other.\nThrough this somewhat revisionist view, it is much easier to place into perspective New Deal\u0026rsquo;s zealot focus on young men, strange deficiency in some areas, and central focus on infrastructure. In that regard, the New Deal worked very well to bring a failing economy back to a semblance of normalcy for the privileged few.\n","permalink":"https://www.jemoka.com/posts/kbheffects_of_the_new_deal/","tags":null,"title":"effects of the New Deal"},{"categories":null,"contents":"eigenvalues are values which scales eigenvectors\nconstituents linear map \\(T\\) \u0026ldquo;eigenvectors\u0026rdquo; \\(v\\) \u0026ldquo;eigenvalues\u0026rdquo; \\(\\lambda_{j}\\) requirements \\begin{equation} Tv = \\lambda_{j}v \\end{equation}\nadditional information finding eigenvalue \\begin{equation} \\lambda_{j} \\in Spec(T) \\Rightarrow det(\\lambda_{j}I-T) = 0 \\end{equation}\nThe right polynomial \\(det(\\lambda_{j} I-T) = 0\\) is named the \u0026ldquo;characteristic polynomial.\u0026rdquo;\nnatural choordinates Given the eigenvectors \\((x+,y+), (x-,y-)\\), we can change coordinates of your matrix into the natural choordinates.\n\\begin{equation} A = \\begin{pmatrix} x+ \u0026amp; x- \\\\y+ \u0026amp; y- \\end{pmatrix} \\begin{pmatrix} \\lambda+ \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda- \\end{pmatrix} \\begin{pmatrix} x+ \u0026amp; x- \\\\y+ \u0026amp; y- \\end{pmatrix}^{-1} \\end{equation}\nThis makes scaling matricides much much easier. If you think about multiplying the above matrix \\(n\\) times, the inverse and non-inverse cancells out.\n","permalink":"https://www.jemoka.com/posts/kbheigenvalue/","tags":null,"title":"eigenvalue"},{"categories":null,"contents":"The Elastic Modulus is a measurement of how much deformation takes place given some force on the system. Formally, it is the slope of the stress-strain curve, defined by:\n\\begin{equation} E = \\frac{stress}{strain} \\end{equation}\nThe units in pascals as it is: force per area (pascals) divided by deformation (dimensionless, as it is a fraction of old shape over new shape \\(\\frac{V}{V}=1\\)).\nDepending on how its measured, it is called different things:\nYoung\u0026rsquo;s Modulus: tensile elasticity\u0026mdash;tendency for object to deform along an axis with force applied (usually that is just called the Elastic Modulus) Shear\u0026rsquo;s Modulus: shear elasticity\u0026mdash;tendency of an object to shear (deform in shape with the constant volume) with force applied Bulk Modulus: volumetric elasticity\u0026mdash;tendency for an object to deform in all directions when uniformly loaded ","permalink":"https://www.jemoka.com/posts/kbhelastic_modulus/","tags":null,"title":"Elastic Modulus"},{"categories":null,"contents":"Eleanor Roosevelt is the first lady of the US.\nCreated minimum wage Wrote a weekly column named My Day, in 135 newspapers 2x a week broadcast ","permalink":"https://www.jemoka.com/posts/kbheleanor_roosevelt/","tags":null,"title":"Eleanor Roosevelt"},{"categories":null,"contents":"A civil rights movement organizer that founded SNICK.\n","permalink":"https://www.jemoka.com/posts/kbhella_baker/","tags":null,"title":"Ella Baker"},{"categories":null,"contents":"Your brain maintaing a stable level of energy. Closely related to glucose homeostatis.\nmethods to achive energy homeostasis by the CNS regulation of the brain AgRP signaling is activated to stimulate food intake when hypoglycemic. ","permalink":"https://www.jemoka.com/posts/kbhenergy_homeostasis/","tags":null,"title":"energy homeostasis"},{"categories":null,"contents":"motivating entanglement file:///Users/houliu/Documents/School Work/The Bible/Quantum/Leonard Susskind, Art Friedman - Quantum Mechanics_ The Theoretical Minimum-Basic Books (2014).pdf\nTake two actors, Alice \\(A\\) and Bob \\(B\\). They each have a space \\(S_A\\) and \\(S_B\\). What if, for instance, we want to create a composite system out of Alice and Bob?\nWe will define elements in the Alice space as being defined by bases \\(H\\) and \\(T\\), where each element \\(a \\in S_a\\) is defined as:\n\\begin{equation} \\alpha_H | H \\big\\} + \\alpha_T | T \\big\\} \\end{equation}\nWhy the weird kets? We will use different kets to be aware of where bases came from; as in, elements in Alicespace is not elements in Bobspace.\nLet\u0026rsquo;s take Bobspace to be a higher dimension, as in, using normal ket vectors:\n\\begin{align} |1\\big\u0026gt; \\\\ |2\\big\u0026gt; \\\\ |3\\big\u0026gt; \\\\ \\cdots \\\\ |6\\big\u0026gt; \\end{align}\n","permalink":"https://www.jemoka.com/posts/kbhentangled/","tags":null,"title":"entanglement"},{"categories":null,"contents":"epigenetics is the ability to make identical cells present distinct phenotipic states.\nWhy? DNA is packaged by charged histone proteins, and they wrap around the nucleosome. Upon acute changes in the environment, cells can change their epigenic states.\nwhimsical adaptations Epigenetic adaptive states in organisms with no clear path adaptation. For instance, a certain lung cancer cell has this ability. So, how do cells decide what genes they would activate?\nAnother example: treating fisheries in caffine\nGrowing in caffine will trigger caffine resistance Remove the caffine would cause some of them to default back, some of them to stay the same way ","permalink":"https://www.jemoka.com/posts/kbhepigenetics/","tags":null,"title":"epigenetics"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhequal_rights_act/","tags":null,"title":"Equal Rights Act"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbheugene_wigner/","tags":null,"title":"Eugene Wigner"},{"categories":null,"contents":"A type of cell.\nSample eukareotyic cell gene:\nTATA box promoter 5\u0026rsquo; non-coding sequence Non-coding introns interlaced between exons, unique to eukareotyic cells. Bacteria (prokateotic cells don\u0026rsquo;t contain introns or have small them) 3\u0026rsquo; non-coding sequence ","permalink":"https://www.jemoka.com/posts/kbheukareotyic_cell/","tags":null,"title":"eukareotyic cell"},{"categories":null,"contents":"The Euler-Bernoulli Theory is a theory in dynamics which describes how much a beam deflect given an applied load.\nAssumptions For Euler-Bernoulli Theory to apply in its basic form, we make assumptions.\nThe \u0026ldquo;beam\u0026rdquo; you are bending is modeled as a 1d object; it is only long and is not wide For this page, \\(+x\\) is \u0026ldquo;right\u0026rdquo;, \\(+y\\) is \u0026ldquo;in\u0026rdquo;, and \\(+z\\) is \u0026ldquo;up\u0026rdquo; Probably more, but we only have this so far. the general form of the Euler-Bernoulli Theory assumes a freestanding beam Basic Statement The most basic for the Euler-Bernoulli Equation looks like this:\n\\begin{equation} \\dv[2]x \\qty(EI\\dv[2]{w}{x}) =q(x) \\end{equation}\nwhere, \\(w(x)\\) is the deflection of the beam at some direction \\(z\\) at position \\(x\\). \\(q\\) is the load distribution (force per unit length, similar to pressure which is force per unit area, at each point \\(x\\)). \\(E\\) is the Elastic Modulus of the beam, and \\(I\\) the second moment of area of the beam\u0026rsquo;s cross section.\nNote that \\(I\\) must be calculated with respect to the axis perpendicular to the load. So, for a beam placed longside by the \\(x\\) axis, and pressed down on the \\(z\\) axis, \\(I\\) should be calculated as: \\(\\iint z^{2}\\dd{y}\\dd{z}\\).\nPretty much all the time, the Elastic Modulus \\(E\\) (how rigid your thing is) and second moment of area \\(I\\) (how distributed are the cross-section\u0026rsquo;s mass) are constant; therefore, we factor them out, making:\n\\begin{align} \u0026amp;\\dv[2]x \\qty(EI\\dv[2]{w}{x}) =q(x) \\\\ \\Rightarrow\\ \u0026amp; EI \\qty(\\dv[2]x \\dv[2]{w}{x} )=q(x) \\\\ \\Rightarrow\\ \u0026amp; EI \\dv[4]{w}{x} = q(x) \\end{align}\nThis is also apparently used everywhere in engineering to figure out how bendy something will be given some \\(q\\) put along the beam.\nOk, let\u0026rsquo;s take the original form of this equation and take some integrals to see the edges of this thing:\n\\begin{equation} \\dv[2]{x} \\qty(EI \\dv[2]{w}{x}) = q(x) \\end{equation}\nFirst things first, let\u0026rsquo;s take a single integral:\n\\begin{equation} \\dv{x} \\qty(EI \\dv[2]{w}{x}) = -Q \\end{equation}\nThis is the total shear force on the material (the sum of all forces applied to all points \\(\\int q(x)\\).) We have a sign difference\nold notes\nLet\u0026rsquo;s take some transverse load \\(q(x,t)\\), applied at time \\(t\\) at location \\(x\\). To model the load/bending/vibration of the rod, we first have to know a few more things.\nFirst, figure the Young\u0026rsquo;s Modulus \\(E\\) of the thing that you are bending.\nOf course, we also want to know what shape our thing is; more specifically, we want to know how the point masses in our thing is distributed. So we will also need the second moment of area \\(I\\).\nFinally, we should have \\(m\\) mass per unit length of the rod we are bending.\nThe Euler-Bernoulli Theory tells us that the deflection (distance from the neutral-axis) each point \\(x\\) in the material should get is:\n\\begin{equation} EI \\pdv[4]{w}{x} + m \\pdv[2]{w}{t} = q(x,t) \\end{equation}\nSolving this lovely Differential Equation would tell you how far away each point diverges from the neutral point.\nTracing this out over \\((x,t)\\), we can get some trace of how the thing vibrates by measuring the behavior of \\(\\omega\\).\nfree vibrations in Euler-Bernoulli Theory If no time-varying \\(q\\) exists, we then have:\n\\begin{equation} EI \\pdv[4]{w}{x} + m \\pdv[2]{w}{t} = 0 \\end{equation}\nAnd then some magical Differential Equations happen. I hope to learn them soon.\nThe result here is significant: if we can figure the actual rate of vibrations which we expect.\nHowever, this doesn\u0026rsquo;t really decay\u0026mdash;but funing torks do. How?\nApparently because air resistance\u0026mdash;Zachary Sayyah. So Sasha time.\n","permalink":"https://www.jemoka.com/posts/kbheuler_bernoulli_theory/","tags":null,"title":"Euler-Bernoulli Theory"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbheurope/","tags":null,"title":"Europe"},{"categories":null,"contents":"The correlation is a relation between two random variables.\nStandardize variables to \\(z\\) by dividing The correlation is simply their \u0026ldquo;product\u0026rdquo;: means of positive and negative groups ","permalink":"https://www.jemoka.com/posts/kbhexpectation/","tags":null,"title":"expectation"},{"categories":null,"contents":"\\(\\mathbb{F}^n\\) is the set of all lists of length \\(n\\) with elements of \\(\\mathbb{F}\\). These are a special case of matricies.\nFormally\u0026mdash;\n\\begin{equation} \\mathbb{F}^n = \\{(x1,\\ldots,x_n):x_j\\in\\mathbb{F}, \\forall j =1,\\ldots,n\\} \\end{equation}\nFor some \\((x_1,\\ldots,x_n) \\in \\mathbb{F}^n\\) and \\(j \\in \\{1,\\ldots,n\\}\\), we say \\(x_j\\) is the \\(j^{th}\\) coordinate in \\((x_1,\\ldots,x_n)\\).\nadditional information addition in \\(\\mathbb{F}^n\\) Addition is defined by adding corresponding coordinates:\n\\begin{equation} (x1,\\ldots,x_n) + (y_1,\\ldots,y_n) = (x_1+y_1, \\ldots,x_n+y_n) \\end{equation}\naddition in \\(\\mathbb{F}^n\\) is commutative If we have \\(x,y\\in \\mathbb{F}^n\\), then \\(x+y = y+x\\).\nThe proof of this holds because of how addition works and the fact that you can pairwise commute addition in \\(\\mathbb{F}\\).\n\\begin{align} x+y \u0026amp;= (x_1,\\ldots,x_n) + (y_1,\\ldots,y_n)\\\\ \u0026amp;= (x_1+y_1,\\ldots,x_n+y_n)\\\\ \u0026amp;= (y_1+x_1,\\ldots,y_n+x_n)\\\\ \u0026amp;= (y_1,\\ldots,y_n) + (x_1,\\ldots,x_n)\\\\ \u0026amp;= y+x \\end{align}\nThis is a lesson is why avoiding explicit coordinates is good.\nadditive inverse of \\(\\mathbb{F}^n\\) For \\(x \\in \\mathbb{F}^n\\), the additive inverse of \\(x\\), written as \\(-x\\) is the vector \\(-x\\in \\mathbb{F}^n\\) such that:\n\\begin{equation} x+(-x) = 0 \\end{equation}\nWhich really means that its the additive inverse of each of the coordinates.\nscalar multiplication in \\(\\mathbb{F}^n\\) At present, we are only going to concern ourselves with the product of a number \\(\\lambda\\) and a vector \\(\\mathbb{F}^n\\). This is done by multiplying each coordinate of the vector by \\(\\lambda\\).\n\\begin{equation} \\lambda (x_1,\\ldots,x_n) = (\\lambda x_1, \\lambda, \\lambda x_n) \\end{equation}\nwhere, \\(\\lambda \\in \\mathbb{F}\\), and \\((x_1,\\ldots,x_n) \\in \\mathbb{F}^n\\).\nThe geometric interpretation of this is a scaling operation of vectors.\n","permalink":"https://www.jemoka.com/posts/kbhlists_over_fields/","tags":null,"title":"F^n"},{"categories":null,"contents":"We define a set \\(\\mathbb{F}^{s}\\), which is the set of unit functions that maps from any set \\(S\\) to \\(\\mathbb{F}\\).\ncloseness of addition \\begin{equation} (f+g)(x) = f(x)+g(x), \\forall f,g \\in \\mathbb{F}^{S}, x \\in S \\end{equation}\ncloseness of scalar multiplication \\begin{equation} (\\lambda f)(x)=\\lambda f(x), \\forall \\lambda \\in \\mathbb{F}, f \\in \\mathbb{F}^{S}, x \\in S \\end{equation}\ncommutativity inherits \\(\\mathbb{F}\\) (for the codomain of functions \\(f\\) and \\(g\\))\nassociativity inherits \\(\\mathbb{F}\\) for codomain or is just \\(\\mathbb{F}\\) for scalar\ndistribution inherits distribution in \\(\\mathbb{F}\\) on the codomain again\nadditive identity \\begin{equation} 0(x) = 0 \\end{equation}\nadditive inverse \\begin{equation} (-f)(x) = -f(x) \\end{equation}\nmultiplicative identity \\(1\\) hee hee\n","permalink":"https://www.jemoka.com/posts/kbhfs_is_a_vector_space/","tags":null,"title":"F^s is a Vector Space Over F"},{"categories":null,"contents":"A New Deal program to help long-term families to have home. Tho program lowered down-payment for homes down from \\(50\\%\\) down to only \\(\u0026lt;10\\%\\). This is part of Roosevelt\u0026rsquo;s New Deal to lower interest rates and increased national home ownership rates. This could have been attributed to programs to stabilize home prices. This specifically helped white families: favoured single-family homes.\n","permalink":"https://www.jemoka.com/posts/kbhfederal_housing_administration/","tags":null,"title":"Federal Housing Administration"},{"categories":null,"contents":"The Federal Project Number One is a branch of projects under the WPA which created opportunities for writers, musicians, artists, writers, etc.\n","permalink":"https://www.jemoka.com/posts/kbhfederal_project_number_one/","tags":null,"title":"Federal Project Number One"},{"categories":null,"contents":"A field is a special set.\nconstituents distinct elements of at least \\(0\\) and \\(1\\) operations of addition and multiplication requirements closed commutativity associativity identities (both additive and multiplicative) inverses (both additive and multiplicative) distribution Therefore, \\(\\mathbb{R}\\) is a field, and so is \\(\\mathbb{C}\\) (which we proved in properties of complex arithmetic).\nadditional information Main difference between group: there is one operation is group, a field has two operations.\n","permalink":"https://www.jemoka.com/posts/kbhfield/","tags":null,"title":"field"},{"categories":null,"contents":"There\u0026rsquo;s not one market, there is a host of different markets. This field is filled with interesting markets which unique dynamics. The field can really be seen as an advanced statistics application, but its just happening to be modeling the case of financial dynamics.\nKnowledge Random Walk Hypothesis Brownian Motion Arbitrage Pricing Derivative Pricing options CAPM ","permalink":"https://www.jemoka.com/posts/kbhfinancial_markets_intro/","tags":["index"],"title":"Financial Markets"},{"categories":null,"contents":"We define:\n\\begin{equation} \\mathbb{F}^{\\infty} = \\{(x_1, x_2, \\dots): x_{j} \\in \\mathbb{F}, \\forall j=1,2,\\dots\\} \\end{equation}\nclosure of addition We define addition:\n\\begin{equation} (x_1,x_2,\\dots)+(y_1,y_2, \\dots) = (x_1+y_1,x_2+y_2, \\dots ) \\end{equation}\nEvidently, the output is also of infinite length, and as addition in \\(\\mathbb{F}\\) is closed, then also closed.\nclosure of scalar multiplication We define scalar multiplication:\n\\begin{equation} \\lambda (x_1,x_2, \\dots) = (\\lambda x_1, \\lambda x_2, \\dots ) \\end{equation}\nditto. as above\ncommutativity extensible from commutativity of \\(\\mathbb{F}\\)\nassociativity extensible from associativity of \\(\\mathbb{F}\\), for both operations\ndistribution \\begin{align} \\lambda ((x_1,x_2,\\dots)+(y_1,y_2, \\dots)) \u0026amp;= \\lambda (x_1+y_1,x_2+y_2, \\dots ) \\\\ \u0026amp;= (\\lambda (x_1+y_1),\\lambda (x_2+y_2), \\dots ) \\\\ \u0026amp;= (\\lambda x_1+\\lambda y_1,\\lambda x_2+\\lambda y_2, \\dots) \\\\ \u0026amp;= (\\lambda x_1, \\lambda x_2, \\dots) + (\\lambda y_1, \\lambda y_2, \\dots) \\\\ \u0026amp;= \\lambda (x_1, x_2, \\dots) + \\lambda (y_1, y_2, \\dots) \\end{align}\nditto. for the other direction.\nadditive ID \\begin{equation} (0,0, \\dots ) \\end{equation}\nadditive inverse extensive from \\(\\mathbb{F}\\)\n\\begin{equation} (-a, -b, \\dots ) + (a,b, \\dots ) = 0 \\end{equation}\nscalar multiplicative ID \\(1\\)\n","permalink":"https://www.jemoka.com/posts/kbhfinfty_is_a_vector_space_over_f/","tags":null,"title":"Finfinity is a Vector Space over F"},{"categories":null,"contents":"A graph of states which is closed and connected.\nAlso relating to this is a derived variable. One way to prove reaching any state is via Floyd\u0026rsquo;s Invariant Method.\n","permalink":"https://www.jemoka.com/posts/kbhfinite_state_machine/","tags":null,"title":"Finite State Machine"},{"categories":null,"contents":"A finite-dimensional vector space is a vector space where some actual list (which remember, has finite length) of vectors spans the space.\nAn infinite-demensional vector space is a vector space that\u0026rsquo;s not a finite-dimensional vector space.\nadditional information every finite-dimensional vector space has a basis Begin with a spanning list in the finite-dimensional vector space you are working with. Apply the fact that all spanning lists contains a basis of which you are spanning. Therefore, some elements of that list form a basis of the finite-dimensional vector space you are working with. \\(\\blacksquare\\)\nfinite-dimensional subspaces finite-dimensional subspaces\n","permalink":"https://www.jemoka.com/posts/kbhfinite_dimensional_vector_space/","tags":null,"title":"finite-dimensional vector space"},{"categories":null,"contents":"Fireside Chats are a group of broadcasts by Franklin D. Roosevelt (FDR) which allowed him to speak directly to the people.\n","permalink":"https://www.jemoka.com/posts/kbhfireside_chats/","tags":null,"title":"Fireside Chats"},{"categories":null,"contents":"To prove properties on Finite State Machines, we can construct a proof:\nstating an invariant proving that the invarient is true for all states for all transitions: assume invarient is true before transition and prove that its true after So, essentially induction.\n","permalink":"https://www.jemoka.com/posts/kbhfloyd_s_invariant_method/","tags":null,"title":"Floyd's Invariant Method"},{"categories":null,"contents":"Abstract Alzheimer\u0026rsquo;s Disease (AD) is a demonstrativeness disease marked by declines in cognitive function. Despite early diagnoses being critical for AD prognosis and treatment, currently accepted diagnoses mechanisms for AD requires clinical outpatient testing with a medical professional, which reduces its accessibility. In this work, we propose a possible feature extraction mechanism leveraging the previously demonstrated errors of Hidden Markov-based forced alignment (FA) tools upon cognitively impaired patients as an automated means to quantify linguistic disfluency.\nBackground Annotated linguistic disfluency features, used in combination with semantic features, have been shown ((Antonsson et al. 2021)) to improve the accuracy of AD classification systems. However, manual annotation of disfluency hinders the throughput of AD detection systems. Furthermore, there is a dearth ((Guo et al. 2021)) of data provided with preexisting annotated results.\nExisting acoustic-only approaches ((Lindsay, Tröger, and König 2021; Shah et al. 2021)) frequently places focus on the actual speech features such as silence, energy, rate, or loudness. While this approach has returned promising results ((Wang et al. 2019)), it renders the acoustic data features extracted independent of actual linguistic disfluency. Of course, some approaches (including that in (Wang et al. 2019)) perform separate, manual annotation on both aspects and treat them jointly with late fusion. However, no existing approaches have an effective feature representation that bridges the acoustic-linguistic gap.\nAn incidental effect of Hidden Markov Model (HMM) based Viterbi forced alignment (FA) tools (such as P2FA) is that its quality is shown ((Saz et al. 2009)) to be lowered in cognitively impaired speakers, resulting from a roughly \\(50\\%\\) decrease in power of discrimination between stressed and unstressed vowels. Other ASR and FA approaches ((Tao, Xueqing, and Bian 2010)) has since been designed discriminate against such changes more effectively.\nProposal By encoding FA results of HMM based approaches in embedding space, we introduce a novel feature representation of acoustic information. As FA requires an existing transcript, this method is considered semi-automated because the test must be either administered via a common-transcript, transcribed manually later, or transcribed using ASR techniques. After encoding, the proposed feature can be used in a few ways.\nEuclidean distance The Euclidean Distance approach compares the embedding of the HMM FA vector with a \u0026ldquo;reference\u0026rdquo; benchmark via pythagoras in high dimension.\nThere are two possible modalities by which the \u0026ldquo;reference\u0026rdquo; can be acquired; if the data was sourced via the patient sample reading a standardized transcript, a reference FA sample could be provided via the audio of another individual reading the same transcript screened traditionally screened without AD. Therefore, the \u0026ldquo;deviation from reference\u0026rdquo; would be used as an input feature group to any proposed model architectures.\nAlternatively, as stated before, other FA approaches are less susceptible to lexical hindrances with decreased discriminatory power. Therefore, we could equally take the Euclidean distance between embedded results of two different FA mechanisms\u0026mdash;one shown to be more sustainable to cognitively impaired speakers and one not\u0026mdash;as input features to training architectures.\nCross-Attention One key issue with the Euclidean Distance approach is that the difference between \u0026ldquo;normal\u0026rdquo; pauses, changes in speaker pace, etc. which would be variable between different speakers even controlling for AD prognoses.\nIn computer vision, few-shot classification cross-attention ((Hou et al. 2019)) has shown promising results in discrimination; furthermore, trainable cross-attention ensures more flexible control to non-prognostic verbal disturbances such as a normal change in pace which would otherwise cause a large difference in the Euclidean Distance approach.\nIn practice, a model similar to that proposed by ((Hou et al. 2019)) would be used as the basis to encode (or even discriminate) between pairwise samples of different FA approaches or against a non-AD control, as per highlighted in the section above.\nAs input features Of course, the raw FA embedding can be used as an input feature. There are less prior work on this front as this project would be, as far as we know, proposing the use of forced aligner outputs as a feature input heuristic.\nReferences Antonsson, Malin, Kristina Lundholm Fors, Marie Eckerström, and Dimitrios Kokkinakis. 2021. “Using a Discourse Task to Explore Semantic Ability in Persons with Cognitive Impairment.” Frontiers in Aging Neuroscience 12 (January): 607449. doi:10.3389/fnagi.2020.607449. Guo, Yue, Changye Li, Carol Roan, Serguei Pakhomov, and Trevor Cohen. 2021. “Crossing the ‘Cookie Theft’ Corpus Chasm: Applying What BERT Learns from Outside Data to the ADReSS Challenge Dementia Detection Task.” Frontiers in Computer Science 3 (April): 642517. doi:10.3389/fcomp.2021.642517. Hou, Ruibing, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. 2019. “Cross Attention Network for Few-Shot Classification.” Advances in Neural Information Processing Systems 32. Lindsay, Hali, Johannes Tröger, and Alexandra König. 2021. “Language Impairment in Alzheimer’s Disease—Robust and Explainable Evidence for AD-Related Deterioration of Spontaneous Speech through Multilingual Machine Learning.” Frontiers in Aging Neuroscience 13 (May): 642033. doi:10.3389/fnagi.2021.642033. Saz, Oscar, Javier Simón, W Ricardo Rodr\\’ıguez, Eduardo Lleida, and Carlos Vaquero. 2009. “Analysis of Acoustic Features in Speakers with Cognitive Disorders and Speech Impairments.” Eurasip Journal on Advances in Signal Processing 2009. Springer: 1–11. Shah, Zehra, Jeffrey Sawalha, Mashrura Tasnim, Shi-ang Qi, Eleni Stroulia, and Russell Greiner. 2021. “Learning Language and Acoustic Models for Identifying Alzheimer’s Dementia from Speech.” Frontiers in Computer Science 3 (February): 624659. doi:10.3389/fcomp.2021.624659. Tao, Ye, Li Xueqing, and Wu Bian. 2010. “A Dynamic Alignment Algorithm for Imperfect Speech and Transcript.” Computer Science and Information Systems 7 (1): 75–84. doi:10.2298/CSIS1001075T. Wang, Tianqi, Chongyuan Lian, Jingshen Pan, Quanlei Yan, Feiqi Zhu, Manwa L. Ng, Lan Wang, and Nan Yan. 2019. “Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese.” In Interspeech 2019, 3880–84. ISCA. doi:10.21437/Interspeech.2019-2414. ","permalink":"https://www.jemoka.com/posts/kbhdementiabank_acoustics_project_proposal/","tags":null,"title":"Forced-Alignment Error for Feature Extraction for Acoustic AD Detection"},{"categories":null,"contents":"FDR is an American president.\nFDR and Teddy Roosevelt is Got Polio, which played in his favor =\u0026gt; press agree to not photograph him when he was in a wheelchair Created the New Deal Models himself after his cousin Teddy Roosevelt, and believed that charisma and moral leadership work. \u0026ldquo;Above all, try something\u0026hellip; let the court shoot it if need to.\u0026rdquo;\nHe was able to gain single party control, wh.\nCreated Fireside Chats.\nHis wife, Eleanor Roosevelt, was very controversial.\nlegacy of FDR Never spent enough to end the depression Expanded government regulation, government size, and social welfare Modernization of presidency: sets agenda, initiates legislation Realigned the democratic party (created the progressive democrats) Maintained democracy \u0026lt;=== compared to Authoritarianism ","permalink":"https://www.jemoka.com/posts/kbhfdr/","tags":null,"title":"Franklin D. Roosevelt (FDR)"},{"categories":null,"contents":"Saltwater economists are economists from coastal schools that are mostly classical Keynsians\nFreshwater economists are economists who are mostly Neoclassical Economists\n","permalink":"https://www.jemoka.com/posts/kbhfreshwater_economists/","tags":null,"title":"Freshwater economists"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhfunctor/","tags":null,"title":"functor"},{"categories":null,"contents":"fusion in machine learning is the process of adding features or encoding.\nlate fusion late fusion adds features together to a model in a multi-modal approach by first embedding the features separately\nearly fusion early fusion adds features together to a model in a multi-modal approach by concatenating the features first then embedding\n","permalink":"https://www.jemoka.com/posts/kbhfusion/","tags":null,"title":"fusion (machine learning)"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgeneral_relativity/","tags":null,"title":"general relativity"},{"categories":null,"contents":"a hissyfight with the transformational generative syntax.\ngenerative semantics states that structure is in support of meaning, rather than the other way around that transformational generative syntax suggests.\nThis means that you need to first come up with a meaning then imbew the best structure to support the expression of that meaning.\nThis (along with distributed morphology) is the main opposition of the Lexicalist Hypothesis, and because proof for the existence of semantic primes, also the main opposition of the existence of semantic primes.\n","permalink":"https://www.jemoka.com/posts/kbhgenerative_semantics/","tags":null,"title":"generative semantics"},{"categories":null,"contents":" A genetic algorithm is a search heuristic that is inspired by Charles Darwin\u0026rsquo;s theory of natural evolution.\nIts what Grey\u0026rsquo;s video says. The picking and chucking iterative thing.\n","permalink":"https://www.jemoka.com/posts/kbhgenetic_algorithum/","tags":null,"title":"genetic algorithm"},{"categories":null,"contents":"A Geometric Brownian Motion is a Brownian Motion with a drift.\nIt is determined by:\n\\begin{equation} \\dd{S_{t}} = \\mu S_{t} \\dd{t} + \\sigma \\dd{S_{t}} \\dd{W_{t}} \\end{equation}\nwhere, \\(S_{t}\\) is a Geometric Brownian Motion, \\(\\mu\\) is its drift, \\(\\sigma\\) the volatility, and \\(W_{t}\\) a centered Brownian Motion.\n","permalink":"https://www.jemoka.com/posts/kbhgeometric_brownian_motion/","tags":null,"title":"Geometric Brownian Motion"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgolden_gate_bridge/","tags":null,"title":"Golden Gate Bridge"},{"categories":null,"contents":"a:2:{i:0;s:2:\u0026ldquo;f2\u0026rdquo;;i:1;s:2:\u0026ldquo;f3\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;e2\u0026rdquo;;i:1;s:2:\u0026ldquo;e3\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;e1\u0026rdquo;;i:1;s:2:\u0026ldquo;e2\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;b2\u0026rdquo;;i:1;s:2:\u0026ldquo;b3\u0026rdquo;;}\na:2:{i:0;s:2:\u0026ldquo;c2\u0026rdquo;;i:1;s:2:\u0026ldquo;d8\u0026rdquo;;}\n","permalink":"https://www.jemoka.com/posts/kbhgoogle_nerd_snipe/","tags":null,"title":"Google Nerd Snipe"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgorup/","tags":null,"title":"gorup"},{"categories":null,"contents":"Using constructor theory to test whether or not gravity in quantum theory is just entanglement.\nThis solves problem with gravity.\n","permalink":"https://www.jemoka.com/posts/kbhgravitational_entanglement/","tags":null,"title":"gravitational entanglement"},{"categories":null,"contents":"The Great Depression is a period of time of American depression.\n","permalink":"https://www.jemoka.com/posts/kbhgreat_depression/","tags":null,"title":"Great Depression"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhgreedy_programming/","tags":null,"title":"greedy programming"},{"categories":null,"contents":"Participate in Demo Day.\nGetting something:\nOpportunity to get partnering Networking opportunities, having access to contract manufacturing =\u0026gt; Conrad Challenge, $600 each, $1200\nUser conversation\nSpoke again with CompassionKind: wanting to get 20 units shipped out Spoke with SustainableEnergy for All: started by one of the UN reps. of an African country; wanted to have us featured on Social Media Wanted to connect Start diving into user connections Hiring requests\nFulfilling orders MechE ","permalink":"https://www.jemoka.com/posts/kbhgreenswing_april_checkin/","tags":null,"title":"GreenSwing April Checkin"},{"categories":null,"contents":"In this experiment, an efficient and accurate network of detecting automatically disseminated (bot) content on social platforms is devised. Through the utilisation of parallel convolutional neural network (CNN) which processes variable n-grams of text 15, 20, and 25 tokens in length encoded by Byte Pair Encoding (BPE), the complexities of linguistic content on social platforms are effectively captured and analysed. With validation on two sets of previously unexposed data, the model was able to achieve an accuracy of around 96.6% and 97.4% respectively — meeting or exceeding the performance of other comparable supervised ML solutions to this problem. Through testing, it is concluded that this method of text processing and analysis proves to be an effective way of classifying potentially artificially synthesized user data — aiding the security and integrity of social platforms.\n","permalink":"https://www.jemoka.com/posts/kbhgregarious_abstract/","tags":null,"title":"Gregarious Abstract"},{"categories":null,"contents":"grid search is a hyperparameter tuning technique by trying pairs of all hyperparemeters sequentially\n","permalink":"https://www.jemoka.com/posts/kbhgrid_search/","tags":null,"title":"grid search"},{"categories":null,"contents":"components a set of constituent objects an operation requirements for group closed existence of identity existence of inverses associative additional information identity in group commutates with everything (which is the only commutattion in groups ","permalink":"https://www.jemoka.com/posts/kbhgroup/","tags":null,"title":"group"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.642517\nOne-Liner Used WLS data to augment CTP from ADReSS Challenge and trained it on a BERT with good results.\nNovelty Used WLS data with CTP task to augment ADReSS DementiaBank data Notable Methods WLS data is not labeled, so authors used Semantic Verbal Fluency tests that come with WLS to make a presumed conservative diagnoses. Therefore, control data is more interesting:\nKey Figs Table 2 Data-aug of ADReSS Challenge data with WSL controls (no presumed AD) trained with a BERT. As expected the conservative control data results in better ferf\nNew Concepts ADReSS Challenge is small so use WLS to augment it ","permalink":"https://www.jemoka.com/posts/kbhguo_2021/","tags":["ntj"],"title":"Guo 2021"},{"categories":null,"contents":"Gut bacteria are both adversly affected by 5-Fluoropyrimidine, and but they mtaybe able to inactivate synthesized Fluoropyrimidine.\nPreTA in E. Coli is an example of a bacterial that can do this. See implications of PreTA deactivating Fluoropyrimidine.\n","permalink":"https://www.jemoka.com/posts/kbh5_fluoropyrimidine_maybe_inactivated_by_gut_microbiome/","tags":null,"title":"gut microbiome deactivating Fluoropyrimidine"},{"categories":null,"contents":"Hello Internet is a podcast hosted by Brady Haran and CGP Grey.\n","permalink":"https://www.jemoka.com/posts/kbhhello_internet/","tags":null,"title":"Hello Internet"},{"categories":null,"contents":"Herber Hoover is an American president.\nHerber Hoover\u0026rsquo;s response to the Great Depression Hoover\u0026rsquo;s Programs: too little, too late Makes business pledge to maintain wages, tax cuts, Smoot-halwey Tariff, bank financial support Builds Golden Gate Bridge and the Hoover Dam Rejects the idea of the direct federal relief, which is against FDR\u0026rsquo;s thoughts ","permalink":"https://www.jemoka.com/posts/kbhherber_hoover/","tags":null,"title":"Herber Hoover"},{"categories":null,"contents":" Reading Date Notes New Deal Flip-book \u0026lt;2022-03-24 Thu\u0026gt; New Deal Historian Flipbook Legacy of McCarthyism \u0026lt;2022-04-25 Mon\u0026gt; Legacy of McCarthyism Soviet Perspective on the Cold War \u0026lt;2022-04-29 Fri\u0026gt; Soviet Perspective on Cold War MLK and Malcom X \u0026lt;2022-05-10 Tue\u0026gt; MLK and Malcom X Reading Origins of American Conservatism \u0026lt;2022-05-27 Fri\u0026gt; Origins of American Conservatism ","permalink":"https://www.jemoka.com/posts/kbhhistory_readings_index/","tags":["index"],"title":"History Readings Index"},{"categories":null,"contents":"Homogeneity is a measure of how similar many things are.\n","permalink":"https://www.jemoka.com/posts/kbhhomogeneity/","tags":null,"title":"homogeneity"},{"categories":null,"contents":"Honoré\u0026rsquo;s Statistic is a statistical measure of vocabulary complexity, it is a test of Semantic Verbal Fluency and is commonly used for cognitive impairment detection.\nThe statistic is defined as:\n\\begin{equation} HS = 100 \\log \\frac{N}{1-\\frac{N_{uni}}{U}} \\end{equation}\nwhere, \\(N\\) is the total number of words, \\(U\\) the total number of distinct words, \\(N_{uni}\\) the number of total distinct words used only once.\nThe idea here is that a higher diversity of vocabulary shows higher Semantic Verbal Fluency.\n","permalink":"https://www.jemoka.com/posts/kbhhonore_s_statistic/","tags":null,"title":"Honoré's Statistic"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhhoover_dam/","tags":null,"title":"Hoover Dam"},{"categories":null,"contents":"Hoovervile are homeless encampments named after Herber Hoover, where homeless people band together after loosing jobs in the Great Depression.\n","permalink":"https://www.jemoka.com/posts/kbhhooverviles/","tags":null,"title":"Hoovervile"},{"categories":null,"contents":"me\n","permalink":"https://www.jemoka.com/posts/kbhhoujun_liu/","tags":null,"title":"Houjun Liu"},{"categories":null,"contents":"A reading: (Krugman 2009)\nReflection The discussion here of the conflict between \u0026ldquo;saltwater\u0026rdquo; and \u0026ldquo;freshwater\u0026rdquo; (Keynesian and Neoclassical) economists is very interesting when evaluated from the perspective of our recent impending recession.\nOne particular statement that resonated with me in the essay was the fact that a crisis simply \u0026ldquo;pushed the freshwater economists into further absurdity.\u0026rdquo; It is interesting to see that, once a theory has been well-established and insulated in a community, it becomes much more difficult to parcel out as something that could be wrong.\nAs the same time, the forcibly-correcting \u0026ldquo;fudge\u0026rdquo; inconsistencies of the Keynesian model is also a strong weakness which perhaps further exacerbated the freshwater economists\u0026rsquo; dissent into their models. Modeling human behavior has been consistently quite messy, so it is unsurprising that both neoclassical and Keynesian economists strayed away from those models.\nCircling back to the COVID-trigger economic downturn: we definitely see a push towards increased \u0026ldquo;absurdity\u0026rdquo; in terms of increased polarization in the US; but not only that, the deeply rooted idea of \u0026ldquo;pandemics don\u0026rsquo;t affect the States\u0026rdquo; or at least \u0026ldquo;the Feds/our supply chain have preparation for absurd events\u0026rdquo; is again shown to be false\u0026mdash;despite the Obaman re-discovery of Keynesian management earlier.\nThis all raises a question: under what circumstances is a tangibly \u0026ldquo;better\u0026rdquo; result going to surface and be accepted when one model is tangibly perfect yet wrong, the other requiring flawed corrections or unrigorous analysis. Must we reject one model completely before the other one can be used?\nI don\u0026rsquo;t believe behavioral economics, though providing a partial solution as Krugman outlines, is the be-and-end-all of macroeconomic models during a depression. All of the models which were theorized (bar pure neoclassicalist \u0026ldquo;perfect agents\u0026rdquo;) ostensibly do one thing: trying to \u0026ldquo;rationally\u0026rdquo; model the \u0026ldquo;irrational\u0026rdquo; behavior of market participants. I don\u0026rsquo;t believe that this is ultimately going to be feasible on a macroeconomic scale to create models that will last (sans repeated, empirical testing\u0026mdash;but there are not enough depressions to go around.) Perhaps, then, the basic Keynesian idea of simply creating fiscal corrections may very well be the best second thing.\nReading notes the main problem was the fact that nobody saw a catastrophie coming More important was the profession’s blindness to the very possibility of catastrophic failures in a market economy.\npeople either believed that the market would never go wrong or the Fed fixes everything free-market economies never go astray and those who believed that economies may stray now and then but that any major deviations from the path of prosperity could and would be corrected by the all-powerful Fed.\nThe economists thought the humans are perfectly rational, and the fact that they are not is what leads to failures Unfortunately, this romanticized and sanitized vision of the economy led most economists to ignore all the things that can go wrong. They turned a blind eye to the limitations of human rationality that often lead to bubbles and busts\nKeynsian Economics was not trying to entirely replace markets Keynes did not, despite what you may have heard, want the government to run the economy. \u0026hellip; He wanted to fix capitalism, not replace it.\nMilton Friedman lead the return to Neoclassical Economics The neoclassical revival was initially led by Milton Friedman of the University of Chicago, who asserted as early as 1953 that neoclassical economics works well enough as a description of the way the economy actually functions\nNeoclassical Economics with the monetarist theory under Milton asserted that keeping the money supply growing is all that needed Monetarists asserted, however, that a very limited, circumscribed form of government intervention — namely, instructing central banks to keep the nation’s money supply, the sum of cash in circulation and bank deposits, growing on a steady path — is all that’s required to prevent depressions.\nMilton Freedman believes that large-scale expansion would lead to inflation and high unimployment excessively expansionary policies, he predicted, would lead to a combination of inﬂation and high unemployment\nAnti-Keynesian seniments overtook Freedman\u0026rsquo;s original proposition Eventually, however, the anti-Keynesian counterrevolution went far beyond Friedman’s position, which came to seem relatively moderate compared with what his successors were saying.\n#question why is this obvious? for obvious reasons\nBecause the new economists beliefed that the market is right, the advise was for business to max stock price ﬁnance economists believed that we should put the capital development of the nation in the hands of what Keynes had called a “casino.”\nMajor stock events didn\u0026rsquo;t blunt the disregard to Keynesian policy These events, however, which Keynes would have considered evidence of the unreliability of markets, did little to blunt the force of a beautiful idea.\nNew \u0026ldquo;perfect\u0026rdquo; economic models earned large respect in industry mild-mannered business-school professors could and did become Wall Street rocket scientists, earning Wall Street paychecks.\nNew models often analyzed financial systems independently of their real-world worth Finance economists rarely asked the seemingly obvious (though not easily answered) question of whether asset prices made sense given real-world fundamentals like earnings. Instead, they asked only whether asset prices made sense given other asset prices\nMacro split into two factions: the Keynes recessionists or the anti-Keynesians macroeconomics has divided into two great factions: “saltwater” economists (mainly in coastal U.S. universities), who have a more or less Keynesian vision of what recessions are all about; and “freshwater” economists (mainly at inland schools), who consider that vision nonsense.\nFreshwater economists\u0026rsquo; theory: recessions were just people confused? Nobel laureate Robert Lucas, argued that recessions were caused by temporary confusion: workers and companies had trouble distinguishing overall changes in the level of prices\nUnder freshwater theories, unemployment is just people electing not to work due to unfavorable environment ampliﬁed by the rational response of workers, who voluntarily work more when the environment is favorable and less when it’s unfavorable. Unemployment is a deliberate decision by workers to take time off.\n\u0026hellip;\nPut baldly like that, this theory sounds foolish — was the Great Depression really the Great Vacation?\nThe new Keysians still kept more or less to non-dramatic thinking They tried to keep their deviations from neoclassical orthodoxy as limited as possible. This meant that there was no room in the prevailing models for such things as bubbles and banking-system collapse.\nNew Keysians believed entirely in the Fed, without need for large fiscal policy They believed that monetary policy, administered by the technocrats at the Fed, could provide whatever remedies the economy needed.\nPeople just thought that there can\u0026rsquo;t be a bubble in housing What’s striking, when you reread Greenspan’s assurances, is that they weren’t based on evidence — they were based on the a priori assertion that there simply can’t be a bubble in housing.\nObama\u0026rsquo;s economic policies are much more on the Keynes side Such Keynesian thinking underlies the Obama administration’s economic policies — and the freshwater economists are furious.\nFailure of neoclassicalist theory is that breaking Keynsian economical behavior requires perfect rationality, which is absurd if you start from the assumption that people are perfectly rational and markets are perfectly efﬁcient, you have to conclude that unemployment is voluntary and recessions are desirable.\nEconomists thought that economics would have been perfect Economics, as a ﬁeld, got in trouble because economists were seduced by the vision of a perfect, frictionless market system.\nBehavioral Economics Behavioral Economics is a study of economics which hinges on the irrationality of human behavior. Its an answer to both the Neoclassical Economics\u0026rsquo; poor assumption that humans and markets are perfect, but also Keynsian Economics\u0026rsquo;s increasingly large need for a random \u0026ldquo;fudge\u0026rdquo; to get their models working right.\npillars of Behavioral Economics \u0026ldquo;Many real-world investors bear little resemblance to the cool calculators of efﬁcient-market theory: they’re all too subject to herd behavior, to bouts of irrational exuberance and unwarranted panic.\u0026rdquo; \u0026ldquo;even those who try to base their decisions on cool calculation often ﬁnd that they can’t, that problems of trust, credibility and limited collateral force them to run with the herd.\u0026rdquo; Good arbitrageurs are just forced out of the economy in large downward spirals As a result, the smart money is forced out of the market, and prices may go into a downward spiral.\n","permalink":"https://www.jemoka.com/posts/kbhhow_did_economists_get_it_so_wrong/","tags":null,"title":"How Did Economists Get It So Wrong?"},{"categories":null,"contents":"hypothesis testing is the mechanism by which a hypothesis is tested statistically.\nThe core logic of hypothesis testing: have a metric, do tests, calculate probability that the outcome could have happened given the metric is true.\nExamples include\nt-test (for sample means) z-test (for sample proportions) chi-square test (for sample categories) Common to all hypothesis tests are the following terms.\nnull hypothesis A null hypothesis is a \u0026ldquo;no difference\u0026rdquo; hypothesis created as a part of hypothesis testing. It is usually stated as an equality.\nalternative hypothesis The alternative hypothesis is the \u0026ldquo;new news\u0026rdquo; hypothesis created as a part of hypothesis testing, whereby the confirmation would introduce new information.\np-value the p-value of a hypothesis test is the probability of the results acquired taking place given if the null hypothesis. That is:\n\\begin{equation} p(\\hat{p} | H_0\\ true) \\end{equation}\nTo figure out the above probability, you could either simulate the occurrence and look at a histogram (more common for AP Statistics anyways) or measure a few other statistics. We will talk about them later.\nTo use p-value as a hypothesis test, the sample has to meet the conditions for inference.\nType I Error A Type I Error takes place when you reject the null hypothesis during hypothesis testing even while its true: i.e., a false positive.\nThe probability of having a Type I Error is the significance level of the test.\nType II Error A Type II Error takes place when you accept the null hypothesis during hypothesis testing even while its false.\nThe probability of having a Type II Error is the conjugate of the power of a test.\nsignificance level significance level is the level by which one would accept a p-value is being indicative of the success of a test. We usually use the letter \\(\\alpha\\) to denote this.\npower (statistics) power is a statistic calculable during hypothesis testing. Its the probability of rejecting the null hypothesis given the null hypothesis is false. Also known as the conjugate of the Type II Error.\npower increases as significance level increases, but then the probability of a Type I Error increases as well.\n","permalink":"https://www.jemoka.com/posts/kbhhypothesis_testing/","tags":null,"title":"hypothesis testing"},{"categories":null,"contents":"identities allows another number to retain its identity after an operation.\nWhat identities are applicable is group dependent. Identities are almost always object dependent.\n","permalink":"https://www.jemoka.com/posts/kbhidentity/","tags":null,"title":"identity"},{"categories":null,"contents":"\u0026lt;\u0026gt; NUS-HIST301 American History\nThe idea of identity politics is proposed, that politics became associated with sub-population of identities:\nBlack Pride Movement Chicano Activism The American Indian movement Termination of reservation system Pan-Indian Rights Alcatraz and Wounded Knee Occupations LGBT movement Stonewall GLF starts marching Asian American Yellow Peril Model minority movement NOW Femanism Acts The Equal Rights Act almost possible, and then Phyllis Schlafly happened Environmental Movement Silent Spring Cuyahoga River on fire Richard Nixon creates the EPA Earth Day ","permalink":"https://www.jemoka.com/posts/kbhactivism_during_the_1970s/","tags":null,"title":"identity politics"},{"categories":null,"contents":"to prove that something goes both ways: given \\(A\\Rightarrow B\\), and \\(A \\Leftarrow B\\), \\(A \\Leftrightarrow B\\).\n","permalink":"https://www.jemoka.com/posts/kbhequivalence/","tags":null,"title":"if and only if"},{"categories":null,"contents":"The Inbox is an Inbox for mobile org captures.\n","permalink":"https://www.jemoka.com/posts/kbhinbox/","tags":null,"title":"Inbox"},{"categories":null,"contents":"Here\u0026rsquo;s a list of all indexes:\nProjects Index Research Index Production Index About This should be reflected on a fancier way on my home page.\n","permalink":"https://www.jemoka.com/posts/kbhindex_index/","tags":["index"],"title":"Index Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhinflectional_words/","tags":null,"title":"inflectional words"},{"categories":null,"contents":"Information Units are unique entities mentioned during an utterance; for a sentence like \u0026ldquo;There is a boy. The boy is a brother. He is stealing a cookie. The sister is watching.\u0026rdquo;, \u0026ldquo;boy, cookie, sister\u0026rdquo; are possible IUs.\n","permalink":"https://www.jemoka.com/posts/kbhiu/","tags":null,"title":"Information Units (Linguistics)"},{"categories":null,"contents":"an integer (\\(\\mathbb{Z}\\)) is the natural numbers, zero, and negative numbers: \u0026hellip;,-4,-3,-2,-1,0,1,2,2,3\n","permalink":"https://www.jemoka.com/posts/kbhinteger/","tags":null,"title":"integer"},{"categories":null,"contents":"The integrating factor \\(\\rho(x)\\) is a value that helps undo the product rule. For which:\n\\begin{equation} log(\\rho(x)) = \\int P(x)dx \\end{equation}\nfor some function \\(P(x)\\).\nSeparating the \\(\\rho(x)\\) out, we have therefore:\n\\begin{equation} e^{\\int P dx} = \\rho(x) \\end{equation}\nWhy is this helpful and undoes the product rule? This is because of a very interesting property of how \\(\\rho(x)\\) behaves.\n","permalink":"https://www.jemoka.com/posts/kbhintegrating_factor/","tags":null,"title":"integrating factor"},{"categories":null,"contents":"the inverse is the the opposite of an operation. As in, if you apply the inverse of an operation to the result of applying the original with the same operation it will cancel it.\nThat is,\n\\begin{equation} A * B * B^{-1} = A \\end{equation}\n\\(B^{-1}\\) is then the inverse of \\(B\\) for the \\(*\\) operation. This is operation dependent.\n","permalink":"https://www.jemoka.com/posts/kbhinverses/","tags":null,"title":"inverse"},{"categories":null,"contents":"irrational numbers are real numbers that are not rational numbers.\nFormally:\n\\begin{equation} \\mathbb{C} = \\mathbb{R} \\backslash \\mathbb{Q} \\end{equation}\nwhere, \\(\\backslash\\) is subtracting two sets.\n","permalink":"https://www.jemoka.com/posts/kbhirrational_number/","tags":null,"title":"irrational number"},{"categories":null,"contents":" Date Notes \u0026lt;2022-04-13 Wed\u0026gt; PCP April Checkin \u0026lt;2022-04-13 Wed\u0026gt; Alivio April Checkin \u0026lt;2022-04-16 Sat\u0026gt; GreenSwing April Checkin \u0026lt;2022-04-25 Mon\u0026gt; Pollen April Checkin \u0026lt;2022-04-30 Sat\u0026gt; Logan\u0026rsquo;s Team Checkin \u0026lt;2022-05-02 Mon\u0026gt; Anna\u0026rsquo;s Team Checkin TODO Stack Get asthma kids leads for Alivio GreenSwing Hiring: Fufilling Orders, MechE Conrad money? Get Mentors for Pollen =\u0026gt; Figma lady ","permalink":"https://www.jemoka.com/posts/kbhistudio_meeting_notes/","tags":null,"title":"iStudio Meeting Notes"},{"categories":null,"contents":" https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma\nfor integrating Differential Equations with Brownian Motion.\n","permalink":"https://www.jemoka.com/posts/kbhito_intergral/","tags":null,"title":"Itô Intergral"},{"categories":null,"contents":"Punchlines Screw you, I\u0026rsquo;m not Stupid\u0026hellip;. I\u0026rsquo;m just Chinese. Joke\u0026rsquo;s on you, I\u0026rsquo;m both Chinese AND stupid. Setups Why is it that toilets have a refractory period? The satanic church fights back against the Texas abortion ban. Completed jokes Where did Texas gun control funding go? its illegal to own more than 6 d*l**s there ","permalink":"https://www.jemoka.com/posts/kbhjokes/","tags":null,"title":"jokes"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.642633\nOne-Liner Developed a kitchen sink of diagnoses tools and correlated it with biomarkers.\nNovelty The kitchen sink of data collection (phones, tablet, eye tracker, microphone, wristband) and the kitchen sink of noninvasive data imaging, psych, speech assesment, clinical metadata.\nNotable Methods Here\u0026rsquo;s their kitchen sink\nI have no idea why a thermal camera is needed\nKey Figs Here are the features they extracted\nDeveloped the features collected via a method similar to action research, did two passes and refined/added information after preliminary analysis. Figure above also include info about whether or not the measurement was task specific.\nand there are the biomarkers and medical data they collected\nAnd then they correlated their kitchen sink with biomarker from the tap\nNew Concepts spinal tap Notes ","permalink":"https://www.jemoka.com/posts/kbhjonell_2021/","tags":["ntj"],"title":"Jonell 2021"},{"categories":null,"contents":" Orbits of planetary bodies are ellipses with the sun at one of the two foci Drawing a line from the sun to the orbiting body, they would sweep out equal areas Planets that are closer to the sun have much shorter periods than that Squares of the periods of the planets is equal to the cubes of the distance from the planet to the sun ","permalink":"https://www.jemoka.com/posts/kbhkepler_s_laws_of_planetary_motion/","tags":null,"title":"Kepler's Laws of Planetary Motion"},{"categories":null,"contents":"Keynsian Politics is a economy strategy to support large projects via the government to boost economic output (i.e. that the economy needs a minder, but is generally free-sustaining.)\nSee also: Keynsian Economics was not trying to entirely replace markets\n","permalink":"https://www.jemoka.com/posts/kbhkeynsian_politics/","tags":null,"title":"Keynsian Politics"},{"categories":null,"contents":"KLA is a semiconductor process control company. https://www.kla.com/ Rick Wallace is the CEO.\n135000 employees 8.2B of revenue 72-300 tools 15% of revenue in R\u0026amp;D Their main business is in automatically inspecting chips and wafers in time.\n","permalink":"https://www.jemoka.com/posts/kbhkla/","tags":null,"title":"KLA"},{"categories":null,"contents":"knowledgebase testing is a space to test the knowledgebase! Other utility and maintained pages include random and Index.\n","permalink":"https://www.jemoka.com/posts/kbhknowledgebase_testing/","tags":null,"title":"knowledgebase testing page"},{"categories":null,"contents":"A KS test is a hypothesis test that measures if two groups of samples are drawn from the same distribution.\n","permalink":"https://www.jemoka.com/posts/kbhkolmogorov_smirnov_test/","tags":null,"title":"Kolmogorov-Smirnov test"},{"categories":null,"contents":"Want mechanics? No. You get energy.\nFirst, recall the stationary-action principle. To define a system in Lagrangian Mechanics, we define a smooth function \\(L\\), called the \u0026ldquo;Lagrangian\u0026rdquo;, and some configuration space (axis) \\(M\\).\nBy convention, \\(L=T-V\\). \\(T\\) is the kinetic energy in the system, and \\(V\\) is the potential energy in the system.\nBy the stationary-action principle, then, we require \\(L\\) to remain at a critical point (max, min, saddle.) This fact allows us to calculate the equations of motion by hold \\(L\\) at such a point, and evolving the \\((T,V)\\) pair to remain at that point.\nThe notion of solving for optimal \\((T,V)\\), which will give us the equations of motion, is why Lagrangian multipliers were invented.\nNow, here\u0026rsquo;s a few results which help you deal with the Lagrangian.\nConservation of Momentum Note that momentum is always conserved.\nRecall that:\n\\begin{equation} F = m a = m \\dv{v}{t} = \\dv{mv}{t} \\end{equation}\nwhen \\(m\\) is constant, which it almost certainly is.\nRecall the definition of momentum:\n\\begin{equation} p := mv \\end{equation}\nTherefore, we have that:\n\\begin{equation} F = \\dv{p}{t} \\end{equation}\nGreat, now let\u0026rsquo;s recall what energy is:\n\\begin{equation} W = \\int F\\dd{x} \\end{equation}\nSubstituting our definitions of force:\n\\begin{equation} W = \\int \\dv{p}{t}\\dd{x} = \\int \\dd{p}\\dv{x}{t} = \\int v \\dd{p} \\end{equation}\n[something something something ask leonard]\nWe end up with:\n\\begin{equation} \\pdv{W}{v} = p \\end{equation}\nhow? IDK. But you then usually would use this by taking the derivative of the Lagrangian by velocity, then figuring it lingual to \\(0\\).\nBeam Theory We begin with this Euler-Lagrange expression:\nthese are a series of expressions derived to semiautomatically solve Largrangian expressions of expressions derived to semiautomatically solve Largrangian expressions: they are the pre-figured-out stationary-action principle \u0026ldquo;stationary points\u0026rdquo; with the least energy.\nWe want to create a Lagrangian of our system, and plug it in there.\nWe define the Lagrangian for this system to be\nRecall that the Lagrangian is defined by all kinetic energy sum minus all potential energy sum. Will investigate deeper later, but the first term is obviously the kinetic energy (1/2 mass-density velocity squared), then the subtracted potential energy term is the spring potential of the system (1/2 kx^2).\nThen there\u0026rsquo;s this third term. No idea.\nWe then try to plug stuff into that Euler-Lagrange expression. We can calculate for ourselves that:\nFinally, then:\nliterally\u0026hellip; the end. We just move stuff around and that\u0026rsquo;s literally it.\n","permalink":"https://www.jemoka.com/posts/kbhlagrangian_mechanics/","tags":null,"title":"Lagrangian Mechanics"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624694\nOne-Liner Proposed a large multimodal approach to embed auditory info + biomarkers for baseline classification.\nNovelty Developed a massively multimodal audio-to-embedding correlation system that maps audio to biomarker information collected (mood, memory, respiratory) and demonstrated its ability to discriminate cough results for COVID. (they were looking for AD; whoopsies)\nNotable Methods Developed a feature extraction model for AD detection named Open Voice Brain Model Collected a dataset on people coughing and correlated it with biomarkers Key Figs Figure 2 This is MULTI-MODAL as heck\nThis figure tells us the large network the came up with.\nTable 2 and 3 The descriminator tacked on the end of the network is transfer-trained to different tasks. It shows promising results for cough-to-COVID classification\nNew Concepts OVBM Lyu 2018 Notes Biomarker correlation Is biomarker data something that is commonly used as a feature extraction/benchmark tool?\n","permalink":"https://www.jemoka.com/posts/kbhlaguarta_2021/","tags":["ntj"],"title":"Laguarta 2021"},{"categories":null,"contents":"the\n","permalink":"https://www.jemoka.com/posts/kbhlaw_of_cosines/","tags":null,"title":"law of cosines"},{"categories":null,"contents":"LOOCV is a cross validation method whereby the entire dataset bar one sample is used for training; then, validation is ran on one sample. This is repeated \\(N\\) times (with a fresh model and a fresh item left out) to get a distribution of one-shot validation results that is an approximately-normal curve centered around the mean validation result from many one-shot samples.\n","permalink":"https://www.jemoka.com/posts/kbhloo/","tags":null,"title":"Leave-One-Out Cross Validation"},{"categories":null,"contents":"Reading notes :claim: Mccarthyism was a process that de-politicized America Since political activities could get you in trouble, prudent folk avoided them\nSocial conformaty became standard middle-class Americans became social conformists\nCommunism serves as a form of balance checking, which Mccathyism lost With their demise, the nation lost the institutional network that had created a public space where serious alternatives to the status quo could be presented.\nModerate-left was also diminished Moreover, with the disappearance of a vigorous movement on their left, moderate reform groups were more exposed to right-wing attacks and thus rendered less effective.\nMccarthyism also diminshed America\u0026rsquo;s liberal modernization Measures like national health insurance, a social reform embraced by the rest of the industrialized world, simply fell by the wayside.\nCold-war opposition became quelled by mccarthism Opposition to the cold war had been so thoroughly identified with communism that it was no longer possible to challenge the basic assumptions of American foreign policy without incurring suspicions of disloyalty\nThat there may have been more international collaboration if mccarthism was not done early on American policymakers feared to acknowledge the official existence of the People\u0026rsquo;s Republic of China until Richard Nixon, who was uniquely impervious to charges of being soft on communism, did so as president in 1971\nControvercial issues were avoided intellecturally and artistically Similarly, the blacklist contributed to the reluctance of the film industry to grapple with controversial social or political issues. In the intellectual world, cold war liberals also avoided controversy.\nThat \u0026ldquo;ideology\u0026rdquo; became irrelavent, pure pragmatism took hold They celebrated the \u0026ldquo;end of ideology,\u0026rdquo; claiming that the United States\u0026rsquo; uniquely pragmatic approach to politics made the problems that had once concerned left- wing ideologists irrelevant.\nState power became expanded federal agents attacked individual rights and extended state power into movie studios, universities, labor unions, and many other ostensibly independent institutions.\nThat Mccarthism produced a threat to demcrocy in itself McCarthyism alone did not cause these outrages; but the assault on democracy that began during the 1940s and 1950s with the collaboration of private institutions and public agencies in suppressing the alleged threat of domestic communism was an important early contribution.\n","permalink":"https://www.jemoka.com/posts/kbhlegacy_of_mccarthyism/","tags":null,"title":"Legacy of McCarthyism"},{"categories":null,"contents":"Any two basis of finite-dimensional vector space have the same length.\nconstituents A finite-dimensional vector space \\(V\\) Basis \\(B_1\\), \\(B_2\\) be bases in \\(V\\) requirements Given \\(B_1\\), \\(B_2\\) are basis in \\(V\\), we know that they are both linearly independent and spans \\(V\\). We have that the length of linearly-independent list \\(\\leq\\) length of spanning list.\nLet\u0026rsquo;s take first \\(B_1\\) as linearly independent and \\(B_2\\) as spanning:\nWe have then \\(len(B_1) \\leq len(B_2)\\)\nSwapping roles:\nWe have then \\(len(B_2) \\leq len(B_1)\\)\nAs both of this conditions are true, we have that \\(len(B_1)=len(B_{2})\\). \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhlength_of_basis_doesn_t_depend_on_basis/","tags":null,"title":"Length of Basis Doesn't Depend on Basis"},{"categories":null,"contents":"The Lexicalization Hypothesis is a hypothesis proposed by Chomsky that states that syntactic transformations can only apply on syntatic constituents; therefore, the rules of putting words together is different from the rules that puts phrases together. This theory stands in opposition to generative semantics.\nThere are two versions of the Lexicalization Hypothesis:\nStrong Lexicalization Hypothesis The Strong Lexicalization Hypothesis states that both derivational words (changes meaning, bench=\u0026gt;benching) or inflectional words (changes grammar, eat=\u0026gt;eating) cannot be put together via syntatical rules. (Geeraerts 2009)\nWeak Lexicalization Hypothesis Weak Lexicalization Hypothesis states that semantic rules cannot work in the formation of derivational words only.\n","permalink":"https://www.jemoka.com/posts/kbhlexicalization_hypothesis/","tags":null,"title":"Lexicalization Hypothesis"},{"categories":null,"contents":" Poster-modern search for individualism ","permalink":"https://www.jemoka.com/posts/kbhliberal_center/","tags":null,"title":"Liberal Center"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhlina/","tags":null,"title":"lina"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.642033\nOne-Liner Proposed cross-linguistic markers shared for AD patients between English and French; evaluated features found with standard ML.\nNovelty Multi-lingual, cross-linguistic analysis.\nNotable Methods Looked at common patters between the two languages Linguistic results scored by IUs on CTP task Key Figs Figure 1 This figure tells us the various approaches measured.\nTable 2 Here\u0026rsquo;s a list of semantic features extracted\nTable 3 Here\u0026rsquo;s a list of NLP features extracted. Bolded items represent P \u0026lt;0.001 correlation for AD/NonAD difference between English and French.\nSame thing but semantic features\nsame thing but acoustic features. As we can see, acoustic features didn\u0026rsquo;t do much.\nNew Concepts CTP IU Notes ","permalink":"https://www.jemoka.com/posts/kbhlindsay_2021/","tags":["ntj"],"title":"Lindsay 2021"},{"categories":null,"contents":"Gaussian Elimination Quiz: 09/20/2022 Demonstrate that matrices\u0026rsquo; multiplication are not commutative (error: didn\u0026rsquo;t consider \\(m\\times m\\)) Which \\(2\\times 2\\) matrices under multiplication form a group? (error: closure need to proved on invertable matrices under multiplication, not just \\(2\\times 2\\)) Deriving Rotation matrices (error: clockwise vs counter-clockwise) ","permalink":"https://www.jemoka.com/posts/kbhlinear_algebra_errors/","tags":null,"title":"Linear Algebra Errors"},{"categories":null,"contents":"The bible stays the same: (Axler 1997)\nWe will be less exploratory, Axler will pretty much tell us. However, we should try to say stuff in the class every single class period.\nThere is a ban on numbers over 4 on this class.\nBest Practices Ask questions Talk to each other Make mistakes From Riley: know the Proof Design Patterns Non-Axler but Important Things we explicitly are told to know, but is not immediately in Axler. You bet you determinants are going to be here.\ngroup matricies dot product and cross product solving systems binary operation Homework See NUS-MATH530 Homework Index\nQuiz Errors Linear Algebra Errors\n1 Axler 1.A Axler 1.B Axler 1.C 2 Axler 2.A Axler 2.B Axler 2.C 3 4 5 Misc Knowledge algebra vector integer additive identity References Axler, Sheldon. 1997. Linear Algebra Done Right. Undergraduate Texts in Mathematics. Springer New York. doi:10.1007/b97662. ","permalink":"https://www.jemoka.com/posts/kbhlinear_algebra_index/","tags":["index"],"title":"Linear Algebra Index"},{"categories":null,"contents":"A Linear Combination of vectors is a\u0026hellip; guess what? Any vector formed by a combination of vectors at arbitrary scales.\nconstituents A list of vectors \\(v_1, \\dots,v_{m}\\) Scalars \\(a_1, \\dots, v_{m} \\in \\mathbb{F}\\) requirements A Linear Combination is defined formally by:\n\\begin{equation} v = a_1v_1+\\dots+a_{m}v_{m} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhlinear_combination/","tags":null,"title":"linear combination"},{"categories":null,"contents":"Here it is:\n\\begin{equation} a\\frac{dy}{dx} + by = c \\end{equation}\nFor some constants \\(a,b,c\\). The name is pretty obvious, because we have constants and the highest power on everything is \\(1\\). Its first-order because the derivative is only the first-order derivative.\nlinear (diffeq) We technically call it \u0026ldquo;linear\u0026rdquo; because: if there are two possible solutions \\(y_1(x)\\) \\(y_2(x)\\), a linear combination \\(Ay_1(x)+By_2(x)\\) should also be a solution. Its \u0026ldquo;linear\u0026rdquo; because linear combinations work.\nsolving separable differential equations A separable differential equation means that we can separate the derivative by itself and separate its two components. For the example above, we have that:\n\\begin{equation} \\frac{dy}{dx} = \\frac{c-by}{a} \\end{equation}\nWe can naturally separate this:\n\\begin{equation} \\frac{a}{c-by}dy = dx \\end{equation}\nAnd then we can finally take the integral on both sides:\n\\begin{equation} \\int \\frac{a}{c-by}dy = \\int dx \\end{equation}\nWait wait wait but why is this possible? Why is it that we can separate a \\(\\frac{dy}{dx}\\) such that \\(dy\\) and \\(dx\\) is isolatable? Remember:\n\\begin{equation} \\frac{dy}{dx} = \\lim_{h\\to 0} \\frac{y(x+h)-y(x)}{h} \\end{equation}\nno where is the differentials seperatable! Apparently Ted\u0026rsquo;s undergrads didn\u0026rsquo;t know this either. So here\u0026rsquo;s a reading on it.\nWhat if its non-seperable? See Linear Non-Seperable Equation\n","permalink":"https://www.jemoka.com/posts/kbhlinear_constant_coefficient_equation/","tags":null,"title":"Linear Constant-Coefficient Equation"},{"categories":null,"contents":"Linear Dependence Lemma is AFAIK one of the more important results of elementary linear algebra.\nstatement Suppose \\(v_1, \\dots v_{m}\\) is an linearly dependent list in \\(V\\); then \\(\\exists j \\in \\{1, 2, \\dots m\\}\\) such that\u0026hellip;\n\\(v_{j} \\in span(v_1, \\dots, v_{j-1})\\) the span of the list constructed by removing \\(v_{j}\\) from \\(v_1, \\dots v_{m}\\) equals the span of \\(v_1, \\dots v_{m}\\) itself intuition: \u0026ldquo;in a linearly dependent list of vectors, one of the vectors is in the span of the previous ones, and we can throw it out without changing the span.\u0026rdquo;\nproof By definition of linear dependence, given the list \\((v_1, \\dots v_{m}\\)) is linearly dependent, there exists some not-all-zero \\(a_1, \\dots, a_{m} \\in \\mathbb{F}\\) such that:\n\\begin{equation} a_1v_1+\\dots +a_{m}v_{m} = 0 \\end{equation}\nLet \\(a_{j}\\) be the last non-zero scalar in the expression (making the term actually exist). You can, in this circumstance, chuck everything to the right and divide by \\(a_{j}\\) to recover \\(v_{j}\\):\n\\begin{equation} v_{j}= -\\frac{a_1}{a_{j}} v_1 - \\dots -\\frac{a_{j-1}}{a_{j}}v_{j-1} \\end{equation}\nWe were able to construct \\(v_{j}\\) as a linear combination of \\(v_{1}, \\dots v_{j-1}\\), therefore:\n\\begin{equation} v_{j} \\in span(v_1, \\dots, v_{j-1}) \\end{equation}\nshowing \\((1)\\).\nFor \\(2\\), the intuition behind the proof is just that you can take that expression for \\(v_{j}\\) above to replace \\(v_{j}\\), therefore getting rid of one vector but still keeping the same span.\nFormally, \\(\\forall u \\in span(v_1, \\dots v_{m})\\), we can write it as some:\n\\begin{equation} u = c_1v_1 + \\dots c_{j}v_{j} + \\dots + c_{m}v_{m} \\end{equation}\nnow we replace \\(v_{j}\\) with the isolated expression for \\(v_{j}\\) above.\nException: if \\(j=1\\) and \\(v_1=0\\), note that you can just replace \\(v_1\\) with \\(0\\) without doing any special substitution.\nHaving written all arbitrary \\(u \\in span(v_1, \\dots v_{m})\\) as a linear combination of \\(v_1\\dots v_{m}\\) without \u0026hellip; \\(v_{j}\\), we see that the renaming vectors span the same space. \\(\\blacksquare\\)\nissue note that if we chose \\(j=1\\) in the above result, \\(v_1=0\\). Contrapositively, if \\(v_1 \\neq 0\\), \\(j\\neq 1\\). This is because of the fact that:\nif \\(j=1\\), the lemma tells us that \\(v_{1} \\in span(v_{1-1}) \\implies v_1 \\in span()\\). As per definition, the span of the empty set is \\(\\{0\\}\\). Therefore, \\(v_1 \\in \\{0\\} \\implies v_1=0\\).\n","permalink":"https://www.jemoka.com/posts/kbhlinear_dependence_lemma/","tags":null,"title":"Linear Dependence Lemma"},{"categories":null,"contents":"A linearly independent list is a list of vectors such that there is one unique choice of scalars to be able to construct each member of their span.\nBased on the same technique as in the proof that a sum of subsets is a direct sum IFF there is only one way to write \\(0\\), we can show that in a linearly independent list, there is (IFF) only one way to write the zero vector as a linear combination of that list of vectors \u0026mdash;namely, the trivial representation of taking each vector to \\(0\\). In fact, we will actually use that as the formal definition of linear independence.\nThis definition of linear independence is the result of the definition for direct sum.\nSee also Linear Dependence Lemma.\nconstituents A list of vectors \\(v_1, \\dots, v_{m}\\) in \\(V\\) requirements Formally, a linearly independent list is defined by there being only one choice of scalars \\(a_1, \\dots, a_{m} \\in \\mathbb{F}\\) to write \\(0\\) as a linear combination of \\(v_{1},\\dots, v_{m}\\): namely, by taking each \\(a_1, \\dots a_{m}\\) to \\(0\\).\nWe also declare \\(()\\) to be linearly independent.\nadditional information linearly dependent a list is linearly dependent if\u0026hellip;. its not linearly independent.\noh. my. god.\nBased on the same formal definition, this means that a linearly dependent list is defined by the fact that there can be more than one way of writing \\(0\\) as a linear combination of that list of vectors, where one of the ways makes it so that writing \\(0\\) does not require all zero scalars.\nlength of linearly-independent list \\(\\leq\\) length of spanning list A linearly independent list should be smaller or equal in length to a spanning list.\nThe canonical proof is one by induction.\nSuppose \\(u_1, \\dots u_{m}\\) is an linearly independent list in \\(V\\). Take also a list \\(w_1, \\dots w_{n}\\) spans \\(V\\). We desire that \\(m\\leq n\\). We create a list of length \\(n\\) containing all of the \\(w\\) thus far. Our invariant is that \\(len(B) = n\\). This proof essentially uses Floyd\u0026rsquo;s Invariant Method (compsci topic for Jack\u0026rsquo;s understanding only.)\nbase case take the spanning list of \\(V\\) we declared named \\(w_1, \\dots w_{n}\\). Given it spans, adding any other vector in \\(V\\), if \\(w_1, \\dots w_{n}\\) isn\u0026rsquo;t already linearly dependent, will make it linearly dependent. This is because you can write the new vector \\(v \\in V\\) which you add as a linear combination of the previous vectors already as they already span \\(V\\).\nBy the Linear Dependence Lemma, you can remove one of the vectors in the new linearly dependent list while keeping the list still spanning \\(V\\).\nNow, construct the list:\n\\begin{equation} u_1, w_1, \\dots w_{n} \\end{equation}\nwhere, \\(u_{1} \\in V\\) is taken from that linearly independent list in \\(V\\). By the statement above, via applying the Linear Dependence Lemma, we can create a list that spans the same space by taking away one of the \\(w_{j}\\) (we can\u0026rsquo;t take \\(u_1\\) because it is at the first position, and we can\u0026rsquo;t grantee its $0$\u0026mdash;see the issue with the Linear Dependence Lemma). We now have a list \\(B\\) with length \\(n\\) with \\(u_1\\) and the rest of the \\(w\\) not taken away which span \\(V\\)\ncase number \\(j\\) Given a spanning list \\(B\\) of \\(V\\) with length \\(n\\), with some parts \\(u_1, \\dots, u_{j-1}, w_{j}, \\dots w_{n}\\). We now include \\(u_{j}\\) in the list, placing it after \\(u_{j-1}\\). As the list pre-inclusion is already a spanning list of \\(V\\), any new vectors from \\(V\\) added will necessarily be able to be written as a linear combination of the other vectors already in the list. Therefore, we know that\u0026mdash;if not already pre-inclusion\u0026mdash;the list is linearly dependent.\nBecause the first half (\\(u_1,\\dots u_{j}\\)) of this new list is linearly independent (given), the bit that \u0026ldquo;causes\u0026rdquo; the linear dependence is in the \\(w\\) (i.e. each \\(u\\) cannot be written by other \\(u\\).) Therefore, we can say that the first condition of Linear Dependence Lemma allows us to remove one of the \\(w\\) while spanning the same space, creating again a spanning list of length \\(n\\).\ninduction repeat the procedure \\(m\\) times, resulting in all the \\(u_{j}\\) being included in our new list \\(B\\) of length still \\(n\\). Given we contained a list of length \\(m\\) in a list of length \\(n\\), \\(m \\leq n\\).\n","permalink":"https://www.jemoka.com/posts/kbhlinear_independence/","tags":null,"title":"linear independence"},{"categories":null,"contents":"general form of First-Order Differential Equations This will depend on both unknown function \\(x\\), and the independent variable \\(t\\). These could and could not be separable.\n\\begin{equation} \\dv{x}{t} = F(t,x),\\ x(t_{0}) = x_{0} \\end{equation}\nLet\u0026rsquo;s imagine \\(F\\) is \u0026ldquo;bounded\u0026rdquo; and \u0026ldquo;continuous\u0026rdquo; on \\(I \\times \\omega\\), where \\(I\\) is an open interval about \\(t_{0}\\) and \\(\\omega\\) is an open subset of \\(\\mathbb{R}^{n}\\), containing \\(x_{0}\\). \\(F\\) is bounded; the results are bounded??\nfunctions embedded in vector spaces We understand that such First-Order Differential Equations will describe a subset of an infinite dimensional vector space.\nGiven we are dealing with First-Order Differential Equations, each function is a basis (if linear, otherwise, not quite the basis) of the subspace of the larger vector space; \\(+C\\) is how you create parametried variations However, our function is not linear, not all functions would suffice here: non-linear equations are difficult to deal with beacuse the arc length follows a certain pattern General form of a first order linear differential equation A general linear, first-order, first-degree differential equation of the form:\n\\begin{equation} \\dv{y}{x} + P(x)y = Q(x) \\end{equation}\nhas a solution:\n\\begin{equation} y(x) = e^{-\\int P\\dd{x}}\\int e^{\\int P\\dd{x}} Q(x) \\dd{x} \\end{equation}\nthe more general solution (for definite integrals):\n\\begin{equation} x(t) = e^{-A(t)}x_{0} + e^{-A(t)}\\int_{t_{0}}^{t}e^{A(s)}b(s)\\dd{s} \\end{equation}\ngiven the initial condition that \\(x(0) = 0\\). This is from the textbook.\nBefore you go ham and start solving, though, make sure that pesky \\(y\\) term is actually there. If its not, you maybe better served using the seperable methods to solve these things.\nthis is bad This is difficult to deal with this! What?? How?? Why does this work?? See below.\nsolving differential equations The following technique works for ALL first-order linear differential equations:\nTo solve, first put your equation into the standard form:\n\\begin{equation} \\frac{dy}{dx} + P(x)y = Q(x) \\end{equation}\nIf you have an equation like:\n\\begin{equation} a(x) \\dv{y}{x} + b(x)y = c(x) \\end{equation}\na good way to do this is to apply \\(\\frac{1}{a(x)}\\) to both sides, resulting in:\n\\begin{equation} \\dv{y}{x} + \\frac{b(x)}{a(x)} y = \\frac{c(x)}{a(x)} \\end{equation}\nAnd then you can carry on solving like its an equation in standard form.\nTo solve such a generic equation, we here are trying to UNDO the product rule.\nWe first multiply the entire expression by something called the intergrating factor \\(\\rho(x)\\).\n\\begin{equation} \\rho(x) \\left(\\frac{dy}{dx} + P(x)y\\right) = \\rho(x)Q(x) \\end{equation}\nA note on how this \\(\\rho(x)\\) works. This intergrating factor is actually defined with the following rule:\n\\begin{equation} \\log (\\rho (x)) = \\int P(x) \\dd{x} \\end{equation}\n(notably, \\(\\log\\) is actually \\(\\ln\\) in this case.)\nWhy so weird of an expression? This all springs from the fact that \\(\\dv x e^{x} = e^{x}\\). See below on how this fact is stretched (to great lengths) to solve diffeqs.\nFrom the above expression containing \\(\\rho (x)\\), we naturally have that (based on the definition of the natural log, just expanding it out):\n\\begin{equation} e^{\\int P(x)\\dd{x}} = \\rho (x) \\end{equation}\nWhy is this useful? Remember, we are trying to undo the product rule. Let\u0026rsquo;s replace our new definition for \\(\\rho (x)\\) into the above expression we are trying to solve and see what happens!\n\\begin{align} \u0026amp;\\rho (x)\\qty (\\dv{y}{x} + P(x)y) = \\rho (x)Q(x) \\\\ \\Rightarrow\\ \u0026amp; e^{\\int P\\dd{x}} \\qty (\\dv{y}{x} + P(x)y) = e^{\\int P\\dd{x}} Q(x) \\end{align}\nFor a second now, let\u0026rsquo;s just take an aside and deal with the left side. We are starting to almost clearly see the product rule at play here. Let\u0026rsquo;s finish the job by finishing up the rest of the product rule. Remember, we want to go opposite the product rule at the next steps.\n\\begin{align} e^{\\int P\\dd{x}} \\qty (\\dv{y}{x} + P(x)y) \u0026amp;= \\dv{y}{x}e^{\\int p\\dd{x}} + yPe^{\\int P\\dd{x}} \\\\ \u0026amp;= \\dv x \\qty (ye^{\\int P\\dd{x}}) \\end{align}\nWoah! Now we have something clearly in the favor of \\(y\\) separated out. Let\u0026rsquo;s put this back to our original expression.\n\\begin{align} \u0026amp;e^{\\int P\\dd{x}} \\qty (\\dv{y}{x} + P(x)y) = e^{\\int P\\dd{x}} Q(x) \\\\ \\Rightarrow\\ \u0026amp; \\dv x \\qty (ye^{\\int P\\dd{x}}) = e^{\\int P\\dd{x}} Q(x) \\end{align}\nNice. Now, do you see the clear step to isolate \\(y\\) by itself? I do.\n\\begin{align} \u0026amp;\\dv x \\qty (ye^{\\int P\\dd{x}}) = e^{\\int P\\dd{x}} Q(x) \\\\ \\Rightarrow\\ \u0026amp; \\int \\dv x \\qty (ye^{\\int P\\dd{x}}) \\dd{x}= \\int e^{\\int P\\dd{x}} Q(x) \\dd{x}\\\\ \\Rightarrow\\ \u0026amp; ye^{\\int P\\dd{x}} = \\int e^{\\int P\\dd{x}} Q(x) \\dd{x} \\end{align}\nAnd finally, naturally and lastly, we divide the \\(e^{\\int P\\dd{x}}\\) to both sides.\n\\begin{align} \u0026amp; ye^{\\int P\\dd{x}} = \\int e^{\\int P\\dd{x}} Q(x) \\dd{x}\\\\ \\Rightarrow\\ \u0026amp; y = e^{-\\int P\\dd{x}}\\int e^{\\int P\\dd{x}} Q(x) \\dd{x}\\ \\blacksquare \\end{align}\nAnd there you have it. That\u0026rsquo;s the general solution to our diffeq.\n","permalink":"https://www.jemoka.com/posts/kbhlinear_non_seperable_equation/","tags":null,"title":"Linear Non-Seperable Equation"},{"categories":null,"contents":"A list is an ordered collection of \\(n\\) elements.\nrequirements as list length cannot be negative list length cannot be \\(\\infty\\) repetition matters order matters additional info two lists are equal IFF they have same \\(n\\) same elements same order they are different from sets because order matters (therefore, because in/out is no longer a binary) number of entries of the same object matters length is finite ","permalink":"https://www.jemoka.com/posts/kbhlist/","tags":null,"title":"list"},{"categories":null,"contents":" Number Name 31 Herber Hoover 32 Franklin D. Roosevelt (FDR) ","permalink":"https://www.jemoka.com/posts/kbhlist_of_american_presidents/","tags":null,"title":"list of American presidents"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhlog_laws/","tags":null,"title":"log laws"},{"categories":null,"contents":"TODO: connect Logan with a few fire departments\n","permalink":"https://www.jemoka.com/posts/kbhlogan_s_team_check_in/","tags":null,"title":"Logan's Team Checkin"},{"categories":null,"contents":"DOI: 10.1101/2021.03.24.21254263\nOne-Liner Review paper presenting the \\(ADReSS_o\\) challenge and current baselines for three tasks\nNotes Three tasks + state of the art:\nClassification of AD: accuracy \\(78.87\\%\\) Prediction of MMSE score: RMSE \\(5.28\\) Prediction of cognitive decline: accuracy \\(68.75\\%\\) Task 1 AD classification baseline established by decision tree with late fusion\n(LOOCV and test)\nTask 2 MMSE score prediction baseline established by grid search on parameters.\nSVR did best on both counts; results from either model are averaged for prediction.\nTask 3 Same thing here, DT does better but notably its F1 is smaller; data trained with final late fusion\n","permalink":"https://www.jemoka.com/posts/kbhluz_2021/","tags":["ntj"],"title":"Luz 2021"},{"categories":null,"contents":"Seed: walking, loving\nWalking\nSkipping\nShoes\nRoad\nRunning\nForward\nSpeed\nPlane\nTravel\nUnique\nCold\nHouse\nLoving\nCuddling\nKissing\nHolding\nTogether\nStaring\nLonging\nEstablish\nSpending time\nWaving\nWelling\nWalking together, staring forward longing you\nLoving together, skipping forward, a cold house\nCuddling down the avenue, spending time there, Waving by\nEstablish what it\u0026rsquo;s like,\n","permalink":"https://www.jemoka.com/posts/kbhlyrics_ping/","tags":null,"title":"Lyrics: Ping"},{"categories":null,"contents":"Seed: explore, wild\nexplore\nlearn\nresources\nmineral\ndetail\nfeature\nfact\npolice\nduty\nparticulars\ndeposit\nassign\nundertake\nnatural\nenvironment\ncultivate\nregion\nharshly\nuntrusting\nnervous\nincreasing\nchanging\nperiod\nbecome greater\nWe go explore, changing times, parting ways.\nWanting no praise, become greater Than ever\nWe go explore, shining lights moving stars\nFinding no target, we cannot expect to see\nHow can we explore if we can\u0026rsquo;t even feed? Ourselves? Our families? Our digitaries?\nHow can we explore if we can\u0026rsquo;t even seek. Unatural exploration Touching the depths with our feet\nWe go explore, wondrous depths, random seas\nWanting someone, reminicing the never\nWe go explore, purple skies acid rain\nFinding the target, we didn\u0026rsquo;t know to see\nHow can we explore if we can\u0026rsquo;t even feed? Ourselves? Our families? Our digitaries?\nHow can we explore if we can\u0026rsquo;t even seek. Unatural exploration Probing the depths with our feet\n","permalink":"https://www.jemoka.com/posts/kbhlyrics_laws/","tags":null,"title":"Lyrics: Unnatural Exploration"},{"categories":null,"contents":"DOI: 10.1109/CISP-BMEI.2018.8633126\nA dataset paper with which auditory info about people talking is collected.\nHere are the state-of-the-art as of Laguarta 2021 on the dataset proposed.\n","permalink":"https://www.jemoka.com/posts/kbhlyu_2018/","tags":null,"title":"Lyu 2018"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.623607\nOne-Liner Trained a bimodal model on speech/text with GRU on speech and CNN-LSTM on text.\nNovelty A post-2019 NLP paper that doesn\u0026rsquo;t use transformers! (so faster (they used CNN-LSTM) lighter easier) \u0026ldquo;Our work sheds light on why the accuracy of these models drops to 72.92% on the ADReSS dataset, whereas, they gave state of the art results on the DementiaBank dataset.\u0026rdquo; Notable Methods Bi-Modal audio and transcript processing vis a vi Shah 2021, but with a CNN-LSTM and GRU on the other side.\nKey Figs Figure 1: Proposed Architecture The figure highlights the authors\u0026rsquo; proposed architecture\nFigure 2: confusion matrix In addition to validating prior work by Karlekar 2018 and Di Palo 2019, proposed model C and got accuracy of \\(73.92\\%\\).\n","permalink":"https://www.jemoka.com/posts/kbhmahajan_2021/","tags":["ntj"],"title":"Mahajan 2021"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmahatma_ghandi/","tags":null,"title":"Mahatma Ghandi"},{"categories":null,"contents":"MapReduce is an distributed algorithm.\nMap: \\((in\\_key, in\\_value) \\Rightarrow list(out\\_key, intermediate\\_value)\\). Reduce: Group map outputs by \\(out\\_key\\) \\((out\\_key, list(intermediate\\_value)) \\Rightarrow list(out\\_value)\\) example of MapReduce Say, if you want to count word frequencies in a set of documents.\nMap: \\((document\\_name, document\\_contents) \\Rightarrow list(word, #\\ occurrences)\\) You can see that this can be distributed to multiple processors. You can have each processor count the word frequencies in a single document. We have now broken the contents into divide and conquerable groups.\nReduce: \\((word, list\\ (occurrences\\_per\\_document)) \\Rightarrow (word,sum)\\) We just add up the occurrences that each of the nodes\u0026rsquo; output for word frequency.\n","permalink":"https://www.jemoka.com/posts/kbhmapreduce/","tags":null,"title":"MapReduce"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmarkovian_process/","tags":null,"title":"markovian process"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmartin_luther_king/","tags":null,"title":"Martin Luther King"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2021.642647\nOne-Liner Combined bag-of-words on transcript + ADR on audio to various classifiers for AD; ablated BERT\u0026rsquo;s decesion space for attention to make more easy models in the future.\nNovelty Pre-processed each of the two modalities before fusing it (late fusion) Archieved \\(93.75\\%\\) accuracy on AD detection The data being forced-aligned and fed with late fusion allows one to see what sounds/words the BERT model was focusing on by just focusing on the attention on the words Notable Methods Used classic cookie theft data bag of words to do ADR but for words multimodality but late fusion with one (hot-swappable) classifier Key Figs How they did it This is how the combined the forced aligned (:tada:) audio and transcript together.\nBertbelation Ablated BERT results.\nThe model overall tends to focus on early parts of sentences. y is attention weight, x is position in sentence, blue is TD, red is AD.\nNew Concepts Active Data Representation ","permalink":"https://www.jemoka.com/posts/kbhmartinc_2021/","tags":["ntj"],"title":"Martinc 2021"},{"categories":null,"contents":"The Martingale Model states: if we observed the closing price of the market yesterday, we expect that the market is going to open at the close price yesterday.\nFormally:\n\\begin{equation} E\\qty [X_{k}|X_{k-1}, X_{k-2},\\ldots] = X_{k-1} \\end{equation}\n\u0026ldquo;irrespective of what you know, no matter how long the history, the best expectation of today\u0026rsquo;s price is yesterday\u0026rsquo;s price.\u0026rdquo;\nThis is not a for sure! modeling statement: this is simply the expected value!! That means, after \\(\\infty\\) times of re-running the universe starting \u0026ldquo;yesterday\u0026rdquo;, the new opening price will converge to the last closing price.\nTwo important conclusions:\nIf we know the closing price yesterday (it is observed), the price today will be DETERMINED and not!!! a random variable If the closing price yesterday is a random variable, the price today will be IN-DETERMINED and also a random variable Therefore, the \u0026ldquo;randomness is fair\u0026rdquo;, and therefore the \u0026ldquo;market is not drifting in favor/against you.\u0026rdquo;\nThe Martingale Model comes from the idea that \u0026ldquo;true gambling is true equal conditions (money, opponents, bystanders, situations, die, and dice.)\u0026rdquo; Therefore, any amount of bias towards one direction/party is advantageous for that person.\nIn fact, it was theorized that an efficient market should follow exactly this behavior.\nchanges in history Of course, the difference between the expression:\n\\begin{equation} E\\qty [X_{k}|X_{k-1}, X_{k-2},\\ldots] = X_{k-1} \\end{equation}\nversus\n\\begin{equation} E\\qty [X_{k}|X_{k-1}] = X_{k-1} \\end{equation}\nis pretty big. The two will only be the same if the markets is assumed to be a markovian process.\nMartingale historical conditioning Ok, if we are told that the process is Martingale, but we only have two days ago, what do we have?\ni.e. what if we want to know:\n\\begin{equation} E\\qty [X_{k} | X_{k-2}] = ? \\end{equation}\nTurns out, there\u0026rsquo;s a small trick you can do. Without even Martingale, we can claim that:\n\\begin{equation} E\\qty [X_{k} | X_{k-2}] = \\sum_{x} E\\qty [X_{k} | X_{k-1}, X_{k-1} = x] \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\end{equation}\nThat, the price today is just the sum of all possible prices for day \\(k-1\\) we name small \\(x\\) times the probability \\(Pr\\) that it actually happens given the existing \\(k-2\\) observation.\nOf course, given the Martingale Model now, given some possible price in day \\(k-1\\) named \\(x\\), price in \\(k\\) is also \\(x\\). Therefore:\n\\begin{equation} E[X_{k}|X_{k-1},X_{k-1} = x] =x \\end{equation}\nApplying this, then, we have\n\\begin{equation} \\sum_{x} E\\qty [X_{k} | X_{k-1}, X_{k-1} = x] \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) = \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\end{equation}\nThe right sum, then, is just the expected value of \\(X_{k-1}\\) given \\(X_{k-2}\\)!! Meaning:\n\\begin{equation} \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) = E[X_{k-1} | X_{k-2}] \\end{equation}\nNow, we are in a Martingale Model. Therefore:\n\\begin{equation} \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) = E[X_{k-1} | X_{k-2}] = X_{k-2} \\end{equation}\nAnd so, putting it all together, we have:\n\\begin{align} E\\qty [X_{k} | X_{k-2}] \u0026amp;= \\sum_{x} E\\qty [X_{k} | X_{k-1}, X_{k-1} = x] \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\\\ \u0026amp;= \\sum_{x} x \\cdot Pr \\qty(X_{k-1}=x|X_{k-2}) \\\\ \u0026amp;= E[X_{k-1} | X_{k-2}] \\\\ \u0026amp;= X_{k-2} \\end{align}\nAmazing. So Martingale holds over time\n","permalink":"https://www.jemoka.com/posts/kbhmartingale_model/","tags":null,"title":"Martingale Model"},{"categories":null,"contents":"we never do them anyways\nadditional information multiplying matricies its always row-by-column, move down rows first then columns multiply element-wise and add (row times column and add) invertability Matrices whose determinants are not \\(0\\) (i.e. it is invertable) is called \u0026ldquo;nonsingular matrix\u0026rdquo;. If it doesn\u0026rsquo;t have an inverse, it is called a singular matrix.\nGaussian elimination The point of Gaussian elimination is to solve/identiy-ify a linear equation. Take, if you have a matrix expression:\n\\begin{equation} Ax = b \\end{equation}\nWe can apply \\(A^{-1}\\) to both side, we then have:\n\\begin{equation} A^{-1}Ax = A^{-1} b \\end{equation}\nApplying the definition of the identity:\n\\begin{equation} Ix = A^{-1}b \\end{equation}\nTherefore, to solve for some \\(A^{-1}\\), which would yield \\(x\\).\ndeterminants For a matrix, for instance, like:\n\\begin{equation} \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{bmatrix} \\end{equation}\nWe wish to find the matrix\u0026rsquo;s determinant; we write it down as:\n\\begin{equation} \\begin{vmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{vmatrix} \\end{equation}\ngeometric interpretation of determinants Geometrically, determinants are how matrices send a unit object after its mapping; i.e. how does it transform the area of a unit square.\nelementary matrix elementary matricies are slight variations from the identity matrix which performs the elementary row operations:\nswap rows add a row to another scale rows ","permalink":"https://www.jemoka.com/posts/kbhmatricies/","tags":null,"title":"matricies"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624558\nOne-Liner analyzed spontaneous speech transcripts (only!) from TD and AD patients with fastText and CNN; best was \\(83.33\\%\\) acc.\nNovelty threw the NLP kitchen sink to transcripts fastText CNN (with vary n-gram kernel 2,3,4,5 sizes) Notable Methods embeddings seaded by GloVe fastText are much faster, but CNN won out Key Figs the qual results PAR (participant), INV (investigator)\nNotes Hey look a review of the field:\n","permalink":"https://www.jemoka.com/posts/kbhmeghanani_2021/","tags":["ntj"],"title":"Meghanani 2021"},{"categories":null,"contents":"Applying the MFA aligner upon the Pitt (cookie only) data and performing statistics upon the calculated disfluency information. The ultimate goal is to replicate Wang 2019.\nThe code is available here.\nThe (unvalidated, draft) results are reported below:\nMean value reported, standard deviation in parens. For our data, \\(N=422\\), cases balanced.\nVariable AD (Pitt, ours) MCI (Wang) Control (ours) Control (Wang) Silence Duration 28.10 (21.28) 13.55 (5.53) 18.06 (12.52) 7.71 (5.03) Speech Duration* 23.77 (14.11) 46.64 (5.79) 27.23 (15.3) 53.63 (7.82) Voice-Silence Ratio 1.79 (4.88) 4.43 (2.78) 5.78 (31.95) 10.11 (6.05) Verbal Rate 1.59 (0.61) 1.56 (0.40) 1.989 (0.51) 1.91 (0.43) *speech duration would obviously vary with file length\nFurther statistical quantification also tells us some more things. Although the data does not make a good classifier, I performed two tests: a Kolmogorov-Smirnov test for goodness of fit, and a good \u0026lsquo;ol Pearson\u0026rsquo;s correlation with AD/control target. p-values are reported below.\nKS test silence duration: \\(1.31 \\times 10^{-5}\\) speech duration: \\(2.98 \\times 10^{-3}\\) voice-silence ratio: \\(2.01 \\times 10^{-7}\\) verbal rate: \\(4.32 \\times 10^{-10}\\) Pearson\u0026rsquo;s silence duration: \\(4.15 \\times 10^{-8}\\) speech duration: \\(0.164\\) voice-silence ratio: \\(0.732\\) verbal rate: \\(1.22 \\times 10^{-12}\\) As per the values reported in Wang 2019, we can see that\u0026mdash;apart from audio metadata\u0026mdash;verbal rate is a strongly correlated indicator against MCI/AD. We can reasonably say that Wang 2019\u0026rsquo;s data collection can be automated with reasonable success using batchalign + MFA.\nBroken ML I applied an RBF Support-Vector machine to classify AD/control based only on the two most highly correlated variables: verbal rate and silence duration. The results were disappointing.\nOn test data, N=42, balanced labels:\nSVC: \\(61.9\\%\\) Random forest: also \\(61.9\\%\\) We have fairly disappointing results. Here\u0026rsquo;s my hypothesis of why:\nif you take a look at this figure, we can see two main distributions\nSo, if we, like Wang 2019, used statistics on independence (they used chi-square, I used KS test), we will come up that the distributions are different.\nHowever, if you take a look at a randomly sampled set of validation data (crosses on the figure), you can see that a lot of them lands in the \u0026ldquo;mostly control\u0026rdquo; area: making the classifier not super useful.\nWe can therefore catch a lot of the \u0026ldquo;slow talking, long pausing\u0026rdquo; patients, but most speaking fluently will possibly need semantic information for prediction.\nI have some preliminary results on Pitt+ERNIE (a kind of BERT) that indicate that a key semantic factor is \u0026ldquo;on-topicness.\u0026rdquo; However, Pitt does not contain a lot of off-topic control data (say, the fluency task, which it has for dementia) for me to validate those claims easily. I will continue work on that front.\n","permalink":"https://www.jemoka.com/posts/kbhmfa_disfluency_measurement/","tags":null,"title":"MFA Disfluency Measurement"},{"categories":null,"contents":" Lanzi WNL (August 12) 1%. Selection Seed 7. Houjun. 82.64% ± 4.48% with a 95% confidence. Lanzi MCI (August 12) 1%. Selection Seed 7. Houjun. 78.70% ± 7.85% with a 95% confidence. Lanzi WNL (August 13) 1%. Selection Seed 7; syllabic balanced. Houjun. Within which, 90.97%±3.40% of multi-syllabic words were correctly identified 86.28%±4.08% of mono-syllabic words were correctly identified 88.63%±2.65% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test. Lanzi MCI (August 13) 1%. Selection Seed 7; syllabic balanced. Houjun. Within which, 76.85%±8.08% of multi-syllabic words were correctly identified 72.22%±8.58% of mono-syllabic words were correctly identified 74.54%±5.86% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test. Lanzi WNL (August 13) 1%. Selection Seed 7; syllabic balanced; 3-tier labeling. Houjun. Within which, 96.75%±2.10% of multi-syllabic words were correctly identified 90.61%±3.46% of mono-syllabic words were correctly identified 93.68%±2.03% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test.\nWithin sucesseses, 16.57% are partial.\nLanzi MCI (August 13) 1%. Selection Seed 7; syllabic balanced; 3-tier labeling. Houjun. Within which, 91.67%±5.30% of multi-syllabic words were correctly identified 78.70%±7.85% of mono-syllabic words were correctly identified 85.19%±4.78% of all words were correctly identified at a confidence interval of 95% based on a single-variable t test.\nWithin sucesseses, 18.48% are partial.\n","permalink":"https://www.jemoka.com/posts/kbhmfa_performance_statistics/","tags":null,"title":"MFA Performance Statistics"},{"categories":null,"contents":"Mia is a student at the Nueva School\n","permalink":"https://www.jemoka.com/posts/kbhmia_tavares/","tags":null,"title":"Mia Tavares"},{"categories":null,"contents":"Micah Brown is a student at The Nueva School, also the host of Project80, among other things.\n","permalink":"https://www.jemoka.com/posts/kbhmicah_brown/","tags":null,"title":"Micah Brown"},{"categories":null,"contents":"Milton Freedman is an economist.\n","permalink":"https://www.jemoka.com/posts/kbhmilton_freedman/","tags":null,"title":"Milton Freedman"},{"categories":null,"contents":"MMSE is not mean squared error! It is a short mental state test to measure one\u0026rsquo;s neuralpsycological capabilities; frequently used as a first line by a psycologist.\n","permalink":"https://www.jemoka.com/posts/kbhmmse/","tags":null,"title":"Mini-Mental State Examination"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhminimum_spanning_tree/","tags":null,"title":"minimum spanning tree"},{"categories":null,"contents":"How many disturbance users can coveather take without crashing? Let\u0026rsquo;s find out.\nCode Util function to mapreduce a list:\ndef multiplyList(l) : # Multiply elements one by one result = 1 for x in l: result = result * x return result We first set a user count:\nN = var(\u0026#34;N\u0026#34;) # Pool size val_percent = var(\u0026#34;val_percent\u0026#34;) # Pools val_pool = N*val_percent user_pool = N*(1-val_percent) # Disturbance disturbance_percent = var(\u0026#34;disturbance_percent\u0026#34;) # Validation Pools + Disburbance val_disturbance_pool = disturbance_percent*val_pool val_normal_pool = (1-disturbance_percent)*val_pool # Chance of three or more disturbance attestors # which is equal to one minus chance of zero, one, or two disturbance attesors no_disturbance_attestor = (val_normal_pool/val_pool)*((val_normal_pool-1)/(val_pool-1))*((val_normal_pool-2)/(val_pool-2))*((val_normal_pool-3)/(val_pool-3)) one_disturbance = [] for disturbance_point in range(0,4): res = [] res.append((val_disturbance_pool)/(val_pool-disturbance_point)) for pre_disturbance in range(0,disturbance_point): res.append((val_normal_pool-pre_disturbance)/(val_pool-pre_disturbance)) for post_disturbance in range(disturbance_point+1,4): res.append((val_normal_pool-post_disturbance)/(val_pool-post_disturbance)) one_disturbance.append(multiplyList(res)) one_disturbance_attestor = sum(one_disturbance) two_disturbance = [] for disturbance_point_i in range(0,4): for disturbance_point_j in range(disturbance_point_i+1,4): res = [] res.append((val_disturbance_pool)/(val_pool-disturbance_point_i)) res.append((val_disturbance_pool-1)/(val_pool-disturbance_point_j)) for pre_i_disturbance in range(0,disturbance_point_i): res.append((val_normal_pool-pre_disturbance)/(val_pool-pre_disturbance)) for sandwich in range(disturbance_point_i+1,disturbance_point_j): res.append((val_normal_pool-post_disturbance)/(val_pool-sandwich)) for post_j_disturbance in range(disturbance_point_j+1,4): res.append((val_normal_pool-post_disturbance)/(val_pool-post_j_disturbance)) two_disturbance.append(multiplyList(res)) two_disturbance_attestor = sum(two_disturbance) distubrance_chance(N, val_percent, disturbance_percent) = expand(1-(no_disturbance_attestor+one_disturbance_attestor+two_disturbance_attestor)) # no_disturbance_attestor (N*(disturbance_percent - 1)*val_percent + 3)*(N*(disturbance_percent - 1)*val_percent + 2)*(N*(disturbance_percent - 1)*val_percent + 1)*(disturbance_percent - 1)/((N*val_percent - 1)*(N*val_percent - 2)*(N*val_percent - 3)) z = var(\u0026#34;z\u0026#34;) val_dist(val_percent, disturbance_percent) = distubrance_chance(100, val_percent, disturbance_percent) implicit_plot3d(val_dist-z, (val_percent,0.1,1), (disturbance_percent, 0,1), (z, 0,1) ,frame=True,axes_labels=[\u0026#39;Validation\u0026#39;,\u0026#39;Disturbance\u0026#39;, \u0026#39;Chance\u0026#39;],axes=False, color=(val_dist,colormaps.Blues)) Launched html viewer for Graphics3d Object z = var(\u0026#34;z\u0026#34;) n_dist(N, disturbance_percent) = distubrance_chance(N, 0.1, disturbance_percent) show(implicit_plot3d(n_dist-z, (N,100,100000), (disturbance_percent, 0,1), (z, 0,1) ,frame=True,axes_labels=[\u0026#39;N\u0026#39;,\u0026#39;Disturbance\u0026#39;, \u0026#39;Chance\u0026#39;],axes=False, color=(n_dist,colormaps.Blues)), aspect_ratio=[1,100000,100000], plot_points=100) Launched html viewer for Graphics3d Object n_dir(N) = distubrance_chance(N, 0.1, 0.1) # plot(n_dir, (N,100,100000),axes_labels=[\u0026#39;N\u0026#39;, \u0026#39;Disturbance\u0026#39;], thickness=1) # solve(distubrance_chance(100, N, 0.1)==0.01, N, to_poly_solve=True) # implicit_plot(distubrance_chance(100, N, 0.1)==0.01, (N, 0,1), (z, 0, # solve(distubrance_chance(N, val_perc, 0.1)==0.01, val_perc, to_poly_solve=True) # implicit_plot(solve(distubrance_chance(N, val_perc, 0.1)==0.01, val_perc)[0]) # val_perc = var(\u0026#34;var_perc\u0026#34;) show(implicit_plot(distubrance_chance(N, val_perc, 0.1)==0.01, (N, 15, 1000), (val_perc, 0,1), plot_points=300,axes_labels=[\u0026#39;N\u0026#39;,\u0026#39;Val Ratio\u0026#39;],axes=False), aspect_ratio=800) # solve(distubrance_chance(800, val_perc, 0.1)==0.01, val_perc, to_poly_solve=True) \u0026lt;/Users/houliu/.sage/temp/baboon.jemoka.com/64368/tmp_9bdcu2si.pn\u0026gt;\n","permalink":"https://www.jemoka.com/posts/kbhminimum_user_base_requirements_for_coveather/","tags":null,"title":"minimum user base requirements for coveather"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhminimum_wage/","tags":null,"title":"minimum wage"},{"categories":null,"contents":"Reading notes Malcom X\u0026rsquo;s father was an active prechear in the scene Malcom X and MLK are both made mostly charactures out of context Malcom X had a belligent upbringing with a belligent father, whereas MLK lived in relative comfort as a son of a successful minister Malcom was sent into white foster families as his mother became institutionalized Becasue of his experience in foster system, Malcom tried to pass/be white King\u0026rsquo;s nonviolent priciples not understood and became conflicted with ideas of local leaders Malcom found a father figure in the Nation of Islam, changing his name in prison MLK had more positive African American role models in life Malcom X disallusioned with the policy of nonengagement by the nation of islam Malcom X had support over racial seperatism Nation of Islam wanted to create a completely seperate Black state, promoting Black Nationalism secret Malcom X wanted break because of skeptism again Eli Mohammed Malcom charged MLK with infiltration Martin believes that the process of voilence is a form of naïve expression King believes that the \u0026ldquo;strong demagogic oratory\u0026rdquo; of Malcom was detrimental and extremist Martin believes that the personal nature of assults from Malcom maybe result in physical assult Malcom was suspended during 1963, and became independent\u0026mdash;wanted to combine religion and politics like King Malcom began forging ties with millitan Black movement Martin regretted that integration has not proceeded, but believed it would have been difficult anyways Rejected nonviolent and intergrational movement People saw King and X\u0026rsquo;s ideas inrecosiliable But, King and X themselves made a possible shared ending by the end Believes that suicides were cut short Racial pride was a centering point: while Malcom saw it as something to be harbored, Martin saw it as inate ","permalink":"https://www.jemoka.com/posts/kbhmlk_and_malcom_x_reading/","tags":null,"title":"MLK and Malcom X Reading"},{"categories":null,"contents":"modalization is the\n","permalink":"https://www.jemoka.com/posts/kbhmodalization/","tags":null,"title":"modalization"},{"categories":null,"contents":"Monetarist theory is a theory of economics proposed by Milton Freedman which asserts that Keynsian economics only applies in the limited case that central bank need to keep the money supply growing; otherwise, the free market can handle itself.\nTherefore the Monetarist theorists propose that the stock market crash of 1929 was caused that the US monetary fund did a bad job of actually controlling the funds, and didn\u0026rsquo;t inject enough money into economy.\nSee also the opposite: demand-driven theory.\n","permalink":"https://www.jemoka.com/posts/kbhmonetarist_theory/","tags":null,"title":"Monetarist theory"},{"categories":null,"contents":"The fallout of the Rosa Parks incident, which is when many of Montgomery residents.\nThe boycotts were developed by Martin Luther King.\n","permalink":"https://www.jemoka.com/posts/kbhmontomery_bus_boycott/","tags":null,"title":"Montgomery Bus Boycott"},{"categories":null,"contents":"The multiplicative identity allows another number to retain its identity after multiplying. Its \\(1\\) [for fields?].\n","permalink":"https://www.jemoka.com/posts/kbhmultiplicative_identity/","tags":null,"title":"multiplicative identity"},{"categories":null,"contents":"TBD\n","permalink":"https://www.jemoka.com/posts/kbhmultiplying/","tags":null,"title":"multiplying"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhmy_day/","tags":null,"title":"My Day"},{"categories":null,"contents":"NACC is a large, longitudinal dataset for neurodegentitive disease as a project in collaboration with Dr. Alyssa Weakley at UC Davis.\nDr. Alyssa Weakley is interested in\nEarly Cognitive Change Mild Cognitive Impairment (MCI) \u0026ldquo;How early can we detect, using NACC, change?\u0026rdquo;\ndataset construction Participants are given a battery of mental capacity tests, these values are tracked over time There are also family member questionnaire Neuroimaging and biomarker data Other things tracked in the data\u0026mdash;\nAmyloid levels of spinal fluid Detecting even earlier focus good to focus on specifically alzheimer\u0026rsquo;s type dementia (so, ignore things on lewy body disease) Using clinical diagnosis as the dependent variable, but good to see the autopsy results Items 3 and 7 are independent codes; if alzhimer\u0026rsquo;s is measured, MCI is not measured. visa versa.\n","permalink":"https://www.jemoka.com/posts/kbhnacc/","tags":null,"title":"NACC"},{"categories":null,"contents":"natural numbers (\\(\\mathbb{N}\\)) are the counting numbers: 1,2,3,4\u0026hellip;.\nZero is not part of it; this produces interesting results like set of natural number under addition is not a group because there is no identity (tbh nor inverse (inverse of 1 is -1 which is not in the set.))\n","permalink":"https://www.jemoka.com/posts/kbhnatural_numbers/","tags":null,"title":"natural number"},{"categories":null,"contents":"The nsm theory is a theory that\u0026hellip;\nclaims that there exists a set of semantic primes and logic universal across languages which is indefinable by other words within the language which, as a corollary, resolves the epistemological problem that if all words are defined by other words in the language there will (why?) be no connection to the real world the theory of NSM rests on\u0026hellip;\ntwo pillars of NSM theory existence of semantic primes The existence of semantic primes is codified more formally as the strong version of the Lexicalization Hypothesis.\nIssues with it: problems with semantic primes\nthe ability to perform the act of reductive paraphrase Issues with that: problems with reductive paraphrasing\noh cool! (Bohnemeyer 2004)\nAlso the fact that NSM is first found in English means that there is a certain anglo-centrism that comes with the language.\n","permalink":"https://www.jemoka.com/posts/kbhnatural_semantic_metalanguage/","tags":null,"title":"Natural Semantic Metalanguage"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhnatural_transformations/","tags":null,"title":"natural transformation"},{"categories":null,"contents":"needfinding as a process of finding need.\nneedfinding with Rick Wallace needfinding with Rick Wallace. You don\u0026rsquo;t find out what they need, but you find what they need and how to fix it. (duh?)\n","permalink":"https://www.jemoka.com/posts/kbhneedfinding/","tags":null,"title":"needfinding"},{"categories":null,"contents":"Neoclassical Economics is a view of economics that disregards the Keynsian Politics theory of the economy needs a minder started by Milton Freedman. It believes that free market economy will prevail.\n","permalink":"https://www.jemoka.com/posts/kbhneoclassical_economics/","tags":null,"title":"Neoclassical Economics"},{"categories":null,"contents":"A set of policy by Franklin D. Roosevelt (FDR) which helped saving the economy during the Great Depression.\nSaving the Banks Unemployment Relief Industrial Recovery Agriculture Creates the WPA. Also the Social Security Administration. Also created Rural Electrification Administration\nMany people were still left out.\n","permalink":"https://www.jemoka.com/posts/kbhnew_deal/","tags":null,"title":"New Deal"},{"categories":null,"contents":"A reformist, counterculture movement during the \u0026rsquo;80s lead by Ronald Reagan. Its a new response to the neoliberalism which aligned the blocks of Evangelical Christians (25% of voters) and Business leaders (powerful leaders.)\nAmerican liberalism expands under the new right as well.\nPresident as a party leader: Reagan is often shown as shining beaken of the Republican Party Leadership\u0026mdash;won every single state except Georgia .\n","permalink":"https://www.jemoka.com/posts/kbhnew_right/","tags":null,"title":"New Right"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhchomsky/","tags":null,"title":"Noam Chomsky"},{"categories":null,"contents":"In this project, we aim to derive situations for the existence of a differential equation for when a family of functions do not intersect. We were able to derive a full solution for the result in linear equations, and we offer an exploration of a partial solution for non-linear cases.\nFunction Families Fundamentally, function families are functions parameterized by some \\(C\\), which has the shape:\n\\begin{equation} y(x, \\dots, c) = f(x, \\dots)+c \\end{equation}\nThrough this result, we can figure a statement for \u0026ldquo;intersection.\u0026rdquo; If two functions intersect, their difference will be \\(0\\); if there is a non-trivial solution (that \\(c_1\\neq c_2\\) \u0026mdash; that, they are not the same function\u0026mdash;still makes \\(y_{C_1} = y_{C_2}\\)), the function family interact.\nWe can test this by subtracting two arbitrary members from the desired family. If it results that \\(c_1-c_2=0 \\implies c_1=c_2\\), we can say that the family does not intersect: that there are no non-trivial solutions to the function having no difference.\nSingle-Order Linear Differential Equations Here, we prove the fact that single-order linear differential equations do not produce solutions that intersect. We have the following single-order linear differential equation:\n\\begin{equation} \\dv{y}{x} + P(x) = Q(x) \\end{equation}\nIf, as desired, our function has a analytical solution (without an integral), we will make both terms differentiable.\n\\begin{equation} \\dv{y}{x} + P\u0026rsquo;(x) = Q\u0026rsquo;(x) \\end{equation}\nRecall the general solution of this expression:\n\\begin{align} y \u0026amp;= e^{-\\int P\u0026rsquo;(x)\\dd{x}} \\int e^{\\int P\u0026rsquo;(x)\\dd{x}} Q\u0026rsquo;(x)\\dd{x} \\\\ \u0026amp;= e^{-(P(x)+C_1)} \\int e^{P(x)+C_1} Q\u0026rsquo;(x)\\dd{x} \\end{align}\nOf course, we can separate the constants \\(e^{C_1}\\) out.\n\\begin{align} y \u0026amp;= e^{-(P(x)+C_1)} \\int e^{P(x)+C_1} Q\u0026rsquo;(x)\\dd{x} \\\\ \u0026amp;= e^{-P(x)} \\int e^{P(x)} Q\u0026rsquo;(x)\\dd{x} \\end{align}\nNow, it is the case that, for the most part, \\(e^{P(x)}Q\u0026rsquo;(x)\\) may not be integral-differentiable. Applying the fundamental theorem, we still have that as the integral function, with some \u0026ldquo;differentiated\u0026rdquo; term which we will call \\(a(x)\\): below\n\\begin{align} y \u0026amp;= e^{-P(x)}(a(x) +C) \\\\ \u0026amp;= e^{-P(x)}a(x) +Ce^{-P(x)} \\end{align}\nExcellent. Now, let\u0026rsquo;s do the subtraction test devised above; if we have that \\(C_1-C_2=0\\) given \\(y_1-y_2=0\\), then we can ensure that the function family do not intersect.\n\\begin{align} y_1 - y_2 =0 \u0026amp;= (e^{-P(x)}a(x) +C_{1}e^{-P(x)})-(e^{-P(x)}a(x) +C_{2}e^{-P(x)}) \\\\ \u0026amp;= C_{1}e^{-P(x)}-C_{2}e^{-P(x)} \\\\ \u0026amp;= (C_{1}-C_{2})e^{-P(x)} \\end{align}\nWe now have that:\n\\begin{equation} 0 = (C_1+C_2)e^{-P(x)} \\end{equation}\nNotably, the codomain of \\(e^{x}\\) is \\((0, \\infty)\\). Having never reached \\(0\\), we have that \\(0=C_1-C_2\\), as desired. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnon_intersecting_graphs/","tags":null,"title":"Non-Intersecting Graphs (Single Order)"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhnonsingular_matricies/","tags":null,"title":"nonsingular matricies"},{"categories":null,"contents":"The nonviolence movement a method of protest which is developed by Mahatma Ghandi and leveraged by Martin Luther King in the civil rights movement.\nThe idea is to achieve civil disobedience and allowing oneself to be punished so egregiously without inciting violence so at to elicit sympathy across the nation.\nThe civil rights movement leveraged training tactics and training to ensure its participants would be completely nonviolent and so elicit the correct response.\n","permalink":"https://www.jemoka.com/posts/kbhnonviolence_movement/","tags":null,"title":"nonviolence movement"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhnormal_distribution/","tags":null,"title":"normal distribution"},{"categories":null,"contents":"Foreword Hi there, internet traveler.\nThe time is 2015/2016, I was either in 5th or 6th grade. At that time, I was barely beginning to be actually comfortable using the language of English.\nOne of the ways I practiced English, which is also a habit I continue to do today, is to write. I write mostly expository prose now, but, back then, shining with childish naïvete, I decided to write a multi-part story as a means of practicing English.\nAt the time, I was fortunately supported by four very helpful adults\u0026mdash;ESL instructors, teachers from the local government\u0026rsquo;s ESL program, local students, family friends\u0026mdash;who have supported me and edited this silly story as a means of helping me better my command of English.\nIronically, this story is set in 2018, I think, two years after when I wrote it. Its now 2022, almost 7 years after. Make of that what you will.\nTherefore\u0026mdash;\nNorman, an epic tale told in N parts\nWritten by yours truly, Houjun Liu, circa 2016.\nEdited by: Lynne Zummo, Dorit Hahn, Susan Cole, and Jennifer Fan.\nTypesetted: May 10th, 2022. Menlo Park, California.\nPrologue: James Peter On a sunny day, in a small house at 1623 Wesson Ave, James lay on a dirty, tiny bed. Suddenly a dog was in James’ sight. James stood up, stared at the dog. It was a small, brown, white, fuzzy dog with a tiny stump. The dog walked around James’ bed, looking silly.\n“Let’s call you Norman! It is a good name for you!”\n“There is no dog allowed in my house, get’em out! RIGHT now, or I will get YOU out!” shouted Mr. Miller.\n“Dude,” a voice came from James’ mind. Mr. Miller, the owner of the Wacky Hair Salon, who is James’ uncle, barged into James’ room, continuously shouting.\n“Get’em out, RIGHT NOW! NOW! YOU HEAR ME?”\nJames, staring at Norman, just didn’t care.\nNorman seemed to not understand all this. He followed Mr. Miller to the window, and \u0026hellip; just as suddenly as he had come in, he was thrown out by Mr. Miller.\nWhile Norman was wandering around, James started crying.\nMonths passed…\nPart 1 On a cold winter afternoon, Mr. Miller is sending James to an orphanage as punishment for doing “bad” things. James just doesn’t understand this. He SIMPLY wants Norman to come back!\nWhen they arrive, James finally realizes why he didn’t have parents. The truth is dreadful: his dad went crazy from programming in binary code.\n“I will go crazy, too,” James thinks. “It is not an easy job, no sir.” His mom’s situation was even worse, for she was killed by the African disease Ebola.\nHe trudges into the front building with Dr. Brains, and sees children that had been starved, gone mad, and even had been wandering around hopelessly! Many questions flew into James’ mind: Will I go crazy, too? Will I be starving, too? Will I also be wandering around like a zombie?! Feeling scared, he starts to wander, feel hungry, and starve like the other kids ……\n“Wait ! NO, I can’t do that,” thought James.\nDr. Brains takes James to walk around the orphanage, he realizes it is actually a better place to be rather than 1623 Wesson Ave. He sees cats, he sees ducks, he sees horses, he sees a playground, and he sees…\nNORMAN!\nPart 2 Dr. Brains, who looks bewildered, is staring at him.\n“How can you know him? You just arrived here!”.\n“Long story…,” explains James. “I once met the dog, and he was thrown out by Mr. Miller from where I used to live.”\n“So, this is PART of our orphanage. As you see, it is big. We now should thank the donor, who passed away, Dr. James Rover Peter…” There is a little pause, then Dr. Brains continues.\n“Who is YOUR dad!”\nThey continue walking until they get to a building labeled ‘EDU_K-4’.\n“This is the K-4 grade educational multifunctional building,” explains Dr. Brains, “where you will be staying for half a year. Then you will move to this building for study.”\nDr. Brains is now pointing at a building labeled ‘EX-EDU_5-12’.\nThey continue to walk until they get to another building labeled ‘OPH_LV #20312’. It is a small, lovely building, much like an apartment. “This is where you will live, in room # 20312_004,” says Dr. Brains while he hands James a key. Then he gives him a packet, which reads: Vanilla orphanage grand rules and schedule.\n“This is all you need, good day! I will leave you here.”\nJames watches Dr. Brains until he is out of sight.\nHe walks straight into the room. It looks clean, neat, like a 3-star hotel. There is a twin size bed, a desk, and a restroom. He sits down and starts to read the packet:\nChapter 1: Grand rules Welcome to Vanilla Orphanage! This is a place where you can enjoy yourself, explore yourself, and get prepared for the world!\nBut, there MUST be some rules in our orphanage to keep you and your classmates safe.\nFirst of all, you MUST not run in the front building.\nSecond, no talking is allowed while a grand meeting is taking place (see chapter 2 for more info).\nThird, follow your schedule all the time.\nFourth, if you have an emergency, use the emergency call phone (You don’t need to dial it, it will automatically connect to Vanilla Orphanage Hospital). But if you can walk and speak normally, go to Vanilla Orphanage Hospital for more help.\nFifth, the use of a regular telephone is only allowed three times a day. If your teacher calls you, it won’t count. You can only use a regular telephone for calling inside-the-orphanage friends, no outside call is allowed. To see the interior telephone numbers, see chapter 3.\nChapter 2: Grand schedule + your personal schedule Grand schedule\nYour personal schedule\nMeet me every OTHER Sunday at 15:00 at grand office starting 1/2/2019.\nGrand meeting will take place every first day of the month at the big hall in the front building. Everyone will attend the grand meeting; it lasts the whole day.\nDinner, Lunch, and Breakfast will be served at Front Building.\nDr. Flynn (k-4 Sciences) 4242-5000-2525 Dr. Jones (5-12 Sciences) 2134-1000-1045 Dr. Foster (k-4 Math) 2456-6206-6200 Ms. Garcia (5-12 Math) 1341-4000-4012 Mrs. Newman (k-4 Talk-it-out assistant) 2563-6374-7407 Mrs. Willems (5-12 Talk-it-out assistant) 8908-6997-9000 Dr. Brains (Headmaster) 2563-0035-3526\nPart 3 A brief day, he does whatever he is told, follows the schedule, does the work. But, something that amazes James is that the food is actually YUMMY.\nHe does enjoy eating at vanilla orphanage. Normally, it is like a buffet, but a limited one. You only can have one serving of meat, 2 vegetables, a delicious main dish (e.g. cooked rice, cooked noodles …).\nBy the table, James sees students laugh at each other, talk with each other, and, from far away, he sees a little brown-white puppy is running to a girl with curly hair, and stops.\nNorman!!!!!!!!!!!!!\nIt is funny that the girl asks exactly the same thing as Dr. Brains asked:” How can you know him?” He explains the whole story why he knows Norman and asks his very own and very first question to the very first student he meets at the vanilla orphanage: “How did he get here?”.\n“Long story,” says the girl, “he first arrived here because of our save the dogs project, Calvin and I found him.”\n“And who are you? I’m James”.\n“Sorry, I forgot about that, my name is Amelia!”.\nA tall, black haired student comes and joins them. “Hi there, what’s up? I heard someone mention my name.”\n“Oh, we were just talking about the dog. Our new friend, James, gave him a name: Norman,” responds Amelia.\n“Guess what?” asked Calvin, “I taught him Chinese!”.\n“Oh interesting, show us!” says Amelia.\n“狗儿，请来一下; I told him to come.” Suddenly, Norman comes and starts running around Calvin. “你的名字叫做 Norman; I told him that his name is Norman,” says Calvin. The dog starts moving around in a funny way, which James feels weird about. “Oh, don’t worry about that, that’s the Funny-Brown-Hula-Stump-Wiggle-Wag-Dance that I taught him,” Says Amelia.\nPart 4 Dong, dong, dong, dong…… The school bell rings, everyone gets up to do everything they need. It’s Sunday. According to Dr. Brains, James needs to meet Dr. Brains at the grand office.\nWhen he arrives, Dr. Brains says nothing but a greeting, and he hands James a slip of paper that says:\nThe organization of Brainiacs: 52345 Brainful way, North town, CA 94780\n“What is the org…”. “Stop! I will explain everything right after!” explains Dr. Brains. “Just remember what this parchment says!”\nHe hands him a telephone, and says, “Dial 52325, when you hear a beep, dial 900. Answer every question it’s asking you.”\nHe does what he is told, then a girl’s voice says: “Welcome to the new member registration center of TOFB, or the Organization of Brainiacs. Please answer the question: What is your address? “James states the orphanage’s address. “What is your reason to join?” Dr. Brains says quietly,” Invited.”\nWho invited you?” James answers:”Dr. Brains.” “ Welcome, again, new member. Please take the blood needle that appears in front of you and use it to poke your left ring finger.” James does this, and the voice says, ”Thanks for joining! Please hang up the phone!”\n“Understand this?” Dr. Brains says, ”Let’s go!”. “But, go where?” asks James. ”T-O-F-B,” replies Dr. Brains. They walk straight into a box, where James spots a device. Dr. Brains pushes a button on the device, and suddenly, James feels dizzy. They are spinning. They spin faster and faster. Finally, he hears a pop, then, suddenly, he falls into another device which is like a poison chamber. He and Dr. Brains open the door, and he sees a small, transparent house that reads T-O-F-B.\nPart 5 They walk straight into the house, and see a small elevator that is made out of glass. While they walk into the elevator, James feels something is seriously wrong. First, this is a one-story building, and unlike the 5th avenue apple retail store, it has no underground floor. Second, the elevator has NO button, how can Dr. Brains go anywhere with this elevator?\nDr. Brains seems solemn, he carefully looks at the emergency speaker, then, suddenly, James hears a loud CRACK. Then the elevator starts getting darker and darker. After 5 seconds, it is not transparent anymore.\nThe elevator starts to go down deeper and deeper. Then a screen pops up.”Hello, WELCOME to the Organization of Brainiacs. Please scan your card…” says a voice. He doesn’t have a card!! He looks around to find Dr. Brains, but, he is gone!\n“Where else can he be?”James thinks,”there is no way out!”. Suddenly, smoke fills the elevator, James first doesn’t realize what it is, but suddenly, he knows it.”Oh oh!” thinks James, ”IT IS GAS!!!!!”\nChapter 2: T-O-F-B Way underground, Dr. Brains hesitates. “OOOOOOOOOOPS! I forgot James in the elevator..” , he thinks, ”and the killer gas X03-00 would be deadly.”\nHe rushes to the “hacker center”, and shouts, ”You guys! STOP the elevator! And STOP the gas! Open the doors! Clear out the gas! He is NOT a criminal!”\nEverybody freezes, and some whisper, ”Oops, x03-00 gas can knock a human out in 10 seconds”.\nPart 6 Back in the elevator, James barely has time to call the emergency. “Does Dr. Brains mean to kill me?” he thinks, ”or is this a test for me?” He has more things to worry about than that. However, the good news is that Dr.Brains and his team hurry to the elevator just in time, which is when he gets knocked off. They give him the medicine that will neutralize the effect of the gas, and then they hurry to prepare the WELCOME event of the new T-O-F-B members in this season.\nSoon after, James wakes up, safe and sound. Dr. Brains is right by him.\n“Sorry for the accident, but here, welcome to T-O-F-B”, Dr. Brains says with a little smile.\nThere is a little awkward moment when he and Dr. Brains both try to say something, but no sounds come out. It doesn’t last long, just a few seconds. Then Dr. Brains continues, ”The Organization of Brainiacs is a little like what you see in the movie M-I-B. We basically are the only legal group in human and alien law that can meet, communicate with, and study the aliens from outer universe. You know one of our aliens: Norman. He actually can speak Hidoeneese AND English.”\n“But what is Hidoeneese?” James asks.\n“Hidoeneese is the language of the Hidonipothan.” Dr.Brains says.\n“And what is Hidonipothan?” he asks, again.\n“Long story short, it’s kind of an alien tribe. Later at breakfast, Norman will explain. By the way, he likes his name Norman.” Dr. Brains responds.\n“What? Breakfast? It’s already morning?” James asks.\n“Well yes, you have been knocked out by gas for almost 12 hours, and now it is 6:00 in the morning,” Dr. Brains says, ”you still can get about 3 hours rest. Everyone in T-O-F-B sleeps late and wakes up late. And one last thing, I will give you the NEW MEMBER #04 packet so you can learn more about T-O-F-B.”\nHe hands him a packet, just like the packet in the orphanage. But it is hand written.\nWelcome, new member, we are proud that you are here. As the founder of T-O-F-B, I will introduce you to the few basics of daily life.\nFirst, you all have an outside “job”, which you will still perform. Since you are a child, AS I KNOW, we will just keep you up-to-date and call you via the headphones that we will give you. We won’t interrupt your class, unless it is an emergency, I promise. You will be meeting once a month so it won’t affect any of your grades.\nSecond, in T-O-F-B, we treat any child like an adult. It means a large work load, but you can also access any part of our centre freely with your BNPS. But in some areas, we want you to have adult supervision.\nYour supervisor is:\nGrave Hono ( Dr. Brains, as a substitute name in the human world)\nWe will give you a map and what you should do later.\nDr. Ranboro\n9/23/2018\nPart 7 He falls asleep……… He dreams about aliens attacking the centre, and only Dr. Brains, Dr. Ranboro, Norman, Amelia, Calvin and a guy who he didn’t know survived. He thinks it’s just a dream, but what he doesn’t know is, this day is coming closer and closer.\n“Wakey, Wakey!” Dr. Brains shouts, laughing” JJJJJJJJAAAAAAMMMMMMMEEEEEEEESSSSSSSSSSSS!!!!!!!!!!!!”. James finally wakes up, and mumbles, ”What the heck in the world was this?”\nDr. Brains seems to be confused. “You didn’t recognize my voice? Wake up, Buddy! Get dressed! The welcoming party is waiting!!”.\nHe gets dressed, hurries to follow Dr. Brains, and they go outside to a “secretive” room that is labeled “G—CHECK, BNPS ROOM”. They go in, and he sees a bunch of devices that are new to him. He sits down, just as Dr. Brains ordered, and Dr. Brains brings a needle to his face, straight into his eyes. “Watch out!” James shouts. He doesn’t even have time to think, as the needle goes in and out of his eyes. Dr. Brains says, “Good, we already got the DNA, scanned the iris, scanned the brain map. Ok, 2 last things, then we are good to go!” He does a bunch of scans on James’ finger, and he enters a password into a machine. “Ok, one last thing. Print your BNPS and tattoo it to your shoulder!” Dr. Brains says. The machine reads “bring human to the tattoo station …… step 3/5”. Dr. Brains orders him to put his shoulder into a cylinder. He feels a little pressure and his shoulder pops out of the machine. He sees a little piece of metal on his shoulder and it reads ”TOFB.1029358612.JP/////////” The machine also prints out a metal card. “Don’t lose it!” Dr. Brains says, “it is your ID here!”\nThey walk out of the room and into the elevator. It is an elevator like the one in the TOFB’s entrance. The one that changes color and transparency, only much more slowly. When it tells him to scan the card, he knows better than to not do so. The elevator seems smart, and it asks “Homo, and James! Morning! Which level area do you want to go to?”. Dr. Brains responds, “Dining room number three, formal, both of us”. The elevator responds with a “TOFB wishes you a pleasant day!” When the door opens again, they enter a large area, like the first level of a 5 star hotel. Everything is white: people’s clothes, the ground, the staircase, the light, etc,. He sees Dr.Brains’ clothes change to white! He says, “Dr.Brains! Your outfit changed color!” “Yours did, too!” Dr. Brains responds. James looks down at his clothes. His had actually, as Dr. Brains said, changed color and texture.\nThey eat their breakfast—salmon, soup and broccoli, and Dr. Brains announces to him, “OK, now let’s do some work stuff”. They head back to the living area, and they wash themselves. Then they head to the meeting area. Norman, Dr. Ranboro and the other guy James sees in his dream are waving to James-and-Dr.Brains-in-the-black-suit-and-a-tie.\nPart 8 “So”, Dr. Ranboro says, “Welcome! Thank you for joining the organiza………?!!!\u0026gt;?*\u0026amp;%*\u0026amp;^%\u0026amp;^%%∆˙ßå˚µß∂˙”. FFF! A small arrow flies though the walls and hits Dr. Ranboro, making his words into nonsense. “å∆∆ß¬—å˚å!!!!!!¡¡¡¡¡¡?¿!¡……Jams…….main sq……is com…..tel..hom……nnnoor…….¡¡¡!!!???¿¿¿å∂ß˚˚˚˚∆ƒå˙”, he says. James can barely understand, but he knows one thing, they will tell him about the main sq…whatsoever.\n“Let’s jump into the topic,” Norman says. “The main sq… is actually an attack called The Main Square Rattle, or what we call TMSR. It’s started because another kind of alien, The Froakan, wants to use humans as slaves, own the TOFB AND the Hidonipothan.The only way to stop that is to get the battle-rattler and rattle it. But if The F’s got the rattler and rattle it, well, we will all freeze and do what they want, like a bunch of zombies. The state of being a zombie is called ratling. Sadly, there isn’t a known cure yet for ratle. But Dr. Brains is working on it! Lastly, the battle-rattler is locked in the Ratle Mountains. And the only way to enter the Ratle Mountain is by using Dr. Ranboro’s key. Otherwise, you will have very little chance to get out alive! And that’s why they shot Dr. Ranboro. As a matter of fact, the arrow is poisonous. If we don’t send him to hospital now, he will become a baby in 72 hours.”\nTalking about Dr. Ranboro, James notices Dr. Ranboro’s hair getting darker and darker from the old-man-white. They send him to the hospital about 5 minutes later.\nChapter 3:That’s called war After another ride on the “TOTP—0111”, which is the “squeeze box” to get to the North Town, they are back at Dr. Brains’ office. But something weird has happened, only students in OPH_LV # 40000 - OPH_LV # 49999 are still in the orphanage. Dr. Brains tries to find out why, but he can’t. And that’s when all of the humans in the orphanage hear a gigantic laugh coming from nowhere.”HHHAAAHHHAAAAHAHHAAA! This is your day, Homo, your death ceremony!HHAAHAHA!”\nMonths passed again……..\nPart 9 The daily live is almost the same as before, just that a part of the students in the orphanage is missing. But live is still very simple. Tasty food, friendly teachers, and visits to TOFB every other week.\nOne day, James is in his math class.\n“So when 2 is raised to the……”\n“Beep! Beep!”\nHis secret headphones from TOFB send a message request to him.\n“Beep! Beep!”\n“Didn’t that Ranboro guy say they won’t interrupt our classes?” James thought.\n“And let’s do some prob….”\n“Beep! Beep!”\nJames requests a bathroom break and answers the headphones in the boys’ restroom.\n“It’s an emergency!!! The Froakans are getting closer to the rattler!!! Help!!!! James, take Homo and get here now!” Norman cries.\nAs fast as he can, he rushes to Dr. Brains’ office, grabs Dr. Brains and locks him and himself into the TOTB-0111.\nAnd as fast as lightning, they are here, in the North town.\nThey rush into the elevator, he swipes his and Dr. Brains’ card and rushes to Dr. Ranboro’s office.\n“Quick! They will rattle it in like…like 20 minutes and we will all ratle!!!”, Norman hollers.\nAnd again, as fast as lightning, they get war-dressed and get into the fastest transport system in TOFB.\nAs James looks down, he is wearing a strong iron chest plate that reads ’T-O-F-B///////The Smarter one’. And on his shoulder, there is a cord which extends from his Digital ID to the chest plate. There is a screen in his chest plate that is unbreakable. There is a soft protection layer, then there is a swimming layer, then the pressure layer, an iron pad, an air supply on his side if the enemy spreads poisonous gas, and an armor on the outside.\nAmazingly, these things only weigh 1 pound and fit perfectly.\nHe is war-trained, so he knows exactly what to do with this fancy outfit. The screen is the main control, the outfit will detect the environment and change to the perfect layer.\nUploaded ate 10/25/2015 [sic.]\nPart 10 The ride seems to be long, but it’s actually only 5 minutes. They will enter the Ratle Mountains from the North End, which is the second-safest route into the mountains without Dr. Ranboro’s key.\nAnd there they are, in the Ratle Mountains. They are led by Mr. Giose, who was the other guy in his dream when he came to the TOFB the first time. The other four warriors are Norman, Dr. Ranboro, Dr. Brains and James. The first 20 miles are short and boring. Nothing happens. But after the 29th mile mark, they enter a cave.\nThe cave is dark. There are only few lights flashing. They are not worried, until they hear a scream.\n“OOOOOOO! Eeeek!”\n“Ahhhhhhhhhhhh! ZZZ! ZZZ…ZZZ..ZZZ…ZZZ…ZZZ…zzz…” The voice is getting smaller and smaller.\n“It is the sleeping spider! It will knock out a human in NO time!” Mr. Giose shouts.\nJames and the whole crew know what to do. They press a few buttons on their screens, and their helmets of their armor dissolve into the air. What is left behind, is the air filtering system.\n“Three! Two! One! It’s gas,” Norman says, playfully.\nDr. Brains spreads out the SSG gas, which will, hopefully, knock out the sleeping spider.\nThat wastes a LOT of time. Before they know it, they all starts to ratle.\nIt is James who feels it first. He feels extremely and uncontrollably happy. He starts running around and talking to other people in a rude way. To himself, it feels like as if he is drifting into unconsciousness.\nThen the same happens to Dr. Brains, and then Norman, followed by Mr. Giose. Luckily, Dr. Ranboro called the TOFB’s team 2 to come for help before he changes, too.\nI never knew what happened after this incident until the year of 2021. Since James was ratling, he couldn’t remember the whole year of 2020. He recovered on the day of 10/26/2021. Dr. Foster, who works at the orphanage AND at TOFB found a cure using Chinese Herbal Tea.\nWell, let’s jump into the time machine. Backward to 2014!!\nChapter 4:Childhood We jump into the time machine, and swoosh. Here we are, in the year of 2014. We are standing in front of 1623 Wesson Ave. It is a sunny day. The Peters are getting ready for a trip to Africa. James greets his uncle, who will look after the house while they are gone. Mrs. Peter is packing hastily. And Mr. Peter is bringing his computer, because, weirdly, he is starting to like CODING in BINARY CODE. Nothing more to say, so here the story goes.\nPart 11 “Hurrrrrryyyy!” Mr. Peter shouts. “Or else we will be late for the plane!”\nThe Peters hurry to the bus stand, waiting for the airport express.\nAfter about an hour ride, they finally arrive at the San Francisco International Airport.\nThey check in. And they hurry to the security check. At the security check, Mrs. Peter thinks she forgot something. Yes, she forgot to bring ANY medication for the disease Ebola.\n","permalink":"https://www.jemoka.com/posts/kbhnorman_an_epic_tale_in_n_parts/","tags":null,"title":"Norman: An Epic Tale in N Parts"},{"categories":null,"contents":"\u0026ldquo;Doing NSM analysis is a demanding process and there is no mechanical procedure for it. Published explications have often been through a dozen or more iterations over several months\u0026rdquo; \u0026mdash; (Heine, Narrog, and Goddard 2015)\nApproach and XD Introduction and Theory The Natural Semantic Metalanguage (NSM) approach (Wierzbicka 1974) is a long-standing hypothetical theory in structural semantics which claims that all human languages share a common set of primitive lexical units\u0026mdash;usually words, but, in some languages, short connected phrases\u0026mdash;through which all other words in each language can be defined.\nFor NSM to hold, two main results must be demonstrated. (Heine, Narrog, and Goddard 2015) The theory\u0026rsquo;s validity hinges, first, upon the existence of semantic primes\u0026mdash;a series of primitive lexical units both indefinable via other words in the same language but also is universally lexicalized across all languages. Second, the theory\u0026rsquo;s confirmation requires the ability to perform \u0026ldquo;reductive paraphrasing\u0026rdquo;, the process of defining all other words in a language with respect to the universal semantic primes\u0026rsquo; manifest in that language.\nIf proven as fact, the NSM theory and its implications has reaching implications into the long-standing (footnote: not to mention often personally fierce) conflict between the newer theories of generative semantics\u0026mdash;where structure of language is created in support of meaning\u0026mdash;and Noam Chomsky\u0026rsquo;s transformational generative syntax\u0026mdash;where meaning is filled to precomputed structure, which NSM suggests (Harris 2021).\nThe difficulty of forming adequate investigations in the area of NSM is due the theory itself being exceedingly hard to falsify\u0026mdash;the principle method through which NSM is demonstrated is via the manual (i.e. non-standardized) lexicalization of semantic primes and a partial demonstration of their relations (Geeraerts 2009) to other words in the language. Whenever one irregularity in the theory is identified (Bohnemeyer 1998), the proponents of the theory simply respond with another update to the (non standardized) set of reductive paraphrasing rules to account for the irregularity (NO_ITEM_DATA:goddard1998bad.)\nYet, there are repeated empirical (again, non-standardized) confirmations of the existence of the original set (Wierzbicka 1974) of semantic primes in other languages (Chappell 2002; Peeters 1994; Travis 2002); there are also numerous demonstrations of the proposed applications (Goddard 2012) of the theory in structural semantics. These facts has therefore maintained the relevance of NSM in current linguistic study but rendered the theory without a very clear path forward. Due to this reason, recent research has placed larger focus on functional (cognitive linguistical) theories (Divjak, Levshina, and Klavan 2016) and largely overlooked structuralist arguments like the NSM.\nBroad Goals and Approach To complement the very large body of work already in the identification of semantic primes for NSM in numerous languages, we aim in this project to investigate the process of reductive paraphrasing to provide a baseline evaluation of the feasibility of NSM as a theory. The approach proposed below is intended to very generally test the practicality of the act of reductive paraphrasing from the published set of primes: whether paraphrasing from those primes is even broadly possible across the entire lexicon of the few languages for which it is purported to be possible. This test remains needed because, quite counter-intuitively, metalanguage theorists have been constructing lexicalizations for non-prime words on an \u0026ldquo;as-needed\u0026rdquo; basis such as in (Wierzbicka 2007). No lexicon-wide demonstrations of lexicalizability has been performed (i.e. reductive paraphrasing all words down to the primes) as the current approach of manual definition of words from primes is significantly time-consuming and requires careful consideration of NSM\u0026rsquo;s semantic grammar between primes.\nWe aim perform a lexicon-wide test of reductive paraphrasing computationally via much newer approaches in computational linguistics, specifically model-based Natural Language Processing (NLP).\nIn order to isolate the exact problem of reductive paraphrasing, we first will have to highlight a few key assumptions by the NSM theory and therefore this project.\nThe semantic metalanguage theory is itself built on the assumption that \u0026ldquo;each language is its own metalanguage\u0026rdquo; (Goddard 2002)\u0026mdash;that human languages are broadly lexicalizable by itself (i.e. one can write an English dictionary by only using English.) We believe that the examination of this assumption is not within scope of the study and\u0026mdash;given it is fairly universally true from a practical standpoint (i.e. English dictionaries exist)\u0026mdash;we will take it as fact. We will use this fact further as the control for the feasibility of the approach, as discussed in the section below.\nThe remaining assumptions of NSM to be tested here, then, is that 1) semantic primes exist and 2) the original set of NSM primes published (Wierzbicka 1974) (and in subsequent studies in various other languages highlighted before) are correct and, through reductive paraphrase, can lexicalize every word in the lexicon.\nAims and Experimental Design In this study, we aim to develop a computational protocol for lexicon-wide testing of the possibility of performing reductive paraphrasing for every word in the lexicon given a set of purported semantic primes. Practically, this means that we are trying create a model to test whether all words in a language is lexicalizable when restricted to only using a chosen subset of primes in the same language.\nTo create a truly replicable test for lexicalizability under restriction, we turn to probabilistic NLP approaches. We propose the following metric for lexicalizability: a word is \u0026ldquo;lexicalizable\u0026rdquo; under some set of semantic primes if there is a lossless mapping between a linear combination of the primes\u0026rsquo; latent embeddings to the word in lexicon space.\nUnder this model, all words in the lexicon are lexicalizable by the set of primes being tested if there is a lossless projection of the bases of the lexical space to the primes\u0026rsquo; latent embedding space.\nThat is, given we have a latent embedding space of \\(n\\) semantic primes \\(P^n\\) and some lexicon \\(W\\) with \\(m\\) words, we aim to identify a linear mapping \\(M\\) such that:\n\\begin{equation} Mp = e_{W_j}\\ |\\ p \\in P^n, \\forall j=1\\ldots m \\end{equation}\nwhere, \\(e_{W_j}\\) is the \\(j\\) th standard basis of \\(W\\) (i.e. \\(j\\) th word in the lexicon.)\nThis projection is not, in principle, impossible. In the high-dimensional space of the entire lexicon, individual lexicalized words represent only the basis vectors of the space (and indeed in one-hot encodings for deep learning they are shown as the standard-basis of the lexicon-wide space.) Whereas in the lower-dimensional subspace of primes, a linear combination of primes can be used to represent each lexicalized word in the full lexicon.\nSuccess in identifying a feasible \\(M \\in \\mathcal{L}(P, W)\\) for a given \\(P\\) and \\(W\\) indicates the feasibility of finding a linear combination in \\(P\\) which maps to all \\(w \\in W\\), which means reductive paraphrase of \\(w\\) to a set of primes in \\(P\\) is possible as there is a direct \u0026ldquo;translation\u0026rdquo; (namely, \\(W\\)) from \\(P\\) to \\(W\\).\nTo actually compute \\(W\\) given \\(P\\) and \\(M\\), we leverage the well-established Transformer encoder-decoder architecture for language modeling (Vaswani et al. 2017). Furthermore, we frame the problem as one of unsupervised multi-lingual translation without alignments.\nThe basis of the model proposed to be used to obtain \\(W\\) is (Artetxe et al. 2018), a unsupervised multi-lingual translation model.\nFigure from (Artetxe et al. 2018).\nAs we are performing the task with word embeddings, not sentences like that of (Artetxe et al. 2018), the cross-attention lookup vector will serve no purpose (be \\(0\\)) (Niu, Zhong, and Yu 2021) and hence removed.\nFor the sake of standardization, we will call \\(P\\) the primary language/lexicon \\(L1\\), and \\(W\\) the second language/lexicon \\(L2\\). The basic hypothesis provided by (Artetxe et al. 2018) is that, through alternating samples of \\(L1\\) and \\(L2\\) through the model against their corresponding decoders using a shared encoder and separate decoders, the shared encoder is trained to perform the task of autoencoding for both lexicons at once. Therefore, at prediction time, to get the \u0026ldquo;translation\u0026rdquo; of an input, one simply applies the decoder of the desired lexicon to obtain a result.\nDuring training, the input to the shared encoder can either be a word from either \\(P\\) or $W$\u0026mdash;sampled with equal probability. If the input is from \\(P\\), we connect the output of the shared encoder with the \\(L1\\) decoder and train with the objective of recovering the input. Essentially, we are using the model as a alternate method of training a variational auto-encoder (Klys, Snell, and Zemel 2018) with alternating decoders given the lexicon being analyzed.\nThis task is trivial if the embedding space after the shared encoder is exactly as wide as both lexicon. However, we will restrict the output dimension of the shared encoder to \\(dim(P)\\) which after training we will call the latent embedding space of \\(L1\\); this name is verified and justified as a part of the feasibility check below.\nWe will also use the backtranslation mechanism proposed by (Artetxe et al. 2018) during training: whereby the autoencoded output from \\(L1\\) is used as target for the same input as \\(L2\\) (as well as the reverse), mimicking the process of translation.\nAfter training, the \\(L2\\) decoder would then be the candidate \\(W\\), mapping from the (proposed) latent embedding space of \\(P\\) to the lexicon \\(W\\).\nFollowing both (Artetxe et al. 2018; Conneau and Lample 2019) we will use cross-entropy as the objective function of training.\nFeasibility Checkpoint We first need to show that, as expected, the model architecture proposed above\u0026mdash;upon convergence\u0026mdash;will create a latent embedding for \\(L1\\) after encoding if the output size for encoding is \\(dim(L1)\\) (defined to be equal to \\(dim(P)\\)).\nA trivial test of whether the encoding output is desirably the embedding space of \\(L1\\) is that, through training with a toy mapping \\(P=W=L1=L2\\), we would expect both decoders to be an one-to-one mapping that simply copies the input. That is, after training with \\(P=W\\), we should see that activating one input in the shared post-encoding space should activate one or close to one feature only in both decoder\u0026rsquo;s output space.\nNumerically, this means that the result obtained from taking the mean entropy of both outputs given a singular input activation should be statistically insignificantly different from \\(0\\).\nThat is, we expect that given trained decoders \\(L_1\\) and \\(L_2\\), and standard bases of \\(W=P\\) named \\(e\\), we should see that:\n\\begin{equation} \\frac{\\log(L_1e_j) + \\log(L_2e_j)}{2} \\approx 0: \\forall j = 1\\ldots dim(W) \\end{equation}\nWe expect this result because, through gradient-descent, the quickest minima reachable to capture variation in the input perfectly is the copying task; therefore, we should expect here that if the post-encoding distribution is the same distribution as the input, the model\u0026rsquo;s decoders will fit to the copying task. If the post-encoding distribution is different from the input, the model\u0026rsquo;s decoders would then have to actually perform nontrivial mappings to achieve the desired autoencoding result.\nCheckpoint 2 + Hypothesis 1 The following is the first novel result that we can show with the new architecture. We first hypothesize that the model should converge when training to the target of the (already linguistically accepted, as aforementioned) result that English words are themselves a metalanguage.\nFor \\(dim(W)\\) iterations (similar to (Webb et al. 2011)), we will leave a word chosen at random out of the lexicon of \\(P\\). This operation results in \\(dim(P) = dim(W)-1\\). We will then train the model until a local minima is reached and measure convergence.\nTo test this hypothesis, we will measure the cross-entropy performance of \\(L2\\) decoder upon the word that is left out. The resulting loss should be statistically insignificantly different from \\(0\\) if the word is successfully lexicalized via the \\(dim(W)-1\\) other words not left out in \\(P\\) in the latent embedding space after encoding.\nIf the hypothesis is not successful, the model cannot converge even on a large subset of the entire lexicon, much less in the limited subset of the 60-word NSM-proposed metalanguage; it is therefore imperative not to continue the study unless convergence at this point can be shown. Importantly, however, failures in this step does not show any claims about reductive paraphrasing as we are simply benchmarking the model against a control linguistic assumption we discussed earlier.\nIn any case, it would be valuable at this point to again perform analyze for post-encoding output to observe any reductive paraphrasing behavior.\nHypothesis 2 At this point, we will set the lexicons to the sets we are actually testing. We will set \\(P\\) to be the list of semantic primes established by (Heine, Narrog, and Goddard 2015), and \\(W\\) to the English lexicon.\nShould lexicalization of all of the English lexicon via the semantic primes only be possible, this model should again converge after training with cross-entropy inappreciably different from \\(0\\). This result would indicate the existence of a \\(W\\) (i.e. \\(L2\\) decoder), indicating the possibility of lexicon-wide reductive paraphrasing.\nInstitution and Experience The actual protocol proposed as a part of this study (namely, creating, training, and calculating metrics from the autoencoder) is a technical concept taught as a part of the regular curriculum of Advanced Machine Learning at Nueva; however, expertise and mentorship may still be required when implementing a complex model topology and training mechanism like the one proposed. The open-ended project structure of the Advanced Machine Learning course supports and sometimes necessitate implementing a model like the one proposed with the help of the CS faculty. Therefore, if additional mentorship is indeed required, there exists support available within the institution.\nThe more difficult skill-set to capture is the knowledge regarding the theories of NSM and the field of structuralist linguistics in general. As of writing, we are not aware of any students which has an active research interest in traditional linguistics; however, this knowledge constitute a far more insignificant portion of the actual mechanics of the project and is more importantly very easily taught. Mentorship is also available here from members of the Mathematics and CS faculty with prior research interest in computational linguistics.\nIn terms of equipment, the most important tool required in working with a large-scale neural network is a matrix-mathematics accelerator; this often takes the form of a consumer graphics card and typical desktop computing setup. For the Machine Learning course taught at Nueva, Google\u0026rsquo;s Colab (and their free graphics card addition) is frequently used to address this need and would at a minimum suffice here. Also, it is based on the personal experience of the author, though by no means definite, that a large selection of students at Nueva has comparable hardware for training available at home.\nProvided network access to the computing accelerator, this experiment can be done under any setting and definitely does not necessitate the use of the biology lab.\nImpact Academic Significance Within the short term, this experiment provides two major results. First, it establishes the use of a bifercated unsupervised encoder-decoder translation model like that proposed by (Artetxe et al. 2018) as a Conditional Variational Autoencoder (CVAE) (Klys, Snell, and Zemel 2018) with the ability to define and train the hidden latent representation after encoding. Although traditional CVAEs are frequently more suited for most output-aware generation tasks, this new scheme supports the direct influence of the latent representations of the encoder instead of using an additional input to both the encoder and decoder to influence such representations, like in traditional CVAEs. This difference is significant for as it creates the where dimensional projection is needed but the content of the latent representation itself is also relevant to the study.\nOf course, the short-term result also includes the direct result of the second tested hypothesis: a systemic, lexicon-wide evaluation of the feasibility of reductive paraphrasing. The study is to develop a computational protocol for lexicon-wide reductive paraphrasing by creating a lossless mapping between a linear combination of the primes\u0026rsquo; latent embeddings to the word in lexicon space.\nIf both initial metrics succeeds and the third, final reduction step with actual semantic primes fail, the result would indicate an inability to create such a lossless mapping, and therefore raise concerns about the lexicon-wide applicability of the reductive paraphrasing on the set of published semantic primes. That, there is not even a locally convergent linear combination of primes that will generally describe all of the lexicon, despite the hypothesis by NSM theorists. This result will be highly impactful for NSM theory in general which necessitates the possibilty of reductive paraphrase (Geeraerts 2009) (Vanhatalo, Tissari, and Idström, n.d.).\nOn the long term, demonstrations of reductive paraphrasing has wide-reaching implications into NSM theory is general (Heine, Narrog, and Goddard 2015; Geeraerts 2009), and the field of language learning. The paraphrasing capacity of the proposed embedding would hypothetically be able to create a semantic mapping between a set of words to one other word; in this way, it is not infeasible to create a language-learning tool with continually larger embedding size to slowly create a larger lexicon in the target user. Early results (Sharma and Goyal 2021) have shown a possible application of such an approach, using supervised machine translation techniques.\nLearning and Teaching One to two students, along with a facilitator, would be an ideal size for this experiment. Primarily, the three main roles will include model engineering, training and validation, and model ablation and testing. The last role requires the most amount of traditional linguistics knowledge as the student\u0026rsquo;s role would be to connect the weights in the model to the applicable theories being tested.\nThe study proposed is an extremely conventional empirical Machine Learning/NLP study. From a pedagogical standpoint for XRT, this study will be a diversion from the traditional wet-lab sciences or survey-based educational/social sciences commonly produced by the lab and lead a new avenue for the Lab\u0026rsquo;s expansion. Within Nueva, empirical research into machine learning is frequently done through independent study or the Intro/Advance machine learning courses\u0026mdash;which were recently expanded due to widening interest at the Upper School.\nParticipation in this project provides its constituent students an opportunity to practice publish-quality ML/NLP in a longer-term and multi-stage project previously not possible through semester-long courses. Students are trained to perform model construction, data selection and cleaning, collection of model validation metrics, as well as model ablation and interpretation: important concepts in ML operations taught but not formalized in the Machine Learning course as the course exercises, while open-ended, isolate only one skill and have expected outcomes.\nGiven the demand and rate of student progression between Intro/Advanced courses in ML each year, developing a suitable approach to propagate true machine-learning research will be relevant to upwards of 30 students each year.\nIncidentally, students also get an exposure to the practice of conventional linguistics and the new trend of applying empirical research NLP back against classic semantics; however, the demand for this exact skill is likely small at Nueva.\nThough the tool used and expanded upon by this experiment is applicable to the NLP research community, it is unfortunately difficult to predict its future applications to XRT or Nueva students without seeing more expansion into the area of ML and NLP by the XRT lab.\nSafety and Ethics The following are the responses to the safety and ethics checklist.\nThis project does not satisfy any triggers of the second-expert protocol. All data needed is from a dictionary (for the English lexicon, e.g. (Fellbaum 2010)) as well as the semantic primes listed in a figure on the article (Heine, Narrog, and Goddard 2015). The data is being generated during compute. The actual compute hardware will need to be stored in either in the cloud (not on-prem), physically in the iLab, or (for personal compute hardware), in students\u0026rsquo; homes. An internet connection and a model training acceleration scheme (such as the free Google Colab) would suffice. None foreseeable See below The experiment is done on the English lexicon. It is difficult to imagine a tangible harm from the experiment. This study provides students with an opportunity to conduct a full research study in ML; XRT has not had this from of projects before and approval would result in a new avenue of research being conducted with XRT. However, if the project is not approved, other ML projects may subsequently surface and students can leverage those opportunities to learn about the practice of empirical ML instead. As with most machine-learning projects, it is customary and appropriate to end with a statement on ML ethics and its implications. This study is a linguistics, lexicon-scale study, and the data sourced is available generally and not subject to copyright or any known data-protection laws. The inputs to the model are combinations of English words, and the model produces singular English words. The benefits of this model involves generating new knowledge about the English lexicon and semantic theories. The only known harm of the model involves the mis-intepretation of its results, creating overreaching generalizations to semantic primality analysis or NSM theories. The model and source code can be released to the general public without broad impact.\nAcknowledgments I would like to thank Brandon Cho at Princeton University and Ted Theodosopoulos at The Nueva School for the very interesting discussion/argument that resulted in this proposal almost a year ago. I would like to thank Klint Kanopka at Stanford University for his mentorship and discussion of the overall feasibility of the approach and pointing out the path that lead to the proposed model\u0026rsquo;s basis in machine translation. Finally, I would like to thank Prof. Brian MacWhinney at Carnegie Mellon University for pointing out discourse between structuralism/functionalism during our exchanges and for his mentorship in my exploration of computational linguistics.\nReferences Artetxe, Mikel, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2018. “Unsupervised Neural Machine Translation,” 12. Bohnemeyer, Jurgen. 1998. “Temporal Reference from a Radical Pragmatics Perspective: Why Yucatec Does Not Need to Express ’after’ and ’before’.” Walter de Gruyter, Berlin/New York Berlin, New York. Chappell, Hilary. 2002. “5. The Universal Syntax of Semantic Primes in Mandarin Chinese.” In Studies in Language Companion Series, 243–322. Studies in Language Companion Series. John Benjamins Publishing Company. doi:10.1075/slcs.60.12cha. Conneau, Alexis, and Guillaume Lample. 2019. “Cross-Lingual Language Model Pretraining,” 11. Divjak, Dagmar, Natalia Levshina, and Jane Klavan. 2016. Cognitive Linguistics 27 (4): 447–63. doi:doi:10.1515/cog-2016-0095. Fellbaum, Christiane. 2010. “Wordnet.” In Theory and Applications of Ontology: Computer Applications, 231–43. Springer. Geeraerts, Dirk. 2009. “Neostructuralist Semantics.” In Theories of Lexical Semantics, 124–78. Theories of Lexical Semantics. Oxford University Press. doi:10.1093/acprof:oso/9780198700302.003.0004. Goddard, Cliff. 2002. “The Search for the Shared Semantic Core of All Languages.” In Meaning and Universal Grammar: Theory and Empirical Findings. John Benjamins Publishing Company. ———. 2012. “Semantic Primes, Semantic Molecules, Semantic Templates: Key Concepts in the NSM Approach to Lexical Typology.” Linguistics 50 (3). doi:10.1515/ling-2012-0022. Harris, Randy Allen. 2021. The Linguistics Wars: Chomsky, Lakoff, and the Battle over Deep Structure. Oxford University Press. Heine, Bernd, Heiko Narrog, and Cliff Goddard. 2015. “The Natural Semantic Metalanguage Approach.” In The Oxford Handbook of Linguistic Analysis, edited by Bernd Heine and Heiko Narrog. Oxford University Press. doi:10.1093/oxfordhb/9780199677078.013.0018. Klys, Jack, Jake Snell, and Richard Zemel. 2018. “Learning Latent Subspaces in Variational Autoencoders,” 11. Niu, Zhaoyang, Guoqiang Zhong, and Hui Yu. 2021. “A Review on the Attention Mechanism of Deep Learning.” Neurocomputing 452 (September): 48–62. doi:10.1016/j.neucom.2021.03.091. Peeters, Bert. 1994. “16 Semantic and Lexical Universals in French.” In Studies in Language Companion Series, 423. Studies in Language Companion Series. John Benjamins Publishing Company. doi:10.1075/slcs.25.20pee. Sharma, Prawaal, and Navneet Goyal. 2021. “Zero-Shot Reductive Paraphrasing for Digitally Semi-Literate.” In Forum for Information Retrieval Evaluation, 91–98. Travis, Catherine E. 2002. “4. La Metalengua Semántica Natural.” In Studies in Language Companion Series, 173–242. Studies in Language Companion Series. John Benjamins Publishing Company. doi:10.1075/slcs.60.11tra. Vanhatalo, Ulla, Heli Tissari, and Anna Idström. n.d. “Revisiting the Universality of Natural Semantic Metalanguage: A View through Finnish,” 28. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need,” 11. Webb, Geoffrey I., Claude Sammut, Claudia Perlich, Tamás Horváth, Stefan Wrobel, Kevin B. Korb, William Stafford Noble, et al. 2011. “Leave-One-Out Cross-Validation.” In Encyclopedia of Machine Learning, edited by Claude Sammut and Geoffrey I. Webb, 600–601. Boston, MA: Springer US. doi:10.1007/978-0-387-30164-8_469. Wierzbicka, Anna. 1974. “Semantic Primitives.” Lingua 34 (4): 365–69. doi:10.1016/0024-3841(74)90004-7. ———. 2007. “Bodies and Their Parts: An NSM Approach to Semantic Typology.” Language Sciences 29 (1): 14–65. doi:10.1016/j.langsci.2006.07.002. NO_ITEM_DATA:goddard1998bad. ","permalink":"https://www.jemoka.com/posts/kbhnsm_proposal/","tags":null,"title":"NSM Proposal"},{"categories":null,"contents":"NUS Secondary School Other Duties AP Statistics Index AP Phys C Mech Index Tuning Forks bioinformatics NUS-MATH580 QIC Date Topic \u0026lt;2022-04-05 Tue\u0026gt; physical qubits, manipulating physical qubits \u0026lt;2022-04-08 Fri\u0026gt; making qubits interact \u0026lt;2022-05-10 Tue\u0026gt; Chiara Marletto \u0026lt;2022-05-24 Tue\u0026gt; Strong Free Will NUS-CS223 Algorithms Backlog: Finite State Machine\nDate Topic \u0026lt;2022-04-07 Thu\u0026gt; stable matching problem, stable matching algorithm \u0026lt;2022-05-02 Mon\u0026gt; dynamic programming, relaxation \u0026lt;2022-05-23 Mon\u0026gt; distributed algorithum, randomized algorithum, complexity theory NUS-HIST301 American History Backlog: New Deal, Franklin D. Roosevelt (FDR), Works Progress Administration, effects of the New Deal, Great Depression, Herber Hoover, disinformation\nDate Topic \u0026lt;2022-04-07 Thu\u0026gt; WWII, propaganda \u0026lt;2022-05-02 Mon\u0026gt; cold war \u0026lt;2022-05-09 Mon\u0026gt; civil rights \u0026lt;2022-05-26 Thu\u0026gt; Richard Nixon \u0026lt;2022-06-01 Wed\u0026gt; Ronald Raegan NUS-PHYS301 Mech Date Topic \u0026lt;2022-04-12 Tue\u0026gt; String Yo-Yo Problem, rotational energy \u0026lt;2022-05-24 Tue\u0026gt; Gyroscopes NUS-ENG401 English Date Topic \u0026lt;2022-04-15 Fri\u0026gt; secondary source comparison activity Essays Bluest Eye Essay Planning I, Tituba Essay Planning NUS-MATH530 Please refer to Linear Algebra Index\nNUS-ECON320 Financial Econometrics Date Topic \u0026lt;2022-08-25 Thu\u0026gt; Financial Markets Intro, ECON320 Architecture NUS-CS350 Software Studio Date Topic \u0026lt;2022-08-25 Thu\u0026gt; User Interviews, User Story \u0026lt;2022-09-07 Wed\u0026gt; Software Engineering, Prototyping \u0026lt;2022-09-12 Mon\u0026gt; Task Estimation \u0026lt;2022-09-15 Thu\u0026gt; Documentation and Specification \u0026lt;2022-09-19 Mon\u0026gt; Testing NUS-MATH570 DiffEq Date Topic \u0026lt;2022-08-26 Fri\u0026gt; DiffEq Intro ","permalink":"https://www.jemoka.com/posts/kbhnueva_courses_index/","tags":["index"],"title":"Nueva Courses Index"},{"categories":null,"contents":"A number can be any of\u0026hellip;\n\\(\\mathbb{N}\\): natural number \\(\\mathbb{Z}\\): integer \\(\\mathbb{Q}\\): rational number \\(\\mathbb{R}\\): real number \\(\\mathbb{P}\\): irrational number \\(\\mathbb{C}\\): complex number ","permalink":"https://www.jemoka.com/posts/kbhnumber/","tags":null,"title":"number"},{"categories":null,"contents":"The code created for this problem can be found here.\nProblem 1 Let\u0026rsquo;s begin with a normal function:\n\\begin{equation} f(x) = (\\sqrt{x}-1)^{2} \\end{equation}\nTaking just a normal Riemann sum, we see that, as expected, it converges to about \\(0.167\\) by the following values between bounds \\([0,1]\\) at different \\(N\\):\nN Value 10 0.23 100 0.172 1000 0.167 10000 0.167 100000 0.167 Problem 2 First, as we are implementing a discrete random walk, here\u0026rsquo;s a fun example; \\(p=0.51\\), \\(\\epsilon=0.001\\).\nWhat is particularly interesting about this case is that, due the probability of change being slightly above \\(50\\%\\), we can see that the sequence has an overall positive growth pattern; however, as far as daily returns is concerned, there is almost no value from day-to-day gains in the market.\nTo actually analyze the our expected value for the probability distributions in number of steps \\(T\\) to travel from \\(0\\) to \\(1\\), as a function of \\(p, \\epsilon\\), we perform the following computation:\nExpected Value of T We set:\n\\begin{equation} \\Delta = \\begin{cases} +\\epsilon, P=p\\\\ -\\epsilon, P=1-p \\end{cases} \\end{equation}\nTherefore, for \\(T\\) as a function from \\(0\\) to \\(1\\), we have:\n\\begin{align} E(T)\u0026amp;=\\frac{1}{E\\qty(\\Delta) } \\\\ \u0026amp;= \\frac{1}{p\\epsilon-(1-p)\\epsilon } \\\\ \u0026amp;= \\frac{1}{\\epsilon (2p-1)} \\end{align}\nNow we will calculate the Variance in \\(T\\):\n\\begin{align} Var(T) \u0026amp;= \\frac{1}{Var(\\Delta)} \\end{align}\nWhere, \\(Var(\\Delta)\\) is calculated by:\n\\begin{align} Var(\\Delta) \u0026amp;= E(\\Delta^{2})-E^{2}(\\Delta) \\\\ \u0026amp;= \\qty(\\epsilon^{2} (2p-1)) - \\qty(\\epsilon (2p-1))^{2} \\end{align}\nAnd therefore:\n\\begin{equation} Var(T) = \\frac{1}{\\qty(\\epsilon^{2} (2p-1)) - \\qty(\\epsilon (2p-1))^{2}} \\end{equation}\nProblem 3 Yes, as we expect, that as \\(\\epsilon\\) decreases, the actual steps \\(T\\) it takes to travel from \\([0,1]\\) increases by an order of magnitude. Given \\(10\\) trials, with \\(p=0.51\\) and \\(\\epsilon = \\{0.1,0.01\\}\\), we have that:\n\\(\\epsilon\\) Mean \\(T\\) Std. \\(T\\) 0.1 570.8 1051.142 0.01 3848.2 1457.180 We can see this on the expected value calculations as well, that:\n\\begin{equation} \\lim_{\\epsilon \\to 0} E(T) = \\frac{1}{\\epsilon (2p-1)} = \\infty \\end{equation}\nThis is not true for the case of \\(p=0.5\\), where the limit will create an undefined behavior with \\(0\\infty\\), and l\u0026rsquo;hospital\u0026rsquo;s rule upon \\(\\epsilon\\) doesn\u0026rsquo;t apply here.\nProblem 4 Yes, the quadratic variation converges towards \\(0\\). Similarly as before, with \\(p=0.51\\) and \\(\\epsilon = \\{0.1,0.01,0.001\\}\\), our quadratic variations are:\n\\(\\epsilon\\) quadratic variation 0.1 5.02 0.01 0.32 0.001 0.05 It seems like that, as long as the path terminates and epsilon becomes smaller, the sum of squared difference will converge towards \\(0\\).\nThis means that, for all \\(p\u0026gt;0.5\\), the squared differences will be convergent. However, for \\(p\\leq 0.5\\), the squared differences are arguably still convergent but the sequence doesn\u0026rsquo;t terminate.\nProblem 5 To allow negative values, we changed the function to:\n\\begin{equation} f(x) = ({x}-1)^{2} \\end{equation}\nThe results of running the three expressions with \\(p=0.51\\), \\(\\epsilon=\\{0.1, 0.01, 0.001\\}\\), similarly to before, respectively are as follows:\n\\(\\epsilon\\) \\(f(x_{i})\\) \\(f(x_{i+1})\\) \\(f\\qty(\\frac{x_{i+1}-x_{i}}{2})\\) 0.1 3.03 -2.37 -1.85 0.01 1.7 -1 -0.17 0.001 0.359 0.307 0.938 It seems like\u0026mdash;while all three of these results converge\u0026mdash;they converge to distinctly different limits. Of course, this result also depends on \\(p\\), as the probability determines whether the path is even complete in the first place, which will of course affect the convergence here.\n","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_stochastic_integration/","tags":null,"title":"NUS-ECON320 Stochastic Integration"},{"categories":null,"contents":"Let \\(X\\) denote price and \\(Y\\) denote volatility. The two objects obey the following process:\n\\begin{equation} \\begin{cases} \\dd{X} = \\mu X \\dd{t} + XY \\dd{W} \\\\ \\dd{Y} = \\sigma Y \\dd{B} \\end{cases} \\end{equation}\nwhere, \\(W\\) and \\(B\\) are correlated Brownian motions with correlation \\(\\rho\\) \u0026mdash; \\(E[(\\dd{W})(\\dd{B})] = \\rho \\dd{t}\\).\nLet\u0026rsquo;s work with \\(Y\\) first. We understand that \\(Y\\) is some continuous variable \\(e^{a}\\). Therefore, \\(\\dv{Y}{t}=ae^{a}\\). Therefore, \\(dY = ae^{a}dt\\). Finally, then \\(\\frac{\\dd{Y}}{Y} = \\frac{ae^{a}}{e^{a}}\\dd{t} = a\\).\nFinally, then, because we defined \\(Y=e^{a} \\implies \\ln Y = a = \\frac{\\dd{Y}}{Y}\\).\nSo, we have that:\n\\begin{align} \u0026amp;\\dd{Y} = \\sigma Y\\dd{B} \\\\ \\Rightarrow\\ \u0026amp; \\dd{\\log Y} = \\frac{\\sigma Y\\dd{B}}{Y} = \\sigma \\dd{B} \\end{align}\nThis tells that the change in log returns in \\(Y\\) is normal (as \\(B\\) is a Brownian Motion), with a standard deviation of \\(\\sigma\\). Therefore:\n\\begin{equation} \\dd{\\log Y} \\sim \\mathcal{N}(0, \\sigma^{2} \\dd{t}) \\end{equation}\nWe therefore see that the log-returns of \\(Y\\) is a normal with variance \\(\\sigma^{2}\\), making \\(Y\\) itself a Brownian Motion with center \\(0\\) and variance \\(\\sigma^{2}\\).\nSo now, tackling the expression above in \\(X\\), we will do the same exact thing as above and divide by \\(X\\):\n\\begin{equation} \\dd{\\log X} = \\mu \\dd{t} + Y\\dd{W} \\end{equation}\nSo we can see that \\(X\\) is a Geometric Brownian Motion as a sum of two random variables\u0026mdash;its volatility is determined by \\(Y\\) with a time-drift \\(\\mu \\dd{t}\\).\nWe see that we are almost ready to have an analytical solution here, because the top expression is applying some function \\(f=\\log\\) to a stochastic differential equation by time; however, the right side \\(Y\\) here is not quite a constant (it is itself a stochastic process), so we can\u0026rsquo;t simply apply an Itô Intergral and call it a day.\nSo instead, we will proceed to a Monte-Carlo simulation of the results to verify as much as we can.\nWe will begin by setting the sane values for variances\u0026mdash;having \\(0.1\\%\\) drift and \\(1\\%\\) variance in variance, and the two Brownian motions being inverses of each other \\(\\rho = 0.5\\).\nmu = 0.001 sigma = 0.01 rho = 0.5 (mu,sigma,rho) (0.001, 0.01, 0.5) We will seed a standard Brownian motion; as the two random motions are covariate, we can use the value of one to generate another: therefore we will return both at once.\nfrom numpy.random import normal def dbdw(): dB = normal() dW = dB + normal(0, (1-rho)**2) return (dB, dW) dbdw() (-1.0246010237177643, -1.281335746614678) Excellent.\nWe will now simulate the system we were given:\n\\begin{equation} \\begin{cases} \\dd{X} = \\mu X \\dd{t} + XY \\dd{W} \\\\ \\dd{Y} = \\sigma Y \\dd{B} \\end{cases} \\end{equation}\nLet\u0026rsquo;s set the number of trials to \\(10000\\).\nN = 1000 We will measure the convergence of \\(\\bar{\\dd{X}}\\) and \\(\\bar{\\dd{Y}}\\): we will tally each value at each time \\(t\\) as well as compare their expected values over time.\nWe will first seed our systems at \\(1\\%\\) variance and \\(1\\) dollar of price.\nX = 1 Y = 0.01 Now, it\u0026rsquo;s actual simulation time!\n# history of Y and X X_hist = [] Y_hist = [] # history of dx dX_hist = [] dY_hist = [] # current expected value EdX = 0 EdY = 0 # difference in E EdX_diff = 0 EdY_diff = 0 # for n loops, we simulate for _ in range(N): # get a source of randmess dB, dW = dbdw() # get the current dx and dw _dX = mu*X+X*Y*dW _dY = sigma*Y*dB # apply it X += _dX Y += _dY # tally it Y_hist.append(Y) X_hist.append(X) dX_hist.append(_dX) dY_hist.append(_dY) # calculate new expected value # we don\u0026#39;t store it immediately b/c we want to check convergence _EdX = sum(dX_hist)/len(dX_hist) _EdY = sum(dY_hist)/len(dY_hist) EdX_diff = abs(_EdX-EdX) EdY_diff = abs(_EdY-EdY) # store new expected value EdX = _EdX EdY = _EdY Let\u0026rsquo;s observe a few values! For starters, let\u0026rsquo;s measure our new expected values.\nEdX 0.0013333651336800837 EdY -1.225482645599256e-06 And, let\u0026rsquo;s check if we have converged by seeing if the difference is a reasonably small value:\n(EdX_diff, EdY_diff) (2.578663659035343e-05, 1.2183875816528115e-07) Looks like both of our variables have converged. Now, let\u0026rsquo;s plot a few things. Let\u0026rsquo;s first build a table with our data.\nimport pandas as pd data = pd.DataFrame({\u0026#34;price\u0026#34;: X_hist, \u0026#34;variance\u0026#34;: Y_hist}) data[\u0026#34;time\u0026#34;] = data.index data price variance time 0 0.998644 0.009974 0 1 0.980393 0.009796 1 2 0.998355 0.009967 2 3 0.994514 0.009913 3 4 1.001363 0.009961 4 .. ... ... ... 995 2.323640 0.008778 995 996 2.321473 0.008715 996 997 2.343427 0.008818 997 998 2.306271 0.008654 998 999 2.333365 0.008775 999 [1000 rows x 3 columns] We will use this to continue the rest of our analysis. For data augmentation, we will also calculate the natural logs of the change to get the rate of change.\nimport numpy as np data[\u0026#34;price_log\u0026#34;] = np.log(data.price) data[\u0026#34;variance_log\u0026#34;] = np.log(data.variance) data[\u0026#34;price_log_change\u0026#34;] = data.price_log - data.price_log.shift(1) data[\u0026#34;variance_log_change\u0026#34;] = data.variance_log - data.variance_log.shift(1) # drop the first row we have w/o change data = data.dropna() data price variance ... price_log_change variance_log_change 1 0.980393 0.009796 ... -0.018444 -0.018005 2 0.998355 0.009967 ... 0.018155 0.017332 3 0.994514 0.009913 ... -0.003855 -0.005443 4 1.001363 0.009961 ... 0.006863 0.004801 5 0.991306 0.009895 ... -0.010094 -0.006639 .. ... ... ... ... ... 995 2.323640 0.008778 ... 0.002148 0.002124 996 2.321473 0.008715 ... -0.000933 -0.007227 997 2.343427 0.008818 ... 0.009413 0.011765 998 2.306271 0.008654 ... -0.015983 -0.018804 999 2.333365 0.008775 ... 0.011680 0.013827 [999 rows x 7 columns] Let\u0026rsquo;s begin by plotting what we have:\nimport seaborn as sns import matplotlib.pyplot as plt sns.set() We will plot price and variation on two axes.\nplt.gcf().clear() sns.lineplot(x=data.time, y=data.price, color=\u0026#34;g\u0026#34;) ax2 = plt.twinx() sns.lineplot(x=data.time, y=data.variance, color=\u0026#34;b\u0026#34;, ax=ax2) plt.show() Where, the blue line represents the percent variance over time and the green line represents the price. Given the \\(0.1\\%\\) drift we provided, we can see that our simulated market grows steadily in the 1000 data point.\nWe can then plot the log (percent) changes.\nplt.gcf().clear() sns.lineplot(x=data.time, y=data.price_log_change, color=\u0026#34;g\u0026#34;) ax2 = plt.twinx() sns.lineplot(x=data.time, y=data.variance_log_change, color=\u0026#34;b\u0026#34;, ax=ax2) plt.show() As you can see\u0026mdash;we have fairly strong random variables, centered around \\(0\\). Having verified that our drift and variables behave in the way that we expect, we can proceed with analysis.\nWe can use a single-variable \\(t\\) test to figure the \\(99\\%\\) confidence band of the result. To do this, we first need to calculate the mean and standardized deviation of the price percent change (log difference).\nlog_mean, log_std = (data.price_log_change.mean(), data.price_log_change.std()) (log_mean, log_std) (0.0008495184126335735, 0.008471735971085885) And now, we will calculate the\nfrom scipy.stats import t lower_bound, upper_bound = t.interval(0.99, len(data)-1, loc=log_mean, scale=log_std) lower_bound -0.021014037766751738 Therefore, with \\(99\\%\\) confidence, we can say that our asset\u0026mdash;given its current parameters, and an \\(N=1000\\) Monte-Carlo simulation\u0026mdash;will not have a more than \\(2.1\\%\\) drop in value.\nWe will use a hedged option to minimize loss. We will use this value to determine the maximum loss for an European put option, maturing in \\(T\\) time, such that the exercise thereof will be hedged against drops of asset price.\nFirst, we will determine the cost of a correctly hedged European put option.\nWe will define \\(S_{0}\\) as the current price of the asset. We will use \\(P\\) as the price of the put option.\nWe desire the strike price of the option to be:\n\\begin{equation} K = S_{0} + P \\end{equation}\nthat is: the price of the put option we desire here will recuperate the price to trade the option and protect against loss. We will symbolically solve for the price of such an option.\nNote that the codeblocks switches here from standard Python to SageMath.\nWe first define the standard normal cumulative distribution.\nfrom sage.symbolic.integration.integral import definite_integral z = var(\u0026#34;z\u0026#34;) N(x) = 1/sqrt(2*pi)*definite_integral(e^(-z^2/2), z, -infinity, x) We will then leverage the Euopean call Black-Scholes model to calculate the optimal put price. We first instantiate variables \\(T\\), and we will set current time to be \\(0\\).\nWe will use \\(v\\) for \\(\\sigma\\), the volatility of the security. We will use \\(S\\) for current price. Lastly, we define \\(P\\) to be our put price. We will call \\(r\\) our risk-free rate.\nTo determine the discount factor, we first implement symbolically our expression for desired strike price.\nv,T,S,P,r = var(\u0026#34;v T S P r\u0026#34;) K = S+P K P + S Great. Now we will implement our discount factors.\nd1 = 1/v*sqrt(T) * (ln(S/K) + (r+v^2/2)*(T)) d2 = d1-v*T d1, d2 (1/2*((v^2 + 2*r)*T + 2*log(S/(P + S)))*sqrt(T)/v, -T*v + 1/2*((v^2 + 2*r)*T + 2*log(S/(P + S)))*sqrt(T)/v) And lastly, we will implement the Black-Scholes expression for puts as a logical expression.\nexpr = P == N(-d2)*K*e^(-r*T)-N(-d1)*S expr \\begin{equation} P = \\frac{{\\left({\\left(\\operatorname{erf}\\left(\\frac{\\sqrt{2} {\\left(T v^{2} + 2 \\, T r + 2 \\, \\log\\left(\\frac{S}{P + S}\\right)\\right)} \\sqrt{T}}{4 \\, v}\\right) - 1\\right)} e^{\\left(T r\\right)} - \\operatorname{erf}\\left(-\\frac{\\sqrt{2} {\\left(2 \\, T v^{2} - {\\left(T v^{2} + 2 \\, T r + 2 \\, \\log\\left(\\frac{S}{P + S}\\right)\\right)} \\sqrt{T}\\right)}}{4 \\, v}\\right) + 1\\right)} S}{\\operatorname{erf}\\left(-\\frac{\\sqrt{2} {\\left(2 \\, T v^{2} - {\\left(T v^{2} + 2 \\, T r + 2 \\, \\log\\left(\\frac{S}{P + S}\\right)\\right)} \\sqrt{T}\\right)}}{4 \\, v}\\right) + 2 \\, e^{\\left(T r\\right)} - 1} \\end{equation}\nNumerical solutions to this expression\u0026mdash;fitting for each of the values from before\u0026mdash;would then indicate the correct price of the option to generate the hedging effect desired.\n","permalink":"https://www.jemoka.com/posts/kbhnus_econ320_volatility_hedging/","tags":null,"title":"NUS-ECON320 Volatility Hedging"},{"categories":null,"contents":"General Information Due Date Topic Important Documents 9/29 Lit. Devices I, Tituba Prompt In an interview, Maryse Conde explains, \u0026ldquo;I was attracted to write the particular story of Tituba because this woman was unjustly treated by history. I felt the need to give her a reality that was denied to her because of her color and her gender.\u0026rdquo; Choose one or two literary devices and explain how Conde uses it/them in the novel to give Tituba her subjecthood. Examples could be: narrative voice, allusion, irony, dialogue, etc.\nClaim Synthesis Quotes Bin Birth Determines Capacity That birth determines the capacity for one to do Evil\nThere was one thing, however, that I didn\u0026rsquo;t know: evil is a gift received at birth. There\u0026rsquo;s no acquiring it. Those of us who have not come into this world armed with spurs and fangs are losers in every combat. (73)\nMama Yaya highlights that misfortune lies in the center of life derived from birth\nMisfortune, as you know, is our constant companion. We are born with it, we lie with it, and we squabble with it for the same withered breast. It eats the codfish from our calabash. But we\u0026rsquo;re tough, us n\u0026mdash;! (85)\nBelieves that having choice in birth is what would make it fulfilling\n(Irony between \u0026ldquo;gift\u0026rdquo; and \u0026ldquo;choice\u0026rdquo;)\nI began to doubt seriously Mama Yaya\u0026rsquo;s basic conviction that life is a gift. Life would only be a gift if each of us could choose the womb that carried us. \u0026hellip; If one day I am born again, let it be in the steely army of conquerors! (120)\nTituba believes that she is born as a healer\nThe terror of these people seemed like an injustice to me. They should have greeted me with shouts of joy and welcome and presented me with a list of illnesses that I would have tried my utmost to cure. I was born to heal, not to frighten. (12)\n\u0026ldquo;Births\u0026rdquo; Other People Elizabeth Parris \u0026ldquo;reborn\u0026rdquo; after Tituba\u0026rsquo;s Care\nUp till then I had not called on the supernatural to care for Elizabeth Parris. \u0026hellip; hat night I decided to use my powers. \u0026hellip; In the morning the color returned to Goodwife Parris\u0026rsquo;s cheeks. She asked for a little water. Toward midday she managed to feed herself. And in the evening she went to sleep like a newborn babe. (45)\nThe \u0026ldquo;evil\u0026rdquo; of abortion transferred from Tituba into Betsy\nI made her swear not to tell anyone and at dusk I plunged her up to her neck in a liquid to which I had given all the properties of amniotic fluid. \u0026hellip; Plunging Betsey into this scalding hot bath, it seemed to me that these same hands, that not long ago had dealt death were now giving life, and I was purifying myself of the murder of my child. (63)\nHer upper lip curled up into an ugly pout, revealing her sick gums. \u0026ldquo;You, do good? You\u0026rsquo;re a Negress, Tituba! You can only do evil. You are evil itself.\u0026rdquo; \u0026hellip; \u0026ldquo;That bath you had me take; what was in it? The blood of a newborn baby that died from one of your spells?\u0026rdquo; I was aghast. (77)\nRebirth After Death (like the actual book) Tituba\u0026rsquo;s Freeing from Prison into Benjamin is Described an Rebirth\nHe smiled cynically. \u0026ldquo;A man who hasn\u0026rsquo;t got very much money. You know how much slaves are selling for at the present time? Twenty-five pounds!\u0026rdquo; Our conversation stopped there, but now I knew the fate awaiting me. Another master, another bondage. (120)\nThen with one skillful blow of the mallet he smashed my chains to pieces. He did the same thing with my wrists while I screamed. \u0026hellip; I screamed, and this scream, the terrified cry of a newborn baby, heralded my return to this world. I had to learn how to walk again. \u0026hellip; Few people have the misfortune to be born twice. (122)\nTituba\u0026rsquo;s \u0026ldquo;real\u0026rdquo; story begins only after death\nAnd that is the story of my life. Such a bitter, bitter story. My real story starts where this one leaves off and it has no end. (175)\nSuccessful rebirth only without birth\nI watched her grow up and stumble around on her shaky legs, exploring the pur- gatory of the plantation, finding her delight in the shape of a cloud, the drooping foliage of an ylang-ylang, or the taste of a bitter orange. \u0026hellip; A child I didn\u0026rsquo;t give birth to but whom I chose! What motherhood could be nobler! (177)\nMisc Book opens with the framing of her being born\nI was born from this act of aggression. From this act of hatred and contempt. (3)\nPackage insert praises death as something positive\n\u0026ldquo;Death is a porte whereby we pass to joye; Lyfe is a lake that drowneth all in payne \u0026ndash;John Harrington\u0026rdquo; (Cover Insert)\nPlans for Abortion\nThere is no happiness in motherhood for a slave. It is little more than the expulsion of an innocent baby, who will have no chance to change its fate, into a world of slavery and abjection\u0026hellip;. That night, my baby was carried out of my womb in a flow of black blood. I saw him wave his arms like a tadpole in distress and I burst into tears. (52)\nSubclaim Development Tituba Realizes Birth is Involuntary Birth into live is a deterministic process for which those being \u0026ldquo;born\u0026rdquo; have no agency over. For African folks, Mama Yaya claims that misfortune is one such deterministic factor of their birth. Although Mama Yaya disagrees, Tituba believes that life is not a gift unless it is deterministic (this is of course ironic, because you don\u0026rsquo;t choose your gifts.) Despite the indeterminism, Tituba believes that she is born as a healer She leverages (Re)Birth to change others, to poor results Perhaps as an attempt to help others control (\u0026ldquo;choose\u0026rdquo;) birth, she uses her power to reborn people; like\nElizabeth Parris Betsy Parris But Psych! Both of them turned on her. Especially Betsy Parris.\nAlso her child was aborted.\nThe work literally provides rebirth of Tituba and empowers her to give birth despite her abortion Tituba Herself raised her station against those of Benjamin. Yet, this was not voluntary (see quote in section) and designed by Condé in the story.\nTituba\u0026rsquo;s \u0026ldquo;real\u0026rdquo; story begins only after forcible death/\u0026ldquo;rebirth\u0026rdquo;, and she (author?) considers it nobel to give this new form of birth without giving birth (which didn\u0026rsquo;t happen in real world), but without the request of the born either.\nConclusiony bit So this whole book, beginning at her birth, covered by a celebration of alternative birth illustrates such a process of providing agency.\nThe Claim The motif of birth and rebirth plays an important role in Maryse Condé\u0026rsquo;s work I, Tituba. Despite Tituba\u0026rsquo;s own failed attempt at controlling the (re)birth of herself and others to better their fate in history, Condé offers Tituba a renewed empowerment in birth by both illustrating her \u0026ldquo;rebirth\u0026rdquo; and providing her a chance to elect a descendant she wasn\u0026rsquo;t originally able to bear.\n","permalink":"https://www.jemoka.com/posts/kbhi_tituba_essay_planning/","tags":null,"title":"NUS-ENG401 I, Tituba Essay Planning"},{"categories":null,"contents":"Statement Suppose \\(U_1\\), \\(U_2\\), and \\(W\\) are subspaces of \\(V\\), such that:\n\\begin{equation} \\begin{cases} V = U_1 \\oplus W\\\\ V = U_2 \\oplus W \\end{cases} \\end{equation}\nProve or give a counterexample that \\(U_1=U_2\\)\nIntuition The statement is not true. The definition of direct sums makes it such that, \\(\\forall v \\in V\\), there exists a unique representation of \\(v\\) with \\(u_{1i}+w_{i} = v\\) for \\(u_{1j}\\in U_1, w_{j} \\in W\\) as well as another unique representation \\(u_{2i} + w_{i}=v\\) for \\(u_{2j} \\in U_{2}, w_{j} \\in W\\).\nHowever, the definition of direct sums doesn\u0026rsquo;t guarantee that the distinct unique representations are equivalent; although \\(V\\) can only be represented uniquely by EITHER a sum of \\(U_1+W\\) or \\(U_2+W\\), it does not mean that each \\(v \\in V\\) itself has only one unique representation.\nCounterexample In constructing a counterexample, we turn to the fact that the sums of two variables creates a third free variable; therefore, we can figure two distinct ways of creating a third, final free variable that construct an equivalent space.\nConstructing \\(U_1\\) as a subspace We begin with constructing:\n\\begin{equation} U_1= \\left\\{\\begin{pmatrix} x_1\\\\y_1\\\\2y_1 \\end{pmatrix}, x_1,y_1 \\in \\mathbb{F} \\right\\} \\end{equation}\nBy setting both free variables to \\(0\\), we construct the additive identity. Then:\n\\begin{equation} \\lambda \\begin{pmatrix} x_1 \\\\ y_1 \\\\ 2y_1 \\end{pmatrix} = \\begin{pmatrix} \\lambda x_1 \\\\ \\lambda y_1\\\\ 2(\\lambda y_1) \\end{pmatrix} \\end{equation}\nby multiplication in \\(\\mathbb{F}\\), scalar multiplication, commutativity, and associativity. We can show closure under addition by inheriting the operation in \\(\\mathbb{F}\\) as well as applying distributive to the factor of \\(2\\).\nTherefore, we show that \\(U_1\\) is a subspace of \\(\\mathbb{F}^{3}\\).\nConstructing \\(U_2\\) as a subspace Then, we construct:\n\\begin{equation} U_2=\\left\\{\\begin{pmatrix} x_1 \\\\ y_1 \\\\ 0 \\end{pmatrix}, x_1,y_1\\in \\mathbb{F} \\right\\} \\end{equation}\nWe again have \\(0\\) by setting free variables to create the additive identity. Addition and scalar multiplication is closed by inheriting them from \\(\\mathbb{F}\\) (and the fact that \\(0\\) is the additive inverse and therefore \\(\\lambda 0 = 0\\)).\nTherefore, \\(U_2\\) is a subspace as well in \\(\\mathbb{F}^{3}\\).\nConstructing \\(W\\) as a subspace Finally, we have:\n\\begin{equation} W = \\left\\{\\begin{pmatrix} 0 \\\\ 0 \\\\z_1 \\end{pmatrix}, z_1\\in \\mathbb{F} \\right\\} \\end{equation}\nBy setting \\(z_1=0\\), we have the additive identity. As with above, addition and scalar multiplication is closed through inheritance and that \\(\\lambda 0=0\\).\nConstructing Sum of Subsets Let\u0026rsquo;s construct:\n\\begin{equation} U_1+W = V \\end{equation}\nTake \\(u_1 \\in U_1, w \\in W\\), attempting to construct a \\(v\\in V\\), we have that:\n\\begin{equation} \\begin{pmatrix} x_{1} \\\\ y_1 \\\\ 2y_1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ z_1 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ y_1 \\\\ 2y_1+z_1 \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} \\end{equation}\nConstructing Direct Sum For all vectors in \\(\\mathbb{F}^{3}\\), this is an equivalence with 3 free variables and 3 expressions\u0026mdash;rendering each vector in \\(\\mathbb{F}^{3}\\) to have a representation by \\(U_1+W\\). We can see this also with the unique \\(0\\) test:\nWe see that for:\n\\begin{equation} 0 \\in U_1+W \\end{equation}\nTo solve for some \\(u_1 \\in U, w \\in W : u_1+w = 0\\) we have that:\n\\begin{equation} \\begin{pmatrix} x_{1} \\\\ y_1 \\\\ 2y_1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ z_1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\end{equation}\nwhere the first vector is in \\(U_1\\) and the second is in \\(W\\). The first two expressions tell us that \\(x_1=y_1=0\\); the final equation requires that \\(2y_1+z_1=0+z_1=0\\Rightarrow z_1=0\\) .\nTherefore, the only way to write \\(0\\) is to take each element in the sum to \\(0\\) (i.e. in this case \\(u_1=w=0 \\implies u_1+w = 0\\)), making the above a direct sum.\nTherefore:\n\\begin{equation} U_1 \\oplus W = V \\end{equation}\nIn almost the same manner, we can show that:\n\\begin{equation} U_2\\oplus W = V \\end{equation}\nThat, for some \\(u_2\\in U_2, w \\in W, v \\in V\\):\n\\begin{equation} \\begin{pmatrix} x_1\\\\y_1\\\\0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ z_1 \\end{pmatrix} = \\begin{pmatrix} x_1\\\\y_1\\\\z_1 \\end{pmatrix} \\end{equation}\nfor the first vector in \\(U_2\\), the second in \\(W\\). In fact, this is the statement made in example 1.41.\nCreating the Counterexample Finally, we have that:\n\\begin{equation} \\left\\{\\begin{pmatrix} x_1 \\\\ y_1 \\\\ 2y_1 \\end{pmatrix}: x_1,y_1 \\in \\mathbb{F}\\right\\} \\neq\\left\\{\\begin{pmatrix} x_1 \\\\ y_1 \\\\ 0 \\end{pmatrix}: x_1,y_1 \\in \\mathbb{F}\\right\\} \\end{equation}\n\\(\\forall y_1 \\neq 0\\) in the first expression. Therefore, \\(U_1 \\neq U_2\\), finishing the counterexample. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_1_c_proof_preso/","tags":null,"title":"NUS-MATH530 1.C Problem 23"},{"categories":null,"contents":"Dot product Calculations Let\u0026rsquo;s calculate some dot products!\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 0 \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\2 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\1 \\end{pmatrix} = 4 \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} -1 \\\\1 \\end{pmatrix} = 0 \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\1 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = 4 \\end{equation}\nInterpretation Geometrically, the intepretation of the dot product is the magnitude that comes from scaling the bottom projected value by the top value. This is essentially multiplying the proportion of one vector that\u0026rsquo;s parallel to the other by each other.\nCross Product Calculations Cross products!\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\times \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\0 \\end{pmatrix} \\end{equation}\n\\begin{equation} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} \\times \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\0 \\end{pmatrix} \\end{equation}\nThe dot product is the point that is perpendicular to the other two input vectors.\nWhy its not a field We want to check why the multiplication of vectors in \\(\\mathbb{F}^{3}\\) via taking the cross product cannot form a field.\nWe can safely assume that the addition operation of the vectors derive their closure, commutativity, associativity from these properties in \\(\\mathbb{F}\\).\nTherefore, we will verify these properties with multiplication. The only closed multiplication-like operation of vectors is the cross-product. Let\u0026rsquo;s first define the cross-product.\nGiven two vectors in \\(\\mathbb{F}^{3}\\):\n\\begin{equation} \\begin{pmatrix} a \\\\b \\\\ c \\end{pmatrix}, \\begin{pmatrix} d \\\\ e\\\\f \\end{pmatrix} \\end{equation}\nTheir cross product is the vector in \\(\\mathbb{F}^{3}\\) defined by:\n\\begin{equation} \\begin{vmatrix} \\hat{i} \u0026amp; \\hat{j} \u0026amp; \\hat{k} \\\\ a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\end{vmatrix} \\end{equation}\nTaking the actual determinant, we have that:\n\\begin{equation} \\begin{vmatrix} \\hat{i} \u0026amp; \\hat{j} \u0026amp; \\hat{k} \\\\ a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\end{vmatrix} = \\begin{pmatrix} bf-ce \\\\ dc-af \\\\ ac-db \\end{pmatrix} \\end{equation}\nIdentity Let\u0026rsquo;s first figure the identity of this operation. We wish to figure some \\((a,b,c)\\) such that the result of the cross product would be \\((d,e,f)\\).\nGeometrically, the perpendicularity of a vector is the resulting value of the cross product; however, no vector (apart from \\(\\vec{0}\\)) can be perfectly perpendicular to itself exactly. This would indicate that no such identities exist.\nWe can also observe that there is no \\(f\\) term on the bottom of the cross product. This would indicate that no combination of \\((a,b,c)\\) can construct the needed \\(f\\) on the last entry.\nFinally, for a more formal proof.\nProof: there can not exist a field-like identity for a cross product.\nFor the sake of contradiction let\u0026rsquo;s say for some nonzero vector \\(\\vec{v} \\in \\mathbb{F}^{3}\\), there exists some identity named \\(\\vec{e} \\in \\mathbb{F}^{3}\\) that follows the properties of identities in a field.\nWe first have that:\n\\begin{equation} \\vec{e} \\times \\vec{v} = \\vec{v} \\end{equation}\nby the definition of the identity.\nAnd also that:\n\\begin{equation} \\vec{v} \\times \\vec{e}= \\vec{v} \\end{equation}\nby the fact that field-like operations commutes.\nWe have also the property of cross products that:\n\\begin{align} \u0026amp;\\vec{a} \\times \\vec{b} = -(\\vec{b} \\times \\vec{a}) \\\\ \\Rightarrow\\ \u0026amp; \\vec{a} \\times \\vec{b} + \\vec{b} \\times \\vec{a} = 0 \\end{align}\nBy applying the inverse of \\(-(\\vec{b}\\times \\vec{a})\\) to both sides, as cross products are closed and therefore an additive inverse exists.\nTherefore, we have that:\n\\begin{equation} \\vec{v} + \\vec{v} = 0 \\end{equation}\nWe see then \\(\\vec{v}\\) is its own additive inverse. Therefore \\(\\vec{v}\\) itself is also \\(0\\). But we established that \\(\\vec{v}\\) can be non-zero. Reaching contradiction, \\(\\blacksquare\\). (this is iffy)\nCommutativity Because of the fact that two-by-two matricies exists on the diagonals, the cross product is also not commutative. In fact,\nDeterminants The geometric interpretation of the determinants is the change in area inside a vector which it stretches a given vector.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_geometric_intepretations/","tags":null,"title":"NUS-MATH530 Geometric Intepretations"},{"categories":null,"contents":" Date Link \u0026lt;2022-09-09 Fri\u0026gt; NUS-MATH530 Solving Systems \u0026lt;2020-09-09 Wed\u0026gt; NUS-MATH530 Geometric Intepretations \u0026lt;2022-09-13 Tue\u0026gt; NUS-MATH530 Linear Vehicles \u0026lt;2022-09-15 Thu\u0026gt; NUS-MATH530 Plane and 1.B \u0026lt;2022-09-27 Tue\u0026gt; NUS-MATH530 1.C Problem 23 ","permalink":"https://www.jemoka.com/posts/kbhnus_math530_homework_index/","tags":null,"title":"NUS-MATH530 Homework Index"},{"categories":null,"contents":"Infinite Plane Two Vehicles Yes. Though the travel of the two vehicles are not entirely independent, the second vehicle can diagonally traverse the plane while the first vehicle cuts across it. Practically, the question asks whether or not a combination of:\n\\begin{equation} \\alpha \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\beta \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{equation}\nCan form all vectors in \\(\\mathbb{R}^2\\). Expanding that expression out, we have, given some point \\((a,b)\\) that:\n\\begin{equation} \\begin{pmatrix} \\beta \\\\ \\alpha + \\beta \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\end{equation}\nTherefor, we have expressions:\n\\begin{equation} \\begin{cases} \\beta = a \\\\ \\alpha +\\beta = b \\end{cases} \\end{equation}\nSubstituting the definition of \\(\\beta\\) then:\n\\begin{align} \u0026amp;\\alpha + a = b \\\\ \\Rightarrow\\ \u0026amp;\\alpha = b - a \\end{align}\nTherefore, we have that, for all desired locales \\((a,b)\\) we have a fully determined solution:\n\\begin{equation} \\begin{cases} \\alpha = b-a \\\\ \\beta = a \\end{cases} \\end{equation}\nThis means that some direction of travel in both vehicles will suffice.\nGoing Home Not necessarily. Graphically, after shifting yourself to some location upper-right, its impossible to move horizontally in the vertical-only vehicle.\nPractically, the question is asking that, if you are at some beginning location:\n\\begin{equation} \\begin{pmatrix} a \\\\b \\end{pmatrix} \\end{equation}\nCan we devise some travel that follows:\n\\begin{equation} \\alpha \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} a \\\\b \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\end{equation}\nExpanding this out, we have expressions:\n\\begin{equation} \\begin{cases} 0 + a = 0 \\\\ \\alpha + b = 0 \\end{cases} \\end{equation}\nNamely, we have that:\n\\begin{equation} \\begin{cases} a = 0 \\\\ \\alpha = -b \\end{cases} \\end{equation}\nWe are therefore under-determined here; there is a solution only \\(\\forall a=0\\), but for no other \\(a\\).\nInfinite Space Pickup and Hoverboard We have:\n\\begin{equation} \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 3 \\\\ -2 \\\\1 \\end{pmatrix} \\end{equation}\nto go to all directions \\((a,b,c)\\). Let\u0026rsquo;s try solving:\n\\begin{equation} \\begin{cases} \\alpha + 3\\beta = a\\\\ \\alpha -2\\beta = b \\\\ \\beta = c \\end{cases} \\end{equation}\nSubstituting the value for \\(\\beta=c\\), to the above equations, we have:\n\\begin{equation} \\begin{cases} \\alpha + 3c = a \\\\ \\alpha - 2c = b \\\\ \\end{cases} \\end{equation}\nAnd therefore, we have results:\n\\begin{equation} \\begin{cases} \\alpha = a-3c \\\\ \\alpha = b+ 2c \\end{cases} \\end{equation}\nThis equation is again over-determined. Therefore, you cannot get anywhere in your space; you can, however, get to all the points \\((a,b,c)\\) where:\n\\begin{equation} a-3c = b+2c \\end{equation}\nPickup, Hoverboard, AND Jetpack (part 1) We now have:\n\\begin{equation} \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 3 \\\\ -2 \\\\ 1 \\end{pmatrix} + \\gamma \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} \\end{equation}\nto go to all points \\((a,b,c)\\), we now try solving:\n\\begin{equation} \\begin{cases} \\alpha + 3\\beta = a \\\\ \\alpha -2\\beta + \\gamma = b\\\\ \\beta +\\gamma = c \\end{cases} \\end{equation}\nAt this point, it is probably easier to use a matrix to solve this expression. Hence, let\u0026rsquo;s solve:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 1 \u0026amp; -2 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\c \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s first subtract the first column from the second column:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; -5 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} a \\\\ b-a \\\\c \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s now rotate rows \\(2\\) and \\(3\\):\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; -5 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a \\\\c\\\\ b-a \\end{pmatrix} \\end{equation}\nGreat. Now let\u0026rsquo;s subtract thrice the second row towards the first row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; -3 \\\\ 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; -5 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c \\\\c\\\\ b-a \\end{pmatrix} \\end{equation}\nAnd add five times the second row to the last row\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; -3 \\\\ 0 \u0026amp; 1 \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 6 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c \\\\c\\\\ b-a+5c \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s subtract a sixth of the last row to the second row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; -3 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 6 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c \\\\c-\\frac{b-a+5c}{6}\\\\ b-a+5c \\end{pmatrix} \\end{equation}\nAnd add a half to the top row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 6 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c + \\frac{b-a+5c}{2} \\\\c-\\frac{b-a+5c}{6}\\\\ b-a+5c \\end{pmatrix} \\end{equation}\nAnd finally divide the bottom row by \\(6\\):\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} a-3c + \\frac{b-a+5c}{2} \\\\c-\\frac{b-a+5c}{6}\\\\ \\frac{{b-a+5c}}{6} \\end{pmatrix} \\end{equation}\nGreat. So now we have a fully determined solution \\(\\forall \\alpha, \\beta, \\gamma\\). Therefore, given a pair of location which you want to reach \\((a,b,c)\\), we can use the expressions above to solve for the values by which we have to move each vehicle.\nPickup, Hoverboard, AND Jetpack (part 2) We have the same problem, but with new numbers.\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 1 \u0026amp; -2 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\c \\end{pmatrix} \\end{equation}\nAt this point, we really want to be checking of the vectors which form this matrix is a spanning set; that is, after performing Gaussian elimination, do we get back a zero-row? If so, it will restrict some combination of our input \\((a,b,c)\\) to converge to \\(0\\).\nLet\u0026rsquo;s begin by subtracting the first row from second row\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 0 \u0026amp; -5 \u0026amp; -5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\end{equation}\nAnd now, let\u0026rsquo;s divide the middle row by \\(\\frac{-1}{5}\\)\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s then subtract the middle row from the last row:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 5 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{pmatrix} \\end{equation}\nAlready we see an \\(0\\) row emerging. That means that some combination of the variables have to be \\(0\\) for a solution to exist: these vectors do not span the space and therefore we can\u0026rsquo;t get everywhere in space.\nPickup Breaks Down We now want to know if the following vectors would reach \\((0,0,0)\\) after driving the pickup some distance \\(d\\); that is, if we started at some \\((a,b,c)\\), can:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 3 \u0026amp; 0 \\\\ 1 \u0026amp; -2 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 1 \\\\ \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\beta \\\\ \\gamma \\end{pmatrix} = -\\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} \\end{equation}\nyield a solution? We have already performed the Gaussian Elimination above, therefore, we will skip directly to the solution:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\gamma \\\\ \\beta \\end{pmatrix} = -\\begin{pmatrix} a-3c + \\frac{b-a+5c}{2} \\\\c-\\frac{b-a+5c}{6}\\\\ \\frac{{b-a+5c}}{6} \\end{pmatrix} \\end{equation}\nObviously the bottom few rows yield a solution, however, the top row places some limitation on our possible location. Namely, that:\n\\begin{equation} -\\frac{a+b-c}{2} = 0 \\end{equation}\nIf the locations you are at do not behave with these rules, a solution will not be yielded.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_linear_vehicles/","tags":null,"title":"NUS-MATH530 Linear Vehicles"},{"categories":null,"contents":"Equation of a Plane We want to determine all points on the plane formed by two vectors.\nLet\u0026rsquo;s take two vectors \\(\\vec{u} \\in V\\) and \\(\\vec{v} \\in V\\). The orthogonal vector to the both of them (i.e. the normal direction of the plane) is:\n\\begin{equation} \\vec{u}\\times \\vec{v} \\end{equation}\nby the definition of the cross product.\nThe points on the plane, therefore, have to be orthogonal themselves to this normal vector. This means that the dot product of the candidate vector against these vectors should be \\(0\\):\n\\begin{equation} (\\vec{u} \\times \\vec{v}) \\cdot \\begin{pmatrix} x_{1} \\\\ \\dots \\\\ x_{n} \\end{pmatrix} = 0 \\end{equation}\nThis forms the final equation for a plane given two vectors in \\(\\mathbb{F}^{n}\\).\nA.B Exercises Double Negative We desire that \\(-(-v)=v \\forall v \\in V\\)\nBy distributivity in vector spaces, and the fact that \\(0v=0\\), we have that:\n\\begin{equation} v+(-1)v = (1-1)v = 0v = 0 \\end{equation}\nTherefore, \\((-1)v=-v\\).\nWe now have:\n\\begin{equation} -(-v) = -((-1)v) \\end{equation}\nThe scalar multiple of \\(v\\), by definition, is also \\(\\in V\\) if \\(v \\in V\\). Therefore, it itself holds that:\n\\begin{equation} (-1)((-1)v) \\end{equation}\nBy associativity:\n\\begin{equation} (-1\\cdot -1)v \\end{equation}\nFinally:\n\\begin{equation} (-1\\cdot -1)v = (1v) = v\\ \\blacksquare \\end{equation}\nOne of it is zero If \\(a \\in \\mathbb{F}\\), \\(v \\in V\\), and \\(av=0\\), we desire that \\(a=0\\) or \\(v=0\\). We perform casework.\nCase 1: \\(a=0\\) \u0026ndash; we are done.\nCase 2: \\(a \\neq 0\\): As \\(a \\in \\mathbb{F}\\), and \\(a \\neq 0\\), \\(\\exists \\frac{1}{a}: a\\cdot \\frac{1}{a}=1\\).\nTherefore:\n\\begin{align} \u0026amp;av = 0 \\\\ \\Rightarrow\\ \u0026amp; \\frac{1}{a}av = \\frac{1}{a} 0 \\\\ \\Rightarrow\\ \u0026amp; 1v = \\frac{1}{a} 0 \\\\ \\Rightarrow\\ \u0026amp; 1v = 0 \\\\ \\Rightarrow\\ \u0026amp; v=0\\ \\blacksquare \\end{align}\nExistence and Uniqueness Given Equation Given \\(v,w \\in V\\), we desire a unique \\(x\\in V: v+3x=w\\).\nLet\u0026rsquo;s first check existence. Take the expression:\n\\begin{equation} n = \\frac{1}{3} (w-v) \\end{equation}\nAs both \\(v,w \\in V\\), subtraction (addition) and scalar multiplication are defined. Therefore, \\(\\forall w,v \\in V\\), we can construct such an \\(n\\).\nSupplying the expression into \\(v+3x\\) for the definition of \\(x\\):\n\\begin{align} v+3x \u0026amp;= v+3\\qty(\\frac{1}{3}(w-v)) \\\\ \u0026amp;= v+(w-v) \\\\ \u0026amp;= v+w-v \\\\ \u0026amp;= v-v+w \\\\ \u0026amp;= 0+w \\\\ \u0026amp;= w \\end{align}\nby distributivity, associativity, and commutativity in vector spaces, yielding \\(w\\) as desired.\nNow let\u0026rsquo;s check uniqueness.\nSuppose \\(\\exists x_1, x_2: v+3x_1=w\\) and \\(v+3x_2=w\\).\nBy transitivity:\n\\begin{equation} v+3x_1=v+3x_2 \\end{equation}\nApplying \\(-v\\) to both sides:\n\\begin{equation} 3x_1=3x_2 \\end{equation}\nFinally, applying \\(\\frac{1}{3}\\) to both sides:\n\\begin{equation} x_{1}= x_2 \\end{equation}\nTherefore, there only exists one unique \\(x\\) which satisfies the expression. \\(\\blacksquare\\)\nEmpty Set is Not a Vector Space The empty set is not a vector space as it doesn\u0026rsquo;t have an additive identity. \\(\\blacksquare\\)\nAdditive Inverse is also Zero Multiplication We first take the additive inverse expression:\n\\begin{equation} \\forall v \\in V, \\exists -v: v+(-v) = 0 \\end{equation}\nTake now:\n\\begin{equation} 0v \\end{equation}\nWe have that:\n\\begin{align} 0v \u0026amp;= (0+0)v \\\\ \u0026amp;= 0v + 0v \\end{align}\nBy distributivity.\nAs \\(0v \\in V\\), \\(\\exists -0v: 0v+(-0v)=0\\).\n\\begin{align} \u0026amp;0v = 0v+0v \\\\ \\Rightarrow\\ \u0026amp; 0v-0v = 0v+0v-0v \\\\ \\Rightarrow\\ \u0026amp; 0 = 0v \\end{align}\nas desired. Now, we will start from this condition and work out way backwards.\nNote that the statement for additive inverse condition is that:\n\\begin{equation} \\forall v \\in V, \\exists -v: v+(-v) = 0 \\end{equation}\nLet us begin with the expression that:\n\\begin{equation} 0=0v \\end{equation}\nWe have that:\n\\begin{equation} 0=(1-1)v \\end{equation}\nThen, we have by distributivity:\n\\begin{equation} 0 = v + (-1)v \\end{equation}\nscalar multiplication is defined on a vector space. Therefore, we have \\(-1v\\) to construct such an additive inverse \\(\\forall v \\in V\\). \\(\\blacksquare\\)\nWeird Vector Space All operations are defined as given.\nTake scalars \\(t_1, t_2 \\in \\mathbb{R}\\).\n\\begin{equation} (t_1-t_2)\\infty = \\infty \\end{equation}\nYet, if we follow the rules of distribution:\n\\begin{equation} (t_1 -t_2)\\infty = \\infty -\\infty =0 \\end{equation}\nTherefore, distribution doesn\u0026rsquo;t hold on this new structure. It is not a vector space. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_plane_and_1_b/","tags":null,"title":"NUS-MATH530 Plane and 1.B"},{"categories":null,"contents":"Two Variables Let\u0026rsquo;s begin with the equations:\n\\begin{equation} \\begin{cases} 2x+y = 3 \\\\ x - y = 0 \\end{cases} \\end{equation}\nWe will first change this into a matrix equation:\n\\begin{equation} \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} \\end{equation}\nWe need to find, then, the inverse of:\n\\begin{equation} \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} \\end{equation}\nNamely, we need the matrix such that:\n\\begin{equation} M \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = I \\end{equation}\nTo do this, we can use row operations on both sides such that the left side becomes the identity, we are essentially inverting the process of reversing a matrix.\n\\begin{equation} \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\end{equation}\nLet\u0026rsquo;s begin:\n\\begin{align} \u0026amp; \\begin{pmatrix} 2 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 0 \u0026amp; 3 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; -2 \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 0 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\\\ 0 \u0026amp; 1 \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 0 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\\\ \\frac{1}{3} \u0026amp; \\frac{1}{3} \\end{pmatrix} \\\\ \\Rightarrow\\ \u0026amp; \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \u0026amp; \\frac{1}{3} \\\\ \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\end{pmatrix} \\end{align}\nFinally, then, we will applying this matrix to the input:\n\\begin{align} \\begin{pmatrix} \\frac{1}{3} \u0026amp; \\frac{1}{3} \\\\ \\frac{1}{3} \u0026amp; -\\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{align}\nThree Variables We do this again, but now with a much larger matrix. Namely:\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\end{equation}\nI spend a good two hours (yes) trying to invert this. At this point, I know its invertable but I keep making mistakes. However, a solution exists and it is of shape:\n\\begin{equation} \\begin{pmatrix} \\frac{1}{5} \u0026amp; \\frac{1}{5} \u0026amp; \\frac{2}{5} \\\\ \\frac{1}{5} \u0026amp; \\frac{1}{5} \u0026amp; -\\frac{3}{5} \\\\ \\frac{2}{5} \u0026amp; -\\frac{3}{5} \u0026amp; \\frac{4}{5} \\end{pmatrix} \\end{equation}\nAnd, applying the output, we have that:\n\\begin{equation} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} \\end{equation}\nSo complicated of an inverse, for such a simple result\u0026hellip;\nMatrix Multiplication Matrix multiplication is not commutative. While you can, for instance, multiply a \\(2\\times 3\\) by a \\(3\\times 3\\), we cannot do it the other way.\nFor an equation with three variables, you need three equations at a minimum to have at least one solution; you can get at most the number of equations number of solutions with fewer equations. You probably will have no solutions if you have more equations\u0026mdash;the result is likely to be overdetermined; of course, two equations may be the same relation then in which case one is effectively nulled.\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_solving_systems/","tags":null,"title":"NUS-MATH530 Solving Systems"},{"categories":null,"contents":"Proof: identity of a group is unique Assume for contradiction that there exists two identities \\(e_1\\) and \\(e_2\\) which are identities of the group \\(A\\). Take also an \\(a \\in A\\).\nGiven both \\(e_1\\) and \\(e_2\\) are identities, we have that:\n\\begin{equation} a * e_1 = a \\end{equation}\nas well as\n\\begin{equation} a * e_2 = a \\end{equation}\nTherefore, we have by the transitive property that:\n\\begin{equation} a * e_1 = a*e_2 \\end{equation}\nBecause we are in a group, there exists a \\(1/a\\) the inverse of \\(a\\). Applying this inverse to the expression, we have that:\n\\begin{equation} 1/a*a * e_1 = 1/a*a*e_2 \\end{equation}\nTherefore, that:\n\\begin{equation} e_1 = e_2\\ \\blacksquare \\end{equation}\nTherefore, there cannot be two unique identities in a group.\nProof: inverse of an element in a group is unique Take group \\(A\\) and element \\(a\\in A\\), assume for contradiction that there exists two inverses of \\(a\\) named here \\(a\u0026rsquo;_1\\) and \\(a\u0026rsquo;_2\\). Given they are both inverses for \\(a\\), we have that:\n\\begin{equation} a * a\u0026rsquo;_1 = 1 \\end{equation}\nas well as\n\\begin{equation} a * a\u0026rsquo;_2 = 1 \\end{equation}\nTherefore, we have by the transitive property that:\n\\begin{equation} a * a\u0026rsquo;_1 = a*a\u0026rsquo;_2 \\end{equation}\nBecause we are in a group, there exists a \\(1/a\\) the inverse of \\(a\\). Applying this inverse to the expression, we have that:\n\\begin{equation} 1/a*a * a\u0026rsquo;_1 = 1/a*a*a\u0026rsquo;_2 \\end{equation}\nTherefore, that:\n\\begin{equation} a\u0026rsquo;_1 = a\u0026rsquo;_2\\ \\blacksquare \\end{equation}\nTherefore, there cannot be two unique inverses for an element in group.\nProof: additive identity in field cannot have multiplicative inverse For some field \\(F\\) take its additive identity \\(0 \\in F\\). Assume for the sake of contradiction there exists a multiplicative inverse for \\(0\\) named \\(0\u0026rsquo; \\in F\\).\nLet\u0026rsquo;s take some \\(a \\in F\\). By definition of the additive identity, we have:\n\\begin{equation} 0 + a = a \\end{equation}\nWe will apply \\(0\u0026rsquo;\\) to both sides, we having that:\n\\begin{equation} 0\u0026rsquo;(0+a) = 0\u0026rsquo;a \\end{equation}\nDistributing \\(0\u0026rsquo;\\) to both sides, we have:\n\\begin{equation} 1 + 0\u0026rsquo;a = 0\u0026rsquo;a \\end{equation}\nGiven \\(a,0\u0026rsquo; \\in F\\), and multiplication is closed in \\(F\\) being a field, \\(0\u0026rsquo;a \\in F\\); applying \\(-0\u0026rsquo;a \\in F\\) the additive inverse of the result of multiplying together to both sides, we have that:\n\\begin{equation} 1 + 0\u0026rsquo;a - 0\u0026rsquo;a = 0\u0026rsquo;a - 0\u0026rsquo;a \\end{equation}\nAnd therefore:\n\\begin{equation} 1 = 0 \\end{equation}\nwhich is absurd, reaching the desired contradiction. \\(\\blacksquare\\)\nSystem \\begin{equation} \\begin{cases} x + 2y + z = 0 \\\\ 2x + 0y - z = 1 \\\\ x - y + z = 2 \\\\ \\end{cases} \\end{equation}\nWe will subtract the top and bottom expressions to have that:\n\\begin{equation} 3y = -2 \\end{equation}\nAnd to get:\n\\begin{equation} y = \\frac{-2}{3} \\end{equation}\nManipulating the second expression, we have that:\n\\begin{equation} 2x -1 = z \\end{equation}\nSubstituting this expression and \\(y\\) into the third expression, we have:\n\\begin{equation} x + \\frac{2}{3} + 2x -1 = 2 \\end{equation}\nperforming algebraic manipulations:\n\\begin{align} \u0026amp;3x + \\frac{2}{3} = 3 \\\\ \\Rightarrow\\ \u0026amp;3x = \\frac{7}{3} \\\\ \\Rightarrow\\ \u0026amp;x = \\frac{7}{9} \\end{align}\nAnd finally:\n\\begin{equation} \\frac{14}{9}-1 = z = \\frac{5}{9} \\end{equation}\nMultiply \\begin{equation} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0\\end{pmatrix} \\begin{pmatrix} x \\\\ y\\\\ z \\end{pmatrix} = \\begin{pmatrix} x+2y+z \\\\ 2x-z \\\\ x-y \\end{pmatrix} \\end{equation}\nThe inner dimensions (column vs. row) of the matricies have to be the same for them to be multiplied; matrix multiplication is not commutative.\nProof: 2x2 Matrices with Real Entries form a Group Under Addition Closure \\begin{equation} \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp;d \\end{pmatrix} + \\begin{pmatrix} e \u0026amp; f \\\\ g \u0026amp; h \\end{pmatrix} = \\begin{pmatrix} a+e \u0026amp; b+f \\\\ c+g \u0026amp; d+h \\end{pmatrix} \\end{equation}\nIdentity \\begin{equation} \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp;d \\end{pmatrix} + \\begin{pmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{pmatrix} = \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{pmatrix} \\end{equation}\nInverse \\begin{equation} \\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp;d \\end{pmatrix} + \\begin{pmatrix} -a \u0026amp; -b \\\\ -c \u0026amp; -d \\end{pmatrix} = \\begin{pmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{pmatrix} \\end{equation}\nAssociative \\begin{equation} \\left ( \\begin{pmatrix} x_1 \u0026amp; x_2 \\\\ x_3 \u0026amp; x_4 \\end{pmatrix} + \\begin{pmatrix} y_1 \u0026amp; y_2 \\\\ y_3 \u0026amp; y_4 \\end{pmatrix} \\right) + \\begin{pmatrix} z_1 \u0026amp; z_2 \\\\ z_3 \u0026amp; z_4 \\end{pmatrix} = \\begin{pmatrix} (x_1+y_1)+z_1 \u0026amp; (x_2+y_2)+z_2 \\\\ (x_3+y_3)+z_3 \u0026amp; (x_4+y_4)+z_4 \\end{pmatrix} \\end{equation}\nwhich is equal, by associativity in \\(\\mathbb{F}\\), as:\n\\begin{equation} \\begin{pmatrix} x_1+(y_1+z_1) \u0026amp; x_2+(y_2+z_2) \\\\ x_3+(y_3+z_3) \u0026amp; x_4+(y_4+z_4) \\end{pmatrix} \\end{equation}\nAnd finally, this is equal to:\n\\begin{equation} \\begin{pmatrix} x_1 \u0026amp; x_2 \\\\ x_3 \u0026amp; x_4 \\end{pmatrix} + \\left (\\begin{pmatrix} y_1 \u0026amp; y_2 \\\\ y_3 \u0026amp; y_4 \\end{pmatrix} + \\begin{pmatrix} z_1 \u0026amp; z_2 \\\\ z_3 \u0026amp; z_4 \\end{pmatrix} \\right) \\end{equation}\nWe have therefore shown that 2x2 matricies form a group under addition.\nProof: 2x2 Matrices with Real Entries does not from a Group Under Multiplication Inverse The matrix\n\\begin{equation} \\begin{pmatrix} 0 \u0026amp; 0 \\\\ 0 \u0026amp;1 \\end{pmatrix} \\end{equation}\nis not invertable. In that, one cannot apply a matrix to this one to result in the multiplicative identity \\(I_2\\).\n","permalink":"https://www.jemoka.com/posts/kbhnus_math530_some_matrix_manipulation/","tags":null,"title":"NUS-MATH530 Some Matrix Manipulation"},{"categories":null,"contents":"We declare known battery voltage \\(E(t)\\).\nHere are the \\(y\\) values.\n\\begin{equation} \\begin{cases} \\dv{x_1}{t} = y_{4}\\\\ \\dv{x_2}{t} = y_{3}\\\\ \\dv{x_3}{t} = y_{1}\\\\ \\dv{x_4}{t} = y_{2}\\\\ \\end{cases} \\end{equation}\nAnd here are some of the \\(x\\) values.\n\\begin{equation} \\begin{cases} \\dv{x_4}{t}=-\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R} \\\\ \\dv{y_1}{t}=-\\frac{1}{LC}x_2-\\frac{E(t)}{C} \\\\ \\dv{y_4}{t} = -\\frac{R}{L}y_2-\\frac{2E(t)}{L} \\end{cases} \\end{equation}\nRight off the bat, we can see that we can make one substitution. That, given:\n\\begin{equation} \\begin{cases} \\dv{x_4}{t}=-\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R} \\\\ \\dv{x_4}{t} = y_{2} \\end{cases} \\end{equation}\nwe have that:\n\\begin{equation} y_2 = -\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R} \\end{equation}\nThis renders the last expression:\n\\begin{align} \\dv{y_4}{t} \u0026amp;= -\\frac{R}{L}y_2-\\frac{2E(t)}{L} \\\\ \u0026amp;= -\\frac{R}{L}\\qty(-\\frac{2}{RC}x_2-\\frac{1}{RC}x_{3}-\\frac{2E(t)}{R})-\\frac{2E(t)}{L} \\\\ \u0026amp;= \\qty(\\frac{2}{LC}x_2+\\frac{1}{LC}x_{3}+\\frac{2E(t)}{L})-\\frac{2E(t)}{L} \\\\ \u0026amp;= \\frac{2}{LC}x_2+\\frac{1}{LC}x_{3} \\end{align}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_circuts/","tags":null,"title":"NUS-MATH570 Circuits"},{"categories":null,"contents":"We have:\n\\begin{equation} \\frac{2y^{2}}{9-x^{2}} + y \\dv{y}{x} + \\frac{3y}{2-x} = 0 \\end{equation}\nWe want to get rid of things; let\u0026rsquo;s begin by dividing the whole thing by \\(y\\).\n\\begin{equation} \\frac{2y}{9-x^{2}} + \\dv{y}{x} + \\frac{3}{2-x} = 0 \\end{equation}\nFinally, then, moving the right expression to the right, we have:\n\\begin{equation} \\frac{2y}{9-x^{2}} + \\dv{y}{x} = \\frac{-3}{2-x} \\end{equation}\nIn this case, we have functions:\n\\begin{equation} \\begin{cases} P(x) = \\frac{2}{9-x^{2}}\\\\ Q(x) = \\frac{-3}{2-x}\\\\ \\end{cases} \\end{equation}\nTaking first the top integral:\n\\begin{equation} \\int \\frac{2}{9-x^{2}} \\dd{x} = \\frac{1}{3} \\log \\qty(\\frac{x+3}{3-x}) \\end{equation}\nRaising \\(e\\) to that power, we have that:\n\\begin{equation} \\sqrt[3]{e\\frac{x+3}{3-x}} \\end{equation}\nMultiplying \\(Q(x)\\) to that expression, we have that:\n\\begin{equation} \\int \\frac{-3}{2-x}\\sqrt[3]{e\\cdot \\frac{x+3}{3-x}} \\dd{x} \\end{equation}\nTherefore, our entire answer is defined as the integral function that:\n\\begin{equation} y = \\frac{1}{\\sqrt[3]{e\\cdot \\frac{x+3}{3-x}} } \\int \\frac{-3}{2-x}\\sqrt[3]{e\\cdot \\frac{x+3}{3-x}} \\dd{x} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_problem_set_1/","tags":null,"title":"NUS-MATH570 Problem Set 1"},{"categories":null,"contents":"Considering the system:\n\\begin{equation} \\begin{cases} \\dv{x}{t} = -2x+y+(1-\\sigma)z \\\\ \\dv{y}{t} = 3x-y \\\\ \\dv{z}{t} = (3-\\sigma y)x-z\\\\ \\end{cases} \\end{equation}\nwith the initial locations \\((x_0, y_0, z_0)= (-1,1,2)\\).\nWe notice first that the top and bottom expressions as a factor in \\(x\\) multiplied by \\(y\\), which means that our system is not homogenous. Let\u0026rsquo;s expand all the expressions first.\n\\begin{equation} \\begin{cases} \\dv{x}{t} = -2x+y+(1-\\sigma)z \\\\ \\dv{y}{t} = 3x-y \\\\ \\dv{z}{t} = 3x-\\sigma yx-z\\\\ \\end{cases} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_problem_set_2/","tags":null,"title":"NUS-MATH570 Problem Set 2, Problem 1"},{"categories":null,"contents":"Intersects:\n\\begin{equation} f(x) = (x+c)^{2} \\end{equation}\n\\begin{equation} h(x) = c x \\end{equation}\nDoesn\u0026rsquo;t Intersect:\n\\begin{equation} g(x) = c e^{\\frac{x^{4}}{4}}} \\end{equation}\n\\begin{align} \u0026amp;h_1(x)-h_2(x) = c_1x-c_2x \\\\ \\Rightarrow\\ \u0026amp; 0 = c_1x-c_2x \\\\ \\Rightarrow\\ \u0026amp; 0 = x(c_1-c_2) \\end{align}\n\\begin{align} \u0026amp;g_1(x)-g_2(x) = c_1e^{\\frac{x^{4}}{4}} - c_2e^{\\frac{x^{4}}{4}} \\\\ \\Rightarrow\\ \u0026amp; 0 = \\qty(c_1 - c_2)e^{\\frac{x^{4}}{4}} \\\\ \\Rightarrow\\ \u0026amp; 0 = e^{\\frac{x^{4}}{4}}(c_1-c_2) \\end{align}\n\\begin{align} \u0026amp; f_1(x)-f_2(x)=(x+c_1)^{2}-(x+c_2)^{2} \\\\ \\Rightarrow\\ \u0026amp; 0 = (x+c_1)^{2}-(x+c_2)^{2} \\\\ \\Rightarrow\\ \u0026amp; 0 = 2x(c_1-c_2)+{c_1}^{2}+{c_2}^{2} \\end{align}\n\\begin{equation} \\dv{y}{x} + P\u0026rsquo;(x)y = Q\u0026rsquo;(x) \\end{equation}\n\\begin{align} \u0026amp;y = e^{\\int P\u0026rsquo;(x)\\dd{x}} \\int e^{\\int P\u0026rsquo;(x)\\dd{x}} Q\u0026rsquo;(x)\\dd{x} \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)} \\int e^{P(x)}Q\u0026rsquo;(x)\\dd{x} \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)} (\\dots+C) \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)}C + \\dots \\end{align}\n\\begin{equation} h(x) \\in e^{-P(x)}C + \\dots \\end{equation}\n\\begin{equation} g(x) \\in e^{-P(x)}C + \\dots \\end{equation}\n\\begin{align} \u0026amp;0 = (e^{-P(x)}C_1+\\dots)-(e^{-P(x)}C_2 + \\dots) \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)}C_1-e^{-P(x)}C_2 \\\\ \\Rightarrow\\ \u0026amp; e^{-P(x)}(C_1-C_2) = 0 \\end{align}\n","permalink":"https://www.jemoka.com/posts/kbhnus_math570_research_question_1/","tags":null,"title":"NUS-MATH570 Research Question 1"},{"categories":null,"contents":"Vocabulario alejar forzar el fracaso ocultar atemorizarte prescindir exige asegurar Preguntas ¿Hay problemas a largo plazo con el sistema del calificación? ¿Como medimos los éxitos del sistema nueva sin prejuicios las preferencias propias de los estudiantes? ¿Necesita estudiantes motivados para el desarrollo complete del sistema? ¿Hay diferencias sociocultural que puede influir los resultados o el implementación del sistema? ¿Cómo conducta examines del rendimiento de los estudiantes a través escuelas con implementaciones diferencies del sistema? ","permalink":"https://www.jemoka.com/posts/kbhnus_span502_tarea_2/","tags":null,"title":"NUS-SPAN502 Tarea 2"},{"categories":null,"contents":"Vocabularios Nuevos creciente jornada exigir desempeño subir Preguntas ¿En el primer lugar, porqué tenemos semanas de cinco días? ¿Para los ciudades con recursos de educación abundante, hay un necesario real de trabar en jornadas de cuatro días? ¿Tenemos de verdad un sistema responsable para examinar los diferencias de cantidad de educación a través de el palmo entero del proceso de educación de un estudiante? ¿Existen presiones políticas que motivó el propósito? ¿En realidad, existe un problema muy fundamental que causó los problemas que vemos hoy? ","permalink":"https://www.jemoka.com/posts/kbhnus_span502_tarea_4/","tags":null,"title":"NUS-SPAN502 Tarea 4"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhobjects/","tags":null,"title":"object"},{"categories":null,"contents":"The Open Voice Brain Model is a audio processing architecture proposed by Laguarta 2021 for audio/biomarker correlation work.\nHere\u0026rsquo;s a fairly self-explanatory figure:\nThe model outputs an AD diagnoses as well as a longitudinal correlation with Memory, Mood, and Respiratory biomarkers.\nThis is then the embedding that they are proposing for use by other tasks.\n","permalink":"https://www.jemoka.com/posts/kbhopen_voice_brain_model/","tags":null,"title":"Open Voice Brain Model"},{"categories":null,"contents":"OpenSMILE is a proprietary audio feature exaction tool.\nSite.\n","permalink":"https://www.jemoka.com/posts/kbhopensmile/","tags":null,"title":"OpenSMILE"},{"categories":null,"contents":" adding multiplying This is object dependent.\n","permalink":"https://www.jemoka.com/posts/kbhoperation/","tags":null,"title":"operation"},{"categories":null,"contents":"Richard Nixon bombs Vietnam for 13 days to beat the VietCong into submission after the Vietnam War.\n","permalink":"https://www.jemoka.com/posts/kbhoperation_linebacker/","tags":null,"title":"Operation Linebacker"},{"categories":null,"contents":"options are derivatives which gives you the permission to make a transaction at a particular date.\nThere are two main types of options:\ncall: gives permission to buy a security on or before the \u0026ldquo;exercise\u0026rdquo; date puts: gives permission to sell a security on or before the \u0026ldquo;exercise\u0026rdquo; date For this article, we will define \\(S_{t}\\) to be the stock price at the time \\(t\\), \\(K\\) as the option\u0026rsquo;s strike price, \\(C_{t}\\) to be the price of the \u0026ldquo;call\u0026rdquo; option, and \\(P_{t}\\) to be the price of the \u0026ldquo;put\u0026rdquo; option at strike price \\(K\\); lastly \\(T\\) we define as the maturity date.\nNaturally, the actual values \\(C_{t}\\) and \\(P_{t}\\) are:\n\\begin{equation} \\begin{cases} C_{t} = Max[0, S_{T}-K] \\\\ P_{t} = Max[0, K-S_{T}] \\\\ \\end{cases} \\end{equation}\nyou either make no money from the option (market price is more optimal), or make some difference between the strike price and the market price.\nThe nice thing here is that little \\(Max\\) term. An option, unlike a futures contract, has no buying obligation: you don\u0026rsquo;t have to exercise it. The payoff is always non-negative!\nNOTE!!! \\(C_{t}\\) at SMALL \\(t\\) is measured at \\(Max[0,S_{*T*}, K}]\\), using \\(S\\) of LARGE \\(T\\). This is because, even when\u0026mdash;currently\u0026mdash;the stock is trading at $60, the right to buy the stock in \\(T\\) months for $70 is not worthless as the price may go up.\nTo analyze options, we usually use the Black-Scholes Formula.\nAmerican vs European Options American options are excercisable at or before the maturity date. European options are exrcercisable only at the maturity date. Analyze Options as Insurance All insurance contracts are actually a form of an option, so why don\u0026rsquo;t we analyze it as such?\nA put option\u0026mdash;-\nAsset insured: stock Current asset value: \\(S_{0}\\) Term of policy: \\(T\\) Maximum coverage: \\(K\\) Deductible: \\(S_0-K\\) Insurance premium: \\(P_{t}\\) A call option is covariant with a put option; so its isomorphic, and so we will deal with it later.\nA few differences:\nAmerican-style early exercise: (you can\u0026rsquo;t, for normal insurance, exercise it without something happening) Marketability: you can\u0026rsquo;t give normal insurance to other people Dividends: holding a stock pays dividends (an option\u0026rsquo;s value goes down as dividends) ","permalink":"https://www.jemoka.com/posts/kbhoptions/","tags":null,"title":"option"},{"categories":null,"contents":"oral lexical retrival is a class of discourse tasks which asks the subject to convert some semantic understanding (\u0026ldquo;concept\u0026rdquo;) into lexical expressions (\u0026ldquo;words\u0026rdquo;)\n\u0026ldquo;ask a patient to describe a thing.\u0026rdquo;\nExamples of oral lexical retrieval:\nSVF BNT Source: CambridgeCore\n","permalink":"https://www.jemoka.com/posts/kbhoral_lexical_retrival/","tags":null,"title":"oral lexical retrieval"},{"categories":null,"contents":"Reading notes conservatives in America make less sense because America is supposed to be liberal/new For most Europeans who came to America, the whole purpose of their difficult and dis- ruptive journey to the New World was not to conserve European institutions but to leave them behind and to create something new, often an entirely new life\nThree splits of conservatism in America those who are most concerned about economic or fiscal issues, that is, pro-business or “free-enterprise” conservatives those most concerned with religious or social issues, that is, pro-church or “traditional-values” conservatives those most concerned with national-security or defense issues, that is, pro-military or “patriotic” conservatives Ronald Reagan unified the three conservatism It was the achievement of Ronald Reagan that he was able in the late 1970s to unite these three different kinds of conservatism into one grand coalition.\nThree-in-one conservatism is a part of American \u0026ldquo;fusionist strategy\u0026rdquo; This was the culmination of a “fusionist strategy” that had been developing amongst American conservatives since the early 1960s.\nBusiness and social conservatism should contradict each other, though However, as we shall see, pro-business conservatism has always included a tendency toward the disruption and even dissolution of religious ideals and social practices.\nExtreme pro-business should also include globalization and erasure of national identities And in recent decades, pro-business conservatism has also included a tendency toward the dismantling of national boundaries and even dissolution of national identities\n\u0026ldquo;conservatism\u0026rdquo; actually conserved American revolutionary force economically this means that the conservative party in America has always sought to conserve a revolutionary force.\nExtreme economic \u0026ldquo;conservatism\u0026rdquo; should destry social and moral arrangements It destroys religious, social, and ultimately moral arrangements as well.\nReligions conservatism founds on the \u0026ldquo;open-market\u0026rdquo; of protestanism This open market in religious matters, so nicely isomorphic with the open market in economic matters, was a powerful factor gen- erating both a reality and an ideology of free choice in the United States.\nBecause the \u0026ldquo;new\u0026rdquo; became new protestanism, religious conservatism is the re-take over of traditional religions Since these churches were continually being left behind, religious conservatism was associated with once-dominant churches that were now dwindling into a minority, and would later dwindle into marginality\nBecause of mass economic benifit, religous conservatism became subordinated to economic conservatism Even ordinary middle-class Protestants benefited from cheaper labor, in the form of domestic servants. And of course it was the businessmen and middle-class Protestants who controlled the political parties, particularly that party which was supposed to be the more conservative one\nBecause there is nothing to conserve about current system, the thing that\u0026rsquo;s conserved is free choice If something were going to be conserved, it would normally be the no-conscription and low-taxation (and free-choice) system.\nEconomic systems propergated the source of American patriotism This meant that people who thought of themselves as American patriots or nationalists, and who sought to conserve the American nation and to promote American national interests\nAmerican conservatism is actually a form of European liberalism we have seen that, from a European perspective, American conservatism was not conservative at all, but actually was a kind of classical lib- eralism.\nwartime strengthened American values and liberalism Moreover, the wartime experience seemed decisively to vindicate and even enhance the strengths of both the traditional American economic system and traditional American moral principles.\n","permalink":"https://www.jemoka.com/posts/kbhrise_of_american_conservatism/","tags":null,"title":"Origins of American Conservatism"},{"categories":null,"contents":"DOI: 10.3389/fnagi.2020.605317\nOne-Liner An excercize scheme has had some measured effect on theta/alpha ratio and Brain wave frequency on AD patients; prognosis of AD not controlled for.\nNovelty Leveraged physical training scheme and measured EEG effects by quantifying theta/alpha ratio Notable Methods Used theta/alpha ratio as assay for improvement, and found the exercise scheme did so p\u0026lt;0.05 Only tested patients with AD w/o a control for stage Key Figs Figure 1 This figure tells us th N number of participants through the study\nFigure 2 This figure shows us that the excercize intervention has statistically significant results to both Brain Oscillation frequency and Theta/Alpha ratio. The x-axis shows us the pre-and-post bars for TG (treatment) and CG (control); the y-axis quantifies the value measured in a box plot. The subplots are brain oscelation and theta/alpha ratio respectively.\nNew Concepts theta/alpha ratio Notes ","permalink":"https://www.jemoka.com/posts/kbhparvin_2020/","tags":["ntj"],"title":"Parvin 2020"},{"categories":null,"contents":" No Demo Day TODO Email need statement template Needfinding Not all patients want to be treated the same way Attitudes towards heathcare system Fostering strong interaction; facilitate interaction Problem: patients have attitudes that physicians can\u0026rsquo;t effectively communicate.\nAction item: interview doctors and patients\nNeed two need statement.\n","permalink":"https://www.jemoka.com/posts/kbhpcp_april_checkin/","tags":null,"title":"PCP April Checkin"},{"categories":null,"contents":"We will leverage atoms as qubits. So, how do we isolate a qubit from an atom? We will leverage electrons.\nWe will select the lowest energy state as the base state; as there maybe multiple ground states, we will choose \\(|u\\big\u0026gt;\\) and \\(|d\\big\u0026gt;\\) from two of the states.\n","permalink":"https://www.jemoka.com/posts/kbhphysical_qubits/","tags":null,"title":"physical qubits"},{"categories":null,"contents":"physics is the act of explaining what we see in terms of solving for the \u0026ldquo;unseen\u0026rdquo;. For an explanation to be good, it needs to be testable.\nHow exactly does physics work? \u0026ldquo;classical results\u0026rdquo;\nNewton\u0026rsquo;s laws Maxwell\u0026rsquo;s equations General relativity \u0026ldquo;quantum theory\u0026rdquo;\nA new model that actually allows particle inference.\n","permalink":"https://www.jemoka.com/posts/kbhphysics/","tags":null,"title":"physics"},{"categories":null,"contents":"User Story Sejin is the executive administrative assistant at Nueva, working on scheduling Liza, Lee, Terry, and the other admins against the members of the wider community. Sejin spends most of her day scheduling people, of which, the largest time drawn is spent convincing people to move their schedules \u0026ldquo;in favor\u0026rdquo; of that of another person (i.e. manually). The reason why this is done is because her approach to scheduling is one-shot: emailing everybody for general availability, noting in her mind who the high-priority attendees are (say, Liza), and if no times match asking/convincing those in lower priority to move their schedules. Although she enjoys the process of putting events together, she is particularly frustrated that, due to the busy schedules and often back-and-forth emails needed to get and conform everyone\u0026rsquo;s schedule, response rates to complicated scheduling problems are low.\nSejin, during a main brunt of her job of scheduling inter or intra-admin meetings, need a solution to schedule many executives at once with attention to their priority/authority/importance to a meeting as well as the possible fluidity of their schedules. There is an inherit fluidity to scheduling as a master-planner of a few admin\u0026rsquo;s schedules: in that, if needed, she has authority to move entire meetings as long as they are swapped for equivalent times of availability. Hence, a previously bad time may suddenly become available if enough scheduling conflicts is generated, thereby creating the incentive for swapping another meeting away for the one being scheduled and rescheduling other attendees of lower priority.\nCurrent scheduling software does not account for either types of fluidity. Tools like Doodle/When2Meet can accommodate for inherent \u0026ldquo;priority\u0026rdquo;\u0026mdash;with Sejin choosing the time-slot that would have the most, highest priority individuals scheduled\u0026mdash;but are one-shot planning tools which do not provide space for swapping entire meetings out to make scheduling work better. Other tools like Calendly or simple iCal does not provide any semblance of priority or \u0026ldquo;multi-possibility\u0026rdquo; for meetings, though does indeed provide the time-blocking capability to swap two meetings at will. These problems result in Sejin needing to just create large email chains to resolve scheduling problems. Also, no scheduling tools provide an opportunity to manually \u0026ldquo;convince\u0026rdquo; or request someone to make time due to the constraints that presented during first-round scheduling. Lastly, scheduling software does not space-block. Sometimes there is a physical capacity/de-duplication limit to spaces, which cannot be accounted for.\nOnce the initial scheduling and emailing processes are automated, Sejin can spend more time focusing on what she actually enjoys: thinking about the process of an event and its details. Schedule can now be an afterthought, an event which happens in the background which is eventually reported to her on the online portal as she is planning the details of the event.\nProposal Fundamentally, this is a fractional knapsack problem. \u0026ldquo;How do we maximize the maximum amount of attendance of maximum amounts of important people?\u0026rdquo;\nFrom a target market, I think a good target would be medium organization assistants: Sejin\u0026rsquo;s concerns really only become a problem when you are scheduling for a one (or few) vs. many situation where there is a stable group of people you are scheduling for, who wants to meet with each other or other people outside.\nAs far as UX, this tool should not require log-in except for the master planner (i.e. our user.) Participants in meeting should be able to freely enter their schedules or create evergreen accounts to manage their scheduling. (This is not as well thought out at the moment.)\nLastly, for the tech stack, I don\u0026rsquo;t think I have the ability to finish the entire stack by myself. From a MVP perspective (if we are trying to satisfy all needs), there needs to be a system optimizing a constantly shifting fractional knapsack, a way to put and store availability information, and a way to automate the requesting/convincing of scheduling change (e.g.. \u0026ldquo;MyApp Notification! Liza is not available at the one time you selected, but everyone else is available at this different time. Can you make this time? Y/N\u0026rdquo;) . Ideally, we would also send iCal invites in the end.\n","permalink":"https://www.jemoka.com/posts/kbhpitch_a_project/","tags":null,"title":"Pitch a Project"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhpolio/","tags":null,"title":"Polio"},{"categories":null,"contents":"A polynomial is a polynomial\nconstituents a function \\(p: \\mathbb{F} \\to \\mathbb{F}\\) coefficient \\(a_0, \\dots, a_{m} \\in \\mathbb{F}\\) requirements A polynomial is defined by:\n\\begin{equation} p(z)=a_0+a_1z+a_2z^{2}+\\dots +a_{m}z^{m} \\end{equation}\nfor all \\(z \\in \\mathbb{F}\\)\nadditional information degree of a polynomial \\(\\deg p\\) A polynomial\u0026rsquo;s degree is the value of the highest non-zero exponent. That is, for a polynomial:\n\\begin{equation} p(z) = a_0+a_1z+\\dots +a_{m}z^{m} \\end{equation}\nwith \\(a_{m} \\neq 0\\), the degree of it is \\(m\\). We write \\(\\deg p = m\\).\nA polynomial \\(=0\\) is defined to have degree \\(-\\infty\\)\nOf course, a polynomial with degree \\(n\\), times a polynomial of degree \\(m\\), has degree \\(mn\\). We see that:\n\\begin{equation} x^{n}x^{m} = x^{n+m} \\end{equation}\n\\(\\mathcal{P}(\\mathbb{F})\\) \\(\\mathcal{P}(\\mathbb{F})\\) is the set of all polynomials with coefficients in \\(\\mathbb{F}\\).\n\\(\\mathcal{P}(\\mathbb{F})\\) is a vector space over \\(\\mathbb{F}\\) We first see that polynomials are functions from \\(\\mathbb{F}\\to \\mathbb{F}\\). We have shown previously that F^s is a Vector Space Over F.\nTherefore, we can first say that \\(\\mathcal{P}(\\mathbb{F}) \\subset \\mathbb{F}^{\\mathbb{F}}\\).\nLastly, we simply have to show that \\(\\mathcal{P}(\\mathbb{F})\\) is a subspace.\nzero exists by taking all \\(a_{m} = 0\\) addition is closed by inheriting commutativity and distributivity in \\(\\mathbb{F}\\) scalar multiplication is closed by distributivity Having satisfied the conditions of subspace, \\(\\mathcal{P}(\\mathbb{F})\\) is a vector space. \\(\\blacksquare\\)\n\\(\\mathcal{P}_{m}(\\mathbb{F})\\) For \\(m\\geq 0\\), \\(\\mathcal{P}_{m}(\\mathbb{F})\\) denotes the set of all polynomials with coefficients \\(\\mathbb{F}\\) and degree at most \\(m\\).\n","permalink":"https://www.jemoka.com/posts/kbhpolynomial/","tags":null,"title":"polynomial"},{"categories":null,"contents":"For some \\(a \\in \\mathbb{F}\\), we define \\(a^m\\) to be \\(a\\) multiplied with itself \\(m\\) times.\nadditional information \\((a^m)^n = a^{mn}\\) \\((ab)^m = a^mb^m\\) ","permalink":"https://www.jemoka.com/posts/kbhpower_math/","tags":null,"title":"power (math)"},{"categories":null,"contents":"We can now use power series to also solve differential equations.\n\\begin{equation} \\dv{x}{t} = 0; x(0)=1 \\end{equation}\nWe wish to have a power-series solution of shape:\n\\begin{equation} x(t) = \\sum_{k=0}^{\\infty }a_{k}t^{k} \\end{equation}\nWe want to find the coefficients \\(a_{k}\\). If you can find such a function that fits this form, they both 1) converge and 20 behave the same way as \\(e^{x}\\) does in Simple Differential Equations.\nanalytic functions Functions which can be described with a power series are called analytic functions.\n","permalink":"https://www.jemoka.com/posts/kbhpower_series/","tags":null,"title":"power series"},{"categories":null,"contents":"gravity sucks.\ngeneral relativity claims that our best theory of how gravity work does not work with non-\n","permalink":"https://www.jemoka.com/posts/kbhproblem_with_gravity/","tags":null,"title":"problem with gravity"},{"categories":null,"contents":"This is a work-in-progress page listing all of my production projects.\nYappin: Podcast https://anchor.fm/yappin/\n20MinuteRants: Blog https://20mr.substack.com/\nProject80: Podcast See Project80.\nNorman Stories: Fiction https://hidonipothan.substack.com/\n(left) Director - Hillview Broadcasting: Production Studio https://hillview.tv/\n","permalink":"https://www.jemoka.com/posts/kbhproduction_index/","tags":["index"],"title":"Production Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhprof_xin_liu/","tags":null,"title":"Prof. Xin Liu"},{"categories":null,"contents":"Project80 is a podcast hosted by Houjun Liu, Anoushka Krishnan, Micah Brown, Mia Tavares, among others.\nCollege Application w.r.t. Project80 Cheese mission statement: Project80 is a good way of creating a self-propegating set of learning that would serve to benefit and educate future generations in hopes of creating a more equitable planet.\n","permalink":"https://www.jemoka.com/posts/kbhproject80/","tags":null,"title":"Project80"},{"categories":null,"contents":"Natural science education resources traditionally teach only codified theory. While theory education is crucial, much of academic science takes place via scrutinizing contested scientific discourse. Due to such resources’ content complexity, high school students are rarely exposed to current, debatable, and relevant science. In response, we introduce Project80: a systemic, student-run protocol to synthesize the latest primary literature in a sub-field into approachable, produced multimedia educational content. The protocol is run by a team of 7 students over the course of 1 month. Students running the protocol consume complex scientific literature, distill relevant data and findings, and synthesize a culminating product of audiovisual content to supplement existing biology and chemistry pedagogy. The system runs independently with limited faculty involvement. Our analysis indicates that the multimedia content created by this protocol will be relevant to roughly 30 courses locally at our institution and will have further extensions in secondary education beyond.\n","permalink":"https://www.jemoka.com/posts/kbhproject80_abstract/","tags":null,"title":"Project80 Abstract"},{"categories":null,"contents":"Projects Index is a index that contains a list of almost all projects for which I have ever worked on. Major categories are highlighted from chapter titles.\nResearch Projects I have spent the last 6 years or so working as an actively-publishing data science researcher; my research interests are mainly in textual data mining, semantic analysis, L2 learning, and science education. As a part of my work with Professor Brian Macwinney, I have also recently taken up interest in acoustic modeling.\nFor a list of my recent research, please head to the Research Index.\nMedia Production Projects I produce a lot of media (videos, podcasts, blogs, live events/talks) as a part of publicizing my work or for other purposes. For those types of projects, head on over to Production Index.\nLarge-Scale Endeavors Condution An open-source task management app. Website.\nMotivation: I got really tired with most other to-do apps after swapping them out over and over again, until I got fed up and built one with some friends.\nRole: Co-Founder, Lead Developer. Technologies: React, Ionic, Firebase, Typescript, Swift, PostgreSQL Key facts: 10,000+ users, 8-person team, featured in the Bay Area almanac, praised by Asana’s head of developer relations for “open-source advocacy” MODAP A R\u0026amp;D team for fireline safety during emergency fires. Repository.\nMotivation: a friend approached me with an opportunity to help our local community, especially with the increased influx of fires.\nRole: Team Lead Technologies: Rust, Torch, ARM, electronics (i2C, UART, messaging protocols, etc.) Key facts: coordinated 5 engineers in developing new technology, supported by Dr. Robert G. Gann, Deputy Director, Center of Excellence for Advanced Technology Aerial Firefighting at the state of Colorado as well as Captain Mason of CalFire CMU batchalign A pipeline for the automated preparation of annotated CHAT transcripts from raw audio. Repository.\nMotivation: my work over the summer.\nRole: Author Technologies: Torch, Huggingface, NLTK, CLAN, computational linguistics Key facts: work developed with and maintained under Prof. Brian MacWhinney at CMU\u0026rsquo;s psycolinguistics department. AIBridge A bootcamp for non-CS students in data science. Website\nMotivation:\nRole: Co-Founder, Lecturer Technologies: Python, ScyPy, Scikit-learn, Pandas Key facts: worked with Prof. Xin Liu at UC Davis to develop an introductary one-week bootcamp in ML. We piloted the program this summer at Davis to an in-person group of 20 PhD students in food science sponsored by AIFS. Full-Stack Projects tractotato CommonLisp macroset for time tracking. Repo.\nMotivation: I wanted to learn CommonLisp macros syntax after reading the Land of Lisp book.\nRole: author Technologies: CommonLisp Scratchathon Portal Portal to submit projects for a scratch hackathon I hosted. Repo.\nMotivation: my friends McGuy and fuelvin, both content creators on Scratch on YouTube, put together a Scratch hackathon summer of 2020. This is the submission portal.\nRole: author Technologies: React, Vercel, Firebase syzygy Library rethinking to-do list dating to be more flexible and powerful. Repo.\nMotivation: a friend and I wanted to innovate beyond the scope of Condution to see how we can abstract away a to-do list system to its bare minimum.\nRole: co-founder, co-author Technologies: Rust positron Library for building lightweight native apps using web tech. Repo.\nMotivation: I wanted to re-make electron to be more lightweight using Suckless\u0026rsquo; Surf browser concept.\nRole: author Technologies: C++, GTK OS/Driver Development Broadcom Wifi/Bluetooth 4377 Chip Linux Driver A driver patchset to support cutting-edge Broadcom 4377 Wifi/Bluetooth chips. Repo.\nMotivation: I needed to be able to use Wifi on my laptop while running Arch Linux.\nRole: author Technologies: C, (small amounts of) Assembly Key facts: integrated into the t2linux pipeline used to make WiFi possible on Linux for most MacBooks released after 2018 Distributed Algorithms and Parallel Computing coveather An encrypted, anonymized system for protected health information verification. Preprint, Repo, and internal note.\nMotivation: I wanted to be able to make vaccine passports more feasible because the current COVID testing/vaccine verification scheme is really bad.\nRole: author Technologies: Clojure, core.async concurrency, Monte-Carlo simulations, blockchain, PGP Key facts: project won first place at the California STEM Fair, and got special recognition from the Yale Science and Engineering assoc. Total award $3000. multischedule A multiple-asynchronous scheduling and delegation algorithm. Repo.\nMotivation: (didn\u0026rsquo;t even come close to getting there) I wanted to create a way to solve or simplify debugging loop overrun problems in robotics codebases.\nRole: author Technologies: Clojure, core.async concurrency rotifer A work-in-progress distributed algorithm for taproot. Repo.\nMotivation: I wanted to make taproot even more distributed if possible.\nRole: author Technologies: Clojure, XML, UDP, ICE simian Exploring OT/CRDT and collaborative text editing for taproot. Repo.\nMotivation: I wanted to learn about how apps like Google Docs work, and explore Operational Transformation/CRDT, in hopes of putting it into taproot.\nRole: author Technologies: Clojure, OT, CRDT aron A distributed multi-dimensional optimization tool. Repo.\nMotivation: Nueva\u0026rsquo;s course scheduling was quite a mess, and I wanted to help. It is a very complex problem and this project is in the freezer at the moment.\nRole: author Technologies: CommonLisp mitte Easy UDP sockets. Repo, Docs.\nMotivation: a friend and I wanted to explore UDP.\nRole: co-author Technologies: Rust, UDP, ICE (connection) Cryptography and security See also: coveather.\njrainbow An implementation of a MD5 rainbow table. Repo, Crate.\nMotivation: I wanted to understand how Rainbow Tables worked.\nRole: author Technologies: Rust, MD5 Note-taking Systems and \\(\\LaTeX\\) improvements taproot A shared zettlekasten of notes and learning resources put together by some friends and I. there has been a few iterations. Current Repo, Current Site, Legacy Site, Even More Legacy Site.\nMotivation: I started writing nice \\(\\LaTeX\\) PDFs of my homework, and some friends wanted to have access to it. Later when I mentioned it, another friend had a similar need; so we asked many people to pool our notes and work together to share.\nRole: co-founder, co-lead, developer Technologies: Next.JS, XeLaTeX, GNU Make, Firn, Hugo, Emacs Org, Org-Publish, Markdown blag The zettlekasten you are currently in! My currently maintained personal knowledgebase. Repo, Site.\nMotivation: I wanted to experiment with more advanced note-taking techniques after developing taproot, and it ended up superseeding the note-taking abilities of taproot.\nRole: author Technologies: Next.js, Emacs Org, Hugo gdoc.el A utility to enable GNU Emacs to edit Google Doc documents based on the gdrive utility. Repo.\nMotivation: I wanted to edit Google Docs in Emacs!\nRole: author Technologies: GNU Emacs, elisp interesting Things that my friends and I find interesting, chucked on the web and builds itself. Repo, Site. No longer maintained.\nMotivation: many text channels were too clogged with stuff my friend group found interesting, so I wanted to take initiative to collect them.\nRole: co-founder, author Technologies: Next.js, Vercel, remark, CommonMark Markdown Public Configurations borg Automatically configure terminals. Repo.\nMotivation: I needed a way to copy my system terminal config onto a system quickly.\nRole: author Technologies: Bash, Zsh, OhMyZsh .config A group of sane configuration files. Repo.\nMotivation: some Redditors asked for my Config, and I thought I\u0026rsquo;d share it to benefit the community; also for personal backup.\nRole: author, maintainer Technologies: Unix administration, Perl, Ruby, LISP .emacs.d Simple, powerful, and semantic GNU Emacs configuration for personal use. Repo.\nMotivation: I wanted to track my progress in developing a working Emacs config.\nRole: author, maintainer Technologies: GNU Emacs, elisp ","permalink":"https://www.jemoka.com/posts/kbhprojects/","tags":["index"],"title":"Projects Index"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhproof/","tags":null,"title":"proof"},{"categories":null,"contents":"base case Prove some base case \\(n_0\\)\ninductive step Prove that, given \\(n\\), \\(n_{j} \\implies n_{j+1}\\).\n","permalink":"https://www.jemoka.com/posts/kbhproof_by_induction/","tags":null,"title":"proof by induction"},{"categories":null,"contents":"Based on the wise words of a crab, I will start writing down some Proof Design Patterns I saw over Axler.\ninheriting properties (splitting, doing, merging) \u0026ldquo;complex numbers inherit commutativity via real numbers\u0026rdquo;\nconstruct then generalize for uniqueness and existence\ntry to remember to go backwards\nto prove IFF\nzero is cool, and here too!, also \\(1-1=0\\)\n\\(0v = 0\\) \\(1-1 = 0\\) \\(v-v=0\\) a.k.a. \\(v+(-v)=0\\) \\(v+0 = v\\) distributivity is epic: it is essentially the only tool to connect scalar multiplication and addition in a vector space\n\u0026ldquo;smallest\u0026rdquo; double containement proofs to show set equivalence: prove one way, then prove the converse (\\(a \\subset b, b\\subset a \\Rightarrow a=b\\))\ncouple hints\nstep 1: identify hypothesis (assumptions) desired conclusion (results, trying/to/proof) step 2: define write down precise, mathematical notations ","permalink":"https://www.jemoka.com/posts/kbhproof_design_patterns-1/","tags":null,"title":"Proof Design Patterns"},{"categories":null,"contents":"Based on the wise words of a crab, I will start writing down some Proof Design Patterns I saw over Axler.\ninheriting properties (splitting, doing, merging) \u0026ldquo;complex numbers inherit commutativity via real numbers\u0026rdquo;\nconstruct then generalize for uniqueness and existence\ntry to remember to go backwards\nto prove IFF\nzero is cool, and here too!, also \\(1-1=0\\)\n\\(0v = 0\\) \\(1-1 = 0\\) \\(v-v=0\\) a.k.a. \\(v+(-v)=0\\) \\(v+0 = v\\) distributivity is epic: it is essentially the only tool to connect scalar multiplication and addition in a vector space\n\u0026ldquo;smallest\u0026rdquo; double containement proofs to show set equivalence: prove one way, then prove the converse (\\(a \\subset b, b\\subset a \\Rightarrow a=b\\))\ncouple hints\nstep 1: identify hypothesis (assumptions) desired conclusion (results, trying/to/proof) step 2: define write down precise, mathematical notations proving uniqueness: set up two distinct results, show that they are the same\nproving negation: if the \u0026ldquo;negative\u0026rdquo; is distinct, but the direct case is more nebulous, use proves by contradiction\nproof by induction\nmessing with list length\nespecially if you are dealing with polynomials, try factoring tools to help includes length of linearly-independent list \\(\\leq\\) length of spanning list ","permalink":"https://www.jemoka.com/posts/kbhproof_design_patterns/","tags":null,"title":"Proof Design Patterns"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhproof_of_work/","tags":null,"title":"proof of work"},{"categories":null,"contents":"propaganda is a form of advertising which:\npropaganda persuades people into believe in a cause often defies reason to reach into ?? See examples:\nUS WWII Propaganda techniques for propaganda Name calling Generalities Transferring of authority Public testimonial Attachment to plane folks Bandwagoning (FOMO) Fear Bad logic Unwanted extrapolation ","permalink":"https://www.jemoka.com/posts/kbhpropaganda/","tags":null,"title":"propaganda"},{"categories":null,"contents":"the fast you are willing to prototype, the more willing you are to fail, the faster you will get to a successful partial solution you can refine and repeat.\nhow to prototype faster? In order of decreasing slowness\u0026mdash;-\nbuild out the whole product\u0026hellip; building the minimum viable product\u0026hellip; skeleton prototyping (Figma)\u0026hellip; Pen and paper\u0026hellip; Talking about it The trade-off: each level gives increased fidelity: its closer to what will actually ship, so you can get better+detailed feedback.\n","permalink":"https://www.jemoka.com/posts/kbhprototyping/","tags":null,"title":"Prototyping"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhcorrelation/","tags":null,"title":"quantum correlation"},{"categories":null,"contents":"The computation model behind quantum theory. It proposes quantum computers, proposed during the 80s. Theoretically, quantum computers have quantum supremacy, which is exciting. It is a theory that works with counterfactual information.\nquantum computer A quantum computer is a computer that uses quantum effects to perform Turing-like computations\nquantum supremacy That a quantum computer outperforms all classical computers\nuniversal computer \u0026ldquo;a programmable system whose repertoire includes all physically possible computations\u0026rdquo; \u0026mdash; Turing.\nYou will realize that modern computers are not actually capable of all computations\u0026mdash;apparently, they can\u0026rsquo;t make itself.\nTherefore, to actually achieve this, we have to make a more general type of computer: a constructor \u0026mdash; a universal quantum constructor.\n","permalink":"https://www.jemoka.com/posts/kbhquantum_information_theory/","tags":null,"title":"quantum information theory"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhquantum_supremecy/","tags":null,"title":"quantum supremecy"},{"categories":null,"contents":"quantum theory allows us to understand physics; it reconciliations the classical world with the quantum world.\nClassical particles, in the double slit experiment, would just straight go through and bounce off Actual particles (quantum) like light, under quantum theory, would actually exhibit interference via wave-like hebahior The measurement of quantum theory is done via quantum information theory.\n","permalink":"https://www.jemoka.com/posts/kbhquantum_theory/","tags":null,"title":"quantum theory"},{"categories":null,"contents":"A qubit is a two-layer quantum theory system.\nA classical bit is something that can be set between two values, a qubit between a much higher dimension.\n","permalink":"https://www.jemoka.com/posts/kbhqubits/","tags":null,"title":"qubit"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhr_n_meeting_with_angi/","tags":null,"title":"R@N Meeting with Angi"},{"categories":null,"contents":"Let\u0026rsquo;s compute what \\(e^{tA}\\) should look like, where \\(t\\) is some scalar and \\(A\\) is a diagonalizable matrix. This is a supplement to Second-Order Linear Differential Equations.\nLet \\(v_1\\dots v_{m}\\) be the eigenvectors of \\(A\\). Let \\(\\lambda_{1}\\dots\\lambda_{m}\\) be the eigenvalues.\nRecall that we can therefore diagonalize \\(A\\) as:\n\\begin{equation} A = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nread: change of choordinates into the eigenbases, scale by the eigenvalues, then change back to normal choordinates.\nNow, imagine if we are multiplying \\(A\\) by itself manymany times; what will that look like?\n\\begin{equation} A^{n} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1}\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\dots \\end{equation}\nThe middle parts, nicely, cancels out! Its a matrix applied to its inverse! So, we get rid of it\n\\begin{equation} A^{n} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(\\dmat{\\lambda_{1}, \\dots, \\lambda_{m}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\dots \\end{equation}\nNow, we are multiplying diagonal matricies against itself! If you work out the mechanics of matrix multiplication, you will note that each element simply gets scaled to higher powers (the matricies are diagonal!)! So then, we have:\n\\begin{equation} A^{n} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{{\\lambda_{1}}^{n}, \\dots, {\\lambda_{m}}^{n}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nNice.\nRecall also the Tayler expasion of \\(e^{x}\\); we will apply it to to \\(e^{tA}\\):\n\\begin{equation} e^{tA} = \\sum_{k=0}^{\\infty} \\frac{1}{k!}(tA)^{k} = \\sum_{k=0}^{\\infty} \\frac{t^{k}}{k!}A^{k} \\end{equation}\nOk. We now apply our definition of \\(A^{n}\\) derived above:\n\\begin{equation} e^{tA} = \\sum_{k=0}^{\\infty} \\frac{t^{k}}{k!}\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{{\\lambda_{1}}^{k}, \\dots, {\\lambda_{m}}^{k}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nSee now that \\(\\mqty(v_1 \u0026amp; \\dots \u0026amp;v_{m})\\) and its inverse is both constant in the sum, so we take it out:\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\qty(\\sum_{k=0}^{\\infty}\\frac{t^{k}}{k!} \\mqty(\\dmat{{\\lambda_{1}}^{k}, \\dots, {\\lambda_{m}}^{k}}))\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nAnd now, the actual mechanics of adding a matrix is just adding it elementwise, so we will put the summations into the matrix:\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{\\sum_{k=0}^{\\infty}\\frac{t^{k}}{k!} {\\lambda_{1}}^{k}, \\dots, \\sum_{k=0}^{\\infty}\\frac{t^{k}}{k!} {\\lambda_{m}}^{k}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\nNote now that each value in that matrix is just the Tayler expansion of \\(e^{k_{\\lambda_{j}}}\\) (take a moment to pause if this is not immediately obvious; think about what each element in that diagonal matrix look like and what the Tayler polynomial \\(e^{x}\\) should look like. Perhaps what some arbitrary \\(e^{ab}\\) should looks like.\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhraising_e_to_a_matrix/","tags":null,"title":"raising e to a matrix"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrandom_variables/","tags":null,"title":"random variable"},{"categories":null,"contents":"The Random Walk Hypothesis is a financial econometric hypothesis that stocks have the same distribution and independent of each other: that stocks are a random variable and not predictable in a macro space.\nTo set up the random walk hypothesis, let\u0026rsquo;s begin with some time \\(t\\), an asset return \\(r_t\\), some time elapsed \\(k\\), and some future asset return \\(r_{t+k}\\).\nWe will create two random variables \\(f(r_t)\\) and \\(g(r_{t+k})\\), which \\(f\\) and \\(g\\) are arbitrary functions we applied to analyze the return at that time.\nThe Random Walk Hypothesis tells us that, at any two unrelated given time, you cannot use the behavior of \\(r_t\\) to predict anything about \\(r_{t+k}\\), under any kind of analysis \\(f\\) or \\(g\\), that:\n\\begin{equation} Cov[f(r_t), g(r_{t+k})] = 0 \\end{equation}\nSo, all of the Random Walk Hypothesis models would leverage the above result, that the two time info don\u0026rsquo;t evolve together and they are independently, randomly distributed: they are random variables.\nFor the market to be a typical Random Walk, the central limit theorem has to hold on the value of return. This usually possible, but if the variance of the return is not finite, the return will not hold the central limit theorem which means that the return will not be normal. Of course the return does not have to hold central limit theorem, then we use other convergence distributions but still model it in the Random Walk Hypothesis as a random variable.\nreturn (FinMetrics) Importantly: its not the price that follows the random walk; it is the RETURN that follows the walk; if it was the price, then its possible for price to become negative. Return, technically, is defined by:\n\\begin{equation} R_t = \\frac{p_t-p_{t-1}}{p_{t-1}} \\end{equation}\nHowever, we really are interested in the natural log of the prices:\n\\begin{equation} r_t = log(p_t) - log(p_{t-1}) \\approx R_t \\end{equation}\nWe can do this is because, for small \\(x\\), \\(log\\ x \\approx x-1\\).\nWe do this is because, if we were wanting to add the returns over the last \\(n\\) days, in \\(R_t\\) you\u0026rsquo;d have to multiply them:\n\\begin{equation} \\frac{p_{t+1}}{p_t} \\cdot \\frac{p_t}{p_{t-1}} = \\frac{p_{t+1}}{p_{t-1}} \\end{equation}\nThis is bad, because of the central limit theorem. To make a random variable built of normalizing \\(n\\) items, you have to add and not multiply them together over a time range. We want to be able to add.\nTherefore, \\(r_t\\) can achieve the same division by adding (see the log laws).\nBut either way, with enough, we know that \\(r_t\\) is independently, identity distributed.\ntime series analysis Over some days \\(k\\), we have:\n\\begin{equation} Y_{k} = \\sum_{i=1}^{k} x_{i} \\end{equation}\nGiven that \\(x_{i}\\) is distributed randomly: \\(\\{x_{i}\\}_{i=1}^{N}\\). This becomes the foundation of time series analysis. The problem of course becomes harder when the values drift against each other, is nonindependent, etc. We can use the Martingale Model to take generic random walk to a more dependent model.\nCJ test If you have some amount of volacitity measurement, we first know that, by the Random Walk Hypothesis, we have:\n\\begin{equation} X_{k} \\sim N(0,\\sigma^{2}) \\end{equation}\nGiven some future return, you hope that:\n\\begin{equation} Y_{k}=\\sum_{i=1}^{k}X_{k}\\sim N(0,\\sigma^{2}) \\end{equation}\nIf so, if you have like \\(20\\%\\) of log returns, to have a statistically significant return, we have that:\n\\begin{equation} \\sigma =\\frac{0.2}{\\sqrt{12}} \\end{equation}\ngetting a statistically significant difference from it is hard.\n","permalink":"https://www.jemoka.com/posts/kbhrandom_walk/","tags":null,"title":"Random Walk Hypothesis"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhrandom_wol/","tags":null,"title":"random wol"},{"categories":null,"contents":"randomized algorithm is a type of algorithm, similar to relaxation.\nMake a hard problem easier by changing the problem What if, instead of guaranteeing we find the best/correct answer, we only provide some chance of finding the best/correct answer? primality testing primality testing is very important for modern crypto systems; we need to be able to find large prime numbers, and be able to generate them quickly.\ntraditional primality testing We can divide every prime number below \\(\\sqrt x\\). In theory, this is pretty fast, but we need to know all the primes we need to test.\nThis would therefore take \\(O(\\sqrt{x})\\) time.\nmiller-rabin primality testing miller-rabin primality testing is a primality testing randomized algorithm.\nConstruct a set of equations, each one requiring an exponentiation and a division If any of them is false, the number is composite If they are all true, the probability that the number is composite is reduced to \\(\\frac{1}{4}\\). If we run miller-rabin 10 times \\(O(10)=O(1)\\), the number is \\(1-\\left(\\frac{1}{4}\\right)^{10}\\) chance of being prime.\nThis is of course much much faster than traditional primality testing.\nModern cryptographic system uses this.\n","permalink":"https://www.jemoka.com/posts/kbhrandomized_algorithum/","tags":null,"title":"randomized algorithm"},{"categories":null,"contents":"rational numbers are ratios:\n\\begin{equation} \\mathbb{Q} = \\left\\{\\frac{a}{b} \\middle| a,b\\in \\mathbb{Z}, b\\neq 0\\right\\} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhrational_number/","tags":null,"title":"rational number"},{"categories":null,"contents":"\\(\\mathbb{R}\\) real numbers are numbers generatable by a possibly infinite sum of powers of 10.\n","permalink":"https://www.jemoka.com/posts/kbhreal_number/","tags":null,"title":"real number"},{"categories":null,"contents":"in NSM, reductive paraphrase is the act of reducing all utterances in a language into semantic primes.\nThis is usually done with the application of an inherent, universal grammar: the conceptual grammar of semantic primes.\nproblems with reductive paraphrasing In the experiment conducted by (Labov 1973), Labov (according to (Geeraerts 2009), manuscript not found) showed that the boundaries of cup vs. mug are not clearly delineated.\n","permalink":"https://www.jemoka.com/posts/kbhreductive_paraphrase/","tags":null,"title":"reductive paraphrase"},{"categories":null,"contents":"Thanks for opening Jack\u0026rsquo;s long rambly PDF. Please read all of it; I wanted to get this out there before anything else so I apologize in advance for a letter that\u0026rsquo;s on the longer side and I didn\u0026rsquo;t have time to write a shorter one.\nBefore you begin, please read Michael\u0026rsquo;s AMAZING notes on our pitch to get the context. It\u0026rsquo;s amazing. I will not repeat here anything mentioned there.\nPat yourself on the back Oh god was that a difficult semester. We got through many a challenges and worked together to solve most of them. That\u0026rsquo;s cool. We also built a thing that the XRT team liked; so that\u0026rsquo;s cool too.\nSome of you (in the meeting) will already have known, but we are greenlit to go into phase -1! What does that mean? What changes? How can you help? Will meetings finally end on time? When will Jack finish asking silly questions? Find out more\u0026hellip; below.\nBut not too hard Just to reiterate our master deliverable as a team (like how this pitch is culminating the deliverable assigned to us on 1/6), we have until July 8th, 2022 to pitch, again:\nWhat exactly are we doing, in one line, in laymen\u0026rsquo;s terms? Why is it helpful? Clarify the roles and responsibilities for the \u0026ldquo;master faculty member\u0026rdquo;, what time commitments and value they add, and what they have to drop to support the program How can we derive legitimacy for what we are doing? (see below) For me, he also added the derivable of talking more slowly. Presumably, De wants us to come with a glossy pitch too.\nBe legit Why do we need \u0026ldquo;legitimacy\u0026rdquo;? We need motivation for kids to do this, and Nueva\u0026rsquo;s rubber stamp would be a good way to do so. this is the focus of how we are asking Lisa to greenlight phase 2 (see below)\nA valid answer for \u0026ldquo;legitimacy\u0026rdquo; is \u0026ldquo;adding the list of skills students achieved on their transcript.\u0026rdquo; Is this a good answer? Not at the moment. Its very unmotivated (this response does not pass the \u0026ldquo;why is that helpful?\u0026rdquo; test).\nAnd follow the yellow-brick road There is going to be a three stage roadmap.\nPhase -1: developing answers to PREPARE to pitch to Liza the idea, asking her to give feedback WITHOUT any of the \u0026ldquo;asks\u0026rdquo; (legitimacy, faculty time, etc.) Phase 1: building a down-scaled version of the program somewhere. Ted has mentioned interest in this, so we maybe able to co-opt some or all of his classes. Developing details and proof-of-feasibility to pitch to Liza again, this time WITH the asks to roll out to the whole school Phase 2: roll out to the whole school and prey to the Research Gods But not the leader I can\u0026rsquo;t be around forever. We are in phase -1; I will probably be gone in the middle of phase 1. We will probably have to have a faculty supporting this program unofficially for sometime, which will be a big ask.\nThis means we have to make some program changes in anticipation\u0026mdash;\nSeek a corpus callosotomy \u0026ldquo;R@N\u0026rdquo; is now separated form \u0026ldquo;Nueva Research Program.\u0026rdquo; \u0026ldquo;R@N\u0026rdquo;\u0026rsquo;s purpose is a working group to build the \u0026ldquo;Nueva Research Program.\u0026rdquo;\nWe need to separate the two as soon as possible, so that means soon. As soon as after the 7/8 deadline, I hope to make this happen. This means changes changes to our leadership structure.\nAs node A.2 outlines, \u0026ldquo;Nueva Research Program\u0026rdquo; meetings have three stable positions.\nTeams’ Stable — Responsible for managing the count, content, and quality of active Research at Nueva projects, as well as the proces of matching team members to teams. (2-3 hrs/wk) Content Stable — Responsible for managing the content of the training program and review teams. Responsible for updating nodes. Runs meetings. (1-2 hrs/wk) Participant Stable — Responsible for managing the count and recruitment of new students into the program, and identifying key experts and mentors to help build new nodes or support the program. Responsible for participant sheet (1-2 hrs/wk) As well as three review teams\nHypothesis Sciences (key mentor: TBD) Non-Hypothesis Sciences (key mentor: Ted) Literacy, Soft Skills, and Development (key mentor: TBD) In a meeting (TBD) before 7/8, we will organize ourselves into three pairs again. Each pair will choose one \u0026ldquo;stable\u0026rdquo; role and one \u0026ldquo;review team\u0026rdquo; role\u0026mdash;essentially acting as a joint-power head for the new program and a review team in itself.\nWe will split our meetings from then on in half; the first bit dealing with R@N, which I will run; the second, ACTUALLY DOING Nueva Research Programs\u0026rsquo; work, lead by the \u0026ldquo;content stable\u0026rdquo; team. This also means that we will separate the two work docs.\nOh, yeah, also, if you have gotten this far; the headings of this document forms a pretty bad poem. Please send this poem to me privately on a direct message. Thank you.\nPresumably, much of the early \u0026ldquo;nueva research program\u0026rdquo; meetings will be solely the participant stable thinking about recruiting metrics and content stable voting on new nodes. That\u0026rsquo;s OK. The protocol\u0026rsquo;s there to be changed if needed.\nBut not without your consent Although we want each and every one of you on the team (evidenced by the fact that we will be pretty screwed if anyone leaves), your main academics comes first. Please talk to me privately if you have any concerns, no harm no foul.\nAlrighty. Let\u0026rsquo;s find a time to meet.\nhttps://www.when2meet.com/?15887080-XHXI8\nI kinda want to meet y\u0026rsquo;all physically over coffee if you want; but if not virtual is all good.\nThanks again for everything!\n\u0026mdash;Jack\n","permalink":"https://www.jemoka.com/posts/kbhresearch_at_nueva_notes_06_09_2022/","tags":null,"title":"Regarding R@N"},{"categories":null,"contents":"background info Recall asymtotic analysis. We remember that:\nconstant time \u0026lt; logarithmic time \u0026lt; linear time \u0026lt; polynomial time \u0026lt; exponential time The question? What happens if dynamic programming is too slow/not good enough for the problem? What if dynamic programming is not needed; instead, why don\u0026rsquo;t we just settle for a pretty good solution?\nTake, for instance, Nueva Courses. The optimal solution is \u0026ldquo;most students get their highest possible preferences.\u0026rdquo; However, this is impractical and pretty much impossible. Instead, what if we endeavor to figure a schedule that generally maximize happiness?\nrelaxation methods constraint relaxation constraint relaxation is a relaxation method to remove extra constraints.\nMotivating problem: traveling salesman problem\nVisit all towns in a given location Travel the minimal distance to do so Cannot visit any town more than once Calculating the basic, naive solution to find all roads is \\(O(n!)\\). Best known solution is \\(O(2^nn^2)\\), which is still slow. Its also an \\(NP\\) hard problem.\nHence, to actually solve it in a reasonable time, we are going to make two relaxations.\nThe salesmen can visit a town more than once The salesmen can teleport to visited towns By these two relations, we convert traveling salesmen to the minimum spanning tree problem.\nWe now (how?) that solving MST is no worse than optimal TSP. We will solve MST, then use that problem as the upper bound of solution to TSP.\ncontinuous relaxation continuous relaxation is a relaxation method to convert difficult discrete problems into continuous ones.\nMotivating problem: set cover\nYou are having a party, and you want your friends to get a nice paper invite.\nyou will send invitations to some subsets of your friends tell them to send invitations to all your mutual friends with them What\u0026rsquo;s the minimum number of friends to invite, and who?\nSet-cover is also hard, and also NP hard. The problem is that sending invitation is discrete.\nHence, to solve, we make it possible to solve for fractions of invitations. Hence, we can prove that our solution is guaranteed to be within bounds\nLagrangian relaxation Lagrangian relaxation is a relaxation method to convert hard-limit constrains into flexible penalization (negative values).\nMotivating problem: shortest paths problem with a constraint.\nYou need to drive the shortest number of miles as well as doing it in a hard constraint to complete the solution in a certain time.\nWe can instead relax the problem into overtime driving being a negative value in the solution.\n","permalink":"https://www.jemoka.com/posts/kbhrelaxation_algorithums/","tags":null,"title":"relaxation (algorithms)"},{"categories":null,"contents":"In this experiment, a model was devised, trained, and evaluated to automate psychotherapist/client text conversations through the use of state-of-the-art, Seq2Seq Transformer-based Natural Language Generation (NLG) systems. Through training the model upon a mix of the Cornell Movie Dialogue Corpus for language understanding and an open-source, anonymized, and public licensed psychotherapeutic dataset, the model achieved statistically significant performance in published, standardized qualitative benchmarks against human-written validation data - meeting or exceeding human-written responses\u0026rsquo; performance in 59.7% and 67.1% of the test set for two independent test methods respectively. Although the model cannot replace the work of psychotherapists entirely, its ability to synthesize human-appearing utterances for the majority of the test set serves as a promising step towards communizing and easing stigma at the psychotherapeutic point-of-care.\n","permalink":"https://www.jemoka.com/posts/kbhreplier_abstract/","tags":null,"title":"Replier Abstract"},{"categories":null,"contents":"I have done various published academic research projects in the fields of natural language processing and science education. Specifically, I have an interest in textual data mining, semantic analysis, L2 acquisition, and science education.\nComputational Linguistics ConDef/Dictembed Wikipedia is a surprisingly good dictionary, and so we can mine it for building context-aware dictionary. Repository, Paper.\nTitle: ConDef: Automated Context-Aware Lexicography Using Large Online Encyclopedias\nCollaborators: Zachary Sayyah - Nueva School\nStatus: Accepted for Oral Presentation and Publication\nVenue: SAI 2022 Computing Conference\nAbstract: ConDef Abstract\nReplier Using a logistic-increase mechanism to slowly blend data to fine-tune a transformer for psychotherapy. Repo, Link.\nTitle: Towards Automated Psychotherapy via Language ModelingTowards Automated Psychotherapy via Language Modeling\nCollaborators: solo project\nStatus: Pre-Print\nVenue: Cornell ArXiV\nAbstract: Replier Abstract\nGregarious Using BPE over a huge convolutional neural network with skip connections for highly-accurate identification of chat-bots on the internet. Repo, Link.\nTitle: Byte-Pair and N-Gram Convolutional Methods of Analysing Automatically Disseminated Content on Social Platforms\nCollaborators: solo project\nStatus: Pre-Print\nVenue: Open Science Foundation Preprints\nAbstract: Gregarious Abstract\nBRANDON/nsm Investigating into the Natural Semantic Metalanguage theory, and how we can use deep-learning methods to deal with prooving/disprooving the Lexicalist Hypothesis. Repo.\nCollaborators: Brandon Cho - Nueva School/Princeton\nWork-in-progress.\npolitisort Sorting and generating politically-motivated utterances. Repo.\nCollaborators: Zachary Sayyah - Nueva School\nWork-in-progress.\ndementia A task similar to ADReSS Challenge, training on acoustic and possibly linguistic features. internal link\nCollaborators: solo project\nPI: Prof. Brian Macwinney - CMU\nWork-in-progress.\nScience Education Project80 A student-driven podcast protocol which trains students to digest scientific research. Link, Internal Link.\nTitle: Project 80: a reproducible, student-driven framework for creating multimedia educational resources from primary literature\nCollaborators: Anoushka Krishnan, Micah Brown - Nueva School\nLab: Paul Hauser - Nueva School\nPI: Luke De - Nueva School\nStatus: Published\nVenue: EB2022/FASEB Journal\nAbstract: Project80 Abstract\nResearch@Nueva A student-lead, student-taught independent program that trains high-school students as researchers and facilitates publish-quality student research.\nCollaborators: Michael, Flint, Kian, Vinca, Oliver - Nueva School\nWork-in-progress.\nParallel Computing/Blockchain Coveather See also coveather. Link.\nTitle: Encrypted, Anonymized System for Protected Health Information Verification Built via Proof of Stake\nCollaborators: solo project\nStatus: Pre-Print and Oral Presentation at the California STEM Fair\nVenue: Cornell ArXiV\nAbstract: Coveather Abstract\n","permalink":"https://www.jemoka.com/posts/kbhresearch_index/","tags":["index"],"title":"Research Index"},{"categories":null,"contents":"A reticle is a photomask/template for a lithography system (like a negative). KLA was the first company to automatically inspect wafers and reticles.\n","permalink":"https://www.jemoka.com/posts/kbhreticle/","tags":null,"title":"reticle"},{"categories":null,"contents":"Richard Nixon is an American president, but pretty much is the watergate guy.\nServed in House and Senate Eisenhower\u0026rsquo;s VP for 8 years Lost first to JFK Richard Nixon is a pragmatist; he pushes economy out of presession via Keynsian Politics.\nRichard Nixon also realized that the large southern population can be motivated via racist policies, so he shifted the .\npolitical positions of Richard Nixon Richard Nixon\u0026rsquo;s Treatment against the Vietnam War Richard Nixon\u0026rsquo;s Foreign Policy ","permalink":"https://www.jemoka.com/posts/kbhrichard_nixon/","tags":null,"title":"Richard Nixon"},{"categories":null,"contents":"Richard Nixon\u0026rsquo;s foreign policy is marked by the \u0026ldquo;Nixon Doctrine\u0026rdquo;: shifting the burden of military containment to allies.\nSupports China as a means against USSR Negotiate with the USSR to lower tension Shifts focus into building and supporting allies ","permalink":"https://www.jemoka.com/posts/kbhrichard_nixon_s_foreign_policy/","tags":null,"title":"Richard Nixon's Foreign Policy"},{"categories":null,"contents":"Richard Nixon proposed the strategy of vietnamization as a treatment to the Vietnam War. He also expanded to Cambodia. To beat the Viet Cong into submission, he initialized the Operation Linebacker campaign.\n","permalink":"https://www.jemoka.com/posts/kbhrichard_nixon_s_treatment_against_the_vietnam_war/","tags":null,"title":"Richard Nixon's Treatment against the Vietnam War"},{"categories":null,"contents":"Rick Wallace is the CEO of KLA.\n","permalink":"https://www.jemoka.com/posts/kbhrick_wallace/","tags":null,"title":"Rick Wallace"},{"categories":null,"contents":"Ronald Reagan is a president of the United States. He rises a wave of the New Right.\nComes out of Hollywood and was CA governor Reagan was a democrat, but McCarthyism lead him Reagan was an FBI informer for McCarthyism investigations Reagan was the first two-term president since 1961, was able to maintain more power compared to others \u0026ldquo;The Great Communicator\u0026rdquo; Reagan politics \u0026ldquo;Government isn\u0026rsquo;t the solution to the problem, its the problem.\u0026rdquo;\nwished for limited politics states rights condemned welfare and \u0026ldquo;welfare cheats\u0026rdquo; (the undertone of racist appeal) Evangelical undertones, family values, moral majority Against affirmative action Supply-side economics: \u0026ldquo;getting rid of taxes will allow more people to spend\u0026rdquo; Anti-Soviet rhetoric Creates the largest increase in welfare spending, gutting about $1.5Bn.\nReagan policy changes Lowering taxes: 70% of tax to 28% of taxes Increase defense budget: 1 trillion to 3 trillion Rising inequality, 1% controlled 40% of wealth (double from the 1970s) Reagan Foreign Policy Ronald Reagan creates the largest military build-up in history (larger than Korea and Vietnam.)\nReasserted Command-in-Chief abilities Creates the National Security Council (for whom the ) Comitted the US to supporting the anti-Marxist insurrections around the world Credited with falling the USSR Supreme Court Interview Process A new interview process for the supreme court designed by Ronald Reagan, creating an extensive process to vet conservative. Reagan swapped out 50% of the Federal judicial process.\nReagan\u0026rsquo;s Legacy Inflation dropped\nUSSR Collapse\nMilitary complex expanded\nIncomes rose\nInequality widened\nWelfare slashed\nDebt\nConcentrated power in the white house\nCentralized conservative agenda\n","permalink":"https://www.jemoka.com/posts/kbhronald_raegan/","tags":null,"title":"Ronald Reagan"},{"categories":null,"contents":"The Rosa Parks bus incident is the instigator which needed to act on an issue to challenge the civil rights movement.\nShe participated in many civil rights agitations, and became the instigator .\n","permalink":"https://www.jemoka.com/posts/kbhrosa_parks/","tags":null,"title":"Rosa Parks"},{"categories":null,"contents":"On the dynamics of Tuning Forks. (Rossing, Russell, and Brown 1992)\nCharacterizing Tuning Forks Aluminum, tines 10mm apart. Four main groups of vibration:\nSymmetrical In-Plane Antisymmetrical In-Plane Symmetrical Out-Of-Plane Antisymmetrical Out-Of-Plane (a) and (c) are in the first group; (b) is in the second group, where the fork just warps.\nDeriving Tuning Forks\u0026rsquo; Frequency As per before, we can treat tuning forks acting in clang and fundamental modes as a good\u0026rsquo;ol fashioned cantilever beam.\nThe frequency action of a cantilever beam is defined as follows:\nOtherwise, for asymmetric modes, we can use the same exact expression but with uniform rods unfixed at either end:\nNote that density is not uniform at this point (because the bottom handle-y bit.)\n","permalink":"https://www.jemoka.com/posts/kbhrossing_1990/","tags":null,"title":"Rossing 1990"},{"categories":null,"contents":"total kinetic energy \\begin{equation} KE_{rigid} = \\frac{1}{2} M{V_{cm}}^2 + \\frac{1}{2} I_{CM}{\\omega_{CM}}^2 \\end{equation}\ntorque from gravity For even non rigid bodies, the following follows:\n\\begin{equation} \\vec{\\tau}_g = \\vec{R}_{CM} \\times M\\vec{g} \\end{equation}\nActually, this follows for any \\(f\\) (like \\(g\\)) evenly applied across point masses.\npotential energy \\begin{equation} \\Delta PE_g = mg\\Delta h \\end{equation}\nwhere, \\(\\Delta h\\) is the travel of center of mass. Regardless of whether or not its point.\n","permalink":"https://www.jemoka.com/posts/kbhrotational_energy/","tags":null,"title":"rotational energy theorem"},{"categories":null,"contents":"Rural Electrification Administration create electrification throughout cities. Most of American infrastructure still 1930s.\n","permalink":"https://www.jemoka.com/posts/kbhrural_electrification_administration/","tags":null,"title":"Rural Electrification Administration"},{"categories":null,"contents":"Observations from studying the comedian Russel Howard.\nStretching analogies Using language/motion/figure do describe something on the opposite end of the spectrum Take, for instance, age: 5Y/O: \u0026ldquo;cheers mum, wasen\u0026rsquo;t on my to-do list\u0026rdquo; A surprisingly sentimental dog: \u0026ldquo;because when I wake up tomorrow I want to see you, and I want to go for a lovely walk\u0026rdquo; Large motions + deadpan after Endless extrapolations of a normal setup: setup: Russian hackers were controlling people\u0026rsquo;s toys; punchline: \u0026ldquo;5 men were dildo\u0026rsquo;d to death, we don\u0026rsquo;t have a recording but here are their final words \u0026mdash; \u0026lsquo;oh yeaaah\u0026rsquo;, \u0026lsquo;oh fuck yeaaah\u0026rsquo;\u0026rdquo; Setup: gweneth paltro Punchline: \u0026ldquo;put an egg up there, you will feel more femenine. no! you will feel like a chicken\u0026rdquo;\nMultiple use of setups: \u0026ldquo;happy birthday too you\u0026rdquo; Peach ","permalink":"https://www.jemoka.com/posts/kbhrussel_howard/","tags":null,"title":"Russel Howard"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624594\n(Sadeghian, Schaffer, and Zahorian 2021)\nOne-Liner Using a genetic algorithm, picked features to optimize fore; achieved \\(94\\%\\) with just MMSE data alone (ok like duh me too). Developed ASR tool to aid.\nNovelty Developed an ASR methodology for speech, complete with punctuations Used a genetic algorithm to do feature selection; NNs performed worse because \u0026ldquo;space is smaller???\u0026rdquo; Notable Methods Used a GRU to insert punctuations The paper leveraged the nuke that is a bidirectional GRU, ATTENTION,\nKey Figs Fully automated ANN transcript does pretty well in terms of classifier AD/NL.\nNew Concepts fusion genetic algorithm MMSE Notes very confusing (too many things going on at once)\n","permalink":"https://www.jemoka.com/posts/kbhsadeghian_2021/","tags":["ntj"],"title":"Sadeghian 2021"},{"categories":null,"contents":"Demo day No value add for demo-day winner Competition makes you want to prepare more \u0026ldquo;this much budget for an enriching experience\u0026rdquo; Mentor Conversations None yet\nIntegration Integration into soundscape Hiring Need help designing a PCB\n","permalink":"https://www.jemoka.com/posts/kbhsalus_april_checkin/","tags":null,"title":"Salus April Checkin"},{"categories":null,"contents":"Scalar multiplication is the process of multiplying a scalar to an element in a set.\nconstituents A set \\(V\\) Some \\(\\lambda \\in \\mathbb{F}\\) Each \\(v \\in V\\) requirements scalar multiplication is defined by a function that results in \\(\\lambda v \\in V\\) (maps back to the space!) to each \\(\\lambda \\in \\mathbb{F}\\) and each \\(v \\in V\\).\nadditional information See also scalar multiplication in \\(\\mathbb{F}^n\\).\n","permalink":"https://www.jemoka.com/posts/kbhscalar_multiplication/","tags":null,"title":"scalar multiplication"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\n","permalink":"https://www.jemoka.com/search/","tags":null,"title":"Search Results"},{"categories":null,"contents":"The second moment of area is a value which\u0026mdash;given an origin\u0026mdash;describes how point masses are distributed around that origin. (i.e. a number for how point masses are distributed). It is in units \\(m^{4}\\).\nTake, for instance, the following picture:\nWe have defined an origin at \\((0,0)\\) of the figure above. Furthermore, we have some \\(\\rho_{i}\\) which is the distance from that origin to each of the infinitesimal areas \\(\\dd{A}\\).\nThen, the second moment of area is defined as:\n\\begin{equation} I = \\iint_{R} \\rho^{2} \\dd{A} \\end{equation}\nThis\u0026hellip; would make sense.\n","permalink":"https://www.jemoka.com/posts/kbhsecond_moment_of_area/","tags":null,"title":"second moment of area"},{"categories":null,"contents":"the trick Here is a pretty ubiquitous trick to solve differential equations of the second order differential equations. It is used to change a second order differential equation to a First-Order Differential Equations.\nIf you have a differential equation of the shape:\n\\begin{equation} x^{\u0026rsquo;\u0026rsquo;} = f(x,x\u0026rsquo;) \\end{equation}\nthat, the second derivative is strictly a function between the first derivative value and the current value.\nWe are going to define a notation \\(x\u0026rsquo; = v\\), which makes sense.\nSo, we will describe:\n\\begin{equation} x^{\u0026rsquo;\u0026rsquo;} = \\dv{v}{t} = \\dv{v}{x} \\dv{x}{t} = v\\dv{v}{x} \\end{equation}\nSo therefore, we have:\n\\begin{equation} x^{\u0026rsquo;\u0026rsquo;} = v\\dv{v}{x} = f(x,v) \\end{equation}\nSo turns out, the original input \\(t\\) is, given a specific equation above, we have no need to know it.\nTo actually go about solving it, see solving homogeneous higher-order differential equations.\n","permalink":"https://www.jemoka.com/posts/kbhsecond_order_differential_equations/","tags":null,"title":"second order differential equation"},{"categories":null,"contents":"Here\u0026rsquo;s a general form:\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = f(t) \\end{equation}\nsolving homogeneous higher-order differential equations This problem because easier if the right side is \\(0\\).\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = 0 \\end{equation}\nThe general goal to solve in this case is to make this a system of First-Order Differential Equations.\nTo do this, we begin by making:\n\\begin{equation} y = \\dv{x}{t} \\end{equation}\nTherefore, we can change the first equation:\n\\begin{equation} a \\dv{y}{t} + by + cx = 0 \\end{equation}\nSolving both of these conditions, we form a system of linear equations:\n\\begin{align} \u0026amp;\\dv{x}{t}=y \\\\ \u0026amp;\\dv{y}{t} = \\frac{-c}{a}x-\\frac{b}{a}y \\end{align}\nWe are now first-order, so we can put this into a matrix equation:\n\\begin{equation} \\dv t \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \u0026amp; 1 \\\\ -\\frac{c}{a} \u0026amp; \\frac{-b}{a} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\end{equation}\nNow! We have an equation:\n\\begin{equation} \\dv{t}v = Av \\end{equation}\nThe result above shows that the transformations \\(\\dv{t}\\) and \\(A\\) are isomorphic. Therefore, we now attempt to characterize \\(A\\) to solve this expression.\nLet\u0026rsquo;s begin. We will first shove that \\(v\\) on top of the differential for aesthetics:\n\\begin{equation} \\dv{v}{t} = Av \\end{equation}\nThis expression is actually nicely seperable, so we shall endeavor to separate it:\n\\begin{equation} \\dd{v} = Av\\dd{t} \\end{equation}\nOf course, \\(v\\) is a function of \\(t\\). Therefore, the right side would be woefully complicated. Therefore, we shall do this handwavy thing where we go:\n\\begin{equation} \\frac{1}{v}\\dd{v} = A\\dd{t} \\end{equation}\nNow, \\(A\\) is not a function in \\(t\\) \u0026mdash; its just some constants! So, we can integrate this safely without much trouble:\n\\begin{equation} \\int \\frac{1}{v}\\dd{v} =\\int A\\dd{t} \\end{equation}\nTo get:\n\\begin{equation} \\ln v = t A + C \\end{equation}\nNote the order as \\(t\\) is a constant. Finally, we will invert the natural log and get \\(v\\) back:\n\\begin{equation} v = e^{tA+C} \\end{equation}\nExcellent. We will now apply some log/exponent laws:\n\\begin{equation} v = e^{tA}e^{C} = e^{tA}C \\end{equation}\nthis is so very handwavy. \\(C\\) is technically a vector here\u0026hellip; long story and iffy understanding\nOk, how do we go about solving \\(x\\)?\nNote now that \\(v=(x\\ y)\\), so we will expand that:\n\\begin{equation} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = e^{tA}\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} \\end{equation}\nwhere, as we defined above \\(y=\\dv{x}{t}\\) (each integral needing a different constant.)\nNow. remember that \\(A\\) is diagonalizable; and so will \\(tA\\) (citation needed, but intuition is that scaling eigenvalues do nothing anyways). So, to make this exponentiation easier, we will diagonalize it.\nWe now have that\n\\begin{equation} e^{tA} = \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} \\end{equation}\n(how?)\nOk. Finally, we will take the binroller that is \u0026ldquo;constancy\u0026rdquo; and apply it to \\(e^{tA}\\). This took quite a bit of time for me to get, so feel free to take some time to get it too.\nThis all hinges upon the fact that \\(C\\) is a constant, so multiplying any constant to it still makes it \\(C\\).\nSo far, we have that:\n\\begin{equation} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = e^{tA}\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} = \\qty(\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} )\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} \\end{equation}\nRemember, now, that \\(v_1\\dots v_{}\\) and its inverses are nothing but vectors filled with a lot of scalars. And any scalar \\(\\alpha\\) times a constant still results in the (a new) constant: \\(\\alpha C =C\\). So, we will steamroll \\(\\mqty(x_0\u0026amp;y_0)\\) over the right side eigenbases matrix (multiplying a constant vector to any\u0026rsquo;ol matrix will just get a new set of constants back) to get:\n\\begin{align} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \u0026amp;= \\qty(\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}})\\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})^{-1} )\\begin{pmatrix} x_0 \\\\ y_0 \\end{pmatrix} \\\\ \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}}) \\begin{pmatrix} C_1 \\\\ C_2 \\end{pmatrix} \\end{align}\nNow, the middle thing has \\(t\\) in it! (the input!) So, we can\u0026rsquo;t just steamroll now. We have to preserve the middle part.\n\\begin{align} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m})\\mqty(\\dmat{e^{t\\lambda_{1}}, \\dots, e^{t\\lambda_{m}}}) \\begin{pmatrix} C_1 \\\\ C_2 \\end{pmatrix} \\\\ \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m}) \\begin{pmatrix} C_1 e^{t\\lambda_{1}} \\\\ C_2 e^{t\\lambda_{2}} \\end{pmatrix} \\end{align}\nAnd finally, we keep steamrolling:\n\\begin{align} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \u0026amp;= \\mqty(v_1\u0026amp; \\dots\u0026amp; v_{m}) \\begin{pmatrix} C_1 e^{t\\lambda_{1}} \\\\ C_2 e^{t\\lambda_{2}} \\end{pmatrix}\\\\ \u0026amp;= \\mqty({C_{1_{x}} e^{t\\lambda_{1}} + C_{2_{x}} e^{t\\lambda_{2}}} \\\\ {C_{1_{y}} e^{t\\lambda_{1}} + C_{2_{y}} e^{t\\lambda_{2}}}) \\end{align}\nThere is absolutely no difference in nature between \\(C_{j_{x}}\\) and \\(C_{j_{y}}\\) except for the fact that they are different constants (which we got by multiplying \\(v_1 \\dots v_{m}\\)) to it.\nOk so:\n\\begin{equation} \\begin{cases} x = C_{1_{x}} e^{t\\lambda_{1}} + C_{2_{x}} e^{t\\lambda_{2}}\\\\ y = C_{1_{y}} e^{t\\lambda_{1}} + C_{2_{y}} e^{t\\lambda_{2}}\\\\ \\end{cases} \\end{equation}\nconstructing the characteristic equation, as desired.\nmethod of undetermined coefficients Ok. This mechanism hinges upon the fact that linear combinations of differential equation solutions are solutions themselves. You can show this to yourself by illustrating diffeq solutions as subspaces of F^S, which are linear objects.\nTherefore, for a non-homogeneous second-order linear equation, we attempt to find two sets of solutions\u0026mdash;\nnamely, the general solution to the homogeneous case (using method above):\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = 0 \\end{equation}\nas well attempting to fit particular solutions to the general case:\n\\begin{equation} a\\dv[2]{x}{t} + b \\dv{x}{t} + cx = f(t) \\end{equation}\nthe linear combination of both solutions would construct the final solution space.\nWe already know how to do step 1\u0026mdash;solve homogeneous higher-order differential equations\u0026mdash;so we won\u0026rsquo;t harp on it here. However, how do we find particular solutions to the general equations?\nWell, we guess! Here\u0026rsquo;s a general table to help illustrate how:\n\\(f(t)\\) \\(x(t)\\) \\(ae^{bt}\\) \\(Ae^{bt}\\) \\(a \\cos (ct) + b\\sin (ct)\\) \\(A\\cos(ct) + B\\sin (ct)\\) \\(kt^{n}\\) \\(A_{n}t^{n} + A_{n-1}x^{n-1} \\dots + A_{0}\\) you can show these to yourself by taking derivatives. \\(a,b,c, k,A,B\\) are distinct constants.\nNow, once you make an educated guess for what \\(x(t)\\) is, perhaps aided by the homogeneous solution, you would take the number of derivatives needed to plug it back to the original expression. Then, equate the left expression and right \\(f(t)\\) and match coefficients of equal-degree terms to solve for the final constants \\(A\\), \\(B\\), etc.\nAfter you finally got the specific solution for \\(A\\) and \\(B\\) , we add the degree of freedom back by adding the homogenous solution in.\nLook for \u0026ldquo;Example 1 (again)\u0026rdquo; on this page (silly, I know, but worth it) to see end-to-end such a solution.\n","permalink":"https://www.jemoka.com/posts/kbhsecond_order_linear_differential_equation/","tags":null,"title":"Second-Order Linear Differential Equations"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhselective_service_system/","tags":null,"title":"Selective Service System"},{"categories":null,"contents":"In NSM, semantic primes are the most fundimental \u0026ldquo;lexical units\u0026rdquo; (so they can be words, or morphemes, etc. the size doesn\u0026rsquo;t matter) across languages.\nThey are the \u0026ldquo;core of a universal mental lexicon\u0026rdquo;.\nThere are\u0026hellip;\nguidelines for identifying semantic primes A semantic prime has to be found in every(ish?) natural language A semantic prime has to be indefinable by other primes proof for the existence of semantic primes Proof: given if the Strong Lexicalization Hypothesis holds, semantic primes must exist.\nAssume for the sake of contradiction no semantic primes exist.\nBecause Strong Lexicalization Hypothesis holds, there does not exist syntactic transformations which can take original single words and transform them into newly lexicalized words to express a different meaning.\nAt the same time, again because of the Strong Lexicalization Hypothesis, one must only leverage syntactic transformation on syntatic constituents when forming ideas.\nTherefore, given a word to lexicalize, it has to be defined by an syntatic transformation on a set of previously lexicalized words.\n(by definition) there are no words lexicalizable from the empty set of words.\nTherefore, there exists some word that needs to be lexicalized by words that are not previously defined, which is absurd. (instead, these words are lexicalized via semantic primes.)\nQED\nproblems with semantic primes the list has grown over time the problem of allolexy: formal restrictions of a language resulting in the same concept needing to be radicalized multiple times (I vs. me) finding semantic primes According to (Geeraerts 2009), (Goddard 2009) provides a \u0026ldquo;practical\u0026rdquo; (though flawed) way of establishing primes. Something to do with large-scale comparisons in \u0026ldquo;whole metalanguage studies\u0026rdquo;, which requires pairwise language comparison\nLocating primes are seen as an enforcement of NSM theories (Vanhatalo, Tissari, and Idström, n.d.). Recent prime locations: in Amharic (Amberber 2008), East Cree (Junker 2008), French (Peeters 1994), Japanese (Onishi 1994), Korean (Yoon 2008), Lao (Enfield 2002), Mandarin (Chappell 2002), Mangaaba-Mbula (Bugenhagen 2002), Malay (Goddard 2002), Polish (Wierzbicka 2002), Russian (Gladkova 2010, for the latest set, see the NSM home page), Spanish (Travis 2002), and Thai (Diller 1994).\n","permalink":"https://www.jemoka.com/posts/kbhsemantic_primes/","tags":null,"title":"semantic prime"},{"categories":null,"contents":"SVF is a standardized Discourse-Completion Task for verbal recall and fluency. It is administered by asking the participant to recall a bunch of words from within a category within 60 seconds.\n","permalink":"https://www.jemoka.com/posts/kbhsemantic_verbal_fluency/","tags":null,"title":"Semantic Verbal Fluency"},{"categories":null,"contents":"The semiconductor industry is a growing industry, the beginning of the semiconductor industry was actually in the silicon valley.\nWe are now taking a look at a reticle.\nalgorithms used in the semiconductor industry Per KLA \u0026mdash;\nClassification Random forest Boosted decision trees MLPs CNNs Reference generation GANs (WAT) VAEs Natural Grouping and Clustering auto-encoders manual feature extractors ","permalink":"https://www.jemoka.com/posts/kbhsemiconductor/","tags":null,"title":"semiconductor"},{"categories":null,"contents":"A set is an unordered collection of objects, which maybe infinitely long. It is generated with \\(\\{, \\}\\). For instance, most numbers are sets.\nconstituents a collection of objects requirements repetition does not matter order does not matter additional information ","permalink":"https://www.jemoka.com/posts/kbhset/","tags":null,"title":"set"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhsets/","tags":null,"title":"sets"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624659\nOne-Liner Multi-feature late fusion of NLP results (by normalizing text and n-gram processing) with OpenSMILE embedding results.\nNovelty NLP transcript normalization (see methods) and OpenSMILE; otherwise similar to Martinc 2021. Same gist but different data-prep.\nNotable Methods N-gram processed the input features Used WordNet to replace words with roots Key Figs New Concepts OpenSMILE ","permalink":"https://www.jemoka.com/posts/kbhshah_2021/","tags":["ntj"],"title":"Shah 2021"},{"categories":null,"contents":"Here is the most simple Differential Equation one could imagine:\n\\begin{equation} \\dv{x}{t} = f(t,x) \\end{equation}\nOr, perhaps, we have a second order differential equation which is the same thing but in the second degree:\n\\begin{equation} \\dv[2]{x}{t} = f\\qty(t,x,\\dv{x}{t}) \\end{equation}\nThen in which case, we have that the first most simple type of differential equation to be as follows:\n\\begin{equation} \\dv{x}{t} = x(t) \\end{equation}\nIf we can solve this, we can generalize this to most of other First-Order Differential Equations.\nwhere, the function \\(f(t,x)=x(t)\\).\n\\begin{align} \u0026amp; \\dv{x}{t} = x(t) \\\\ \\Rightarrow\\ \u0026amp; \\frac{1}{x(t)}\\dd{x} = \\dd{t} \\end{align}\nAt this point, you may ask yourself, why not construct it such that we have \\(\\dd{x} = x(t)\\dd{t}\\)? Well, its because our \\(x\\) is a variable in \\(t\\), so if we constructed it that way we\u0026rsquo;d have to integrate a function \\(\\dd{t}\\) with usub and the reverse chain rule, etc. etc. If we are instead integrating it on \\(\\dd{x}\\), it becomes much easier because our variable of interest no longer considers the \\(t\\).\nContinuing on, then:\n\\begin{align} \u0026amp;\\frac{1}{x(t)}\\dd{x} = \\dd{t} \\\\ \\Rightarrow\\ \u0026amp;\\int \\frac{1}{x(t)}\\dd{x} = \\int \\dd{t} \\\\ \\Rightarrow\\ \u0026amp; \\ln (x(t)) = t \\\\ \\Rightarrow\\ \u0026amp; x(t) = e^{t} \\end{align}\nAwesome. It should\u0026rsquo;t be hard also to see that, generally:\n\\begin{equation} x(t) = e^{ct} \\end{equation}\nis the solution to all equations \\(\\dv{x}{t} = cx\\).\nTurns out (not proven in the book), this holds for complex valued equations as well. So, we have some:\n\\begin{align} \u0026amp;x(t) = e^{it} \\\\ \\Rightarrow\\ \u0026amp; \\dv{x}{t} = ix \\end{align}\nOf course, from elementary calculus we also learned the fact that \\(e^{x}\\) can be represented as a power series; so check that out for now we connect it.\nThis equation leads us to solve:\n\\begin{equation} \\dv{x}{t} + ax = b(t) \\end{equation}\nIn order to do this, we neeed to find a replacement of the property that:\n\\begin{equation} \\dv t\\qty(e^{at}x) = e^{at}\\qty(\\dv{x}{t} +at) \\end{equation}\nA more general result of the above form is\n\\begin{equation} \\dv{x}{t} + a(t)x = b(t) \\end{equation}\nThis is fine, but now we need to leverage to chain rule to have \\(\\dv t a(t)\\) would be simply changing the above result to \\(a\u0026rsquo;(t)\\).\nBut anyways through this we will end up with the same solution we get from solving differential equations.\n","permalink":"https://www.jemoka.com/posts/kbhsimple_differential_equations/","tags":null,"title":"Simple Differential Equations"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhsingle_party_control/","tags":null,"title":"single party control"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhslopes/","tags":null,"title":"slope (statistics)"},{"categories":null,"contents":"a function is called smoo\n","permalink":"https://www.jemoka.com/posts/kbhsmooth_function/","tags":null,"title":"smooth function"},{"categories":null,"contents":"Social Security Administration is a welfare program to directly give cash to those who are in need.\n","permalink":"https://www.jemoka.com/posts/kbhsocial_security_administration/","tags":null,"title":"Social Security Administration"},{"categories":null,"contents":"Here\u0026rsquo;s a bit of a guide to start in software development. It is mostly links to other resources that would help.\nIntroductory Remarks Nobody \u0026ldquo;learns\u0026rdquo; software development. Even in job interviews, people expect you to have \u0026ldquo;worked\u0026rdquo; in software development. The industry, as a whole, drives via \u0026ldquo;learn-by-doing\u0026rdquo;, so its best to start thinking about what you want to achieve with software dev in terms of projects, then look specifically for resources to help you achieve those. Once you Google enough, et viola! You will have the skills needed to tackle another project.\nCommon Tooling There are some common tooling that is standard across all of software development.\nGoogle Google it! 99.98% of programming skills center around google-fu. Learn to Google unknown terms and get a better sense of the picture. The same rule applies through this guide as well.\nStackExchange A group of very mean people put together a very helpful family of websites which are essentially vicious forum boards. They are the StackExchange family of boards.\nThe most famous of which, and the one focused on programming, is called StackOverflow. StackOverflow (\u0026ldquo;SO\u0026rdquo;) is an extremely helpful resource for browsing any question you may have. For instance, if your code crashes with a stack trace, Googling the error and set site:stackoverflow.com will get you pretty far.\nIf you ask a question, though, be prepared to get yelled at though, the likely reason is that your question is already answered.\nmacOS Get a macBook. Even the cheapest one.\nDevelopment on Windows is like cooking on campfire. Doable, useful for specific things, but not great overall. If you must use a PC, put Debian/Ubuntu/some easy to use Linux on it. Windows is just terrible.\nI should add that Microsoft started doing Windows Subsystem for Linux: https://docs.microsoft.com/en-us/windows/wsl/install, which apparently have been pretty good. So worth taking a shot if you are stuck on Windows.\n*nix Terminal BSD/UNIX terminal is a tool that essentially skips the fancy user interface (UI) which your operating system draws and directly runs things \u0026ldquo;organically.\u0026rdquo; If you see something in a guide that says like:\n\u0026ldquo;please execute\u0026rdquo;\npython3 test.py or perhaps\nwget https://wandb.ai/jemoka \u0026gt;\u0026gt; test they are probably asking you to type it (\u0026ldquo;execute it\u0026rdquo;) into the Terminal and hit enter.\nRead this guide put together by the Ubuntu people, it\u0026rsquo;s very good. To open the terminal on your macOS device, open an app called Terminal.app. On Ubuntu, I believe its also an app called terminal.\nIDE An \u0026ldquo;IDE\u0026rdquo; is an Integrated Development Environment. It is where code is written. Fortunately, this is an easy one: use VSCode. There is literally no better tool out there for beginners and advanced users; no wonder it has 70% market share.\nSidenote: But Jack? What do you use? I use something called emacs for very specific reasons. Please don\u0026rsquo;t unless you really want misery and to learn a whole language to configure it.\nComputer Language Architecture This is how an idea turns into \u0026ldquo;stuff\u0026rdquo; on your screen.\nHuman programming languages (\u0026ldquo;Python\u0026rdquo;), are a very readable sort of code. No computers can actually read it. Usually, code you write goes through a three-step process before its able to be ran.\nFirst, the language you write gets converted by a \u0026ldquo;compiler\u0026rdquo; or \u0026ldquo;interpreter\u0026rdquo;, specialized pieces of code that takes human programming languages into a more machine-readable form of code named \u0026ldquo;assembly\u0026rdquo; or \u0026ldquo;byte code\u0026rdquo; respectively, called the \u0026ldquo;intermediate\u0026rdquo;.\nFor now, think of the difference between compilers and interpreters as translating code either all-at-once (compilers) or line-by-line (interpreters). Because the former has a grander view of the whole, languages that use a compiler (\u0026ldquo;compiled languages\u0026rdquo;) are faster. Although, many programmers find languages that use in interpreter (\u0026ldquo;interpreted language\u0026rdquo;) easier because they can spot problems line by line.\nBut wait! There\u0026rsquo;s more. Assembly and byte-code (what compilers and interpreters generate) are not actually runnable by a computer. Yet another piece of software called a \u0026ldquo;runtime\u0026rdquo; takes the reasonably-machine-readable code and actually performs the required operations.\nSome runtimes for languages like C++ uses the raw x86 CPU, which is the stereotypical \u0026ldquo;binary\u0026rdquo; zeros-and-ones. Some other languages, say Java, uses horribly complex runtimes that amounts to a whole virtual machine.\nHere\u0026rsquo;s a bit of a table.\nLanguage C/I Compiler/Interpreter Intermediate Runtime Python I python python bytecode python Java C javac java object java VM JavaScript I V8 (literally) js bytecode web browser! C/C++ C gcc/clang x86 asm x86 cpu Wonder what the runtimes for languages like Java are built in? C/C++. Eventually it all becomes x86 cpu instructions but its like a layer cake. This is why Python and friends are called a \u0026ldquo;higher level language\u0026rdquo;.\ngit Git is where all the code is!\nGit is a decentralized \u0026ldquo;version-control\u0026rdquo; system. It is basically a timestamp-backup system of code with messages and branching.\nGitHub is a website where people like to back up their code. Here\u0026rsquo;s my profile on GitHub.\nManaging Git is pretty\u0026hellip; Involved. It, for instance, assumes familiarity with the Terminal as described above. I suggest learning it, though. Here are some good resources:\nMorgan\u0026rsquo;s very very good git tutorial\u0026hellip; On GitHub! And this article on some commands you should know! Industry-Specific Skills What you start with doesn\u0026rsquo;t matter, but start with something Its easiest to learn programming if you have a project in mind. So, find a project in mind\u0026mdash;what it is, though, doesn\u0026rsquo;t matter. The concepts across programming are highly transferable, but the actual skill is easiest to learn if you are learning w.r.t. a project.\nData science, prototyping, and machine learning Python would be your friend for all things of programming where the act of programming is a means to an end. That is: if you are writing code to do something that\u0026rsquo;s not inherently software (data science, machine learning, heck, also manipulating quantum qubits), Python is your friend.\nIts a language that\u0026rsquo;s designed to be easy to write: is a very do-as-I-say language that sacrifices efficiency and elegance for getting crap done. This is how I started programming. This is the book I started with. It teaches Python through programming a series of small projects that are mostly Terminal games.\nTo learn data science, Nueva\u0026rsquo;s very own data science course give very good conceptual framework. A typical first project is to recognize pictures of handwritten digits, for which there is a good guide. I also started something called AIBridge with AIFS, so if we ever publish the recordings I will put them here.\nGoogle also: pip, ipython, Jupyter.\nBackend engineering Backend engineering is the science of dealing with databases and writing API (application programming interfaces). I don\u0026rsquo;t suggest starting with this, but if you are particularly interested in databases, you could!\nTo master backend engineering, first learn a database manipulation language. For 99.98% of the industry, this means mysql. The link directs to a pretty good guide.\nFurthermore, the language with which backend is written is Java. I hate to say it, but despite Java\u0026rsquo;s terribleness (don\u0026rsquo;t worry about it ;)) its very dependable. Here\u0026rsquo;s a book on Java. In general, I really like all of the books from no starch press.\nFrontend and Web engineering Do you like making stuff move? Do you like drawing buttons? Front end maybe for you. The most basic type of front-end engineering is making websites.\nStart by making a \u0026ldquo;vanilla website\u0026rdquo;: HTML (what\u0026rsquo;s on the page), CSS (what colours and sizes), JavaScript (how it moves) is the standard trio of languages to start. freeCodeCamp (a great Medium blog, check their stuff out) has a good guide on the matter.\nHowever, as you progress in your journey, you will find these tools woefully inadequate. Hence, most people writing web front end move on to something called a \u0026ldquo;JavaScript Framework\u0026rdquo;, a tool to generate a \u0026ldquo;vanilla\u0026rdquo; website from some more easily manipulable JS (changing the text on the page moves from a four line operation (indexing, selecting, grabbing, changing) to a one-liner (state.text=new text)).\nA popular JS framework is ReactJS. Check them out.\nFullstack Engineering Frontend + Backend.\nGame development Game development is honestly one of the most horribly complicated and richly science-y part of CS. I am not super experience in game development but learning C++ and mastering Unity, the game engine. Oh, right, game dev is the only, and I repeat only (with invisible footnotes and qualifications) reason why you should be writing code on Windows.\nA friend is good at game dev, I can make an intro if needed.\nGood Luck! Remember: Google-fu and project-based curiosity is your friend. Let me know if you have questions.\n","permalink":"https://www.jemoka.com/posts/kbhsoftware_dev_starter_pack/","tags":null,"title":"software dev starter pack"},{"categories":null,"contents":"H\nWaterfall The waterfall specification gets written before any code written. We hand off spec and code directly to tester, and code should behave like spec.\nCode specification exactly Spec does not update Code happens only after stuff is done\nAgile Agile are designed to work with minimum specification before code. Spec is updated constantly as code changes and get user feedback.\n","permalink":"https://www.jemoka.com/posts/kbhsoftware_development_methodologies/","tags":null,"title":"Software Development Methodologies"},{"categories":null,"contents":"process of Engineering: chronological order User interviews/stories Documentation/Specification Task estimation Design \u0026amp; architecture Testing Project Management Build and Release engineering fundamental trade-off of Software Engineering The MIT vs. New Jersey problem: in Software Engineering, you can only choose one of FAST or ROBUST.\nProblem Fast (\u0026ldquo;Bell Labs/NJ\u0026rdquo;) Robust (\u0026ldquo;MIT\u0026rdquo;) Specs Whatever it looks like screens, states, UI elements documented; transitions Time \u0026ldquo;whenever\u0026rdquo; precise projections, track work and dependencies Testing \u0026ldquo;ran it + didn\u0026rsquo;t crash\u0026rdquo; black, white box, code overage, edge/adv. cases Modular Giant function object/data model, grouped function, abstraction barriers Failure Unpredictable + silent Graceful, noisy, error reporting + logging Language Scripting, high level Low-level, assembly/bare metal, control, can be difficult Proto. Many/Quickly Few/Slowly Being Done Now Later Source: here.\nhow to choose? Which is the better approach? There isn\u0026rsquo;t one. However, here are some critical questions for you to answer:\nDeadline: what happens if you don\u0026rsquo;t finish today? Release cycle: if you ship a bug, how long can you fix it? Consequences: if the software malfunctions, how bad is it? Life-cycle: how long will the software get used? So\u0026mdash;\nAs consequences for deadline gets worse, trend towards fast; as consequences for failure gets worse, trend towards robust.\n","permalink":"https://www.jemoka.com/posts/kbhsoftware_engineering/","tags":null,"title":"Software Engineering"},{"categories":null,"contents":"So let\u0026rsquo;s say given a system:\n\\begin{equation} \\begin{cases} x + 2y + z = 0 \\\\ 2x + 0y - z = 1 \\\\ x - y + 0z = 2 \\end{cases} \\end{equation}\nWe can represent this using a matricies.\n\\begin{equation} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix} \\end{equation}\nWe will use Gaussian elimination. We will begin by multiplying the top row by \\(-2\\).\n\\begin{equation} \\begin{pmatrix} -2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} =\\begin{pmatrix} -2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\2 \\end{pmatrix} \\end{equation}\nAnd then we add row one to row two; we will not write out the transformation matrix:\n\\begin{equation} \\begin{pmatrix} -2 \u0026amp;-4 \u0026amp;-2 \\\\ 2 \u0026amp;-0 \u0026amp;-1 \\\\ 1 \u0026amp;-1 \u0026amp;0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\2 \\end{pmatrix} \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhsolving_systems/","tags":null,"title":"solving systems"},{"categories":null,"contents":" \u0026ldquo;laws.als\u0026rdquo;: \u0026ldquo;drumuomup\u0026rdquo; \u0026ldquo;ping.als\u0026rdquo;: \u0026ldquo;walking down the street, eating children\u0026rdquo;\u0026quot;\u0026quot; \u0026ldquo;planets.als\u0026rdquo;: \u0026ldquo;sing a song among the starlight\u0026rdquo; \u0026ldquo;songs.als\u0026rdquo;: \u0026ldquo;thank you klint for your discussion\u0026rdquo; Other things I have to finish \u0026ldquo;Tunel2.als\u0026rdquo; ","permalink":"https://www.jemoka.com/posts/kbhsongs_that_need_lyrics/","tags":null,"title":"Songs that need Lyrics"},{"categories":null,"contents":"Reading notes Because feeling for self-endowment, they wish to build socialist society As Communists considered themselves as a vanguard of the revolutionary proletariat – their “aim” was to build socialist society in the whole world.\nSocialist had necesity against capitalist aggression The Soviet approaches towards historical descriptions of the twentieth century showed that with the emergence of the new type of state – socialist one – it became a target for capitalist aggression.\nSocialist revolution requires the creation of socialist society against the world It was first positive move towards realization of the Soviet foreign policy main idea: the world socialist revolution and creation of the socialist society in the whole world.\nThe Soviets believe that the US wants to take over world The US had plans to dominate in the entire world.\nThat the US was intentionally sturggling with socialism All US post-war foreign policy doctrines were aimed on the struggle with socialism\nthat soviets believed that US was exclusivly fighting socialism We can summarize – that on Soviet point of view all American presidents of Cold War period were creating their own doctrines, and all of them were anti-communist and anti-Soviet\nSoviets believes that the US made the first move Soviet concept first vivid steps, which signalized about the start of the confrontation between East and West, were steps made by the West.\nbelieves its a fight against imperialism bipolar confrontation had western roots and the Cold War was the policy of the US and other imperialistic countries against socialist countries.\ncommunism is working towards revolution mankind is a process of revolutionary changes\nthe soviet union believes only it can stop American aggression the Soviet Union was the only power in the world able to stop American ambitions of superpower.\nUSSR believes that itself was the only defender The Soviet Union considered itself as the only defender of the interests of the working class all over the world because it was the first socialist state in history.\nDefinding US and defending imperialism The Imperialistic was the system of capitalist countries: they had a lot of contradictions in their “camp” where each wanted to solve their problems and to defend their own interests by using the others.\nBlack and white view of the world prevailed USSR The entire world was separated into two main categories: friends and enemies. Such black and white world-view was a distinctive feature of Stalin’s way of seeing the world (outside as well as inside the USSR), but even after his death,\n","permalink":"https://www.jemoka.com/posts/kbhsoviet_perspective_on_cold_war/","tags":null,"title":"Soviet Perspective on Cold War"},{"categories":null,"contents":"The span of a bunch of vectors is the set of all linear combinations of that bunch of vectors. We denote it as \\(span(v_1, \\dots v_{m)}\\).\nconstituents for constructing a linear combination a list of vectors \\(v_1,\\dots,v_{m}\\) and scalars \\(a_1, a_2, \\dots, a_{m} \\in \\mathbb{F}\\) requirements \\begin{equation} span(v_{1}..v_{m}) = \\{a_1v_1+\\dots +a_{m}v_{m}:a_1\\dots a_{m} \\in \\mathbb{F}\\} \\end{equation}\nadditional information span is the smallest subspace containing all vectors in the list Part 1: that a span of a list of vectors is a subspace containing those vectors\nBy taking all \\(a_{n}\\) as \\(0\\), we show that the additive identity exists.\nTaking two linear combinations and adding them (i.e. adding two members of the span) is still in the span by commutativity and distributivity (reorganize each constant \\(a_{1}\\) together)\u0026mdash;creating another linear combination and therefore a member of the span.\nScaling a linear combination, by distributivity, just scales the scalars and create yet another linear combination.\nPart 2: a subspace containing the list of vectors contain the span\nsubspaces are closed under scalar multiplication and addition. Therefore, we can just construct every linear combination.\nBy double-containment, a subspace is the smallest subspace containing all vectors. \\(\\blacksquare\\)\nspans If \\(span(v_1, \\dots v_{m})\\) equals \\(V\\), we say that \\(v_1, \\dots, v_{m}\\) spans \\(V\\).\nNOTE! the two things have to be equal\u0026mdash;if the span of a set of vectors is larger than \\(V\\), they do not span \\(V\\).\nlength of linearly-independent list \\(\\leq\\) length of spanning list see here.\n","permalink":"https://www.jemoka.com/posts/kbhspan/","tags":null,"title":"span (linear algebra)"},{"categories":null,"contents":"A spinal tap is a medical procedure whereby cerebralspinal fluid is collected by puncturing the lumbar; used to diagnose problems where biomakers from the brain are needed.\n","permalink":"https://www.jemoka.com/posts/kbhspinal_tap/","tags":null,"title":"spinal tap"},{"categories":null,"contents":"A stack trace is the output of failing code by the runtime to indicate the location of the fault. For instance, in Python:\n--------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-1-0b766d7d4bc7\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 0+\u0026#34;\u0026#34; TypeError: unsupported operand type(s) for +: \u0026#39;int\u0026#39; and \u0026#39;str\u0026#39; ","permalink":"https://www.jemoka.com/posts/kbhstack_trace/","tags":null,"title":"stack trace"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhstandard_error/","tags":null,"title":"standard error"},{"categories":null,"contents":"Everyone and their dog has a blog at this point. Why not me? You see, I don\u0026rsquo;t really like the idea of blogging, but I do enjoy taking notes. I take a crap tonnes of notes, and sometimes people want to see a copy of them.\nIn order to facilitate this, some friends and I created taproot, a collective note-taking effort which also automatically compiled pretty cool previews and an internet site. I still am one of the primary maintainers of taproot.\nWhile working on the project, however, we noticed that the loop-based architecture (instead of being based on events/triggers), lack of duplicity, and requirement of a central build server made it difficult.\nIn this vein, quantumish (also with his own lovely set of notes, tap on the link!) and I were discussing if the essentials of taproot can be built into a static site generator. Hence, this is an experiment (to hopefully be merged with the taproot group) to facilitate this.\n","permalink":"https://www.jemoka.com/posts/kbhstarting_with_why_the_knowledgebase/","tags":null,"title":"Starting With Why: The Knowledgebase"},{"categories":null,"contents":"The stationary-action principle states that, in a dynamic system, the equations of motion of that system is yielded as the \u0026ldquo;stationary points\u0026rdquo; of the system\u0026rsquo;s action. i.e. the points of \u0026ldquo;least\u0026rdquo; action. (i.e. a ball sliding down a ramp is nice, but you don\u0026rsquo;t expect it\u0026mdash;in that system\u0026mdash;to fly off the ramp, do a turn, and then fly down.\n","permalink":"https://www.jemoka.com/posts/kbhstationary_action_principle/","tags":null,"title":"stationary-action principle"},{"categories":null,"contents":"A statistic is a measure of something\n","permalink":"https://www.jemoka.com/posts/kbhstastistic/","tags":null,"title":"statistic"},{"categories":null,"contents":"Stock Issues are policy debate doctrines which divides the debate into 5 subtopical ideas.\nWikipedia\nHarms: what are the problems in the status quo?\nInherency: what are these problems not already being solved? (Or not already being solved in the best way?)\nSignificancy: comparing the advantages and disadvantages of the status quo and your proposed solution, why is the proposed solution more worthy than the status quo?\nThe Ws:\nWhy this? Why is your proposed solution the best (most effective, or most feasible, or fastest, etc.) one?\nWhy now? Why is now the best time to build this solution?\nWhy you? Why are you (and your team) the best builders of this solution?\n","permalink":"https://www.jemoka.com/posts/kbhstock_issues_debate/","tags":null,"title":"Stock Issues (Debate)"},{"categories":null,"contents":"strain is the proportional deformation of a material given some stress applied\n","permalink":"https://www.jemoka.com/posts/kbhstrain/","tags":null,"title":"strain"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhstress/","tags":null,"title":"stress"},{"categories":null,"contents":"Reading Notes Strong Free Will vs. Weak Free Will \u0026mdash; \u0026ldquo;will\u0026rdquo; and \u0026ldquo;bells inequality\u0026rdquo; is a demonstration of indeterminism/randomness between particles \u0026mdash; but indeterminism and randomness a demonstration of will.\nThat if humans have free will, it should be spawened from the indeterminism of elementary particles It asserts, roughly, that if indeed we humans have free will, then elementary particles already have their own small share of this valuable commodity.\nSPIN Axiom SPIN Axiom: Measurements of the squared (components of) spin of a spin 1 particle in three orthogonal directions always give the answers 1, 0, 1 in some order.\nTWIN Axiom Paired particles will come up with same measurements if measured in the same way\nThe TWIN Axiom: For twinned spin 1 particles, suppose experimenter A performs a triple experiment of measuring the squared spin component of particle a in three orthogonal directions x, y, z, while experimenter B measures the twinned par- ticle b in one direction, w . Then if w happens to be in the same direction as one of x, y, z, experimenter B’s measurement will necessarily yield the same answer as the corresponding measurement by A.\nFree as something that cannot be an uncurried function of previous states To say that A’s choice of x, y, z is free means more precisely that it is not determined by (i.e., is not a function of) what has happened at earlier times (in any inertial frame).\nMIN Axiom Choice of direction of measurement of one twinned qubit does not influence the results of the current qubit (unless they happen to align.)\nThe MIN Axiom: Assume that the experiments performed by A and B are space-like separated. Then experimenter B can freely choose any one of the 33 particular directions w , and a’s response is independent of this choice. Similarly and inde- pendently, A can freely choose any one of the 40 triples x, y, z, and b’s response is independent of that choice.\n","permalink":"https://www.jemoka.com/posts/kbhstrong_free_will/","tags":null,"title":"Strong Free Will"},{"categories":null,"contents":"A subspace is a vector space which is a subset of a vector space, using the same addition and scalar multiplication operations. Intuitively, a subspace of \\(\\mathbb{R}^{2}\\) are all the lines through the origin as well as \\(\\{0\\}\\); a subspace of \\(\\mathbb{R}^{3}\\) are all the planes through the origin as well as \\(\\{0\\}\\), etc. etc.\nconstituents vector space \\(V\\) A subset \\(U \\subset V\\) which is itself a vector space requirements You check if \\(U\\) is a subspace of \\(V\\) by checking IFF the following three conditions:\nadditive identity: \\(0 \\in U\\) closed under the same addition as in \\(V\\): \\(u,w \\in U: u+w \\in U\\) closed under scalar multiplication as in \\(V\\): \\(a \\in \\mathbb{F}\\) and \\(u \\in U\\) means \\(au \\in U\\) Yes, by only checking three you can prove everything else.\nadditional information simplified check for subspace commutativity, associativity, distributivity These properties are inherited from \\(V\\) as they hold for every element in \\(V\\) so they will hold for \\(U \\subset V\\).\nadditive inverse Because scalar multiplication is defined, and we proved in Axler 1.B that \\(-1v=-v\\) (proof: \\(v+(-1)v = (1+(-1))v = 0v = 0\\)).\nmultiplicative identity Its still \\(1\\).\n\\(\\blacksquare\\)\nfinite-dimensional subspaces Every subspace of a finite-dimensional vector space is a finite-dimensional vector space.\nWe prove this result again via induction.\nbase case If \\(U=\\{0\\}\\), we know \\(U\\) is finite-dimensional and are done. If not, take some \\(v_1 \\in U\\) and create a list with only \\(v_1\\) thus far; the invariant here is that the list is linearly independent as we see that a list containing this one element as indeed linearly independent.\ncase \\(j\\) If the linearly independent list we created \\(v_1, \\dots v_{j-1}\\) spans \\(U\\), we are done. We have created a finite list which spans \\(U\\), making \\(U\\) finite-dimensional.\nIf not, that means that we can pick some \\(u \\in U\\) that cannot be written as a linear combination of the invariantly linearly independent vectors \\(v_1, \\dots v_{j-1}\\). We append \\(u\\) to the list, naming it \\(v_{j}\\). As \\(v_{j}\\) cannot be written as a linear combination of the original list, appending it to the list doesn\u0026rsquo;t make the list dependent. This means that the list is still linearly independent.\ninduction Therefore, we have constructed a list of increasing length that is linearly independent. By the fact that length of linearly-independent list \\(\\leq\\) length of spanning list, and the fact that the spanning list of \\(V\\) has finite length (it is given that \\(V\\) is a finite-dimensional vector space), the increasingly longer linearly independent list\u0026mdash;building upwards to eventually span \\(U\\) in finite length.\n","permalink":"https://www.jemoka.com/posts/kbhsubspace/","tags":null,"title":"subspace"},{"categories":null,"contents":"The sum of subsets is the definition of addition upon two subsets.\nApparently, the unions of subsets are almost never subspaces (they don\u0026rsquo;t produce linearity?) Therefore, we like to work with sum of subsets more.\nRemember this has arbitrarily many things!! as a part of the content. When defining, remember to open that possibility.\nconstituents Sub-sets of \\(V\\) named \\(U_1, U_2, \\dots, U_{m}\\)\nrequirements The sum of subsets \\(U_1, \\dots, U_{m}\\) is defined as:\n\\begin{equation} U_1, \\dots, U_{m} = \\{u_1+\\dots+u_{m}: u_1\\in U_1, \\dots, u_{m} \\in U_{m}\\} \\end{equation}\n\u0026ldquo;all elements formed by taking one element from each and add it.\u0026rdquo;\nadditional information sum of subspaces is the smallest subspace with both subspaces Suppose \\(U_1, \\dots U_{m}\\) are subspaces of \\(V\\), then \\(U_1+\\dots +U_{m}\\) is the smallest subspace of \\(V\\) containing \\(U_1, \\dots, U_{m}\\).\nProof:\nIs a subspace\u0026mdash;\nclearly \\(0\\) is in the sum. (taking \\(0\\) from each subspace and adding) addition and scalar multiplication inherits (closed in each subspace, then, reapplying definition of sum of subsets) Smallest containing subspace\u0026mdash;\nBecause a subspace is closed under addition, if a subspace contains \\(U_{1}, \\dots, U_{m}\\) you can always add each of the constituent elements manually to form every \\(U_1+\\dots+U_{m}\\).\nConversely, the subspace \\(U_1+\\dots +U_{m}\\) should contain \\(U_1, \\dots, U_{m}\\) by simply setting the coefficients except for the one you are interested in to \\(0\\).\nTherefore, as both subsets contain each other; they are equivalent.\ndimension of sums Let there be two finite-dimensional subspaces: \\(U_1\\) and \\(U_2\\). Then:\n\\begin{equation} \\dim(U_1+U_2)=\\dim U_1+\\dim U_{2} - \\dim(U_1 \\cap U_2) \\end{equation}\nProof:\nlet us form an basis of \\(U_1 \\cap U_{2}\\): \\(u_1, \\dots u_{m}\\); this indicates to us that \\(\\dim(U_1 \\cap U_{2}) = m\\). Being a basis of \\(U_1 \\cap U_{2}\\), it is linearly independent in \\(U_1\\) (which forms a part of the intersection.\nAs any linearly independent list (in this case, in \\(U_1\\)) can be expanded into a basis of \\(U_1\\). Let\u0026rsquo;s say by some vectors \\(v_1 \\dots v_{j}\\). Therefore, we have that:\nThe new basis is \\(u_1, \\dots u_{m}, v_1, \\dots v_{m}\\), and so:\n\\begin{equation} \\dim U_1 = m+j \\end{equation}\nBy the same token, let\u0026rsquo;s just say some \\(w_1, \\dots w_{k}\\) can be used to extend \\(u_1, \\dots u_{m}\\) into a basis of \\(U_2\\) (as \\(u_1, \\dots u_{m}\\) is also an linearly independent list in \\(U_2\\)). So:\n\\begin{equation} \\dim U_{2} = m+k \\end{equation}\nWe desire that \\(\\dim(U_1+U_2)=\\dim U_1+\\dim U_{2} - \\dim(U_1 \\cap U_2)\\). Having constructed all three of the elements, we desire to find a list that is length \\((m+j)+(m+k)-m = m+j+k\\) that forms a basis of \\(U_1+U_2\\), which will complete the proof.\nConveniently, \\(u_1, \\dots u_{m}, v_1, \\dots v_{j}, w_1, \\dots w_{k}\\) nicely is list of length \\(m+j+k\\). Therefore, we desire that that list forms a basis of \\(U_1+U_{2}\\).\nAs pairwise in this list are the basis of \\(U_1\\) and \\(U_2\\), this list can span both \\(U_1\\) and \\(U_2\\) (just zero out the \u0026ldquo;other\u0026rdquo; sublist\u0026mdash;zero \\(w\\) if desiring a basis of \\(U_1\\), \\(v\\) if \\(U_2\\) \u0026mdash;and you have a basis of each space. As \\(U_1+U_2\\) requires plucking a member from each and adding, as this list spans \\(U_1\\) and \\(U_2\\) separately (again, it forms the basis of the each space), we can just use this list to construct individually each component of \\(U_1+U_2\\) then adding it together. Hence, that long combo list spans \\(U_1+U_2\\).\nThe only thing left is to show that the giant list there is linearly independent. Let\u0026rsquo;s construct:\n\\begin{equation} a_1u_1+ \\dots + a_{m}u_{m} + b_1v_1 + \\dots + b_{j}v_{j} + c_1w_1 + \\dots + c_{k}w_{k} = 0 \\end{equation}\nto demonstrate linearly independence,\nMoving the \\(w\\) to the right, we have that:\n\\begin{equation} a_1u_1+ \\dots + a_{m}u_{m} + b_1v_1 + \\dots + b_{j}v_{j} =-(c_1w_1 + \\dots + c_{k}w_{k}) \\end{equation}\nRecall that \\(u_1 \\dots v_{j}\\) are all vectors in \\(U_1\\). Having written \\(-(c_1w_1 + \\dots + c_{k}w_{k})\\) as a linear combination thereof, we say that \\(-(c_1w_1 + \\dots + c_{k}w_{k}) \\in U_1\\) due to closure. But also, \\(w_1 \\dots w_{k} \\in U_2\\) as they form a basis of \\(U_2\\). Hence, \\(-(c_1w_1 + \\dots + c_{k}w_{k}) \\in U_2\\). So, \\(-(c_1w_1 + \\dots + c_{k}w_{k}) \\in U_1 \\cap U_2\\).\nAnd we said that \\(u_1, \\dots u_{m}\\) are a basis for \\(U_1 \\cap U_{2}\\). Therefore, we can write the \\(c_{i}\\) sums as a linear combination of $u$s:\n\\begin{equation} d_1u_1 \\dots + \\dots + d_{m}u_{m} = (c_1w_1 + \\dots + c_{k}w_{k}) \\end{equation}\nNow, moving the right to the left again:\n\\begin{equation} d_1u_1 \\dots + \\dots + d_{m}u_{m} - (c_1w_1 + \\dots + c_{k}w_{k}) = 0 \\end{equation}\nWe have established before that \\(u_1 \\dots w_{k}\\) is a linearly independent list (it is the basis of \\(U_2\\).) So, to write \\(0\\), \\(d_1 = \\dots = c_{k} = 0\\).\nSubstituting back to the original:\n\\begin{align} a_1u_1+ \\dots + a_{m}u_{m} + b_1v_1 + \\dots + b_{j}v_{j} \u0026amp;=-(c_1w_1 + \\dots + c_{k}w_{k}) \\\\ \u0026amp;= 0 \\end{align}\nrecall \\(u_1 \\dots v_{j}\\) is the basis of \\(U_1\\), meaning they are linearly independent. The above expression makes \\(a_1 = \\dots b_{j} = 0\\). Having shown that, to write \\(0\\) via \\(u, v, \\dots w\\) requires all scalars \\(a,b,c=0\\), the list is linearly independent.\nHaving shown that the list of \\(u_1, \\dots v_1, \\dots w_1 \\dots w_{k}\\) spans \\(U_1+U_2\\) and is linearly independent within it, it is a basis.\nIt does indeed have length \\(m+j+k\\), completing the proof. \\(\\blacksquare\\)\n","permalink":"https://www.jemoka.com/posts/kbhsum_of_subsets/","tags":null,"title":"sum of subsets"},{"categories":null,"contents":"confidence intervals, a review:\n\\begin{equation} statistic \\pm z^*\\sigma_{statistic} \\end{equation}\nFrequently, we don\u0026rsquo;t have access to \\(\\sigma\\) and hence have to guestimate. When we have a sample means and a proportion, we have ways of guestimating it from the standard error (available on the single-sample section of the AP Statistics formula sheet.)\nHowever, for means, the standard error involves! \\(\\sigma\\). How do we figure \\(\\sigma\\) when we don\u0026rsquo;t know it? We could use \\(s\\), sample standard deviation, but then we have to adjust \\(z^*\\) otherwise we will have underestimation. Hence, we have to use a statistic called \\(t^*\\).\nWe can use t-values to perform t-test, a hypothesis test of means.\n","permalink":"https://www.jemoka.com/posts/kbht_statistics/","tags":null,"title":"t-statistics"},{"categories":null,"contents":"A t-test is a hypothesis test for statistical significance between two sample means based on t-statistics. Before it can be conducted, it must meet the conditions for inference.\nconditions for inference (t-test) To use t-statistics, you have to meet three conditions just like the conditions for inference used in z-score.\nrandom sampling normal (sample size larger than 30, or if original distribution is confirmed as roughly symmetric about the mean) Independence use a z-statistic to find a p-value Begin by finding a \\(t\\) statistic. Remember that:\n\\begin{equation} t = \\frac{statistic-parameter}{std\\ err} \\end{equation}\nIn this case, when we are dealing with sample means, then, we have:\n\\begin{equation} t = \\frac{\\bar{x}-\\mu_0}{\\frac{S_x}{\\sqrt{n}}} \\end{equation}\nwhere \\(\\bar{x}\\) is the measured mean, \\(\\mu_0\\) is the null hypothesis mean, and \\(S_x\\) the sample\u0026rsquo;s sample standard deviation.\nQuick note:\n\\(SE = \\frac{S}{\\sqrt{n}}\\) because the central limit theorem states that sample means for their own distribution, whose variance equals the original variance divided by the sample size. Hence, the standard deviation of the means would be the sample standard deviation divided by the square root of the sample size.\nOnce you have a \\(t\\) value, you look at the test and what its asking (above the mean? below the mean? etc.) and add up the tail probabilities.\npaired vs two-sample tests A paired t-test looks at pairs of values as statistic in itself (i.e. substracts directly, etc.) Think about it as a compound statistic, so you are doing a \\(t\\) test on one value, it just happened to be composed/calculated by a pair of values. (for instance, \u0026ldquo;difference between mother-father glucose levels.\u0026rdquo;)\nA two-staple t-test looks at two independent events and compares them. Hence, they are two random variables and should be manipulated as such.\nt-tests for regression lines regression lines can be imbibed with predictive power and confidence intervals:\n\\begin{equation} m \\pm t^* SE_b \\end{equation}\nwhere \\(m\\) is the slope and \\(SE_b\\) is the standard error of the regression line.\nNote that the degrees of freedom used for \\(t^*\\) is the number of data points, minus two.\nconditions for inference (slops) Acronym: LINEAR\nLinear Independent (observations are independent or \\(\u0026lt;10\\%\\)) Normal (for a given \\(x\\), \\(y\\) is normally distributed) Equal variance (for any given \\(x\\), it should have a roughly equal standard deviation in \\(y\\)) Random ","permalink":"https://www.jemoka.com/posts/kbht_test/","tags":null,"title":"t-test"},{"categories":null,"contents":"readme conda init zsh (close shell, open again) .mp4 mfa model downloading what\u0026rsquo;s the difference between online docker install and manual install NLTK Huggingface transformers tokenizers (versining) /opt/homebrew/Caskroom/miniforge/base/envs/aligner/lib/python3.9/site-packages/montreal_forced_aligner/corpus/text_corpus.py; getattr(self, k).update(error_dict[k]) AttributeError: \u0026rsquo;list\u0026rsquo; object has no attribute \u0026lsquo;update\u0026rsquo; FileArgumentNotFoundError: ; line 139\nDBA See the data on the frequency of haphax legomina vs. COCA ESPNet need to talk to Ji Yang Andrew\u0026rsquo;s Features Collapse two PAR tiers down Checkpoint per file One corpus prompt per run Handle empty tiers I/P selection crashes! contingency preview the LONGEST segment instead of the top one -i kill in the middle fixes \u0026ldquo;my mom\u0026rsquo;s cryin(g)\u0026rdquo; [\u0026lt;] mm [l648] (also themmm after) \u0026ldquo;made her a nice dress\u0026rdquo; [\u0026lt;] mhm [l1086] \u0026ldquo;when I was a kid I\u0026rdquo; \u0026amp;=laughs [l1278] Others chstring (for uh, mm-hmm)\nretrace (asr\u0026amp;fa folder)\nlowcase (caps)\nrep-join.cut (fixes/)\nnumbers \u0026lt;affirmative\u0026gt; \u0026lsquo;mo data! CallFriend/CallHome (ca-data) ISL? SBCSAE Aphasia + MICASE TBI data Providing a Two-Pass Solution Writing Big description of the pipeline Notion of the pipeline Better tokenization? 8/18 Initial segment repetition Extracting studdering Gramatically problematic mar mar has done a thing and its phoneme level We did it, now automated LEAP data next actions Aphasia (-apraxia?): classification Child data (EllisWeismer) Dementia a ~Multiple @Begin/CHECK problem~\n~Placement of @Options~\n~Strange, missing period~\n~Bracket comments should FOLLOW words instead of PRECEEDING them~\n~%xwor: line~\nSTICK TO DASHES WHEN DISTRIBUTING BATCHALIGN\nend the utterance when it ends (incl. inter-utterance pauses)\n\u0026ldquo;I\u0026rdquo; need to be capitalized\n11005 (LT)\nAlign EllisWeismer\nAlso cool to align:\nfluency IISRP/*\nhttps://en.wikipedia.org/wiki/Speaker_diarisation\nhttps://universaldependencies.org/\nAlzheimer\u0026rsquo;s Project https://dementia.talkbank.org/\nhttps://luzs.gitlab.io/adresso-2021/\nSpecifically: https://dementia.talkbank.org/access/English/Pitt.html\nReview Kathleen Fraser: https://drive.google.com/drive/u/1/folders/1lYTIzzXLXw3LlDG9ZQ7k4RayDiP6eLs1\nHere are the review papers: https://drive.google.com/drive/u/1/folders/1pokU75aKt6vNdeSMpc-HfN9fkLvRyutt\nRead this first: https://drive.google.com/drive/u/1/folders/0B3XZtiQwQW4XMnlFN0ZGUndUamM?resourcekey=0-AlOCZb4q9TyG4KpaMQpeoA\nSome PITT data have 3-4 recordings\nThe best way to diagnosing alzhimers\u0026rsquo; is from language.\nWhy this field is needed: to analyze a pre-post test metric.\nDesired output: existence of dementia (a.k.a alzheimer\u0026rsquo;s\u0026rsquo;).\nOther research to read:\nPenn (julia parish something but they don\u0026rsquo;t stare their data but they smile and things with Mark Libermann type of thing) Learning more about speech text https://my.clevelandclinic.org/health/diagnostics/22327-differential-diagnosis python3 ~/mfa_data/batchalign-dist/batchalign.py ~/mfa_data/my_corpus ~/mfa_data/my_corpus_aligned\nchristan marr paper on MFA on child data\n","permalink":"https://www.jemoka.com/posts/kbhtalkbank/","tags":null,"title":"talkbank"},{"categories":null,"contents":"Lit Survey Pipeline Segmentation ","permalink":"https://www.jemoka.com/posts/kbhtalkbank_pipeline_project/","tags":null,"title":"TalkBank Pipeline Project"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhtariffs/","tags":null,"title":"tariffs"},{"categories":null,"contents":"Step 0: know what you are building.\nbreaking tasks The process of breaking tasks down.\nWe need to research tasks to see how complex they are + how to break them down Research takes time! It should be its own task Over the process of research, the task becomes much simpler estimating tasks Requirement: tasks should always be estimated by the person doing the work.\nTask Estimation should be done each time! tasks shift Estimate only in powers of 2: 30 minutes, 1h, 2h, 4h, 8h, etc. If you never done something before, double the time than you estimate If you are teaching someone to do something, quadruple the time than you estimate Add buffer time (*1.5), especially if you think yourself as a procrastinator Focus is draining! You need breaks. Take breaks. Things will go wrong! Plan for it. time iterating If anything is longer than 8 hours, that\u0026rsquo;s a good sign you need to break it down! Likely that you have to break things down MVP You probably don\u0026rsquo;t have time to build your feature list\nMVP: minimum viable product We need the basic set of features; you probably have more features than you have time to build Prioritize what you build based on\u0026hellip; Dependencies: is this required for other stuff to work Viability: can the product exist without this? Time: how long does it take? Be ruthless about what you cut; talk to your user.\n","permalink":"https://www.jemoka.com/posts/kbhtask_estimation/","tags":null,"title":"Task Estimation"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhteddy_roosevelt/","tags":null,"title":"Teddy Roosevelt"},{"categories":null,"contents":"Given what you claim as a normal distribution, we can test for its normality. Any distribution you claim as normal has to follow that:\n\\begin{equation} np \\geq 10 \u0026amp; n(1-p) \\geq 10 \\end{equation}\nthat number of successes and failures need both be greater than or equal to ten.\n","permalink":"https://www.jemoka.com/posts/kbhtest_for_normality/","tags":null,"title":"test for normality (statistics)"},{"categories":null,"contents":"How many bugs are in 1,000 lines of code?\nTypical code: 1-10 Platform code: 0.1-1 The best\u0026mdash;NASA: 0.01-0.1 Never assume your software doesn\u0026rsquo;t have bugs.\nTest-Driven Development Test before you build!\nSpecs are already written We know what the expected behavior is We can write tests for the expected behavior first All tests fail to start We know we are done writing code when all tests pass \u0026ldquo;NYI\u0026rdquo; (not-yet implemented)\noften, writing test exposes gaps in your specs How NOT! not write tests Random Sampling Pick one or two inputs and show your code works on it Why it doesn\u0026rsquo;t work: there maybe specific inputs that break your code Exhaustive Testing Test for the domain of inputs Why it doesn\u0026rsquo;t work: tests run forever How DO you write tsets Black-Box Testing Pretend the code implementation is a black box All you know is what the specification; and what the input/output produces White-Box Testing You can see the implementation You test for specific edge cases Off-by-one, running time, specific inputs, etc. Malicious Testing What happens if a user is trying to break your system Sometimes, this is known as \u0026ldquo;pen-testing\u0026rdquo; or \u0026ldquo;white-hack hacking\u0026rdquo; Take CS340 Compsec How BIG are your tests Unit Testing Backbone of testing Typically, that means one test per function Tests choose representative inputs Idempotent: the state of the testing system should be a the beginning and end of the test (tests should revert) (setup + teardown tests) Subsystem Testing Exercise multiple functions working together in a system Often takes longer OK to run these less frequently End-to-End Integration Exercise the entire workflow May involve external libraries, hardware, etc. Regression Testing Isolate the cause of the bug to the smallest possible test case Write a test assuming the bug is fixed Fix the bug Add the test to your test suite How MUCH do we run tests Ideally, run tests every time code is committed Ideally\u0026mdash;run tests that address the function Schedule long tests ","permalink":"https://www.jemoka.com/posts/kbhtesting/","tags":null,"title":"Testing"},{"categories":null,"contents":"The Unreasonable Effectiveness of Mathematics in the Natural Sciences is an article by the famous mathematician Eugene Wigner. (Wigner 1990)\nReflection What I found most peculiarly interesting is the focus on many mathematical/physics texts on the idea of the \u0026ldquo;beauty\u0026rdquo; of the expressions; and, it seems, the clear pleasure that Wigner gets from analyzing the systems with the aforementioned \u0026ldquo;beauty.\u0026rdquo;\nSetting aside whether or not this beauty is \u0026ldquo;deserved\u0026rdquo;/appropriate, I love that my attraction to physics is somewhat similar to what Wigner describes. Under the appropriate conditions, with constraints, it is possible to build a solution to physics problems simply through the evolution of mathematics.\nIt is not to say that the models mathematics provides is correct. I like that Winger ended on the note about how \u0026ldquo;false\u0026rdquo; theories, even despite their falseness, provided shockingly accurate estimations of physical phenomena. Perhaps mathematics provides an almost-fully solid foundation to creating physical systems, but then the entire \u0026ldquo;flaw\u0026rdquo; we see with mathematical modeling is in our (in)ability to provide the limitations to scope.\nFor instance, Bohr\u0026rsquo;s model, an example of \u0026ldquo;falsehood\u0026rdquo; modeled, is an over-limitation to scope which\u0026mdash;thought reducing mathematical complexity\u0026mdash;resulted in a \u0026ldquo;wrong\u0026rdquo; theory. However, the mathematics behind the theory remains to be solid despite the scope limitation, making the result work in a reasonable manner (except for the pitfalls).\nThe inherent concern behind this statement, then, is that there is a case where we can build a perfectly reasonable system to model something, but it turns out that the system is correct only in the limited scope which we are used to operating; when suddenly the scope becomes broken, we are so used to the mathematical tools that we have came to rely on that we don\u0026rsquo;t notice their failures.\nI like that this entire point is brought up before our start in DiffEq, perhaps as a \u0026ldquo;with great power comes great responsibility\u0026rdquo; type of caution to us in terms of how our modeling may go awry while at the same time acting as a preview of the usefulness of the principles provided taken as a whole.\nReading notes Maths show up at entirely random places The first point is that mathematical concepts turn up in entirely unexpected connections. Moreover, they often permit an unexpectedly close and accurate description of the phenomena in these connections.\nWondering whether or not the theory is unique due to its applicability He became skeptical concerning the uniqueness of the coordination between keys and doors.\nThat math is really useful, its weird The first point is that the enormous usefulness of mathematics in the natural sciences is something bordering on the mysterious and that there is no rational explanation for it.\nIt also raises the question of how actually unique our theories are given they are all so applicable Second, it is just this uncanny usefulness of mathematical concepts that raises the question of the uniqueness of our physical theories.\nThe goal of mathematics is maximize the space of usefulness The great mathematician fully, almost ruthlessly, exploits the domain of permissible reasoning and skirts the impermissible.\nRegularity is suprising because its\u0026hellip; regularly found, which is unique The second surprising feature is that the regularity which we are discussing is independent of so many conditions which could have an effect on it.\nLaws of Nature are all highly conditional The principal purpose of the preceding discussion is to point out that the laws of nature are all conditional statements and they relate only to a very small part of our knowledge of the world.\nThat maths is just a fallback for \u0026ldquo;beatiful\u0026rdquo; physics happening the connection is that discussed in mathematics simply because he does not know of any other similar connection.\nApart from invarients, we just scope-limit ourselves to get the remaining bits that we need to make stuff work \u0026ldquo;beautifully\u0026rdquo; propose to refer to the observation which these examples illustrate as the empirical law of epistemology. Together with the laws of invariance of physical theories, it is an indispensable foundation of these theories.\n","permalink":"https://www.jemoka.com/posts/kbhthe_unreasonable_effectiveness_of_mathematics_in_the_natural_sciences/","tags":null,"title":"The Unreasonable Effectiveness of Mathematics in the Natural Sciences"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhtherma/","tags":null,"title":"therma"},{"categories":null,"contents":"The theta/alpha ratio is the ratio between two oscillations measurable by an EEG that is shown to be a possible indicator for AD development.\n","permalink":"https://www.jemoka.com/posts/kbhtheta_alpha_ratio/","tags":null,"title":"theta/alpha ratio"},{"categories":null,"contents":"the transformational generative syntax is a linguistical precept proposed by Noam Chomsky which has the interesting conclusion that meaning is supported by structure, rather than the other way around as generative semantics suggests.\nThis means that you can first come up with generic, independent structure to a sentence, then fill in the sentence with meaning.\nFor instance, \u0026ldquo;colorless green ideas sleep furiously\u0026rdquo; is a sentence Noam Chomsky proposes to have perfect structure but failes to be filled with meaning, supporting the transformational generative syntax theory.\nThis supports the Lexicalist Hypothesis, which is the theory that lexicalization transformations are independent of structural transformations. This would therefore support the proof for the existence of semantic primes.\n","permalink":"https://www.jemoka.com/posts/kbhtransformational_generative_syntax/","tags":null,"title":"transformational generative syntax"},{"categories":null,"contents":"A load perpendicular to the long end of a rod. Think of a metal rod lying flat on the ground; a transverse\n","permalink":"https://www.jemoka.com/posts/kbhtransverse_loaod/","tags":null,"title":"transverse load"},{"categories":null,"contents":"Tuning Forks (funing torks!) is a Tuning Fork. You smack it and it goes \u0026ldquo;biiing!\u0026rdquo;\nLet\u0026rsquo;s figure out how it works. For us to be one same page, let\u0026rsquo;s define some vocab:\nVocab \u0026ldquo;Tine\u0026rdquo;: one of the two prongs of the fork A Cursory Explanation Source: here and here. Both are not very scientific but a good first step.\nFrom a very basic perspective, hiting a tuning fork creates a transverse wave on the tine you hit, which vibrates and then compresses the air around it in a longitudinal fashion at a set frequency, which we hear as a sound.\nOk but then this raises the question of why there\u0026rsquo;s two tines. The explanation this website gives is essentially that the actual mechanism of the Tuning Fork is in squishing the air immediately around the fork, so\u0026hellip;\nif the tines are push towards together, it creates a void in the space it just was; this creates a low pressure rarefaction area if the tines snap back apart, it compresses the air creating compression by squishing the air around it And therefore, the air around the funing tork is essentially being played like a two-way slingy. To adjust the pitch of the Tuning Fork, you lengthen or shorten it: longer tuning forks have larger tines, which vibrate more slowly.\nOk but now many, many questions why does smacking one side of the Tuning Fork make both sides vibrate presumably the base is not vibrating; hence, how does the downward-bendy vibration cause perpendicular oscillation (does it?) A Detour on Rigid Body Harmonic Motion Let\u0026rsquo;s talk about Bending. How does this relate to springs/slinkies? read this. A Better Detour on Cantilever Beams Cantilever Beams\nA Detour on the Temperature We are really worried about two different things here.\nMetal expands/contracts based on the temperature Temperature affects speed of sound A Detour on Material Science Why are our Tuning Forks out of tune? Fun, Relevant Factoids About the World The range of human hearing from a youngen is about 20Hz to 20,000Hz. Look into Young\u0026rsquo;s Modulus\nDensity Second overtones: six and a quarter; why?\nprove the equations given in Rossing 1990\nwhy do high frequencies die faster?\nWhy are they FORKS? What\u0026rsquo;s wrong with one prong\nLagrangian Mechanics\nexperiments to do in the end measuring in water measuring questions to ask why no free vibrations just standing? do the various tuning fork modes compose figure out phys1 physics of strings virbrating ","permalink":"https://www.jemoka.com/posts/kbhtuning_forks/","tags":null,"title":"Tuning Fork"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhu1_c/","tags":null,"title":"u1.c"},{"categories":null,"contents":"Questions of Uniqueness and Existance are important elements in Differential Equations.\nHere\u0026rsquo;s a very general form of a differential equations. First, here\u0026rsquo;s the:\nfunction behavior tests Continuity Weakest statement.\nA function is continuous if and only if:\n\\begin{equation} \\lim_{x \\to y} f(x) =f(y) \\end{equation}\nLipschitz Condition Stronger statement.\nThe Lipschitz Condition is a stronger test of Continuity such that:\n\\begin{equation} || F(t,x)-F(t,y)|| \\leq L|| x- y|| \\end{equation}\nfor all \\(t \\in I\\), \\(x,y \\in \\omega\\), with \\(L \\in (0,\\infty)\\) is a Lipschitz Condition in the dependent variable \\(x\\).\nReshaping this into linear one-dimensional function, we have that:\n\\begin{equation} \\left | \\frac{F(t,x)-F(t,y)}{x-y} \\right | \\leq L \u0026lt; \\infty \\end{equation}\nThe important thing here is that its the same \\(L\\) of convergence \\(\\forall t\\). However, \\(L\\) may not be stable\u0026mdash;in can oscillate\nDifferentiable We finally have the strongest statement.\n\\begin{equation} \\lim_{x \\to y} \\frac{f(x)-f(y)}{x-y} = C \\end{equation}\nTo make something Differentiable, it has to not only converge but converge to a constant \\(C\\).\nExistence and Uniqueness Check for differential equation Assume some \\(F:I \\times \\omega \\to \\mathbb{R}^{n}\\) (a function \\(F\\) whose domain is in some space \\(I \\times \\omega\\)) is bounded and continuous and satisfies the Lipschitz Condition, and let \\(x_{0} \\in \\omega\\), then, there exists \\(T_{0} \u0026gt; 0\\) and a unique solution for \\(x(t)\\) that touches \\(x_{0}\\) to the standard First-Order Differential Equation \\(\\dv{x}{t} = F(t,x), x(t_{0}) = t_{0}\\) for some \\(|t-t_{0}| \u0026lt; T_{0}\\).\nTo actually check that \\(F\\) satisfies Lipschitz Condition, we pretty much usually just go and take the partial derivative w.r.t. \\(x\\) (dependent variable, yes its \\(x\\)) of \\(F\\) on \\(x\\), which\u0026mdash;if exists on some bound\u0026mdash;satisfies the Lipschitz condition on that bound.\nProof So we started at:\n\\begin{equation} \\dv{x}{t} = F(t,x), x(t_{0}) = x_{0} \\end{equation}\nWe can separate this expression and integrate:\n\\begin{align} \u0026amp; \\dv{x}{t} = F(t,x) \\\\ \\Rightarrow\\ \u0026amp; \\dd{x} = F(t,x)\\dd{t} \\\\ \\Rightarrow\\ \u0026amp; \\int_{x_{0)}}^{x(t)} \\dd{x} = \\int_{t_{0}}^{t} F(s,x(s)) \\dd{s} \\\\ \\Rightarrow\\ \u0026amp; x(t)-x_{0}= \\int_{t_{0}}^{t} F(s,x(s)) \\dd{s} \\end{align}\nAt this point, if \\(F\\) is seperable, we can then seperate it out by \\(\\dd{t}\\) and taking the right integral. However, we are only interested in existance and uniquness, so we will do something named\u0026hellip;\nPicard Integration Picard Integration is a inductive iteration scheme which leverages the Lipschitz Condition to show that a function integral converges. Begin with the result that all First-Order Differential Equations have shape (after forcibly separating):\n\\begin{equation} x(t)-x_{0}= \\int_{t_{0}}^{t} F(s,x(s)) \\dd{s} \\end{equation}\nWe hope that the inductive sequence:\n\\begin{equation} x_{n+1}(t) = x_{0} + \\int_{t_{0}}^{t} F(s,x_{n}(s)) \\dd{s} \\end{equation}\nconverges to the same result above (that is, the functions \\(x_{n}(s)\\) stop varying and therefore we converge to a solution \\(x(s)\\) to show existance.\nThis is hard!\nHere\u0026rsquo;s a digression/example:\nif we fix a time \\(t=10\\):\nwe hope to say that:\n\\begin{equation} \\lim_{n \\to \\infty } G_{n}(10) = G(10) \\end{equation}\n\\(\\forall \\epsilon \u0026gt; 0\\), \\(\\exists M \u0026lt; \\infty\\), \\(\\forall n\u0026gt;M\\),\n\\begin{equation} |G_{n}(10)-G(10)| \u0026lt; \\epsilon \\end{equation}\nNow, the thing is, for the integral above to converge uniformly, we hope that \\(M\\) stays fixed \\(\\forall t\\) (that all of the domain converges at once after the same under of the iterations.\nTaking the original expression, and applying the following page of algebra to it:\nFinally, we then apply the Lipschitz Condition because our setup is that \\(F\\) satisfies the Lipschitz Condition, we have that:\n\\begin{equation} ||x_{n+1}(t)-x_{n}(t)|| \\leq L\\int_{x_{0}}^{t} ||x_{n}(s)-x_{n-1}(s)||ds \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhuniqueness_and_existance/","tags":null,"title":"Uniqueness and Existance"},{"categories":null,"contents":"A constructor built out of quantum theory which can replicate itself. It is considered a universal computer.\n","permalink":"https://www.jemoka.com/posts/kbhuniversal_quantum_constructor/","tags":null,"title":"universal quantum constructor"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhuniversity_of_georgia/","tags":null,"title":"University of Georgia"},{"categories":null,"contents":" Investment: Paid for 50% of war bonds Production: ships, tanks, airplanes, etc. \u0026mdash; encourages production Conservation: 5% of the world\u0026rsquo;s population production, 50% of the world\u0026rsquo;s manufactured goods \u0026mdash; rationing, grow goods, etc. ","permalink":"https://www.jemoka.com/posts/kbhus_wwii_propaganda/","tags":null,"title":"US WWII Propaganda"},{"categories":null,"contents":"Goal: understand the user.\nFind out\u0026hellip;\nMotivation Context Deeper need? The goal of user interviews is to understand the user even if they know what they want!\nGood User Interviews Make person feel welcome/safe/appreciated\nAsk open-ended \u0026ldquo;questions\u0026rdquo;\nDescribe a time that\u0026hellip; Tell me more about.. Leave space: awkward silences (not too awkward)\nReally listen!; repress the urge to think of what you want to say next\nRepeat statements back to people\nAsk about examples, context, etc.\nA roadmap 1: create a comfortable entry point 2: go wide, deep into more personal and complex questions 3: focus on the problem, not the solution 4: focus on feelings\u0026mdash;feelings matter, how nice matters 5: end with conclusions and statements for what you User Story The user story should contain\u0026hellip;.\nA main character (your user) Character background (motivation) A plot (context) Climax and Resolution Framework describe the user; who are they; what do they like or not like an iStudio classic need statement finish with a description of the emotional impact of using our software ","permalink":"https://www.jemoka.com/posts/kbhuser_interviews/","tags":null,"title":"User Interviews"},{"categories":null,"contents":" Secrets of Silicon Valley - Horowitz Looking for people who have feel for the problem: people need to believe in the problem Team: can people come with execution? people that are good at startups which are usually not good at later stage stuff Buy a startup and kick out the founders This is very typical Team and idea are easy to decouple Vetting problems Lack of market Technically insatiability \u0026ldquo;Unbelievable stupidity\u0026rdquo;: calcium is so cheap Idea goes through many morphs; getting the credit back People wiling to have a meeting? Decoupling value proposition =\u0026gt; iStudio as a service\nRandom Need: Nueva Alumni Network Maybe set up a Nueva alumni network? What could we do to facilitate the Nueva alumni network; extraction of mutual value from the next work.\nNueva alumni as a service.\nInnovation consultants Ideas are no longer valuable, which ideas to peruse is better. \u0026ldquo;helping people along in their relationship with the idea or with each other.\u0026rdquo; Decoupling solution with the customer with the most value.\n","permalink":"https://www.jemoka.com/posts/kbhvc_thing/","tags":null,"title":"vc thing"},{"categories":null,"contents":"A vector is an element of a vector space. They are also called a point.\n","permalink":"https://www.jemoka.com/posts/kbhvector/","tags":null,"title":"vector"},{"categories":null,"contents":"A vector space is an object between a field and a group; it has two ops\u0026mdash;addition and scalar multiplication. Its not quite a field and its more than a group.\nconstituents A set \\(V\\) An addition on \\(V\\) An scalar multiplication on \\(V\\) such that\u0026hellip;\nrequirements commutativity in add.: \\(u+v=v+u\\) associativity in add. and mult.: \\((u+v)+w=u+(v+w)\\); \\((ab)v=a(bv)\\): \\(\\forall u,v,w \\in V\\) and \\(a,b \\in \\mathbb{F}\\) distributivity: goes both ways \\(a(u+v) = au+av\\) AND!! \\((a+b)v=av+bv\\): \\(\\forall a,b \\in \\mathbb{F}\\) and \\(u,v \\in V\\) additive identity: \\(\\exists 0 \\in V: v+0=v \\forall v \\in V\\) additive inverse: \\(\\forall v \\in V, \\exists w \\in V: v+w=0\\) multiplicative identity: \\(1v=v \\forall v \\in V\\) additional information Elements of a vector space are called vectors or points. vector space \u0026ldquo;over\u0026rdquo; fields Scalar multiplication is not in the set \\(V\\); instead, \u0026ldquo;scalars\u0026rdquo; \\(\\lambda\\) come from this magic faraway land called \\(\\mathbb{F}\\). The choice of \\(\\mathbb{F}\\) for each vector space makes it different; so, when precision is needed, we can say that a vector space is \u0026ldquo;over\u0026rdquo; some \\(\\mathbb{F}\\) which contributes its scalars.\nTherefore:\nA vector space over \\(\\mathbb{R}\\) is called a real vector space A vector space over \\(\\mathbb{C}\\) is called a real vector space ","permalink":"https://www.jemoka.com/posts/kbhvector_space/","tags":null,"title":"vector space"},{"categories":null,"contents":"this is worse ","permalink":"https://www.jemoka.com/posts/kbhcraintech/","tags":null,"title":"VFUA"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhvgg/","tags":null,"title":"VGG"},{"categories":null,"contents":"VGGish is VGG, ish. VGGish is a network based on VGG which is pretrained on the audio-feature-extraction task.\n","permalink":"https://www.jemoka.com/posts/kbhvggish/","tags":null,"title":"VGGish"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhvietnam/","tags":null,"title":"Vietnam"},{"categories":null,"contents":"vietnamization is a political position held by Richard Nixon which is characterized by the slow replacement of American troops with Vietnamese ones.\n","permalink":"https://www.jemoka.com/posts/kbhvietnamization/","tags":null,"title":"vietnamization"},{"categories":null,"contents":"DOI: 10.21437/Interspeech.2019-2414\n","permalink":"https://www.jemoka.com/posts/kbhwang_2019/","tags":null,"title":"Wang 2019"},{"categories":null,"contents":"Richard Nixon does not like democratic policies. Therefore, he had 5 operatives break into the DNC. Woodward and Berstein reports on the issue. Nixon rebounds and fires his investigator.\nThen, he released the \u0026ldquo;smoking gun\u0026rdquo; tape with the middle missing\n","permalink":"https://www.jemoka.com/posts/kbhwatergate/","tags":null,"title":"watergate"},{"categories":null,"contents":"A study with the goal of identifying semantic primes.\n","permalink":"https://www.jemoka.com/posts/kbhwhole_metalanguage_study/","tags":null,"title":"whole metalanguage study"},{"categories":null,"contents":"WPA is the largest relief program ever in the Great Depression New Deal, to promote public infrastructure and create artistic murals. It helped unskilled men to carry out public works infrastructure.\nThe project started 5/1935 and dissolved 6/1943.\n","permalink":"https://www.jemoka.com/posts/kbhwpa/","tags":null,"title":"Works Progress Administration"},{"categories":null,"contents":"","permalink":"https://www.jemoka.com/posts/kbhwriting_index/","tags":null,"title":"Writing Index"},{"categories":null,"contents":"Young\u0026rsquo;s Modulus is a mechanical property that measures the stiffness of a solid material.\nIt measures the ratio between mechanical stress \\(\\sigma\\) and the relative resulting strain \\(\\epsilon\\).\nAnd so, very simply:\n\\begin{equation} E = \\frac{\\sigma }{\\epsilon } \\end{equation}\nThinking about this, silly puddy deforms very easily given a little stress, so it would have low Young\u0026rsquo;s Modulus (\\(\\sigma \\ll \\epsilon\\)); and visa versa. https://aapt.scitation.org/doi/10.1119/1.17116?cookieSet=1\n","permalink":"https://www.jemoka.com/posts/kbhyoung_s_modulus/","tags":null,"title":"Young's Modulus"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2020.624488\nOne-Liner Used an ERNIE trained on transcripts for classification; inclusion of pause encoding made results better.\nNovelty Instead of just looking at actual speech content, look at pauses specific as a feature engineering task \\(89.6\\%\\) on the ADReSS Challenge dataset Notable Methods Applied FA with pause encoding with standard .cha semantics (short pauses, medium pauses, long pauses). Shoved all of this into an ERNIE.\nAssay for performance was LOO\nKey Figs Fig 1 This figure motivates the point that subjects with AD says oh and um more often; which prompted Table 1\nTable 1 Subjects with AD says uh a lot more often; no significance level calculations but ok.\nFigure 5 This figure is the result of a LOO study on the proposed model and presumably others before. X axis is the validation accuracy in question, Y is the density by which the score in X appears in an \\(N=35\\) LOO measurement.\nThis figure tells us that either way the ERNIE model is better than state of the art; furthermore, transcripts with pause encoding did better and did it better more of the time; that\u0026rsquo;s where the 89.6% came from.\nNew Concepts Leave-One-Out cross validation Notes Glorious.\n","permalink":"https://www.jemoka.com/posts/kbhyuan_2021/","tags":["ntj"],"title":"Yuan 2021"},{"categories":null,"contents":"A z-test is a hypothesis test for statistical significance between two sample proportions. Before it can be conducted, it must meet the conditions for inference for a z-test.\nconditions for inference (z-test) has to be random has to be reasonably normal (vis a vi test for normality) each sample has to be independent (or 10% rule) use a z-statistic to find p-value Given a sample proportion, calculate the sample proportion standard deviation (given on the formula sheet) Then, divide the difference between measured and null proportions to figure \\(z\\) that is,\n\\begin{equation} z = \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\end{equation}\nLook up the probability of \\(z\\) taking place on a \\(z\\) table. Then, \\(1-z\\) would yield the \\(p\\) vaule.\n","permalink":"https://www.jemoka.com/posts/kbhz_test/","tags":null,"title":"z-test"},{"categories":null,"contents":"\\(0\\) is a list of length \\(n\\) whose coordinates are all zero\nFormally\u0026mdash;\n\\begin{equation} 0 = (0,\\ldots,0) \\end{equation}\n","permalink":"https://www.jemoka.com/posts/kbhzero/","tags":null,"title":"zero"},{"categories":null,"contents":"A zettlekasten is an atomic notetaking system.\n","permalink":"https://www.jemoka.com/posts/kbhzettlekasten/","tags":null,"title":"zettlekasten"},{"categories":null,"contents":"a zettlekasten index is an index in a zettlekasten file format; it keeps track of all lists of notes. Head to Index Index for an index of indexes in this particular zettlekasten.\n","permalink":"https://www.jemoka.com/posts/kbhzettlekasten_index/","tags":null,"title":"zettlekasten index"},{"categories":null,"contents":"DOI: 10.3389/fcomp.2021.624683\nOne-Liner late fusion of multimodal signal on the CTP task using transformers, mobilnet, yamnet, and mockingjay\nNovelty Similar to Martinc 2021 and Shah 2021 but actually used the the current Neural-Network state of the art Used late fusion again after the base model training Proposed that inconsistency in the diagnoses of MMSE scores could be a great contributing factor to multi-task learning performance hindrance Notable Methods Proposed base model for transfer learning from text based on MobileNet (image), YAMNet (audio), Mockingjay (speech) and BERT (text) Data all sourced from recording/transcribing/recognizing CTP task Key Figs Figure 3 and 4 This figure tells us the late fusion architecture used\nTable 2 Pre-training with an existing dataset had (not statistically quantified) improvement against a randomly seeded model.\nTable 3 Concat/Add fusion methods between audio and text provided even better results; confirms Martinc 2021 on newer data\n","permalink":"https://www.jemoka.com/posts/kbhzhu_2021/","tags":["ntj"],"title":"Zhu 2021"}]