<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Neural Networks</title>
<meta name=description content="Neural Network Unit
A real-valued vector as input, each multiplied by some weights, summed, and squashed by some non-linear transform.
\begin{equation}
z = w\cdot x + b
\end{equation}
and then, we will squash this using it as an &ldquo;activation&rdquo;
\begin{equation}
y = \sigmoid(z)
\end{equation}
One common activation is sigmoid. So, one common formulation would be:
\begin{equation}
y = \frac{1}{1+\exp (- (w \cdot x + b))}
\end{equation}
Tanh
\begin{equation}
y(z) = \frac{e^{z} - e^{-z}}{e^{z}+e^{-z}}
\end{equation}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Neural Networks</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#neural-network-unit>Neural Network Unit</a></li><li><a href=#tanh>Tanh</a></li><li><a href=#relu>relu</a></li><li><a href=#multi-layer-networks>multi-layer networks</a></li><li><a href=#feed-forward-network>feed-forward network</a><ul><li><a href=#embeddings>embeddings</a></li></ul></li><li><a href=#training>Training</a><ul><li><a href=#backpropegation--kbhdeep-learning-dot-md><a href=HAHAHUGOSHORTCODE795s8HBHB>backpropegation</a></a></li></ul></li></ul></nav></aside><main><article><div><h2 id=neural-network-unit>Neural Network Unit</h2><p>A real-valued vector as input, each multiplied by some weights, summed, and squashed by some non-linear transform.</p><p>\begin{equation}
z = w\cdot x + b
\end{equation}</p><p>and then, we will squash this using it as an &ldquo;activation&rdquo;</p><p>\begin{equation}
y = \sigmoid(z)
\end{equation}</p><p>One common activation is <a href=/posts/kbhsigmoid/>sigmoid</a>. So, one common formulation would be:</p><p>\begin{equation}
y = \frac{1}{1+\exp (- (w \cdot x + b))}
\end{equation}</p><h2 id=tanh>Tanh</h2><p>\begin{equation}
y(z) = \frac{e^{z} - e^{-z}}{e^{z}+e^{-z}}
\end{equation}</p><p>This causes &ldquo;saturation&rdquo;&mdash;meaning derivatives to be \(0\) at high values</p><h2 id=relu>relu</h2><p>\begin{equation}
y(z) = \max(z,0)
\end{equation}</p><h2 id=multi-layer-networks>multi-layer networks</h2><p>Single computing units can&rsquo;t compute XOR. Consider a perceptron:</p><p>\begin{equation}
w_1x_1 + w_2x_2 + b = 0
\end{equation}</p><p>meaning:</p><p>\begin{equation}
x_2 = \qty(\frac{-w_1}{w_2})x_1 + \qty(\frac{-b}{w_2})
\end{equation}</p><p>meaning, obtain a line that acts as a <strong>decision boundary</strong>&mdash;we obtain 0 if the input is on one side of the line, and 1 if on the other. XOR, unfortunately, does not have a single linear boundary, its not <strong>linearly <a href=/posts/kbhseperable_diffequ/>seperable</a></strong>.</p><p><a href=/posts/kbhlogistic_regression/>logistic regression</a>, for instance, can&rsquo;t compute XOR because it is linear until squashing.</p><h2 id=feed-forward-network>feed-forward network</h2><p>we can think about <a href=/posts/kbhlogistic_regression/>logistic regression</a> as a one layer network, generalizing over <a href=/posts/kbhsigmoid/>sigmoid</a>:</p><p>\begin{equation}
\text{softmax} = \frac{\exp(z_{i})}{\sum_{j=1}^{k} \exp(z_{j})}
\end{equation}</p><p>and a multinomial <a href=/posts/kbhlogistic_regression/>logistic regression</a> which uses the above. This is considered a &ldquo;layer&rdquo; in the <a href=#feed-forward-network>feed-forward network</a>.</p><p>notation:</p><ul><li>\(W^{(j)}\), weight matrix for layer \(j\)</li><li>\(b^{(j)}\), the bias vector for layer \(j\)</li><li>\(g^{(j)}\), the activation function at \(j\)</li><li>and \(z^{(i)}\), the output at \(i\) (before activation function)</li><li>\(a^{(i)}\), the activation at \(i\)</li></ul><p>instead of bias, we sometimes add a dummy node \(a_{0}\), we will force a value \(1\) at \(a_{0}\) and use its weights as bias.</p><h3 id=embeddings>embeddings</h3><p>We use <a href=/posts/kbhranked_information_retrieval/#vector-space-model>vector-space model</a> to feed words into networks: converting each word first into embeddings, then feeding it into the network</p><p>Fix length problems:</p><ol><li>sentence embedding (mean of all the embeddings)</li><li>element wise max of all the word embeddings to create sentence embedding</li><li>use the max length + pad</li></ol><p>For <a href=/posts/kbhnlp/#language-model>Language Model</a>s, we can use a &ldquo;sliding window&rdquo;; that is:</p><p>\begin{equation}
P(w_{t}|w_{1 \dots t-1}) \approx P(w_{t} | w_{t-N+1 \dots t-1})
\end{equation}</p><h2 id=training>Training</h2><p>For every tuple \((x,y)\), we run a forward pass to obtain \(\hat{y}\). Then, we run the network backwards to update the weights.</p><p>A loss function calculates the negative of the probability of the correct labels.</p><h3 id=backpropegation--kbhdeep-learning-dot-md><a href=/posts/kbhdeep_learning/#backpropegation>backpropegation</a></h3><p><a href=/posts/kbhdeep_learning/#backpropegation>backprop</a></p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>