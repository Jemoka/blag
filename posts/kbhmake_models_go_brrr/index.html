<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Make Models Go Brrr: Model Parallel Whisper</title><meta name=description content="Happy Monday friends.
The deliverable of the week was to make the a ASR model for Batchalign. Essentially, most copies of Whisper is pretty bad at Language Sample Analysis (LSA), because they mostly don&rsquo;t work in terms trying to actually capture the things that people doing LSA want to capture (disfluencies, stuttering, etc.). OpenAI even acknowledged in the paper that they filtered out the disfluencies from their gold transcript to prevent Whisper from writing down too much of them."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Make Models Go Brrr: Model Parallel Whisper</h1><span class=tagbox><span class=tag onclick='window.location.href="/tags/fireside"'><span class=hash>#</span>
<span class=tagname>fireside</span></span></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#a-large-language-model>A <strong>Large</strong> Language Model</a></li><li><a href=#one-model-multiple-cards>One Model, Multiple Cards</a></li></ul></nav></aside><main><article><div><p>Happy Monday friends.</p><p>The deliverable of the week was to make the a ASR model for Batchalign. Essentially, most copies of Whisper is pretty bad at Language Sample Analysis (LSA), because they mostly don&rsquo;t work in terms trying to actually capture the things that people doing LSA want to capture (disfluencies, stuttering, etc.). OpenAI even acknowledged in the paper that they filtered out the disfluencies from their gold transcript to prevent Whisper from writing down too much of them.</p><p>And so&mldr; We roll up our sleeves and do it ourselves.</p><h2 id=a-large-language-model>A <strong>Large</strong> Language Model</h2><p>I didn&rsquo;t want to perform Low-Rank Approximation (LoRA) to heavily when training this model. Folks fine tuning <a href=/posts/kbhllama/>LLaMA</a> will note that the preferred parameters were <a href=https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/>essentially asked the user to make the model matricies Rank 8</a>, across the entire model.</p><p>When trying this in earlier experiments, we failed dramatically as the LoRA&rsquo;d model failed to converge when we hit any smaller rank below 10. However, if we tried to, say, do it above 10, I would OOM.</p><p>I will note: its not like we don&rsquo;t have compute. For this project, I fortunately am able to provision any number of V100 32GB as I see reasonable to train this model. Nevertheless, a lovey dovey parameter heavy 1.5 Billion parameter model is still a sight to behold (and cram into one such GPUs).</p><p>Hence, the most important impetus for making this work without aggressive LoRA and degraded performance is some kind of model parallel training scheme.</p><h2 id=one-model-multiple-cards>One Model, Multiple Cards</h2><figure><img src=/ox-hugo/2023-10-23_10-21-34_screenshot.png></figure><figure><img src=/ox-hugo/2023-10-23_10-21-40_screenshot.png></figure><p>Alr then.</p><p>After investigation, <a href=https://deepspeed.readthedocs.io/en/stable/zero3.html>DeepSpeed</a> seemed pretty promising for a few reasons. The third iteration of its algorithm (Zero-3) has three different main offerings:</p><ol><li>Model parameter sharding (sharding the weights of the model across devices)</li><li>Optimizer state sharding</li><li><strong>Model/Parameter state offload</strong></li></ol><p>The last one caught my eye. Essentially, as long as your chip has the ability to perform a single forward pass, it can train a model under Zero-3. This is because the system is designed, on request, to offload the weights of your model into CPU or NVMe if you want&mdash;and only pull it into the main device for the actual step of forward/backwards passes.</p><p>The thing about DeepSpeed is that its configured in a very non-consistent way, and once you DeepSpeed onto your training script you can&rsquo;t really go back.</p><p>Apparently <a href=https://github.com/huggingface/accelerate>Huggingface Accelerate</a> integrates with it.</p><p>deepspeed
<a href=https://www.reddit.com/r/Oobabooga/comments/13etobg/using_deepspeed_requires_lots_of_manual_tweaking/>https://www.reddit.com/r/Oobabooga/comments/13etobg/using_deepspeed_requires_lots_of_manual_tweaking/</a></p><p>change that assert statement :point_up:</p><p>also set model on train mode</p><p>fp16 doesn&rsquo;t work if you do your own tensor creation within the train loop. your dataloader must return the tensors you put into the model if you want fp16</p><p>some mysterious problem with accelerator.gather for loss reporting?</p><p><del>never got lora to work</del> you have to call <code>model.train()</code> apparently and not use fp16</p><pre tabindex=0><code class=language-nil data-lang=nil>  File &#34;/jet/home/hliuk/.conda/envs/chat-whisper/lib/python3.10/site-packages/torch/nn/modules/conv.py&#34;, line 309, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: weight should have at least three dimensions
</code></pre><p>get_accelerator().empty_cache()</p><p>Never got adam_cpu to complie, so used without zero2 cpu optimizer offload</p><p>&ldquo;q_proj&rdquo;, &ldquo;v_proj&rdquo;, &ldquo;out_proj&rdquo;</p><p>lora saving is hard; if you lora too hard it doesn&rsquo;t converge</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>