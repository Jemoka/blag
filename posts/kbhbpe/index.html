<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Byte-Pair Encoding</title>
<meta name=description content="BPE is a common Subword Tokenization scheme.
Training choose two symbols that are most frequency adjacent merge those two symbols as one symbol throughout the text repeat to step \(1\) until we merge \(k\) times v = set(corpus.characters()) for i in range(k): tl, tr = get_most_common_bigram(v) tnew = f&#34;{tl}{tr}&#34; v.push(tnew) corpus.replace((tl,tr), tnew) return v Most commonly, BPE is not ran alone: it usually run inside space separation systems. Hence, after each word we usually put a special _ token which delineates end of word."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Byte-Pair Encoding</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#training>Training</a></li><li><a href=#inference>Inference</a></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhbpe/>BPE</a> is a common <a href=/posts/kbhtokenization/#subword-tokenization>Subword Tokenization</a> scheme.</p><h2 id=training>Training</h2><ol><li>choose two symbols that are most frequency adjacent</li><li>merge those two symbols as one symbol throughout the text</li><li>repeat to step \(1\) until we merge \(k\) times</li></ol><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>set</span><span style=color:#111>(</span><span style=color:#111>corpus</span><span style=color:#f92672>.</span><span style=color:#111>characters</span><span style=color:#111>())</span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>i</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>k</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>tl</span><span style=color:#111>,</span> <span style=color:#111>tr</span> <span style=color:#f92672>=</span> <span style=color:#111>get_most_common_bigram</span><span style=color:#111>(</span><span style=color:#111>v</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>tnew</span> <span style=color:#f92672>=</span> <span style=color:#d88200>f</span><span style=color:#d88200>&#34;</span><span style=color:#d88200>{</span><span style=color:#111>tl</span><span style=color:#d88200>}{</span><span style=color:#111>tr</span><span style=color:#d88200>}</span><span style=color:#d88200>&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#111>v</span><span style=color:#f92672>.</span><span style=color:#111>push</span><span style=color:#111>(</span><span style=color:#111>tnew</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>corpus</span><span style=color:#f92672>.</span><span style=color:#111>replace</span><span style=color:#111>((</span><span style=color:#111>tl</span><span style=color:#111>,</span><span style=color:#111>tr</span><span style=color:#111>),</span> <span style=color:#111>tnew</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>return</span> <span style=color:#111>v</span>
</span></span></code></pre></div><p>Most commonly, <a href=/posts/kbhbpe/>BPE</a> is not ran alone: it usually run <strong>inside</strong> space separation systems. Hence, after each word we usually put a special <code>_</code> token which delineates end of word.</p><p>Hence: &ldquo;pink fluffy unicorn dancing on rainbows&rdquo; becomes</p><pre tabindex=0><code class=language-nil data-lang=nil>p i n k _ f l u f f y _ u n i c o r n _ d a n c i n g _ o n _ r a i n b o w s
</code></pre><h2 id=inference>Inference</h2><p>During inference time, we apply our stored merges <strong>in the order we learned them</strong>. As in, if we merged <code>er</code> first during training, we should do that first during inference before merging say <code>n er</code>.</p><p>Frequent subwords often ends up being <a href=/posts/kbhmorpheme/>morpheme</a>s.</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>