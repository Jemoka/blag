<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>polynomial interpolation</title>
<meta name=description content="constituents
\(m\) data points \(\qty(x_i,y_{i})\)
requirements
we desire \(c_{j}\) such that:
\begin{equation}
y = c_1 + c_{2} x + c_3 x^{2} + \dots
\end{equation}
Given our set of basis functions \(\phi_{j}(x)\) for input \(x\), our goal is:
\begin{equation}
y = c_1 \phi_{1} + c_2 \phi_{2} + \dots + c_{n}\phi_{n}
\end{equation}
the \(\phi\) are the model function which determines our neural networks.
additional information
Monomial basis and vandermonde Matrix
to do this, we put stuff in matrix form following forms, called the matrix of monomial basis:"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>polynomial interpolation</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#constituents>constituents</a></li><li><a href=#requirements>requirements</a></li><li><a href=#additional-information>additional information</a><ul><li><a href=#monomial-basis-and-vandermonde-matrix>Monomial basis and vandermonde Matrix</a></li><li><a href=#lagrange-basis>Lagrange Basis</a></li><li><a href=#newton-basis>Newton Basis</a></li><li><a href=#overfitting>overfitting</a></li><li><a href=#problem-with-vandermonde--kbhpolynomial-interpolation-dot-md--martix>problem with <a href=HAHAHUGOSHORTCODE1035s7HBHB>Vandermonde</a> martix</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=constituents>constituents</h2><p>\(m\) data points \(\qty(x_i,y_{i})\)</p><h2 id=requirements>requirements</h2><p>we desire \(c_{j}\) such that:</p><p>\begin{equation}
y = c_1 + c_{2} x + c_3 x^{2} + \dots
\end{equation}</p><p>Given our set of basis functions \(\phi_{j}(x)\) for input \(x\), our goal is:</p><p>\begin{equation}
y = c_1 \phi_{1} + c_2 \phi_{2} + \dots + c_{n}\phi_{n}
\end{equation}</p><p>the \(\phi\) are the <a href>model function</a> which determines our <a href=/posts/kbhdeep_learning/>neural network</a>s.</p><h2 id=additional-information>additional information</h2><h3 id=monomial-basis-and-vandermonde-matrix>Monomial basis and vandermonde Matrix</h3><p>to do this, we put stuff in matrix form following forms, called the matrix of <a href=/posts/kbhpolynomial_interpolation/>monomial basis</a>:</p><p>\begin{equation}
\mqty(1 & x_1 & x_1^{2} \\ 1 & x_2 & x_2^{2} \\ & \dots &) \mqty(c_1 \\ c_2 \\ c_3) = \mqty(y_1 \\ y_2 \\ y_3)
\end{equation}</p><p>inverting this matrix gives us the answer; this is the <a href=/posts/kbhpolynomial_interpolation/>Vandermonde</a> matrix. See <a href=#problem-with-vandermonde--kbhpolynomial-interpolation-dot-md--martix>problem with Vandermonde martix</a></p><h3 id=lagrange-basis>Lagrange Basis</h3><p>\begin{equation}
\phi_{k} \qty(x) = \frac{\prod_{i\neq k}^{} x-x_{i}}{\prod_{i \neq k}^{} x_{k}- x_{i}}
\end{equation}</p><p>notice this gives a \(A\) as an identity, but evaluation time is more expensive because you have to do all the multiplication in sequence.</p><p>This also has no problem of being ill-conditioned unlike the <a href=/posts/kbhpolynomial_interpolation/>Vandermonde</a> matrix.</p><p>However, each term is now quadratic.</p><h3 id=newton-basis>Newton Basis</h3><p>\begin{equation}
\phi_{k} \qty(x) = \prod_{i=1}^{k-1} x - x_{i}
\end{equation}</p><p>the first entry is \(1\), the second entry is quadratic, and so on. This is now only quadratic in one term up to \(k=3\).</p><h3 id=overfitting>overfitting</h3><p>For \(m\) data points, you can draw a unique \(m-1\), polynomial which fits the lines exactly.</p><p><a href=/posts/kbhoverfitting/>overfitting</a> can occur, so we perform <a href=/posts/kbhregularization/>regularization</a></p><p>We do this as long as they are not <strong>degenerate</strong>: your points can&rsquo;t be on a line.</p><h3 id=problem-with-vandermonde--kbhpolynomial-interpolation-dot-md--martix>problem with <a href=/posts/kbhpolynomial_interpolation/>Vandermonde</a> martix</h3><p>at higher powers, the squared results tend to be more parallel: this is bad because then small parameter adjustments will require humongous parameter values</p><ul><li>and if two columns of the matrix are parallel, our rank would be at most \(n-1\)</li><li>&mldr;meaning we don&rsquo;t span \(\mathbb{R}^{n}\)</li><li>&mldr;we may not have a solution! because some target output in \(\mathbb{R}^{n}\) may not be hit, or could be hit many times by manipulating the parallel vectors</li></ul><p>in general: if any columns become linearly dependent, they maybe combined in infinite number of ways; that is, we want our Vandermode matrix to have full rank.</p><h4 id=near-singular-matrix-problem>near-singular matrix problem</h4><p><strong>importantly</strong>: if its even <em>close</em> to being singular, we will have this problem.</p><ul><li>with limited precision, we will struggle when columns/linear combinations of columns are too close to being parallel</li><li>they may not be computationally invertible</li><li><a href=/posts/kbhrobustness/>condition number</a>s can help judge how close our matrix is about to be non-invertible</li></ul></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>