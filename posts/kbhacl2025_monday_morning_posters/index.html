<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>ACL2025 Monday Morning Posters</title>
<meta name=description content="ACL2025 Zhang: FaithfulRAG: Fact level conflict modeling
Key insight: RAG performance degrades wen model has context and parametric knowledge mismatch, identifying those and use three step iterative method to improve context faithfulness.
ACL2025 Ding: LLM reasoning capability via scalable question synthesis
Key insight: generate free-from questions conditioned only in BOS, then distill and DPO to get a nice question generation dataset and directly fine tune
ACL2025 Hirsch: localized attribution queries in context grounded generation
Key insight: a new task which benchmarks attribution by first resolving coreferences and then performing context attribution"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>ACL2025 Monday Morning Posters</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#acl2025-zhang-faithfulrag-fact-level-conflict-modeling>ACL2025 Zhang: FaithfulRAG: Fact level conflict modeling</a></li><li><a href=#acl2025-ding-llm-reasoning-capability-via-scalable-question-synthesis>ACL2025 Ding: LLM reasoning capability via scalable question synthesis</a></li><li><a href=#acl2025-hirsch-localized-attribution-queries-in-context-grounded-generation>ACL2025 Hirsch: localized attribution queries in context grounded generation</a></li><li><a href=#acl2025-wen-synthetic-data-strategy-on-domain-specific-retrieval>ACL2025 Wen: synthetic data strategy on domain specific retrieval</a></li><li><a href=#acl2025-xu-updating-small-kv-cache-during-longform-generation>ACL2025 Xu: updating small KV cache during longform generation</a></li><li><a href=#acl2025-feher-retrofitting-large-language-models-with-dynamic-tokenization>ACL2025 Feher: retrofitting large language models with dynamic tokenization</a></li><li><a href=#acl2025-wu-batch-speculative-decoding>ACL2025 Wu: batch speculative decoding</a></li><li><a href=#acl2025-kong-segment-level-direct-preference-optimization>ACL2025 Kong: segment level direct preference optimization</a></li><li><a href=#acl2025-he-idimaticity-in-word-representations>ACL2025 He: idimaticity in word representations</a></li><li><a href=#acl2025-fodor-composition-and-sentence-meaning>ACL2025 Fodor: composition and sentence meaning</a></li></ul></nav></aside><main><article><div><h2 id=acl2025-zhang-faithfulrag-fact-level-conflict-modeling>ACL2025 Zhang: FaithfulRAG: Fact level conflict modeling</h2><p>Key insight: RAG performance degrades wen model has context and parametric knowledge mismatch, identifying those and use three step iterative method to improve context faithfulness.</p><h2 id=acl2025-ding-llm-reasoning-capability-via-scalable-question-synthesis>ACL2025 Ding: LLM reasoning capability via scalable question synthesis</h2><p>Key insight: generate free-from questions conditioned only in BOS, then distill and DPO to get a nice question generation dataset and directly fine tune</p><h2 id=acl2025-hirsch-localized-attribution-queries-in-context-grounded-generation>ACL2025 Hirsch: localized attribution queries in context grounded generation</h2><p>Key insight: a new task which benchmarks attribution by first resolving coreferences and then performing context attribution</p><h2 id=acl2025-wen-synthetic-data-strategy-on-domain-specific-retrieval>ACL2025 Wen: synthetic data strategy on domain specific retrieval</h2><p>Key insight: train your models enough to memorize the context of a specific domain and therefore be able to recall better in particular using document based IDs</p><h2 id=acl2025-xu-updating-small-kv-cache-during-longform-generation>ACL2025 Xu: updating small KV cache during longform generation</h2><p>Key insight: evict members of KV cache that has small attention scores whenever you run out</p><h2 id=acl2025-feher-retrofitting-large-language-models-with-dynamic-tokenization>ACL2025 Feher: retrofitting large language models with dynamic tokenization</h2><p>Key insight: we can tokenized data anew each batch and train a adapter</p><h2 id=acl2025-wu-batch-speculative-decoding>ACL2025 Wu: batch speculative decoding</h2><p>Key insight: generate extra draft tokens in speculative decoding, use those to fill up extra space, greedily selected tokens to re-verify</p><h2 id=acl2025-kong-segment-level-direct-preference-optimization>ACL2025 Kong: segment level direct preference optimization</h2><p>Key insight: entire 2 conversation rollout, use LLM as judge to pick out key segment, then tune in key segments. Key insight is that scores are based on entire conversation, but tuning is only on key segments.</p><h2 id=acl2025-he-idimaticity-in-word-representations>ACL2025 He: idimaticity in word representations</h2><p>Key insight: idioms are represented well in models, but if you do random word replacements, they still are represented well. Which means that mostly it is controlling for lexical overlap</p><h2 id=acl2025-fodor-composition-and-sentence-meaning>ACL2025 Fodor: composition and sentence meaning</h2><p>Key insight: if you ever constructed a dataset that&rsquo;s lexically similar but semantically distant similarity metrics don&rsquo;t work well</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>