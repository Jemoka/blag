<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Rollout with Lookahead</title>
<meta name=description content="Ingredients:
\(\mathcal{P}\) problem (states, transitions, etc.) \(\pi\) a Rollout Policy \(d\) depth (how many next states to look into)&mdash;more is more accurate but slower Use the greedy policy at each state by using the Rollout procedure to estimate your value function at any given state.
Rollout Rollout works by hallucinating a trajectory and calculating the reward. Basis of what some nerds call &ldquo;experience replay&rdquo;.
For some state, Rollout Policy, and depth&mldr;"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Rollout with Lookahead</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#rollout>Rollout</a></li><li><a href=#rollout-policy>Rollout Policy</a></li></ul></nav></aside><main><article><div><p>Ingredients:</p><ul><li>\(\mathcal{P}\) problem (states, transitions, etc.)</li><li>\(\pi\) a <a href=#rollout-policy>Rollout Policy</a></li><li>\(d\) depth (how many next states to look into)&mdash;more is more accurate but slower</li></ul><p>Use the <a href=/posts/kbhaction_value_function/#value-function-policy>greedy policy</a> at each state by using the <a href=#rollout>Rollout</a> procedure to estimate your <a href=/posts/kbhaction_value_function/#id-0b1509e0-4d88-44d1-b6fa-fe8e86d200bb-value-function>value function</a> at any given state.</p><h2 id=rollout>Rollout</h2><p><a href=#rollout>Rollout</a> works by hallucinating a trajectory and calculating the reward. Basis of what some nerds call &ldquo;experience replay&rdquo;.</p><p>For some state, <a href=#rollout-policy>Rollout Policy</a>, and depth&mldr;</p><ol><li>let ret be 0; for i in range depth<ol><li>take action following the <a href=#rollout-policy>Rollout Policy</a></li><li>obtain a sample of possible next state (weighted by the action you took, meaning an instantiation of \(s&rsquo; \sim T(\cdot | s,a)\)) and reward \(R(s,a)\) from current state</li><li>ret += gamma^i * r</li></ol></li><li>return ret</li></ol><h2 id=rollout-policy>Rollout Policy</h2><p>A <a href=#rollout-policy>Rollout Policy</a> is a default <a href=/posts/kbhpolicy/>policy</a> used for lookahead. Usually this <a href=/posts/kbhpolicy/>policy</a> should be designed with domain knowledge; if not, we just use a uniform random next steps.</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>