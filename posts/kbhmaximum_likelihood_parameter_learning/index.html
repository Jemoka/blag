<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Maximum Likelihood Parameter Learning</title>
<meta name=description content="&ldquo;We find the parameter that maximizes the likelihood.&rdquo;
We desire \(\theta\) parameter from some data \(D\). To do this, we simply optimize:
\begin{equation} \hat{\theta} = \arg\max_{\theta}P(D|\theta) \end{equation}
that is, we desire some \(\theta\) that maximize the probability of what&rsquo;s observed. Where:
\begin{equation} P(D|\theta) = \prod_{i} P(o_{i}| \theta) \end{equation}
for each \(o_{i} \in D\). This only works, of course, if each \(o_{i} \in D\) is independent from each other, which we often assume so by calling the samples independently and identically distributed."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Maximum Likelihood Parameter Learning</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#example>Example</a></li><li><a href=#generic-maximum-likelihood-estimate>Generic Maximum Likelihood Estimate</a></li><li><a href=#problems-with-maximum-likelihood-parameter-learning--kbhmaximum-likelihood-parameter-learning-dot-md>Problems with <a href=HAHAHUGOSHORTCODE558s6HBHB>Maximum Likelihood Parameter Learning</a></a></li></ul></nav></aside><main><article><div><p>&ldquo;We find the <a href=/posts/kbhparameter/>parameter</a> that maximizes the likelihood.&rdquo;</p><p>We desire \(\theta\) parameter from some data \(D\). To do this, we simply optimize:</p><p>\begin{equation}
\hat{\theta} = \arg\max_{\theta}P(D|\theta)
\end{equation}</p><p>that is, we desire some \(\theta\) that maximize the <a href=/posts/kbhprobability/>probability</a> of what&rsquo;s observed. Where:</p><p>\begin{equation}
P(D|\theta) = \prod_{i} P(o_{i}| \theta)
\end{equation}</p><p>for each \(o_{i} \in D\). This only works, of course, if each \(o_{i} \in D\) is <a href=/posts/kbhprobability/#independence>independent</a> from each other, which we often assume so by calling the samples <a href>independently and identically distributed</a>.</p><p>The summation is a little unwieldy, so we take the logs and apply log laws to turn the multiplication into a summation:</p><p>\begin{equation}
\hat{\theta} = \arg\max_{\theta} \sum_{i} \log P(o_{i}|\theta)
\end{equation}</p><p>&ldquo;add the log probabilities of each of the outcomes you observed happening according to your unoptimized theta, and maximize it&rdquo;</p><h2 id=example>Example</h2><p>Say we want to train a model to predict whether or not a plane will crash. Suppose our network is very simple:</p><p>\(\theta\) represents if there will be an midair collision. Therefore, we have two disconnected nodes:</p><p>\begin{equation}
P(crash) = \theta
\end{equation}</p><p>\begin{equation}
P(safe) = 1-\theta
\end{equation}</p><p>Now, suppose we observed that there was \(m\) flights and \(n\) midair collisions between them. We can then write then:</p><p>\begin{equation}
P(D|\theta) = \theta^{n}(1-\theta)^{m-n}
\end{equation}</p><p>because \(\theta^{n}(1-\theta)^{m-n}\) is the total probability of the data you are given occurring (\(n\) crashes, \(m-n\) non crashing flights).</p><p>Now, we seek to maximise this value&mdash;because the probability of \(P(D)\) occurring should be \(1\) because \(D\) actually occured.</p><p>Its mostly algebra at this point:</p><figure><img src=/ox-hugo/2023-10-05_10-07-18_screenshot.png></figure><p>Steps:</p><ol><li>we first compute the probability of each of the sample happening according to old \(\theta\) to get \(P(D|\theta)\)</li><li>we then take the log of it to make it a summation</li><li>we then try to maximize \(\theta\) to</li></ol><p>What this tells us is&mldr;</p><h2 id=generic-maximum-likelihood-estimate>Generic Maximum Likelihood Estimate</h2><p>Overall, its kind of unsurprising from the <a href=/posts/kbhprobability/#frequentist-definition-of-probability>Frequentist Definition of Probability</a>, but:</p><p>\begin{equation}
\hat{\theta}_{i} = \frac{n_{i}}{\sum_{j=1}^{k} n_{j}}
\end{equation}</p><p>for some observations \(n_{1:k}\).</p><p>and:</p><p>\begin{equation}
\sigma^{2} = \frac{\sum_{}^{} (o_{i} - \hat{u})^{2}}{m}
\end{equation}</p><h2 id=problems-with-maximum-likelihood-parameter-learning--kbhmaximum-likelihood-parameter-learning-dot-md>Problems with <a href=/posts/kbhmaximum_likelihood_parameter_learning/>Maximum Likelihood Parameter Learning</a></h2><p>This requires a lot of data to make work: for instance&mdash;if we don&rsquo;t have any plane crashes observed in \(n\) files, this scheme would say there&rsquo;s no chance of plane crashes. This is not explicitly true.</p><p>Therefore, we use <a href=/posts/kbhbaysian_parameter_learning/>Baysian Parameter Learning</a>.</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>