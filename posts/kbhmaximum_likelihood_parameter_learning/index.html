<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Maximum Likelihood Parameter Learning</title>
<meta name=description content="&ldquo;We find the parameter that maximizes the likelihood.&rdquo;
for each \(X_{j}\), sum what&rsquo;s the log-likelihood of one \(X_{i}\) take derivative w.r.t. \(\theta\) and set to \(0\) solve for \(\theta\) (this maximizes the log-likelihood!)
If your \(\theta\) is a vector of more than \(1\) thing, take the gradient (i.e. partial derivative against each of your variables) of the thing and solve the place where the gradient is identically \(0\) (each slot is \(0\))."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Maximum Likelihood Parameter Learning</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#log-likelihood>log-likelihood</a><ul><li><a href=#argmax-of-log>argmax of log</a></li></ul></li><li><a href=#example>Example</a></li><li><a href=#generic-maximum-likelihood-estimate>Generic Maximum Likelihood Estimate</a></li><li><a href=#problems-with-maximum-likelihood-parameter-learning--kbhmaximum-likelihood-parameter-learning-dot-md>Problems with <a href=HAHAHUGOSHORTCODE575s8HBHB>Maximum Likelihood Parameter Learning</a></a></li></ul></nav></aside><main><article><div><p>&ldquo;We find the <a href=/posts/kbhparameter/>parameter</a> that maximizes the likelihood.&rdquo;</p><ol><li>for each \(X_{j}\), sum<ol><li>what&rsquo;s the <a href=#log-likelihood>log-likelihood</a> of one \(X_{i}\)</li></ol></li><li>take derivative w.r.t. \(\theta\) and set to \(0\)</li><li>solve for \(\theta\)</li></ol><p>(this maximizes the <a href=#log-likelihood>log-likelihood</a>!)</p><p>If your \(\theta\) is a vector of more than \(1\) thing, take the gradient (i.e. partial derivative against each of your variables) of the thing and solve the place where the gradient is identically \(0\) (each slot is \(0\)). That is, we want:</p><p>\begin{equation}
\mqty[\pdv{LL(\theta)}{\theta_{1}} \\ \pdv{LL(\theta)}{\theta_{2}} \\ \pdv{LL(\theta)}{\theta_{3}} \\ \dots] = \mqty[0 \\ 0 \\0]
\end{equation}</p><ul><li></li></ul><hr><p>We desire \(\theta\) parameter from some data \(D\). To do this, we simply optimize:</p><p>\begin{equation}
\hat{\theta} = \arg\max_{\theta}P(D|\theta)
\end{equation}</p><p>, where:</p><p>\begin{equation}
P(D|\theta) = \prod_{i} P(o_{i}| \theta)
\end{equation}</p><p>for each \(o_{i} \in D\). and \(P\) is <a href=/posts/kbhprobability_mass_function/>PMF</a> or <a href=/posts/kbhprobability_distributions/#probability-density-function>PDF</a> given what you are working with.</p><p>That is, we want the parameter \(\theta\) which maximizes the likelyhood of the data. This only works, of course, if each \(o_{i} \in D\) is <a href=/posts/kbhprobability/#independence>independent</a> from each other, which we can assume so by calling the samples from data <a href=/posts/kbhindependently_and_identically_distributed/>IID</a> (because they are independent draws from the underlying distribution.)</p><h2 id=log-likelihood>log-likelihood</h2><p>The summation above is a little unwieldy, so we take the logs and apply log laws to turn the multiplication into a summation:</p><p>\begin{equation}
\hat{\theta} = \arg\max_{\theta} \sum_{i} \log P(o_{i}|\theta)
\end{equation}</p><p>&ldquo;add the log probabilities of each of the outcomes you observed happening according to your unoptimized theta, and maximize it&rdquo;</p><h3 id=argmax-of-log>argmax of log</h3><p>This holds because <a href=/posts/kbhlog_laws/>log</a> is monotonic (&ldquo;any larger input to a log will lead to a larger value&rdquo;):</p><p>\begin{equation}
\arg\max_{x} f(x) = \arg\max_{x} \log f(x)
\end{equation}</p><h2 id=example>Example</h2><p>Say we want to train a model to predict whether or not a plane will crash. Suppose our network is very simple:</p><p>\(\theta\) represents if there will be an midair collision. Therefore, we have two disconnected nodes:</p><p>\begin{equation}
P(crash) = \theta
\end{equation}</p><p>\begin{equation}
P(safe) = 1-\theta
\end{equation}</p><p>Now, suppose we observed that there was \(m\) flights and \(n\) midair collisions between them. We can then write then:</p><p>\begin{equation}
P(D|\theta) = \theta^{n}(1-\theta)^{m-n}
\end{equation}</p><p>because \(\theta^{n}(1-\theta)^{m-n}\) is the total probability of the data you are given occurring (\(n\) crashes, \(m-n\) non crashing flights).</p><p>Now, we seek to maximise this value&mdash;because the probability of \(P(D)\) occurring should be \(1\) because \(D\) actually occured.</p><p>Its mostly algebra at this point:</p><figure><img src=/ox-hugo/2023-10-05_10-07-18_screenshot.png></figure><p>Steps:</p><ol><li>we first compute the probability of each of the sample happening according to old \(\theta\) to get \(P(D|\theta)\)</li><li>we then take the log of it to make it a summation</li><li>we then try to maximize \(\theta\) to</li></ol><p>What this tells us is&mldr;</p><h2 id=generic-maximum-likelihood-estimate>Generic Maximum Likelihood Estimate</h2><p>Overall, its kind of unsurprising from the <a href=/posts/kbhprobability/#frequentist-definition-of-probability>Frequentist Definition of Probability</a>, but:</p><p>\begin{equation}
\hat{\theta}_{i} = \frac{n_{i}}{\sum_{j=1}^{k} n_{j}}
\end{equation}</p><p>for some observations \(n_{1:k}\).</p><p>and:</p><p>\begin{equation}
\sigma^{2} = \frac{\sum_{}^{} (o_{i} - \hat{u})^{2}}{m}
\end{equation}</p><h2 id=problems-with-maximum-likelihood-parameter-learning--kbhmaximum-likelihood-parameter-learning-dot-md>Problems with <a href=/posts/kbhmaximum_likelihood_parameter_learning/>Maximum Likelihood Parameter Learning</a></h2><p>This requires a lot of data to make work: for instance&mdash;if we don&rsquo;t have any plane crashes observed in \(n\) files, this scheme would say there&rsquo;s no chance of plane crashes. This is not explicitly true.</p><p>Therefore, we use <a href=/posts/kbhbaysian_parameter_learning/>Baysian Parameter Learning</a>.</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>