<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS224N MAY092024</title>
<meta name=description content="Floating Point
4 bytes
\begin{equation}
(-1)^{B} + e^{E-127} \times \qty(1 + \sum_{i=1}^{23} b_{23-i}2^{-i})
\end{equation}
usually \(E\) is a 8 bytes, and 23 digits of \(b\).
With more \(E\), we will have more range, with more \(b\), we will have more precision.
Mixed Precision Training

copy the model in FP32
Run forward pass in FP16
Scale loss to be large enough to not be rounded away
Compute gradients in FP16
Convert the gradients onto FP32
Scale the gradients down
apply to the model

BFloat16
To not need to scale, we can use a scheme that has less precision but the same amount of dynamic range (i.e. allocate the same \(E\), chop off \(b\)) &mdash;no need to scale, just have more dynamic range."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS224N MAY092024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#floating-point>Floating Point</a></li><li><a href=#mixed-precision-training>Mixed Precision Training</a><ul><li><a href=#bfloat16>BFloat16</a></li></ul></li><li><a href=#distributed-data-parallel>Distributed Data Parallel</a><ul><li><a href=#all-reduce>all-reduce</a></li></ul></li><li><a href=#deepspeed-zero>Deepspeed Zero</a><ul><li><a href=#reduce-scatter>reduce-scatter</a></li><li><a href=#all-gather>all gather</a></li><li><a href=#stage-1>Stage 1</a></li><li><a href=#stage-2>Stage 2</a></li><li><a href=#stage-3>Stage 3</a></li></ul></li><li><a href=#lessons>Lessons</a></li><li><a href=#peft>PEFT</a></li></ul></nav></aside><main><article><div><h2 id=floating-point>Floating Point</h2><p>4 bytes</p><p>\begin{equation}
(-1)^{B} + e^{E-127} \times \qty(1 + \sum_{i=1}^{23} b_{23-i}2^{-i})
\end{equation}</p><p>usually \(E\) is a 8 bytes, and 23 digits of \(b\).</p><p>With more \(E\), we will have more range, with more \(b\), we will have more precision.</p><h2 id=mixed-precision-training>Mixed Precision Training</h2><ol><li>copy the model in FP32</li><li>Run forward pass in FP16</li><li>Scale loss to be large enough to not be rounded away</li><li>Compute gradients in FP16</li><li>Convert the gradients onto FP32</li><li>Scale the gradients down</li><li>apply to the model</li></ol><h3 id=bfloat16>BFloat16</h3><p>To not need to scale, we can use a scheme that has less precision but the same amount of dynamic range (i.e. allocate the same \(E\), chop off \(b\)) &mdash;no need to scale, just have more dynamic range.</p><h2 id=distributed-data-parallel>Distributed Data Parallel</h2><ul><li>every GPU has a copy of the model</li><li>each GPU</li></ul><h3 id=all-reduce>all-reduce</h3><p>reduce each copy of the weights down</p><h2 id=deepspeed-zero>Deepspeed Zero</h2><h3 id=reduce-scatter>reduce-scatter</h3><p>squish down and send each part tot the right GPU</p><h3 id=all-gather>all gather</h3><ul><li>send eveything over everybody else</li></ul><h3 id=stage-1>Stage 1</h3><p>We cache a slice of the optimizer state on each GPU.</p><h3 id=stage-2>Stage 2</h3><ul><li>perform a backwards pass</li><li>at each layer, compute gradient</li><li>look up who in the cluster is responsible for that layer</li></ul><h3 id=stage-3>Stage 3</h3><ul><li>divide the model parameters into FSDP units</li><li>shard each unit across multiple GPUs</li><li>run forward pass</li><li>run backward pass</li><li>each GPU updates its own shard using the full gradient from earlier</li></ul><p>(unlike stages 1 and 2, you need to stream in your parameters&mdash;more communication overhead!)</p><h2 id=lessons>Lessons</h2><ul><li>always use mix precision training</li><li>always use bfloat16</li></ul><h2 id=peft>PEFT</h2><p>See <a href=/posts/kbhpeft/>PEFT</a></p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>