<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>maximum a posteriori estimate</title>
<meta name=description content="maximum a posteriori estimate is a parameter learning scheme that uses Beta Distribution and Baysian inference to get a distribution of the posterior of the parameter, and return the argmax (i.e. the mode) of the MAP.
This differs from MLE because we are considering a distribution of possible parameters:
\begin{equation}
p\qty (\theta \mid x_1, \dots, x_{n})
\end{equation}

Calculating a MAP posterior, in general:
\begin{equation}
\theta_{MAP} = \arg\max_{\theta} P(\theta|x_1, \dots, x_{n}) = \arg\max_{\theta} \frac{f(x_1, \dots, x_{n} | \theta) g(\theta)}{h(x_1, \dots, x_{n})}
\end{equation}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>maximum a posteriori estimate</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#map-for-bernoulli-and-binomial-p>MAP for Bernoulli and Binomial \(p\)</a></li><li><a href=#map--kbhmaximum-a-posteriori-estimate-dot-md--for-poisson-and-exponential-lambda><a href=HAHAHUGOSHORTCODE771s13HBHB>MAP</a> for Poisson and Exponential \(\lambda\)</a></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhmaximum_a_posteriori_estimate/>maximum a posteriori estimate</a> is a <a href=/posts/kbhparameter_learning/>parameter learning</a> scheme that uses <a href=/posts/kbhbaysian_parameter_learning/#beta-distribution>Beta Distribution</a> and <a href=/posts/kbhbaysian_network/>Baysian inference</a> to get a distribution of the posterior of the parameter, and return the <a href=/posts/kbhargmax/>argmax</a> (i.e. the mode) of the <a href=/posts/kbhmaximum_a_posteriori_estimate/>MAP</a>.</p><p>This differs from <a href=/posts/kbhmaximum_likelihood_parameter_learning/>MLE</a> because we are considering a <em>distribution</em> of possible parameters:</p><p>\begin{equation}
p\qty (\theta \mid x_1, \dots, x_{n})
\end{equation}</p><hr><p>Calculating a <a href=/posts/kbhmaximum_a_posteriori_estimate/>MAP</a> posterior, in general:</p><p>\begin{equation}
\theta_{MAP} = \arg\max_{\theta} P(\theta|x_1, \dots, x_{n}) = \arg\max_{\theta} \frac{f(x_1, \dots, x_{n} | \theta) g(\theta)}{h(x_1, \dots, x_{n})}
\end{equation}</p><p>We assume that the data points are <a href=/posts/kbhindependently_and_identically_distributed/>IID</a>, and the fact that the bottom of this is constant, we have:</p><p>\begin{equation}
\theta_{MAP} = \arg\max_{\theta} g(\theta) \prod_{i=1}^{n} f(x_{i}|\theta)
\end{equation}</p><p>Usually, we&rsquo;d like to argmax the log:</p><p>\begin{equation}
\theta_{MAP} = \arg\max_{\theta} \qty(\log (g(\theta)) + \sum_{i=1}^{n} \log(f(x_{i}|\theta)) )
\end{equation}</p><p>where, \(g\) is the probability density of \(\theta\) happening given the <strong><strong>prior</strong></strong> belief, and \(f\) is the <a href=/posts/kbhlikelyhood/>likelyhood</a> of \(x_{i}\) given parameter \(\theta\).</p><p>You will note this is just <a href=/posts/kbhmaximum_likelihood_parameter_learning/>Maximum Likelihood Parameter Learning</a> function, plus the log-probability of the parameter prior.</p><h2 id=map-for-bernoulli-and-binomial-p>MAP for Bernoulli and Binomial \(p\)</h2><p>To estimate \(p\), we use the <a href=/posts/kbhbaysian_parameter_learning/#beta-distribution>Beta Distribution</a>:</p><p>The MODE of the beta, which is the <a href=/posts/kbhmaximum_a_posteriori_estimate/>MAP</a> of such a result:</p><p>\begin{equation}
\frac{\alpha -1 }{\alpha + \beta -2}
\end{equation}</p><p>now, for a Laplace posterior \(Beta(2,2)\), we have:</p><p>\begin{equation}
\frac{n+1}{m+n+2}
\end{equation}</p><h2 id=map--kbhmaximum-a-posteriori-estimate-dot-md--for-poisson-and-exponential-lambda><a href=/posts/kbhmaximum_a_posteriori_estimate/>MAP</a> for Poisson and Exponential \(\lambda\)</h2><p>We use the <a href=#map--kbhmaximum-a-posteriori-estimate-dot-md--for-poisson-and-exponential-lambda>gamma distribution</a> as our prior</p><p>\begin{equation}
\Lambda \sim Gamma(\alpha, \beta)
\end{equation}</p><p>where \(\alpha-1\) is the prior event count, and \(\beta\) is the prior time periods.</p><figure><img src=/ox-hugo/2023-11-15_16-16-40_screenshot.png></figure><hr><p>Let&rsquo;s say you have some data points \(x_1, &mldr;x_{k}\), the posterior from from those resulting events:</p><p>\begin{equation}
Gamma(\alpha + n, \beta+k)
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>