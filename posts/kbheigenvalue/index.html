<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>1-d invariant subspace</title><meta name=description content="eigenvalue is the scalar needed to scale the basis element of a one dimensional invariant subspace of a Linear Map to represent the behavior of the map:
\begin{equation}
Tv = \lambda v
\end{equation}
Note we require \(v \neq 0\) because otherwise all scalars count.
eigenvector is a vector that forms the basis list of length 1 of that 1-D invariant subspace under \(T\).
&ldquo;operators own eigenvalues, eigenvalues own eigenvectors&rdquo;
Why is eigenvalue consistent per eigenvector? Because a linear map has to act on the same way to something&rsquo;s basis as it does to the whole space."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>1-d invariant subspace</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li><li><a href=#constituents>constituents</a></li><li><a href=#requirements>requirements</a></li><li><a href=#additional-information>additional information</a><ul><li><a href=#properties-of-eigenvalue--kbheigenvalue-dot-md--s>properties of <a href=HAHAHUGOSHORTCODE418s21HBHB>eigenvalue</a>s</a></li><li><a href=#list-of-eigenvectors-are-linearly-independent--kbhlinear-independence-dot-md>list of eigenvectors are <a href=HAHAHUGOSHORTCODE418s31HBHB>linearly independent</a></a></li><li><a href=#finding-eigenvalues-with-actual-numbers>finding eigenvalues with actual numbers</a></li><li><a href=#natural-choordinates-of-a-map>natural choordinates of a map</a></li><li><a href=#similar-matrices>similar matrices</a></li><li><a href=#invertable-matricies>invertable matricies</a></li><li><a href=#symmetric-matricies-have-a-real-basis-of-eigenvalues>symmetric matricies have a real basis of eigenvalues</a></li></ul></li></ul></nav></aside><main><article><div><p><a href=/posts/kbheigenvalue/>eigenvalue</a> is the scalar needed to scale the <a href=/posts/kbhbasis/>basis</a> element of a one <a href=/posts/kbhdimension/>dimension</a>al <a href=/posts/kbhinvariant_subspace/>invariant subspace</a> of a <a href=/posts/kbhlinear_map/>Linear Map</a> to represent the behavior of the map:</p><p>\begin{equation}
Tv = \lambda v
\end{equation}</p><p>Note we require \(v \neq 0\) because otherwise all scalars count.</p><p><a href=/posts/kbheigenvalue/>eigenvector</a> is a <a href=/posts/kbhvector/>vector</a> that forms the <a href=/posts/kbhbasis/>basis</a> list of length 1 of that 1-D <a href=/posts/kbhinvariant_subspace/>invariant subspace</a> under \(T\).</p><p>&ldquo;<a href=/posts/kbhoperator/>operator</a>s own <a href=/posts/kbheigenvalue/>eigenvalue</a>s, <a href=/posts/kbheigenvalue/>eigenvalue</a>s own <a href=/posts/kbheigenvalue/>eigenvector</a>s&rdquo;</p><p>Why is <a href=/posts/kbheigenvalue/>eigenvalue</a> consistent per <a href=/posts/kbheigenvalue/>eigenvector</a>? Because a linear map has to act on the same way to something&rsquo;s basis as it does to the whole space.</p><h2 id=motivation>Motivation</h2><p>Take some subspace \(U \subset V\):</p><p>\begin{equation}
U = \{\lambda v\ |\ \lambda \in \mathbb{F}, v \in V\} = span(v)
\end{equation}</p><p>Now, if \(T|_{U}\) is an <a href=/posts/kbhoperator/>operator</a> on \(U\), \(U\) would be an <a href=/posts/kbhinvariant_subspace/>invariant subspace</a> of \(T\) of dimension 1 (its <a href=/posts/kbhbasis/>basis</a> being the list \(\{v\}\)).</p><p>Therefore, for some vector \(v \in U\) (basically like various scalings of \(v\)), \(T\) will always send back to \(U\) so we can represent it yet again with another scalar on \(v\), like \(\lambda v\).</p><p>In this case, then, we can write that:</p><p>\begin{equation}
Tv = \lambda v
\end{equation}</p><p>And then the usual definition of <a href=/posts/kbheigenvalue/>eigenvalue</a>s persist.</p><h2 id=constituents>constituents</h2><ul><li>linear map \(T \in \mathcal{L}(V)\)</li><li>vector \(v \in V\), such that \(v \neq 0\)</li><li>scalar \(\lambda \in \mathbb{F}\)</li></ul><h2 id=requirements>requirements</h2><p>If there exists \(v \in V\) such that \(v\neq 0\) and:</p><p>\begin{equation}
Tv = \lambda v
\end{equation}</p><p>then, \(\lambda\) is called an <a href=/posts/kbheigenvalue/>eigenvalue</a>, and \(v\) the <a href=/posts/kbheigenvalue/>eigenvector</a>.</p><h2 id=additional-information>additional information</h2><h3 id=properties-of-eigenvalue--kbheigenvalue-dot-md--s>properties of <a href=/posts/kbheigenvalue/>eigenvalue</a>s</h3><p>Suppose \(V\) in <a href=/posts/kbhfinite_dimensional_vector_space/>finite-dimensional</a>, \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbb{F}\), then:</p><ol><li>\(\lambda\) is an <a href=/posts/kbheigenvalue/>eigenvalue</a> of \(T\)</li><li>\(T - \lambda I\) is not <a href=/posts/kbhinjectivity/>injective</a></li><li>\(T - \lambda I\) is not <a href=/posts/kbhsurjectivity/>surjective</a></li><li>\(T - \lambda I\) is not <a href=/posts/kbhinvertability/>invertable</a></li></ol><p>Showing one shows all.</p><p>Proof:</p><h4 id=1-implies-2>\(1 \implies 2\)</h4><p>Suppose \(\lambda\) is an <a href=/posts/kbheigenvalue/>eigenvalue</a> of \(T\). Then, we have some \(v \in V\) such that:</p><p>\begin{equation}
Tv = \lambda v
\end{equation}</p><p>Now:</p><p>\begin{align}
&amp;Tv = \lambda v \\
\Rightarrow\ & Tv - \lambda v = 0 \\
\Rightarrow\ & Tv - \lambda Iv = 0 \\
\Rightarrow\ & (T-\lambda I)v = 0
\end{align}</p><p>the last step by \((T+S)v = Tv+Sv\), the property of the vector space of \(\mathcal{L}(V)\) (or any \(\mathcal{L}\)).</p><p>And therefore, \(v \in null\ (T-\lambda I)\), and \(v\neq 0\). And so \(null\ (T-\lambda I) \neq \{0\}\) and so \(T-\lambda I\) is not <a href=/posts/kbhinjectivity/>injective</a>, as desired.</p><p>The reverse of this result shows the opposite direction that \(1 \implies 2\).</p><h4 id=the-others>The others</h4><p>\(I \in \mathcal{L}(V)\), \(T \in \mathcal{L}(V)\), \(\mathcal{L}(V)\) is <a href=/posts/kbhclosed/>closed</a>, so \((T - \lambda I) \in \mathcal{L}(V)\), and so it is an operator. Having 2) implies all other conditions of non-injectivity, non-surjectivity, non-invertiblility by <a href=/posts/kbhoperator/#id-e3ff3c90-e719-4c5b-afc4-efcec3169fb2-injectivity-is-id-1af529ce-e2fb-43a4-8f13-aee1dc743b5f-surjectivity-in-id-4ed27ed5-4edc-4ef4-afd7-9b8e3bcd9b96-finite-dimensional-id-36e84a46-76f1-481e-b031-8ab2f0da0aa8-operator-s>injectivity is surjectivity in finite-dimensional operators</a></p><h3 id=list-of-eigenvectors-are-linearly-independent--kbhlinear-independence-dot-md>list of eigenvectors are <a href=/posts/kbhlinear_independence/>linearly independent</a></h3><p>Let \(T \in \mathcal{L}(V)\), suppose \(\lambda_{j}\) are distinct <a href=/posts/kbheigenvalue/>eigenvalue</a>s of \(T\), and \(v_1, \ldots, v_{m}\) the corresponding <a href=/posts/kbheigenvalue/>eigenvector</a>s, then \(v_1, \ldots, v_{m}\) is <a href=/posts/kbhlinear_independence/>linearly independent</a>.</p><p>proof:</p><p>We will show this by contradiction. Suppose \(v_1, \ldots, v_{m}\) are <a href=/posts/kbhlinear_independence/#linearly-dependent>linearly dependent</a>; then, by the <a href=/posts/kbhlinear_dependence_lemma/>Linear Dependence Lemma</a>, \(\exists v_{j}\) such that:</p><p>\begin{equation}
v_{j} \in span(v_1, \dots, v_{j-1})
\end{equation}</p><p>Meaning:</p><p>\begin{equation}
v_{j} = a_1v_1 + \dots + a_{j-1}v_{j-1}
\end{equation}</p><p>Given the list is a list of <a href=/posts/kbheigenvalue/>eigenvalue</a>s, we can apply \(T\) to both sides to get:</p><p>\begin{equation}
\lambda_{j}v_{j} = a_1\lambda_{1}v_1 + \dots + a_{j-1}\lambda_{j-1}v_{j-1}
\end{equation}</p><p>We can also get another definition for \(\lambda_{j} v_{j}\) by simply multiplying the definition for \(v_{j}\) above by \(\lambda_{j}\):</p><p>\begin{align}
&amp;v_{j} = a_1v_1 + \dots + a_{j-1}v_{j-1}\ \text{from above} \\
\Rightarrow\ & \lambda_{j} v_{j} = a_1\lambda_{j}v_1 + \dots + a_{j-1}\lambda_{j}v_{j-1}
\end{align}</p><p>Now, subtracting our two definitions of \(\lambda_{j} v_{j}\), we get:</p><p>\begin{equation}
0 = a_1 (\lambda_{j} - \lambda_{1})v_{1} + \dots +a_{j-1} (\lambda_{j} - \lambda_{j-1})v_{j-1}
\end{equation}</p><p>Recall now that the eigenvalue list \(\lambda_{j}\) are distinct. This means all \(\lambda_{j} - \lambda_{k \neq j} \neq 0\). No \(v_{j} =0\); so if we choose the smallest positive integer for \(j\), the list before it \(v_1, \dots, v_{j-1}\) is <a href=/posts/kbhlinear_independence/>linearly independent</a> (as no value in that list would satisfy the <a href=/posts/kbhlinear_dependence_lemma/>Linear Dependence Lemma</a>). This makes \(a_{j} =\dots =a_{j-1} = 0\).</p><p>And yet, substituting this back into the expression for \(v_{j}\), we have \(v_{j} = 0\), reaching contradiction. So therefore, the list of <a href=/posts/kbheigenvalue/>eigenvector</a>s are <a href=/posts/kbhlinear_independence/>linearly independent.</a> \(\blacksquare\)</p><h4 id=operators-on-finite-dimensional-v-has-at-most-dim-v-eigenvalue--kbheigenvalue-dot-md--s>operators on finite dimensional V has at most dim V <a href=/posts/kbheigenvalue/>eigenvalue</a>s</h4><p>As a corollary of the above result, suppose \(V\) is finite dimensional; then, each <a href=/posts/kbhoperator/>operator</a> on \(V\) has at most \(dim\ V\) distinct <a href=/posts/kbheigenvalue/>eigenvalue</a>s because their <a href=/posts/kbheigenvalue/>eigenvector</a>s form an <a href=/posts/kbhlinear_independence/>linearly independent</a> list and <a href=/posts/kbhlinear_independence/#length-of-linearly-independent-list-leq-length-of-spanning-list>length of linearly-independent list \(\leq\) length of spanning list</a>.</p><h4 id=eigenspaces-are-disjoint>eigenspaces are disjoint</h4><p>the <a href=/posts/kbheigenspace/>eigenspace</a>s of a <a href=/posts/kbhlinear_map/>Linear Map</a> form a <a href=/posts/kbhdirect_sum/>direct sum</a>:</p><p>proof:</p><p>Corollary of result above. Because <a href=/posts/kbheigenvalue/>eigenvector</a>s (i.e. bases) from distinct <a href=/posts/kbheigenspace/>eigenspace</a>s are <a href=/posts/kbhlinear_independence/>linearly independent</a>. So the only way to write \(0\) is by taking each to \(0\). So by taking the bases all to \(0\), you take the \(0\) vector from each space, which shows that the <a href=/posts/kbheigenspace/>eigenspace</a>s are a <a href=/posts/kbhdirect_sum/>direct sum</a>. \(\blacksquare\)</p><h3 id=finding-eigenvalues-with-actual-numbers>finding eigenvalues with actual numbers</h3><p>\begin{equation}
\lambda_{j} \in Spec(T) \Rightarrow det(\lambda_{j}I-T) = 0
\end{equation}</p><p>The right polynomial \(det(\lambda_{j} I-T) = 0\) is named the &ldquo;characteristic polynomial.&rdquo;</p><h3 id=natural-choordinates-of-a-map>natural choordinates of a map</h3><p>Given the eigenvectors \((x+,y+), (x-,y-)\), we can change coordinates of your matrix into the natural choordinates.</p><p>\begin{equation}
A = \begin{pmatrix}
x+ & x- \\y+ & y-
\end{pmatrix} \begin{pmatrix}
\lambda+ & 0 \\ 0 & \lambda-
\end{pmatrix} \begin{pmatrix}
x+ & x- \\y+ & y-
\end{pmatrix}^{-1}
\end{equation}</p><p>This makes scaling matricides much much easier. If you think about multiplying the above matrix \(n\) times, the inverse and non-inverse cancells out.</p><h3 id=similar-matrices>similar matrices</h3><p>Let \(A,B\) be defined:</p><p>\begin{equation}
A = C B C^{-1}
\end{equation}</p><p>and of course:</p><p>\begin{equation}
B = C^{-1} B C
\end{equation}</p><p>where, \(A,B,C \in \mathcal{L}(V)\)</p><p>\(A, B\) has the same <a href=/posts/kbheigenvalue/>eigenvalue</a>s.</p><h3 id=invertable-matricies>invertable matricies</h3><p>Let \(T \in \mathcal{L}(V)\) be <a href=/posts/kbhinvertability/>invertable</a>. If \(\lambda\) is an <a href=/posts/kbheigenvalue/>eigenvalue</a> of \(T\), then \(\frac{1}{\lambda}\) is an <a href=/posts/kbheigenvalue/>eigenvalue</a> of \(T\). Furthermore, \(T\) and \(T^{-1}\) share <a href=/posts/kbheigenvalue/>eigenvector</a>s with eigenvalues \(\lambda\) and \(\frac{1}{\lambda}\)</p><h3 id=symmetric-matricies-have-a-real-basis-of-eigenvalues>symmetric matricies have a real basis of eigenvalues</h3><p>this falls out of the real <a href=/posts/kbhaxler_7_a/#complex-spectral-theorem>spectral theorem</a>.</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>