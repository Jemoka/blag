<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Martinc 2021</title><meta name=description content="DOI: 10.3389/fnagi.2021.642647
One-Liner Combined bag-of-words on transcript + ADR on audio to various classifiers for AD; ablated BERT&rsquo;s decesion space for attention to make more easy models in the future.
Novelty Pre-processed each of the two modalities before fusing it (late fusion) Archieved \(93.75\%\) accuracy on AD detection The data being forced-aligned and fed with late fusion allows one to see what sounds/words the BERT model was focusing on by just focusing on the attention on the words Notable Methods Used classic cookie theft data bag of words to do ADR but for words multimodality but late fusion with one (hot-swappable) classifier Key Figs How they did it This is how the combined the forced aligned (:tada:) audio and transcript together."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Martinc 2021</h1><span class=tagbox><span class=tag onclick='window.location.href="/tags/ntj"'><span class=hash>#</span>
<span class=tagname>ntj</span></span></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#one-liner>One-Liner</a></li><li><a href=#novelty>Novelty</a></li><li><a href=#notable-methods>Notable Methods</a></li><li><a href=#key-figs>Key Figs</a><ul><li><a href=#how-they-did-it>How they did it</a></li><li><a href=#bertbelation>Bertbelation</a></li></ul></li><li><a href=#new-concepts>New Concepts</a></li></ul></nav></aside><main><article><div><p>DOI: 10.3389/fnagi.2021.642647</p><h2 id=one-liner>One-Liner</h2><p>Combined bag-of-words on transcript + <a href=/posts/kbhactive_data_representation/>ADR</a> on audio to various classifiers for AD; ablated BERT&rsquo;s decesion space for attention to make more easy models in the future.</p><h2 id=novelty>Novelty</h2><ul><li>Pre-processed each of the two modalities before fusing it (<a href=/posts/kbhfusion/#late-fusion>late fusion</a>)</li><li>Archieved \(93.75\%\) accuracy on AD detection</li><li>The data being forced-aligned and fed with <a href=/posts/kbhfusion/#late-fusion>late fusion</a> allows one to see what sounds/words the BERT model was focusing on by just focusing on the attention on the words</li></ul><h2 id=notable-methods>Notable Methods</h2><ul><li>Used classic cookie theft data</li><li>bag of words to do <a href=/posts/kbhactive_data_representation/>ADR</a> but for words</li><li>multimodality but <a href=/posts/kbhfusion/#late-fusion>late fusion</a> with one (hot-swappable) classifier</li></ul><h2 id=key-figs>Key Figs</h2><h3 id=how-they-did-it>How they did it</h3><figure><img src=/ox-hugo/2022-06-24_00-20-26_screenshot.png></figure><p>This is how the combined the forced aligned (:tada:) audio and transcript together.</p><h3 id=bertbelation>Bertbelation</h3><p>Ablated BERT results.</p><figure><img src=/ox-hugo/2022-06-24_00-23-36_screenshot.png></figure><p>The model overall tends to focus on early parts of sentences. y is attention weight, x is position in sentence, blue is TD, red is AD.</p><h2 id=new-concepts>New Concepts</h2><ul><li><a href=/posts/kbhactive_data_representation/>Active Data Representation</a></li></ul></div></article></main><footer><p id=footer>&copy; 2019-2022 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>