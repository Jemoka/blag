<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>raising e to a matrix</title><meta name=description content="Let&rsquo;s compute what \(e^{tA}\) should look like, where \(t\) is some scalar and \(A\) is a diagonalizable matrix. This is a supplement to Second-Order Linear Differential Equations.
Let \(v_1\dots v_{m}\) be the eigenvectors of \(A\). Let \(\lambda_{1}\dots\lambda_{m}\) be the eigenvalues.
Recall that we can therefore diagonalize \(A\) as:
\begin{equation} A = \mqty(v_1& \dots& v_{m})\mqty(\dmat{\lambda_{1}, \dots, \lambda_{m}})\mqty(v_1& \dots& v_{m})^{-1} \end{equation}
read: change of choordinates into the eigenbases, scale by the eigenvalues, then change back to normal choordinates."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>raising e to a matrix</h1><span class=tagbox></span></div><main><article><div><p>Let&rsquo;s compute what \(e^{tA}\) should look like, where \(t\) is some scalar and \(A\) is a diagonalizable matrix. This is a supplement to <a href=/posts/kbhsecond_order_linear_differential_equation/>Second-Order Linear Differential Equations</a>.</p><p>Let \(v_1\dots v_{m}\) be the eigenvectors of \(A\). Let \(\lambda_{1}\dots\lambda_{m}\) be the <a href=/posts/kbheigenvalue/>eigenvalue</a>s.</p><p>Recall that we can therefore diagonalize \(A\) as:</p><p>\begin{equation}
A = \mqty(v_1& \dots& v_{m})\mqty(\dmat{\lambda_{1}, \dots, \lambda_{m}})\mqty(v_1& \dots& v_{m})^{-1}
\end{equation}</p><p>read: change of choordinates into the eigenbases, scale by the eigenvalues, then change back to normal choordinates.</p><p>Now, imagine if we are multiplying \(A\) by itself manymany times; what will that look like?</p><p>\begin{equation}
A^{n} = \mqty(v_1& \dots& v_{m})\mqty(\dmat{\lambda_{1}, \dots, \lambda_{m}})\mqty(v_1& \dots& v_{m})^{-1}\mqty(v_1& \dots& v_{m})\mqty(\dmat{\lambda_{1}, \dots, \lambda_{m}})\mqty(v_1& \dots& v_{m})^{-1} \dots
\end{equation}</p><p>The middle parts, nicely, cancels out! Its a matrix applied to its inverse! So, we get rid of it</p><p>\begin{equation}
A^{n} = \mqty(v_1& \dots& v_{m})\mqty(\dmat{\lambda_{1}, \dots, \lambda_{m}})\mqty(\dmat{\lambda_{1}, \dots, \lambda_{m}})\mqty(v_1& \dots& v_{m})^{-1} \dots
\end{equation}</p><p>Now, we are <a href=/posts/kbhmultiplying/>multiplying</a> diagonal <a href=/posts/kbhmatricies/>matricies</a> against itself! If you work out the mechanics of <a href=/posts/kbhmatricies/>matrix</a> <a href=/posts/kbhmultiplying/>multiplication</a>, you will note that each element simply gets scaled to higher powers (the <a href=/posts/kbhmatricies/>matricies</a> are diagonal!)! So then, we have:</p><p>\begin{equation}
A^{n} = \mqty(v_1& \dots& v_{m})\mqty(\dmat{{\lambda_{1}}^{n}, \dots, {\lambda_{m}}^{n}})\mqty(v_1& \dots& v_{m})^{-1}
\end{equation}</p><p>Nice.</p><p>Recall also the Tayler expasion of \(e^{x}\); we will apply it to to \(e^{tA}\):</p><p>\begin{equation}
e^{tA} = \sum_{k=0}^{\infty} \frac{1}{k!}(tA)^{k} = \sum_{k=0}^{\infty} \frac{t^{k}}{k!}A^{k}
\end{equation}</p><p>Ok. We now apply our definition of \(A^{n}\) derived above:</p><p>\begin{equation}
e^{tA} = \sum_{k=0}^{\infty} \frac{t^{k}}{k!}\mqty(v_1& \dots& v_{m})\mqty(\dmat{{\lambda_{1}}^{k}, \dots, {\lambda_{m}}^{k}})\mqty(v_1& \dots& v_{m})^{-1}
\end{equation}</p><p>See now that \(\mqty(v_1 & \dots &v_{m})\) and its inverse is both constant in the sum, so we take it out:</p><p>\begin{equation}
e^{tA} = \mqty(v_1& \dots& v_{m})\qty(\sum_{k=0}^{\infty}\frac{t^{k}}{k!} \mqty(\dmat{{\lambda_{1}}^{k}, \dots, {\lambda_{m}}^{k}}))\mqty(v_1& \dots& v_{m})^{-1}
\end{equation}</p><p>And now, the actual mechanics of adding a <a href=/posts/kbhmatricies/>matrix</a> is just adding it elementwise, so we will put the summations into the matrix:</p><p>\begin{equation}
e^{tA} = \mqty(v_1& \dots& v_{m})\mqty(\dmat{\sum_{k=0}^{\infty}\frac{t^{k}}{k!} {\lambda_{1}}^{k}, \dots, \sum_{k=0}^{\infty}\frac{t^{k}}{k!} {\lambda_{m}}^{k}})\mqty(v_1& \dots& v_{m})^{-1}
\end{equation}</p><p>Note now that each value in that matrix is just the Tayler expansion of \(e^{k_{\lambda_{j}}}\) (take a moment to pause if this is not immediately obvious; think about what each element in that diagonal matrix look like and what the Tayler polynomial \(e^{x}\) should look like. Perhaps what some arbitrary \(e^{ab}\) should looks like.</p><p>\begin{equation}
e^{tA} = \mqty(v_1& \dots& v_{m})\mqty(\dmat{e^{t\lambda_{1}}, \dots, e^{t\lambda_{m}}})\mqty(v_1& \dots& v_{m})^{-1}
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2022 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>