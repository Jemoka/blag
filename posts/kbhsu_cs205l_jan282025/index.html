<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS205L JAN282025</title>
<meta name=description content="Line Search and Steepest Design
Gram-Schmidt For Matrix Orthogonality
You can use Gram-Schmidt to find matrix orthogonality. In particular, for a series of vectors \(s^{(j)}\) forming a matrix \(A\):
\begin{equation}
s^{(q)} = s^{(q)}- \sum_{q&rsquo;=1}^{q-1} \frac{\langle s^{(q)}, s^{(q&rsquo;)} \rangle_{A}}{\langle s^{(q&rsquo;)}, s^{(q&rsquo;)} \rangle_{A}}s^{(q&rsquo;)}
\end{equation}
for Conjugate Gradient, it works out such that only one such dot products is non-zero, so we can write:
\begin{equation}
s^{(q)} = r^{(q)} + \frac{r^{(q)}\cdot r^{(q)}}{r^{(q-1)}\cdot r^{(q-1)}} s^{(q-1)}
\end{equation}
for residual \(r^{(q)}\), and"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS205L JAN282025</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#gram-schmidt-for-matrix-orthogonality>Gram-Schmidt For Matrix Orthogonality</a></li><li><a href=#conjugate-gradient--kbhsu-cs361-apr092024-dot-md><a href=HAHAHUGOSHORTCODE1494s4HBHB>Conjugate Gradient</a></a></li><li><a href=#local-approximations>Local Approximations</a><ul><li><a href=#well-resolved-functions>well-resolved functions</a></li><li><a href=#worries>worries</a></li><li><a href=#splitting-training-dataset-and-averaging>splitting training dataset and averaging</a></li></ul></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhsu_cs361_apr092024/#line-search>Line Search</a> and <a href=/posts/kbhsu_cs205l_jan232025/#steepest-design>Steepest Design</a></p><h2 id=gram-schmidt-for-matrix-orthogonality>Gram-Schmidt For Matrix Orthogonality</h2><p>You can use <a href=/posts/kbhgram_schmidt/>Gram-Schmidt</a> to find matrix orthogonality. In particular, for a series of vectors \(s^{(j)}\) forming a matrix \(A\):</p><p>\begin{equation}
s^{(q)} = s^{(q)}- \sum_{q&rsquo;=1}^{q-1} \frac{\langle s^{(q)}, s^{(q&rsquo;)} \rangle_{A}}{\langle s^{(q&rsquo;)}, s^{(q&rsquo;)} \rangle_{A}}s^{(q&rsquo;)}
\end{equation}</p><p>for <a href=/posts/kbhsu_cs361_apr092024/#conjugate-gradient>Conjugate Gradient</a>, it works out such that only one such dot products is non-zero, so we can write:</p><p>\begin{equation}
s^{(q)} = r^{(q)} + \frac{r^{(q)}\cdot r^{(q)}}{r^{(q-1)}\cdot r^{(q-1)}} s^{(q-1)}
\end{equation}</p><p>for residual \(r^{(q)}\), and</p><h2 id=conjugate-gradient--kbhsu-cs361-apr092024-dot-md><a href=/posts/kbhsu_cs361_apr092024/#conjugate-gradient>Conjugate Gradient</a></h2><p>For \(Ac = b\), let&rsquo;s write:</p><p>Start with \(s^{(i)} = r^{(i)} = b - Ac^{(i)}\)</p><p>Iteration:</p><ul><li>\(\alpha^{(q)} = \frac{r^{(q)}\cdot r^{(q)}}{\langle s^{(q)}, s^{(q)} \rangle}_{A}\)</li><li>\(c^{(q+1)} = r^{(q)}- \alpha^{(q)} As^{(q)}\)</li><li>\(s^{(q+1)} = r^{(q+1)} + \frac{r^{(q+1)} \cdot r^{(q+1)}}{r^{(q)}\cdot r^{(q)}}\cdot s^{(q)}\) (look! the term thing in the right is the only difference between this and gradient descent, by iteratively subtracting the residual iteratively, we compute in the number of steps equal to distinct eigenvalues)</li></ul><p>If you metric is non-symmetric, none of this applies well and hence you should use other methods.</p><h2 id=local-approximations>Local Approximations</h2><p>Taylor expansion: <a href=/posts/kbhsu_math53_feb122024/#taylor-series>Taylor Series</a></p><h3 id=well-resolved-functions>well-resolved functions</h3><ul><li>Regions of a function with less variations require lower sampling rates</li><li>Regions of a function with more variations require higher sampling rates</li></ul><h3 id=worries>worries</h3><ul><li>sampling constantly doesn&rsquo;t capture function&rsquo;s <strong>well-resolvedness</strong> (amount of variation)</li><li>non-piecewise sampling + Taylor expansion breaks during <strong>discontinuity</strong></li></ul><p>piecewise analysis allows you to fix both of these problems</p><h3 id=splitting-training-dataset-and-averaging>splitting training dataset and averaging</h3><ul><li>split data between two pieces</li><li>train 2 separate neural networks in different pieces (suppose different distributions exist in what you are trying to model)</li><li>inference it separately</li></ul><p>To interpolate: k means cluster the inputs together along some dimension (say color), and then try to make a network for each cluster, then we average weights the based on the distance to each cluster.</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>