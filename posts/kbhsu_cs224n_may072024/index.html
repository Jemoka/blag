<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS224N MAY072024</title><meta name=description content="Benchmark tradeoffs

baseline too high: no one can beat it
baseline too low: no differentiation

Close-ended evaluation

do standard ML (&ldquo;accuracy&rdquo;)
because there&rsquo;s one of a few known answers
types of tasks: SST, IMDP, Yelp; SNLI

Most common multi-task benchmark: SuperGLUE
Difficult

what metrics do you choose?
how to aggregate across metrics (average?)
label statistics
spurious correlations

Open-ended evaluations

long generations with too many correct answers (can&rsquo;t directly apply classic ML)
there are better and worse answers (relative)

Content Overlap Metrics
compare lexical similarity between generated and gold text:"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS224N MAY072024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#benchmark-tradeoffs>Benchmark tradeoffs</a></li><li><a href=#close-ended-evaluation>Close-ended evaluation</a><ul><li><a href=#difficult>Difficult</a></li></ul></li><li><a href=#open-ended-evaluations>Open-ended evaluations</a><ul><li><a href=#content-overlap-metrics>Content Overlap Metrics</a></li><li><a href=#model-based-metrics>Model Based Metrics</a></li><li><a href=#humans>Humans!</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=benchmark-tradeoffs>Benchmark tradeoffs</h2><ul><li>baseline too high: no one can beat it</li><li>baseline too low: no differentiation</li></ul><h2 id=close-ended-evaluation>Close-ended evaluation</h2><ul><li>do standard ML (&ldquo;accuracy&rdquo;)</li><li>because there&rsquo;s one of a few known answers</li><li>types of tasks: SST, IMDP, Yelp; SNLI</li></ul><p>Most common multi-task benchmark: SuperGLUE</p><h3 id=difficult>Difficult</h3><ul><li>what metrics do you choose?</li><li>how to aggregate across metrics (average?)</li><li>label statistics</li><li>spurious correlations</li></ul><h2 id=open-ended-evaluations>Open-ended evaluations</h2><ul><li>long generations with too many correct answers (can&rsquo;t directly apply classic ML)</li><li>there are better and worse answers (relative)</li></ul><h3 id=content-overlap-metrics>Content Overlap Metrics</h3><p>compare lexical similarity between generated and gold text:</p><h4 id=usually-n-gram-overlap-metrics>usually n-gram overlap metrics</h4><p>(BLEU (usually considered a precision metric), ROUGE (usually considered a recall metric), METEOR, CIDEr, etc.)</p><ul><li>doesn&rsquo;t consider semantic relatedness</li><li>but is fast!</li></ul><h4 id=semantic-metrics>Semantic metrics</h4><ul><li>BERTSCORE: get contextual embeddings of a sequence using a Bert, do some contextual smart averaging things</li><li>Word Embeddings: averaging all the embeddings and compare them</li><li>BLEURT: pretrain Bert, continual pretrain a Bert on BLEU, then fine tune on human annotation data</li></ul><h3 id=model-based-metrics>Model Based Metrics</h3><p>AlpacaEval and MT-Bench: asking GPT4 to scoring a particular sample.</p><ul><li>self bias worries</li><li>length normalization</li></ul><h3 id=humans>Humans!</h3><p>automatic evaluations need to compared against what humans could have. &ldquo;ask humans to evaluate some axis (&ldquo;fluency&rdquo;, &ldquo;coherence&rdquo;, etc.)&rdquo;</p><ul><li>slow</li><li>expensive</li><li>inter-annotator disagreement</li><li>intra-annotator (time) disagreement</li><li>not reproducable</li><li>is a measure of precision, not recall</li></ul></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>