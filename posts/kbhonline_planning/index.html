<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>online planning</title><meta name=description content="For elements with large possible future state space, we can&rsquo;t just iterate over all states to get a value function for every state, and THEN go about using the greedy policy to perform actions.
Therefore, we employ a technique called receding horizon planning: planning from the current state upwards to a maximum horizon \(d\), figure out what the best action would be given that information for only this state, and then replan."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>online planning</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#monte-carlo-tree-search>monte-carlo tree search</a><ul><li><a href=#monte-carlo-exploration>monte-carlo exploration</a></li></ul></li><li><a href=#generative-model>generative model</a></li><li><a href=#open-loop-planning-vs-close-loop-planning>open-loop planning vs close-loop planning</a><ul><li><a href=#open-loop-planning--model-predictive-control>open loop planning (&ldquo;model predictive control&rdquo;)</a></li></ul></li></ul></nav></aside><main><article><div><p>For elements with large possible future state space, we can&rsquo;t just iterate over all states to get a <a href=/posts/kbhaction_value_function/>value function</a> for every state, and <strong>THEN</strong> go about using the <a href=/posts/kbhaction_value_function/#value-function-policy>greedy policy</a> to perform actions.</p><p>Therefore, we employ a technique called <strong>receding horizon planning</strong>: planning from the current state upwards to a maximum horizon \(d\), figure out what the best action would be given that information for only this state, and then replan.</p><p>Here are the main methods of doing this:</p><ul><li><a href=/posts/kbhrollout_with_lookahead/>Rollout with Lookahead</a>: for each possible next action, sample a transition-weighted random trajectory, use whatever discounted future reward you got for that as your utility</li><li><a href=/posts/kbhforward_search/>Forward Search</a>: for each possible next action, search through each possible next action until you hit the depth required, calculate the instantaneous reward at that point, and backup until you have recorded the sequence of actions that maybe best, and then return the first action in that sequence</li><li><a href=/posts/kbhbranch_and_bound/>Branch and Bound</a></li></ul><h2 id=monte-carlo-tree-search>monte-carlo tree search</h2><ul><li>\(\mathcal{P}\) problem (states, transitions, etc.)</li><li>\(N\) visit counts</li><li>\(Q\) a q-table: <a href=/posts/kbhaction_value_function/>action-value</a> estimates</li><li>\(d\) depth (how many next states to look into)&mdash;more is more accurate but slower</li><li>\(U\) <a href=/posts/kbhaction_value_function/#value-function--kbhaction-value-function-dot-md>value function estimate</a>; usually a <a href=/posts/kbhrollout_with_lookahead/#rollout-policy>Rollout Policy</a>, estimate at some depth \(d\)</li><li>\(c\) exploration constant</li></ul><p>After simulation; we find the best action for our current state from our q-table.</p><p>Simulation algorithm:</p><ul><li>If we have reached the end of depth, simply return the <a href=/posts/kbhutility_theory/>utility</a> from the <a href=/posts/kbhaction_value_function/#value-function--kbhaction-value-function-dot-md>value function</a> estimate</li><li>For some pair of state, action that we just got, if we haven&rsquo;t seen it, we just return the <a href=/posts/kbhaction_value_function/#value-function--kbhaction-value-function-dot-md>value function</a> estimate</li><li>get a promising action from <a href=#monte-carlo-exploration>monte-carlo exploration</a></li><li>sample a next state and reward based on the action you gotten via a <a href=#generative-model>generative model</a></li><li>simulate</li><li>add to the (state, action) count</li><li>update the q table at (state, action): Q[s,a] += (q-</li></ul><h3 id=monte-carlo-exploration>monte-carlo exploration</h3><p>\begin{equation}
\max_{a} Q(s,a) + c \sqrt{ \frac{\log \sum_{a}N(s,a)}{N(s,a)}}
\end{equation}</p><p>where \(c\) is the exploration factor, and \(N\) is the next steps.</p><p>We want to encourage the exploration of things we haven&rsquo;t tried as much. Note that as \(N(s,a)\) is small, the right term is larger. So, if its also not too bad in terms of \(Q\), we will choose it.</p><p>If \(N(s,a)\) is zero, you return the action. You always want to try something at least once.</p><h2 id=generative-model>generative model</h2><p>we perform a random sample of possible next state (weighted by the action you took, meaning an instantiation of \(s&rsquo; \sim T(\cdot | s,a)\)) and reward \(R(s,a)\) from current state</p><h2 id=open-loop-planning-vs-close-loop-planning>open-loop planning vs close-loop planning</h2><h3 id=open-loop-planning--model-predictive-control>open loop planning (&ldquo;model predictive control&rdquo;)</h3><p>Maximize: \(U(a_1, &mldr;, a_{n})\)</p><p>We want to maximize</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>