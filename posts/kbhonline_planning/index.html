<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>online planning</title>
<meta name=description content="For elements with large possible future state space, we can&rsquo;t just iterate over all states to get a for every state, and THEN go about using the to perform actions.
Therefore, we employ a technique called receding horizon planning: planning from the current state upwards to a maximum horizon \(d\), figure out what the best SINGLE action would be given that information for only this state, and then replan.
Here are the main methods of doing this:"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>online planning</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#additional-information>Additional Information</a><ul><li><a href=#generative-model>generative model</a></li><li><a href=#open-loop-planning-vs-close-loop-planning>open-loop planning vs close-loop planning</a></li></ul></li></ul></nav></aside><main><article><div><p>For elements with large possible future state space, we can&rsquo;t just iterate over all states to get a for every state, and <strong>THEN</strong> go about using the to perform actions.</p><p>Therefore, we employ a technique called <strong>receding horizon planning</strong>: planning from the current state upwards to a maximum horizon \(d\), figure out what the best <strong>SINGLE action</strong> would be given that information for only this state, and then replan.</p><p>Here are the main methods of doing this:</p><ul><li><a href=/posts/kbhrollout_with_lookahead/>Rollout with Lookahead</a>: for each possible next action, sample a transition-weighted random trajectory using some policy, use whatever discounted future reward you got for that as your utility</li><li>: for each possible next action, search through each possible next action until you hit the depth required, calculate the instantaneous reward at that point, and backup until you have recorded the sequence of actions that maybe best, and then return the first action in that sequence</li><li>: same algorithm as , but you bound your search based on the theoretical upper-bound of the q-value</li><li>: same core algorithm as , but instead of calculating a based on the , you sample a set of possible next states and average their future utilities</li><li>: use function to come up with a bunch of possible actions to try, and try them with discounts as you try them</li></ul><h2 id=additional-information>Additional Information</h2><h3 id=generative-model>generative model</h3><p>we perform a random sample of possible next state (weighted by the action you took, meaning an instantiation of \(s&rsquo; \sim T(\cdot | s,a)\)) and reward \(R(s,a)\) from current state</p><h3 id=open-loop-planning-vs-close-loop-planning>open-loop planning vs close-loop planning</h3><h4 id=open-loop-planning>open loop planning</h4><p>Instead of doing all the methods above, which all requires state information of the future, <a href=#open-loop-planning>open loop planning</a> uses an exogenously chosen sequence of actions and tries to simply:</p><p>Maximize: \(U(a_1, &mldr;, a_{n})\)</p><p>where the choice of actions doesn&rsquo;t change regardless of eventual state is.</p><p>For high dimensional systems, where is hard to do closed loop systems, this will work better.</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>