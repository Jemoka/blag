<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>word2vec</title>
<meta name=description content="we will train a classifier on a binary prediction task: &ldquo;is context words \(c_{1:L}\) likely to show up near some target word \(W_0\)?&rdquo;
We estimate the probability that \(w_{0}\) occurs within this window based on the product of the probabilities of the similarity of the embeddings between each context word and the target word.
To turn cosine similarity dot products into probability, we squish the dot product via the sigmoid function."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>word2vec</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#properties>properties</a><ul><li><a href=#window-size>window size</a></li><li><a href=#parallelogram-model>parallelogram model</a></li></ul></li><li><a href=#skip-gram-with-negative-sampling>skip-gram with negative sampling</a></li></ul></nav></aside><main><article><div><p>we will train a classifier on a binary prediction task: &ldquo;is context words \(c_{1:L}\) likely to show up near some target word \(W_0\)?&rdquo;</p><p>We estimate the probability that \(w_{0}\) occurs within this window based on the product of the probabilities of the similarity of the embeddings between each context word and the target word.</p><p>To turn <a href=/posts/kbhranked_information_retrieval/#cosine-similarity>cosine similarity</a> <a href=/posts/kbhdot_product/>dot product</a>s into probability, we squish the <a href=/posts/kbhdot_product/>dot product</a> via the <a href=/posts/kbhsigmoid/>sigmoid</a> function.</p><p>importantly, we don&rsquo;t actually use these results. we simply take the resulting embeddings.</p><h2 id=properties>properties</h2><h3 id=window-size>window size</h3><ul><li><strong>smaller windows</strong>: captures more syntax level information</li><li><strong>large windows</strong>: capture more semantic field information</li></ul><h3 id=parallelogram-model>parallelogram model</h3><p>simple way to solve analogies problems with vector semantics: get the difference between two word vectors, and add it somewhere else to get an analogous transformation.</p><ul><li>only words for frequent words</li><li>small distances</li><li>but not quite for large systems</li></ul><h4 id=allocational-harm>allocational harm</h4><p>embeddings bake in existing biases, which leads to bias in hiring practices, etc.</p><h2 id=skip-gram-with-negative-sampling>skip-gram with negative sampling</h2><p><a href=#skip-gram-with-negative-sampling>skip-gram</a> trains vectors separately for word being used as target and word being used as context.</p><p>the mechanism for training the embedding:</p><ul><li>select some \(k\), which is the multiplier of the negative examples (if \(k=2\), ever one positive example will be matched with 2 negative examples)</li><li>sample a target word, and generate positive samples paired by words in its immediate window</li><li>sample window size times \(k\) negative examples, where the noise words are chosen explicitly as not being near our target word, and weighted based on unigram frequency</li></ul><p>for each paired training sample, we minimize the loss via <a href=/posts/kbhcross_entropy_loss/>cross entropy loss</a>:</p><p>\begin{equation}
L_{CE} = -\qty[ \log (\sigma(c_{pos} \cdot w)) + \sum_{i=1}^{k} \log \sigma\qty(-c_{neg} \cdot w)]
\end{equation}</p><p>recall that:</p><p>\begin{equation}
\pdv{L_{CE}}{w} = \qty[\sigma(c_{pos} \cdot w) -1]c_{pos} + \sum_{i=1}^{k} \qty[\sigma(c_{neg_{i}}\cdot w)]c_{neg_{i}}
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>