<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Exploration and Exploitation</title>
<meta name=description content="You are the president, and you are trying to choose the secretary of state. You can only interview people in sequence, and you have to hire on the spot. There are a known number of candidates. We want to maximize the probability of selecting the best candidate. You are given no priors.
How do we know which candidates we explore, and which candidates we exploit. Roughly 37%
Binary Bandit We are playing with \(n\) binary slot machines."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Exploration and Exploitation</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#binary-bandit>Binary Bandit</a><ul><li><a href=#aside-computing-the-distribution>Aside: Computing the Distribution</a></li><li><a href=#approximate-exploration-strategies>Approximate Exploration Strategies</a></li><li><a href=#optimal-exploration>Optimal Exploration</a></li></ul></li></ul></nav></aside><main><article><div><p>You are the president, and you are trying to choose the secretary of state. You can only interview people in sequence, and you have to hire on the spot. There are a known number of candidates. We want to maximize the probability of selecting the best candidate. You are given no priors.</p><p>How do we know which candidates we explore, and which candidates we exploit. Roughly 37%</p><hr><h2 id=binary-bandit>Binary Bandit</h2><p>We are playing with \(n\) binary slot machines.</p><ol><li>arm \(j\) pays off \(1\) with probability \(\theta_{j}\), and pays of \(0\) otherwise. we do not know $Î¸<sub>j</sub>$s exogenously and have to learn it</li><li>we only have \(h\) pulls in total across all \(n\) slot machines</li></ol><p>As we perform \(k\) pulls, we can keep track of a separate <a href=/posts/kbhbaysian_parameter_learning/#beta-distribution>Beta Distribution</a> representing the probability of success for each of the slot machines.</p><p>If our distribution for a given machine is super flat, perhaps its good to try it even if the mean reward is lower if we have</p><h3 id=aside-computing-the-distribution>Aside: Computing the Distribution</h3><p>We want to compute \(\rho_{a}\):</p><p>\begin{equation}
\rho_{a} = P(win_{a} | w_{a}, l_{a}) = \int_{0}^{1} \theta \times Beta(\theta | w_{a}+1, l_{a}+1) \dd{\theta}
\end{equation}</p><p>where, \(w_{a}\) is the number of wins observed for slot machine \(a\), and \(w_{b}\) is the number observed for slot machine \(b\).</p><p>This is exactly the \(\mathbb{E}[Beta(w_{a}+1, l_{a}+1)] = \frac{w_{a}+1}{(w_{a}+1)+(l_{a}+1)}\)</p><h3 id=approximate-exploration-strategies>Approximate Exploration Strategies</h3><ul><li><a href>Undirected Exploration</a></li><li><a href>Directed Exploration</a></li></ul><h3 id=optimal-exploration>Optimal Exploration</h3><p>Suppose we have offline statistic regarding wins and losses as our state:</p><p>\begin{equation}
w_1, l_{1}, \dots, w_{n}, l_{n}
\end{equation}</p><p>We construct a <a href=/posts/kbhaction_value_function/#id-0b1509e0-4d88-44d1-b6fa-fe8e86d200bb-value-function>value function</a>:</p><p>\begin{equation}
U^{*}([w_1, l_{1}, \dots, w_{n}, l_{n}]) = \max_{a} Q^{*}([w_1, l_{1}, \dots, w_{n}, l_{n}], a)
\end{equation}</p><p>our policy is the <a href=/posts/kbhaction_value_function/>action-value</a> policy:</p><p>\begin{equation}
U^{*}([w_1, l_{1}, \dots, w_{n}, l_{n}]) = \arg\max_{a} Q^{*}([w_1, l_{1}, \dots, w_{n}, l_{n}], a)
\end{equation}</p><p>Now, how do we go about calculating the <a href=/posts/kbhaction_value_function/>action-value</a>:</p><p>\begin{align}
Q ([w_1, l_{1}, \dots, w_{n}, l_{n}], a) =\ & \frac{w_{a}+1}{w_{a}+l_{a}+2} (1 + U^{*}(\dots, w_{a}+1, l_{a}, \dots)) \&+ \qty(1-\frac{w_{a}+1}{w_{a}+l_{a}+2})(1 + U^{*}(\dots, w_{a}, l_{a}+1, \dots))
\end{align}</p><p>&ldquo;the probability of you win&rdquo; (expectation of <a href=/posts/kbhbaysian_parameter_learning/#beta-distribution>Beta Distribution</a>), times the instantaneous reward you win + the utility of you doing that.</p><p>To solve this, note that at time \(t=k\), your \(U^{*}\) is \(0\) because you have nothing to pull anymore. Then, you can back up slowly to get each previous state.</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>