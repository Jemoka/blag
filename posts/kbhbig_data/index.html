<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Big Data</title><meta name=description content="Big Data is a term for datasets large enough that traditional data processing applications are inadequate. i.e. when non-parallel processing is inadequate.
That is: &ldquo;Big Data&rdquo; is when Pandas and SQL is inadequate. To handle big data, its very difficult to sequentially go through and process stuff. To make it work, you usually have to perform parallel processing under the hood.
Rules of Thumb of Datasets 1000 Genomes (AWS, 260TB) CommonCraw - the entire web (On PSC!"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Big Data</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#rules-of-thumb-of-datasets>Rules of Thumb of Datasets</a></li><li><a href=#evolution-of-big-data>Evolution of Big Data</a><ul><li><a href=#good-ol-sql>Good Ol&rsquo; SQL</a></li><li><a href=#kv-stores>KV Stores</a></li><li><a href=#document-stores>Document Stores</a></li><li><a href=#wide-column-stores>Wide Column Stores</a></li><li><a href=#graphs>Graphs!</a></li><li><a href=#and-so>And so:</a></li></ul></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhbig_data/>Big Data</a> is a term for datasets large enough that traditional data processing applications are inadequate. i.e. when non-parallel processing is inadequate.</p><p>That is: &ldquo;<a href=/posts/kbhbig_data/>Big Data</a>&rdquo; is when Pandas and SQL is inadequate. To handle big data, its very difficult to sequentially go through and process stuff. To make it work, you usually have to perform parallel processing under the hood.</p><h2 id=rules-of-thumb-of-datasets>Rules of Thumb of Datasets</h2><ul><li>1000 Genomes (AWS, 260TB)</li><li>CommonCraw - the entire web (On PSC! 300-800 TB)</li><li>GDELT - <a href=https://www.gdeltproject.org/>https://www.gdeltproject.org/</a> a dataset that contains everything that&rsquo;s happening in the world right now in terms of news (small!! 2.5 TB per year; however, there is a LOT of fields: 250 Million fields)</li></ul><h2 id=evolution-of-big-data>Evolution of Big Data</h2><h3 id=good-ol-sql>Good Ol&rsquo; SQL</h3><ol><li>schemas are too set in stone (&ldquo;not a fit for <a href=/posts/kbhsoftware_development_methodologies/#agile>Agile</a> development&rdquo; &mdash; a research scientist)</li><li>SQL sharding, when working correctly, is</li></ol><h3 id=kv-stores>KV Stores</h3><p>And this is why we gave up and made Redis (or Amazon DynamoDB, Riak, Memcached) which keeps only Key/Value information. We just make the key really really complicated to support structures: <code>GET cart:joe:15~4...</code></p><p>But the problem with key-value stores isn&rsquo;t good at indexing at all: if we want like to get all of Joe&rsquo;s cart, you can&rsquo;t just <code>GET cart:joe</code> because you can&rsquo;t compare partial hashes.</p><h3 id=document-stores>Document Stores</h3><p>[And something something mongo&rsquo;s document stores but something its bad about those too but CMU can&rsquo;t do tech and the speakers died]</p><h3 id=wide-column-stores>Wide Column Stores</h3><p>Google BigTable type thing</p><p>Just have a wide column of arbitrary width with no schema:</p><table><thead><tr><th>Cart</th><th>Joe</th><th>15~4</th><th></th></tr></thead><tbody><tr><td>Cart</td><td>Robert</td><td>15~3</td><td>More Things!</td></tr><tr><td></td><td>Chicken</td><td>15~2</td><td></td></tr></tbody></table><p>Etc. No idea how you query this but google does it and CMU&rsquo;s speakers died again good on them.</p><h3 id=graphs>Graphs!</h3><p>Neo4j: don&rsquo;t store triples, and have better schemes for encoding. You can then use nice graph query schemes.</p><ul><li>Hard to visualize</li><li>VERY hard to serialize</li><li>Queries are hard</li></ul><h3 id=and-so>And so:</h3><p>And sooooo. Introducing <a href=/posts/kbhspark/>Spark</a>. We don&rsquo;t want to do parallel programming, we want to use traditional databases, so we just make someone else do it on an adapter and just query boring databases with lots of parallel connections.</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>