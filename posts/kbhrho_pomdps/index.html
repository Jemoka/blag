<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>rho-POMDPs</title>
<meta name=description content="POMDPs to solve Active Sensing Problem: where gathering information is the explicit goal and not a means to do something.
Directly reward the reduction of uncertainty: belief-based reward framework which you can just tack onto the existing solvers. To do this, we want to define some reward directly over the belief space which assigns rewards based on uncertainty reduction:
\begin{equation} r(b,a) = \rho(b,a) \end{equation}
and \(\rho\) likely includes some entropy/information."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>rho-POMDPs</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#and-rho-pomdps-adaption-for-piece-wise-linear-convex--pwlc-kbhrho-pomdps-dot-md---objectives>$ρ$-POMDPs adaption for Piece-Wise Linear Convex (<a href=HAHAHUGOSHORTCODE953s3HBHB>PWLC</a>) Objectives</a></li><li><a href=#non-pwlc--kbhrho-pomdps-dot-md--objectives>non-<a href=HAHAHUGOSHORTCODE953s4HBHB>PWLC</a> objectives</a></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhpartially_observable_markov_decision_process/>POMDP</a>s to solve <a href=/posts/kbhrho_pomdps/>Active Sensing Problem</a>: where <strong>gathering information</strong> is the explicit goal and not a means to do something.</p><p>Directly reward the <strong>reduction of uncertainty</strong>: <a href=/posts/kbhbelief/>belief</a>-based reward framework which you can just tack onto the existing solvers. To do this, we want to define some reward directly over the belief space which assigns rewards based on uncertainty reduction:</p><p>\begin{equation}
r(b,a) = \rho(b,a)
\end{equation}</p><p>and \(\rho\) likely includes some entropy/information.</p><h2 id=and-rho-pomdps-adaption-for-piece-wise-linear-convex--pwlc-kbhrho-pomdps-dot-md---objectives>$ρ$-POMDPs adaption for Piece-Wise Linear Convex (<a href=/posts/kbhrho_pomdps/>PWLC</a>) Objectives</h2><p>\begin{equation}
\rho(b,a) = \max_{\alpha \in \Gamma} \mqty[\sum_{s}^{} ??]
\end{equation}</p><p>We want to use \(R\) extra alpha-vectors to compute the value at a state.</p><h2 id=non-pwlc--kbhrho-pomdps-dot-md--objectives>non-<a href=/posts/kbhrho_pomdps/>PWLC</a> objectives</h2><p>Certain stronger-than <a href=/posts/kbhuniqueness_and_existance/#lipschitz-condition>Lipschitz Condition</a> <a href=/posts/kbhuniqueness_and_existance/#continuity>continuity</a> on \(\rho\), we can use a modified version of the bellman updates.</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>