<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Getting Started with PyTorch</title><meta name=description content="(Py)Torch is a great C++/Python library to construct and train complex neural networks. It has taken over academia over the last few years:
(source)
and is slowly taking over industry. Let&rsquo;s learn about how it works!
The chapters below take you through large chapters in a machine-learning journey. But, to do anything, we need to import some stuff which we will need:
import numpy as np import torch Autograd I believe that anybody learning a new ML framework should learn how its differentiation tools work."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><div style="padding:20px 0 30px"><div id=title><h1>Getting Started with PyTorch</h1></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#autograd>Autograd</a></li></ul></nav></aside><main><article><div><p>(Py)Torch is a great C++/Python library to construct and train complex neural networks. It has taken over academia over the last few years:</p><figure><img src=/ox-hugo/2022-11-12_23-39-31_screenshot.png></figure><p>(<a href=https://paperswithcode.com/trends>source</a>)</p><p>and is slowly taking over industry. Let&rsquo;s learn about how it works!</p><p>The chapters below take you through large chapters in a machine-learning journey. But, to do anything, we need to import some stuff which we will need:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>numpy</span> <span style=color:#00a8c8>as</span> <span style=color:#111>np</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span></code></pre></div><h2 id=autograd>Autograd</h2><p>I believe that anybody learning a new ML framework should learn how its differentiation tools work. Yes, this means that we should first understand how it works with not a giant matrix, but with just two simple variables.</p><p>At the heart of PyTorch is the built-in gradient backpropagation facilities. To demonstrate this, let us create two such variables.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>3.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>var_2</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>4.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>(</span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3., requires_grad=True), tensor(4., requires_grad=True))
</span></span></code></pre></div><p>There is secretly a lot going on here, so let&rsquo;s dive in. First, just to get the stickler out of the way, <code>torch.tensor</code> (used here) is the generic variable creator, <code>torch.Tensor</code> (capital!) initializes a proper tensor&mdash;which you will <strong>never</strong> need.</p><p>What is a <code>tensor</code>? A <code>tensor</code> is simply a very efficient matrix that can updates its own values dynamically but keep the same variable name. The above commands creates two such <code>tensor</code>, both being <code>1x1</code> matrices.</p><p>Note that, for the initial values, I used <em>floats!</em> instead of <em>ints</em>. The above code will crash if you use ints: this is because we want the surface on which the matrix changes value to be smooth to make things like gradient descent to work.</p><p>Lastly, we have an argument <code>requires_grad=True</code>. This argument tells PyTorch to keep track of the gradient of the <code>tensor</code>. For now, understand this as &ldquo;permit PyTorch to change this variable if needed.&rdquo; More on that in a sec.</p><p>Naturally, if we have two tensors, we would love to multiply them!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_mult</span> <span style=color:#f92672>=</span> <span style=color:#111>var_1</span><span style=color:#f92672>*</span><span style=color:#111>var_2</span>
</span></span><span style=display:flex><span><span style=color:#111>var_mult</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>Wouldyalookatthat! Another tensor, with the value \(12\).</p><p>Now. Onto the main event. Backpropagation! The point of backprop is that, by just telling Torch that you want one variable in your network to be some value, it can adjust (&ldquo;proper gate backwards&rdquo;) all other variables which contributed to the value of that variable to make sure that reapplying the same computations would arrive at the new, desired values.</p><p>Let&rsquo;s see a practical example. What if we want <code>var_mult</code> to be&mldr; \(7\)? Recall that there are two variables that contributed to the current value of <code>var_mult</code>: 1) <code>var_1</code> and 2) <code>var_2</code> (because <code>var_mult</code> is just the product of those variables!)</p><p>We don&rsquo;t want to be numpties and go about adjusting <code>var_1</code> and <code>var_2</code> ourselves to get <code>var_mult</code> to be \(7\). That&rsquo;s so lame! We want to let <em>PyTorch</em> do it for us&mdash;through the magic of backpropagation.</p><p>Let&rsquo;s first of all figure out how much we need to adjust <code>var_mult</code> in the first place:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>diff</span> <span style=color:#f92672>=</span> <span style=color:#111>var_mult</span><span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#111>diff</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(9., grad_fn=&lt;SubBackward0&gt;)
</span></span></code></pre></div><p>Apparently, \(12\) is \(5\) away from \(7\). Our goal now is to ask PyTorch to adjust <code>var_1</code> and <code>var_2</code> accordingly to make that difference \(0\) (i.e. to make <code>var_mult</code> not different at all from \(7\).)</p><p>Now, this is the beauty of PyTorch. To do this, we just write&mldr;.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>diff</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>None
</span></span></code></pre></div><p>Woosh! Now, PyTorch has&rsquo;t actually changed anything&mdash;</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_mult</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>but, what it did was add a <code>.grad</code> option to each of our variables for which we told that it could update (i.e. that has a <code>requires_grad</code> option.)</p><p>Let&rsquo;s take a look at those variables:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>(</span><span style=color:#111>var_1</span><span style=color:#f92672>.</span><span style=color:#111>grad</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#f92672>.</span><span style=color:#111>grad</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(4.), tensor(3.))
</span></span></code></pre></div></div></article></main></div></div></body></html>