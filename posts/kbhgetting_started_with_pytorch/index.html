<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Getting Started with PyTorch</title><meta name=description content="(Py)Torch is a great C++/Python library to construct and train complex neural networks. It has taken over academia over the last few years:
(source)
and is slowly taking over industry. Let&rsquo;s learn about how it works!
This document is meant to be read cover-to-cover. It makes NO SENSE unless read like that. I focus on building intuition about why PyTorch works, so we will be writing unorthodox code until the very end where we put all ideas together."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><div style="padding:20px 0 30px"><div id=title><h1>Getting Started with PyTorch</h1></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#autograd>Autograd</a></li><li><a href=#gradient-descent>Gradient Descent</a></li><li><a href=#your-first-neural-network>Your First Neural Network</a><ul><li><a href=#layers>Layers</a></li><li><a href=#an-honest-to-goodness-neural-network>An Honest-to-Goodness Neural Network</a></li></ul></li><li><a href=#putting-it-together>Putting it together</a></li></ul></nav></aside><main><article><div><p>(Py)Torch is a great C++/Python library to construct and train complex neural networks. It has taken over academia over the last few years:</p><figure><img src=/ox-hugo/2022-11-12_23-39-31_screenshot.png></figure><p>(<a href=https://paperswithcode.com/trends>source</a>)</p><p>and is slowly taking over industry. Let&rsquo;s learn about how it works!</p><p><strong><strong>This document is meant to be read cover-to-cover. It makes NO SENSE unless read like that. I focus on building intuition about why PyTorch works, so we will be writing unorthodox code until the very end where we put all ideas together.</strong></strong></p><p>The chapters below take you through large chapters in a machine-learning journey. But, to do anything, we need to import some stuff which we will need:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>numpy</span> <span style=color:#00a8c8>as</span> <span style=color:#111>np</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span></code></pre></div><h2 id=autograd>Autograd</h2><p>I believe that anybody learning a new ML framework should learn how its differentiation tools work. Yes, this means that we should first understand how it works with not a giant matrix, but with just two simple variables.</p><p>At the heart of PyTorch is the built-in gradient backpropagation facilities. To demonstrate this, let us create two such variables.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>3.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>var_2</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>4.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>(</span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3., requires_grad=True), tensor(4., requires_grad=True))
</span></span></code></pre></div><p>There is secretly a lot going on here, so let&rsquo;s dive in. First, just to get the stickler out of the way, <code>torch.tensor</code> (used here) is the generic variable creator, <code>torch.Tensor</code> (capital!) initializes a proper tensor&mdash;which you will <strong>never</strong> need.</p><p>What is a <code>tensor</code>? A <code>tensor</code> is simply a very efficient matrix that can updates its own values dynamically but keep the same variable name. The above commands creates two such <code>tensor</code>, both being <code>1x1</code> matrices.</p><p>Note that, for the initial values, I used <em>floats!</em> instead of <em>ints</em>. The above code will crash if you use ints: this is because we want the surface on which the matrix changes value to be smooth to make things like gradient descent to work.</p><p>Lastly, we have an argument <code>requires_grad=True</code>. This argument tells PyTorch to keep track of the gradient of the <code>tensor</code>. For now, understand this as &ldquo;permit PyTorch to change this variable if needed.&rdquo; More on that in a sec.</p><p>Naturally, if we have two tensors, we would love to multiply them!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_mult</span> <span style=color:#f92672>=</span> <span style=color:#111>var_1</span><span style=color:#f92672>*</span><span style=color:#111>var_2</span>
</span></span><span style=display:flex><span><span style=color:#111>var_mult</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>Wouldyalookatthat! Another tensor, with the value \(12\).</p><p>Now. Onto the main event. Backpropagation! The point of backprop is that, by just telling Torch that you want one variable in your network to be some value, it can adjust (&ldquo;proper gate backwards&rdquo;) all other variables which contributed to the value of that variable to make sure that reapplying the same computations would arrive at the new, desired values.</p><p>Let&rsquo;s see a practical example. What if we want <code>var_mult</code> to be&mldr; \(7\)? Recall that there are two variables that contributed to the current value of <code>var_mult</code>: 1) <code>var_1</code> and 2) <code>var_2</code> (because <code>var_mult</code> is just the product of those variables!)</p><p>We don&rsquo;t want to be numpties and go about adjusting <code>var_1</code> and <code>var_2</code> ourselves to get <code>var_mult</code> to be \(7\). That&rsquo;s so lame! We want to let <em>PyTorch</em> do it for us&mdash;through the magic of backpropagation.</p><p>Let&rsquo;s first of all figure out how much we need to adjust <code>var_mult</code> in the first place:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>diff</span> <span style=color:#f92672>=</span> <span style=color:#111>var_mult</span><span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#111>diff</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(9., grad_fn=&lt;SubBackward0&gt;)
</span></span></code></pre></div><p>Apparently, \(12\) is \(5\) away from \(7\). Our goal now is to ask PyTorch to adjust <code>var_1</code> and <code>var_2</code> accordingly to make that difference \(0\) (i.e. to make <code>var_mult</code> not different at all from \(7\).)</p><p>Now, this is the beauty of PyTorch. To do this, we just write&mldr;.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>diff</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>None
</span></span></code></pre></div><p>Woosh! Now, PyTorch has&rsquo;t actually changed anything&mdash;</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_mult</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>but, what it did was add a <code>.grad</code> option to each of our variables for which we told that it could update (i.e. that has a <code>requires_grad</code> option.)</p><p>Let&rsquo;s take a look at those variables:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>(</span><span style=color:#111>var_1</span><span style=color:#f92672>.</span><span style=color:#111>grad</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#f92672>.</span><span style=color:#111>grad</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(4.), tensor(3.))
</span></span></code></pre></div><hr><p>!!!!!!!!the rest of this document is in construction!!!!!!!</p><p>TODO explain what this means.</p><h2 id=gradient-descent>Gradient Descent</h2><p>Ok but manually applying these gradients is kind of hard and also rather silly. To actually do it slowly and measurably, we use optimizer.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.optim</span> <span style=color:#00a8c8>as</span> <span style=color:#111>optim</span>
</span></span></code></pre></div><p>To start an optimizer, you give it all the variables for which it should keep track of updating.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>SGD</span><span style=color:#111>([</span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#111>],</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span><span style=color:#111>,</span> <span style=color:#111>momentum</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>And then, to update gradients, you just have to:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># IMPORTANT</span>
</span></span><span style=display:flex><span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>What&rsquo;s that <code>zero_grad</code>? That clears the gradients from the variables (after applying them with <code>.step()</code>) so that the next update doesn&rsquo;t influence the current one.</p><h2 id=your-first-neural-network>Your First Neural Network</h2><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span></code></pre></div><h3 id=layers>Layers</h3><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>m</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>m</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span><span style=color:#111>,</span> <span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>size</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>Explain what the \(20, 30\) means.</p><p>Ok one layer is just lame. What if you want a bunch of layers?</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m3</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>40</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># function call syntax! Functions call from rigth to left!</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>m3</span><span style=color:#111>(</span><span style=color:#111>m2</span><span style=color:#111>(</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)))</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span><span style=color:#111>,</span> <span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>size</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>And guess what? If you want to adjust the values here, you would just do:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m3</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>40</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># function call syntax! Functions call from rigth to left!</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>m3</span><span style=color:#111>(</span><span style=color:#111>m2</span><span style=color:#111>(</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)))</span>
</span></span><span style=display:flex><span><span style=color:#111>(</span><span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>()</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>12</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>None
</span></span></code></pre></div><p>But wait! What are the options you give to your optimizer?</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>SGD</span><span style=color:#111>([</span><span style=color:#111>m1</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#111>,</span> <span style=color:#111>m1</span><span style=color:#f92672>.</span><span style=color:#111>bias</span> <span style=color:#f92672>...</span> <span style=color:#f92672>...</span> <span style=color:#111>],</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span><span style=color:#111>,</span> <span style=color:#111>momentum</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>That&rsquo;s a <em>lot of variables!!</em> Each linear layer has a \(m\) and a \(b\) (from \(y=mx+b\) fame), and you will end up with a bajillon one of those! Also, that function call syntax, chaining one layer after another, is so knarly! Can we do better? Yes.</p><h3 id=an-honest-to-goodness-neural-network>An Honest-to-Goodness Neural Network</h3><p>PyTorch makes the <code>module</code> framework to make model creator&rsquo;s lives easier. This is the best practice for creating a neural network.</p><p>Let&rsquo;s replicate the example above with the new <code>module</code> framework:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MyNetwork</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># important: runs early calls to make sure that</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># the module is correct</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we declare our layers. We don&#39;t use them yet.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span><span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span><span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m3</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span><span style=color:#ae81ff>40</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># this is a special function that is called when</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># the module is called</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we want to pass our input through to every layer</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># like we did before, but now more declaritively</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m3</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><p>Explain all of this.</p><p>But now, we essentially built our entire network in own &ldquo;layer&rdquo; (actually we literally did, all =Layer=s are just =torch.Module=s) that does the job of all other layers acting together. To use it, we just:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span> <span style=color:#f92672>=</span> <span style=color:#111>MyNetwork</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># function call syntax! Functions call from rigth to left!</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([[-0.1694,  0.0095,  0.4306,  ...,  0.1580,  0.2644,  0.1509],
</span></span><span style=display:flex><span>        [-0.2346, -0.0269, -0.1191,  ...,  0.0229, -0.0819, -0.1452],
</span></span><span style=display:flex><span>        [-0.4871, -0.2868, -0.2488,  ...,  0.0637,  0.1832,  0.0619],
</span></span><span style=display:flex><span>        ...,
</span></span><span style=display:flex><span>        [-0.1323,  0.2531, -0.1086,  ...,  0.0975,  0.0426, -0.2092],
</span></span><span style=display:flex><span>        [-0.4765,  0.1441, -0.0520,  ...,  0.2364,  0.0253, -0.1914],
</span></span><span style=display:flex><span>        [-0.5044, -0.3263,  0.3102,  ...,  0.1938,  0.1427, -0.0587]],
</span></span><span style=display:flex><span>       grad_fn=&lt;AddmmBackward0&gt;)
</span></span></code></pre></div><p>But wait! What are the options you give to your optimizer? Surely you don&rsquo;t have to pass <code>my_network.m1.weight</code>, <code>my_network.m1.bias</code>, etc. etc. to the optimizer, right?</p><p>You don&rsquo;t. One of the things that the <code>super().__init__()</code> did was to register a special function to your network class that keeps track of everything to optimize for. So now, to ask the optimizer to update the entire network, you just have to write:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>SGD</span><span style=color:#111>(</span><span style=color:#111>my_network</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span><span style=color:#111>,</span> <span style=color:#111>momentum</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>optim</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>SGD (
</span></span><span style=display:flex><span>Parameter Group 0
</span></span><span style=display:flex><span>    dampening: 0
</span></span><span style=display:flex><span>    differentiable: False
</span></span><span style=display:flex><span>    foreach: None
</span></span><span style=display:flex><span>    lr: 0.01
</span></span><span style=display:flex><span>    maximize: False
</span></span><span style=display:flex><span>    momentum: 0.9
</span></span><span style=display:flex><span>    nesterov: False
</span></span><span style=display:flex><span>    weight_decay: 0
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>TODO make students recall original backprop example, backprope and step and zero_grad with this new optim.</p><p>Look! Optimizing an entire network works in the <em>exact same way</em> as optimizing two lone variables.</p><h2 id=putting-it-together>Putting it together</h2><p>TODO</p><ol><li>training loop (zero first, call model, get diff/loss, .backward(), .step())</li><li>best practices</li><li>saving and restoring models</li><li>GPU</li></ol></div></article></main></div></div></body></html>