<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS361 APR042024</title>
<meta name=description content="optimization inequalities cannot be strict
Consider:
\begin{align}
\min_{x}&\ x \\
s.t.\ & x > 1
\end{align}
this has NO SOLUTION. (1,1) wouldn&rsquo;t actually be in feasible set. So, we usually specify optimization without a strict inequality.
So, instead, we write:
\begin{align}
\min_{x}&\ x \\
s.t.\ & x \geq  1
\end{align}
Univariate Conditions
First order Necessary Condition
\begin{equation}
\nabla f(x^{*}) = 0
\end{equation}
Second order necessary condition
\begin{equation}
\nabla^{2}f(x^{*}) \geq 0
\end{equation}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS361 APR042024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#optimization-inequalities-cannot-be-strict>optimization inequalities cannot be strict</a></li><li><a href=#univariate-conditions>Univariate Conditions</a><ul><li><a href=#first-order-necessary-condition>First order Necessary Condition</a></li><li><a href=#second-order-necessary-condition>Second order necessary condition</a></li></ul></li><li><a href=#derivative>Derivative</a></li><li><a href=#directional-derivative>Directional Derivative</a></li><li><a href=#numerical-method>Numerical Method</a><ul><li><a href=#finite-difference-method>Finite-Difference Method</a></li><li><a href=#complex-difference-method>Complex-Difference Method</a></li></ul></li><li><a href=#automatic-differentiation--kbhautomatic-differentiation-dot-md><a href=HAHAHUGOSHORTCODE1365s1HBHB>Automatic Differentiation</a></a></li><li><a href=#bracketing>Bracketing</a><ul><li><a href=#unimodality>Unimodality</a></li><li><a href=#bracketing-procedure>Bracketing Procedure</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=optimization-inequalities-cannot-be-strict>optimization inequalities cannot be strict</h2><p>Consider:</p><p>\begin{align}
\min_{x}&\ x \\
s.t.\ & x > 1
\end{align}</p><p>this has <strong>NO SOLUTION</strong>. (1,1) wouldn&rsquo;t actually be in <a href=/posts/kbhsu_cs361_apr022024/#formal-formulation-of-optimization>feasible set</a>. So, we usually specify optimization without a strict inequality.</p><p>So, instead, we write:</p><p>\begin{align}
\min_{x}&\ x \\
s.t.\ & x \geq 1
\end{align}</p><h2 id=univariate-conditions>Univariate Conditions</h2><h3 id=first-order-necessary-condition>First order Necessary Condition</h3><p>\begin{equation}
\nabla f(x^{*}) = 0
\end{equation}</p><h3 id=second-order-necessary-condition>Second order necessary condition</h3><p>\begin{equation}
\nabla^{2}f(x^{*}) \geq 0
\end{equation}</p><h2 id=derivative>Derivative</h2><p>\begin{equation}
f&rsquo;(x) = \frac{\Delta f(x)}{\Delta x}
\end{equation}</p><p>Or gradient; our convention is that gradients are a <strong>COLUMN</strong> vector&mdash;</p><p>\begin{equation}
\nabla f(x) = \mqty(\pdv{f(x)}{x_1} \\ \pdv{f(x)}{x_2} \\ \dots \\ \pdv{f(x)}{x_{n}})
\end{equation}</p><p>Hessian matrix (2nd order partial); its just this, where columns are the second index and rows are the first index.</p><h2 id=directional-derivative>Directional Derivative</h2><p>\begin{align}
\nabla_{s} f(x) &= \lim_{h \to 0} \frac{f(x+hs) - f(x)}{h} \\
&= \lim_{h \to 0} \frac{f(x+\frac{hs}{2}) - f(x- \frac{hs}{2})}{h}
\end{align}</p><p>i.e. this is &ldquo;derivative along a direction&rdquo;</p><h2 id=numerical-method>Numerical Method</h2><h3 id=finite-difference-method>Finite-Difference Method</h3><p>All of these methods suffer from the fact that \(f(x+h) - f(x)\) cancels out at small values of \(x\) and \(h\), because of <strong>floating point errors</strong>. To fix this, use <a href=#complex-difference-method>Complex-Difference Method</a>.</p><h4 id=forward-difference>Forward Difference</h4><p>Recall the Taylor Series about \(f(x+h)\):</p><p>\begin{equation}
f(x+h) = f(x) + \frac{f&rsquo;(x)}{1} h + \frac{f&rsquo;&rsquo;(x)}{2!} h^{2} + \dots
\end{equation}</p><p>Moving it around to get \(f&rsquo;(x)\) by itself:</p><p>\begin{equation}
f&rsquo;(x)h = f(x+h) - f(x) - \frac{f&rsquo;&rsquo;(x)}{2!} h^{2} - \dots
\end{equation}</p><p>So:</p><p>\begin{equation}
f&rsquo;(x) \approx \frac{f(x+h)-f(x)}{h}
\end{equation}</p><p>where $&mldr;$ errors in the end at \(O(h)\). So:</p><p>\begin{equation}
f&rsquo;(x) = \lim_{h \to 0}\frac{f(x+h)-f(x)}{h}
\end{equation}</p><ul><li><p>Error Analysis</p><p>\(\frac{f&rsquo;&rsquo;(x)}{2!}h + &mldr; h^{n}\), the biggest error term lives with \(h\), so this scheme has asymtotic error at \(O(h)\).</p></li></ul><h4 id=central-difference>Central Difference</h4><p>Slightly different formulation, which gives quadratic error \(O(h^{2})\), because the non-squared \(h\) term cancels out:</p><p>\begin{equation}
f&rsquo;(x)= \lim_{h \to 0}\frac{f\qty(x+\frac{h}{2})-f\qty(x-\frac{h}{2})}{h}
\end{equation}</p><h4 id=backward-difference>Backward Difference</h4><p>Forward difference, backward:</p><p>\begin{equation}
f&rsquo;(x) = \lim_{h \to 0} \frac{f(x)-f(x-h)}{h}
\end{equation}</p><h3 id=complex-difference-method>Complex-Difference Method</h3><p>Consider a Taylor approximation of complex difference:</p><p>\begin{equation}
f(x + ih) = f(x) + ih f&rsquo;(x) - h^{2} \frac{f&rsquo;&rsquo;(x)}{2!} - ih^{3} \frac{f&rsquo;&rsquo;&rsquo;(x)}{3!} + \dots
\end{equation}</p><p>Let&rsquo;s again try to get \(f&rsquo;(x)\) by itself; rearranging and thinking for a bit, we will get every other term on the expression above:</p><p>\begin{equation}
f&rsquo;(x) = \frac{\Im (f(x+ih))}{h} + \dots
\end{equation}</p><p>Where the $&mldr;$ errors is at \(O(h^{2})\)</p><p><strong>NOTICE</strong>: we no longer have the cancellation error because we no longer have subtraction.</p><h2 id=automatic-differentiation--kbhautomatic-differentiation-dot-md><a href=/posts/kbhautomatic_differentiation/>Automatic Differentiation</a></h2><p>See <a href=/posts/kbhautomatic_differentiation/>Automatic Differentiation</a></p><h2 id=bracketing>Bracketing</h2><p>Given a unimodal function, the global minimum is guaranteed to be within \([a,c]\) with \(b \in [a,c]\) if we have that \(f(a) > f(b) &lt; f( c)\).</p><p>So let&rsquo;s find this bracket.</p><h3 id=unimodality>Unimodality</h3><p>A function \(f\) is unimodal if:</p><p>\(\exists\) unique \(x^{*}\) such that \(f\) is monotonically decreasing for \(x \leq x^{*}\) and monotonically increasing for \(x \geq x^{*}\)</p><h3 id=bracketing-procedure>Bracketing Procedure</h3><p>If we don&rsquo;t know anything, we might as well start with \(a=-1, b=0, c=1\).</p><p>One of three things:</p><ul><li>we already satisfy \(f(a) > f(b) &lt; f( c)\), well, we are done</li><li>if our left side \(f(a)\) is too low, we will move \(a\) to the left without moving $c$&mdash;doubling the step size every time until it works</li><li>if our right side is too low to the other thing, move it too, &mldr;</li></ul><h4 id=fibonacci-search>Fibonacci Search</h4><p>Say you wanted to evaluate your sequence a finite number of times to maximally lower the interval for bracketing.</p><ul><li><p>Two Evaluations</p><p>At two evaluations, you should pick two points right down the middle very close together; this will cut your interval in half.</p></li></ul><ul><li><p>\(n\) evaluations</p><p>Evaluate intervals with lengths</p><p>\begin{equation}
F_{n} =
\begin{cases}
1, n\leq 2 \\
F_{n-1} + F_{n-2}
\end{cases}
\end{equation}</p><p>as in; say you are allowed \(n\) evaluations; figure the sequence \(\{F_1, \dots, F_{n}\}\), and then partition your space between \(a\) and \(b\) into \(F_{n}\) slices; evaluate at locations \(\frac{F_{n-1}}{F_{n}}\) and \(1- \frac{F_{n-1}}{F_{n}}\), and lop off the half-line which is to the extrema of the higher point.</p></li></ul><h4 id=golden-section-search>Golden Section Search</h4><p>Because of <a href>Binet&rsquo;s Formula</a>, we can write:</p><p>\begin{equation}
\lim_{n \to \infty} \frac{F_{n-1}}{F_{n}} = \frac{1}{\varphi}
\end{equation}</p><p>the inverse of the the golden ratio. So just shrink intervals by that instead.</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>