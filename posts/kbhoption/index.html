<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Option (MDP)</title>
<meta name=description content="an Option (MDP) represents a high level collection of actions. Big Picture: abstract away your big policy into \(n\) small policies, and value-iterate over expected values of the big policies.
Markov Option
A Markov Option is given by a triple \((I, \pi, \beta)\)

\(I \subset S\), the states from which the option maybe started
\(S \times A\), the MDP during that option
\(\beta(s)\), the probability of the option terminating at state \(s\)

one-step options
You can develop one-shot options, which terminates immediate after one action with underlying probability"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Option (MDP)</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#markov-option>Markov Option</a><ul><li><a href=#one-step-options>one-step options</a></li><li><a href=#option-value-fuction>option value fuction</a></li><li><a href=#semi-markov-decision-process>semi-markov decision process</a></li><li><a href=#intra-option-q-learning>intra option q-learning</a></li></ul></li></ul></nav></aside><main><article><div><p>an <a href=/posts/kbhoption/>Option (MDP)</a> represents a high level collection of actions. Big Picture: abstract away your big policy into \(n\) small policies, and value-iterate over expected values of the big policies.</p><h2 id=markov-option>Markov Option</h2><p>A <a href=#markov-option>Markov Option</a> is given by a triple \((I, \pi, \beta)\)</p><ul><li>\(I \subset S\), the states from which the option maybe started</li><li>\(S \times A\), the MDP during that option</li><li>\(\beta(s)\), the probability of the option terminating at state \(s\)</li></ul><h3 id=one-step-options>one-step options</h3><p>You can develop one-shot options, which terminates immediate after one action with underlying probability</p><ul><li>\(I = \{s:a \in A_{s}\}\)</li><li>\(\pi(s,a) = 1\)</li><li>\(\beta(s) = 1\)</li></ul><h3 id=option-value-fuction>option value fuction</h3><p>\begin{equation}
Q^{\mu}(s,o) = \mathbb{E}\qty[r_{t} + \gamma r_{t+1} + \dots]
\end{equation}</p><p>where \(\mu\) is some option selection process</p><h3 id=semi-markov-decision-process>semi-markov decision process</h3><p>a <a href=#semi-markov-decision-process>semi-markov decision process</a> is a system over a bunch of <a href=/posts/kbhoptions/>option</a>s, with time being a factor in option transitions, but the underlying policies still being <a href=/posts/kbhmarkov_decision_process/>MDP</a>s.</p><p>\begin{equation}
T(s&rsquo;, \tau | s,o)
\end{equation}</p><p>where \(\tau\) is time elapsed.</p><p>because option-level termination induces jumps between large scale states, one backup can propagate to a lot of states.</p><h3 id=intra-option-q-learning>intra option q-learning</h3><p>\begin{equation}
Q_{k+1} (s_{i},o) = (1-\alpha_{k})Q_{k}(S_{t}, o) + \alpha_{k} \qty(r_{t+1} + \gamma U_{k}(s_{t+1}, o))
\end{equation}</p><p>where:</p><p>\begin{equation}
U_{k}(s,o) = (1-\beta(s))Q_{k}(s,o) + \beta(s) \max_{o \in O} Q_{k}(s,o&rsquo;)
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>