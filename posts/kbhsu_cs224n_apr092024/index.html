<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS224N APR092024</title>
<meta name=description content="Neural Networks are powerful because of self organization of the intermediate levels.
Neural Network Layer
\begin{equation}
z = Wx + b
\end{equation}
for the output, and the activations:
\begin{equation}
a = f(z)
\end{equation}
where the activation function \(f\) is applied element-wise.
Why are NNs Non-Linear?

there&rsquo;s no representational power with multiple linear (though, there is better learning/convergence properties even with big linear networks!)
most things are non-linear!

Activation Function
We want non-linear and non-threshold (0/1) activation functions because it has a slope&mdash;meaning we can perform gradient-based learning."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS224N APR092024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#neural-network-layer>Neural Network Layer</a></li><li><a href=#why-are-nns-non-linear>Why are NNs Non-Linear?</a></li><li><a href=#activation-function>Activation Function</a><ul><li><a href=#sigmoid--kbhsigmoid-dot-md><a href=HAHAHUGOSHORTCODE1320s0HBHB>sigmoid</a></a></li><li><a href=#tanh>tanh</a></li><li><a href=#hard-tanh>hard tanh</a></li><li><a href=#relu--kbhneural-networks-dot-md><a href=HAHAHUGOSHORTCODE1320s2HBHB>relu</a></a></li><li><a href=#leaky-relu>Leaky ReLU</a></li></ul></li><li><a href=#vectorized-calculus>Vectorized Calculus</a><ul><li><a href=#transposes>Transposes</a></li><li><a href=#useful-jacobians>Useful Jacobians!</a></li><li><a href=#shape-convention>Shape Convention</a></li></ul></li><li><a href=#actual-backprop>Actual Backprop</a></li><li><a href=#check-gradient>Check Gradient</a></li></ul></nav></aside><main><article><div><p>Neural Networks are powerful because of <strong>self organization</strong> of the intermediate levels.</p><h2 id=neural-network-layer>Neural Network Layer</h2><p>\begin{equation}
z = Wx + b
\end{equation}</p><p>for the output, and the activations:</p><p>\begin{equation}
a = f(z)
\end{equation}</p><p>where the activation function \(f\) is applied element-wise.</p><h2 id=why-are-nns-non-linear>Why are NNs Non-Linear?</h2><ol><li>there&rsquo;s no representational power with multiple linear (though, there is better learning/convergence properties even with big linear networks!)</li><li>most things are non-linear!</li></ol><h2 id=activation-function>Activation Function</h2><p>We want <strong>non-linear</strong> and <strong>non-threshold</strong> (0/1) activation functions because it has a <em>slope</em>&mdash;meaning we can perform gradient-based learning.</p><h3 id=sigmoid--kbhsigmoid-dot-md><a href=/posts/kbhsigmoid/>sigmoid</a></h3><p><a href=/posts/kbhsigmoid/>sigmoid</a>: it pushed stuff to 1 or 0.</p><p>\begin{equation}
\frac{1}{1+e^{-z}}
\end{equation}</p><h3 id=tanh>tanh</h3><p>\begin{equation}
tanh(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
\end{equation}</p><p>this is just rescaled logistic: its twice as steep (tanh(z) = 2sigmoid(2z)-1)</p><h3 id=hard-tanh>hard tanh</h3><p>tanh but funny. because exp is hard to compute</p><p>\begin{equation}
HTanh = \begin{cases}
-1, if x &lt; -1 \\
0, if -1 \leq x \leq 1 \\
1, if x > 1\\
\end{cases}
\end{equation}</p><p>this motivates ReLU</p><h3 id=relu--kbhneural-networks-dot-md><a href=/posts/kbhneural_networks/#relu>relu</a></h3><p>slope is 1 so its easy to compute, etc.</p><p>\begin{equation}
ReLU(z) = \max(z,0)
\end{equation}</p><h3 id=leaky-relu>Leaky ReLU</h3><p>&ldquo;ReLU like but not actually dead&rdquo;</p><p>\begin{equation}
LeakReLU = \begin{cases}
x, if x>0 \\
\epsilon x, if x &lt; 0
\end{cases}
\end{equation}</p><p>or slightly more funny ones</p><p>\begin{equation}
swish = x \sigma(x)
\end{equation}</p><p>(this is relu on positive like exp, or a negative at early bits)</p><hr><h2 id=vectorized-calculus>Vectorized Calculus</h2><p>Multi input function&rsquo;s gradient is a vector w.r.t. each input but has a single output</p><p>\begin{equation}
f(\bold{x}) = f(x_1, \dots, x_{n})
\end{equation}</p><p>where we have:</p><p>\begin{equation}
\nabla f = \pdv{f}{\bold{x}} = \qty [ \pdv{f}{x_1}, \pdv{f}{x_2}, \dots]
\end{equation}</p><p>if we have multiple outputs:</p><p>\begin{equation}
\bold{f}(\bold{x}) = \qty[f_1(x_1, \dots, x_{n}), f_2(x_1, \dots, x_{n})\dots ]
\end{equation}</p><p>\begin{equation}
\nabla \bold{f} = \mqty[ \nabla f_1 \\ \nabla f_2 \\ \dots] = \mqty[ \pdv{f_1}{x_1} & \dots & \pdv{f_1}{x_{n}} \\ \nabla f_2 \\ \dots]
\end{equation}</p><h3 id=transposes>Transposes</h3><p>Consider:</p><p>\begin{equation}
\pdv u \qty(u^{\top} h) = h^{\top}
\end{equation}</p><p>but because of shape conventions we call:</p><p>\begin{equation}
\pdv u \qty(u^{\top} h) = h
\end{equation}</p><h3 id=useful-jacobians>Useful Jacobians!</h3><figure><img src=/ox-hugo/2024-04-09_17-04-39_screenshot.png></figure><h4 id=why-is-the-middle-one>Why is the middle one?</h4><p>Because the activations \(f\) are applied elementwise, only the <strong>diagonal</strong> are values and the off-diagonals are all \(0\) (because \(\pdv{h(x_1)}{x_2} = 0\)).</p><h3 id=shape-convention>Shape Convention</h3><p>We will always by <strong>output shape</strong> as the same shape of the <strong>parameters</strong>.</p><figure><img src=/ox-hugo/2024-04-09_17-10-15_screenshot.png></figure><ul><li><strong>shape convention</strong>: derivatives of matricies are the shape</li><li><strong>Jacobian form</strong>: derivatives w.r.t. matricies are row vectors</li></ul><p>we use the first one</p><h2 id=actual-backprop>Actual Backprop</h2><ul><li>create a <a href=/posts/kbhtopological_sort/>topological sort</a> of your computation graph</li><li>calculate each variable in that order</li><li>calculate backwards pass in reverse order</li></ul><h2 id=check-gradient>Check Gradient</h2><p>\begin{equation}
f&rsquo;(x) \approx \frac{f(x+h) - f(x-h)}{2h}
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>