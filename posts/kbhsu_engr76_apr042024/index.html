<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-ENGR76 APR042024</title><meta name=description content="information
Information is the amount of surprise a message provides.
Shannon (1948): a mathematical theory for communication.
information value
The information value, or entropy, of a information source is its probability-weighted average surprise of all possible outcomes:
\begin{equation}
H(X) = \sum_{x \in X}^{} s(P(X=x)) P(X=x)
\end{equation}
properties of entropy

entropy is positive: \(H(x) \geq 0\)
entropy of uniform: for \(M \sim CatUni[1, &mldr;, n]\), \(p_{i} = \frac{1}{|M|} = \frac{1}{n}\), and \(H(M) = \log_{2} |M| = \log_{2} n\)
entropy is bounded: \(0 \leq H(X) \leq H(M)\) where \(|X| = |M|\) and \(M \sim CatUni[1 &mldr; n]\) (&ldquo;uniform distribution has the highest entropy&rdquo;); we will reach the upper bound IFF \(X\) is uniformly distributed.

binary entropy function
For some binary outcome \(X \in \{1,2\}\), where \(P(x=1) = p_1\), \(P(X_2 = 2) = 1-p_1\). We can write:"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-ENGR76 APR042024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#information>information</a><ul><li><a href=#information-value>information value</a></li><li><a href=#information-source>information source</a></li><li><a href=#surprise>surprise</a></li><li><a href=#additional-information>additional information</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=information>information</h2><p><em>Information is the amount of surprise a message provides.</em></p><p>Shannon (1948): a mathematical theory for communication.</p><h3 id=information-value>information value</h3><p>The <a href=#information-value>information value</a>, or <a href=#information-value>entropy</a>, of a <a href=#information-source>information source</a> is its probability-weighted average <a href=#surprise>surprise</a> of all possible outcomes:</p><p>\begin{equation}
H(X) = \sum_{x \in X}^{} s(P(X=x)) P(X=x)
\end{equation}</p><h4 id=properties-of-entropy>properties of entropy</h4><ul><li><strong>entropy is positive</strong>: \(H(x) \geq 0\)</li><li><strong>entropy of uniform</strong>: for \(M \sim CatUni[1, &mldr;, n]\), \(p_{i} = \frac{1}{|M|} = \frac{1}{n}\), and \(H(M) = \log_{2} |M| = \log_{2} n\)</li><li><strong>entropy is bounded</strong>: \(0 \leq H(X) \leq H(M)\) where \(|X| = |M|\) and \(M \sim CatUni[1 &mldr; n]\) (&ldquo;uniform distribution has the highest entropy&rdquo;); we will reach the upper bound IFF \(X\) is uniformly distributed.</li></ul><h4 id=binary-entropy-function>binary entropy function</h4><p>For some binary outcome \(X \in \{1,2\}\), where \(P(x=1) = p_1\), \(P(X_2 = 2) = 1-p_1\). We can write:</p><p>\begin{equation}
H_{2}(p_1) = p_1 \log_{2} \frac{1}{p_1} + (1-p_1) \log_{2} \frac{1}{1-p_1}
\end{equation}</p><p>If you plot this out, we get a cap-like function, whereby at \(H(0) = 0\), \(H(1) = 0\), but \(H(0.5) = 1\) &mdash; information sources are most effective when what&rsquo;s communicated is ambiguous.</p><h3 id=information-source>information source</h3><p>We model an <a href=#information-source>information source</a> as an <a href=/posts/kbhrandom_variables/>random variable</a>. A <a href=/posts/kbhrandom_variables/>random variable</a> can take on any number of information, until you GET the information, and you will get an exact value. Each source has a range of possible values it can communicate (which is the <a href=/posts/kbhsupport/>support</a> of the <a href=/posts/kbhrandom_variables/>random variable</a> representing the information source).</p><p>We will then define the <a href=#surprise>surprise</a> of a piece of information as a (decreasing function? of) the probability corresponding to the event of receiving that information.</p><h3 id=surprise>surprise</h3><p>IMPORTANTLY: this class uses base \(2\), but the base is unimportant.</p><p>\begin{equation}
s(p) = \log_{2} \frac{1}{p}
\end{equation}</p><h4 id=properties-of-surprise>Properties of Surprise</h4><ul><li>log-base-2 <a href=#surprise>surprise</a> has units &ldquo;bits&rdquo;</li><li>\(s(1) = 0\)</li><li>\(p \to 0, s(p) \to \infty\)</li><li>\(s(p) \geq 0\)</li><li>&ldquo;joint surprise&rdquo; \(s(p,q) = s(p) + s(q)\)</li></ul><h4 id=facts-about-surprise>Facts about Surprise</h4><ol><li><a href=#surprise>surprise</a> should probably decrease with increasing \(p\)</li><li><a href=#surprise>surprise</a> should be continuous in \(p\)</li><li>for \(s(p_i), s(q_{j})\), for two events \(p_i\) and \(q_{j}\), the &ldquo;surprise&rdquo; of \(s(pi, q_{j})\) = \(s(p_{i}) + s(q_{j})\)</li><li><a href=#surprise>surprise</a> satisfies the fact that something with probably \(0\) happening, we should be infinitely surprises; if something happens with iincreasing higher probabiity, surprise would be low</li></ol><p>The surprise function is the <strong>unique</strong> function which satisfies all of the above property.</p><h3 id=additional-information>additional information</h3><p><strong>information is relative to the domain which is attempted to be communicated</strong></p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>