<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS205L JAN162025</title>
<meta name=description content="Random properties of eigenthings&mldr;
Hermitian Matrix
A matrix such that:  \(A^{*^{T}} = A\)
\begin{equation}
v^{*^{T}} A = \lambda^{*} v^{*^{T}}
\end{equation}
Meaning; \(Av = \lambda v \implies  \qty(v^{*^{T}} A v = v^{*^{T}} \lambda v) \implies  \lambda^{*} = \lambda\)
Symmetric matrices have this property.
vector deformation
Suppose \(v\) are the eigenvectors of \(A\), some vector \(c = \sum_{k}^{} a_{k} v_{k}\) (as long as they are from different eigenvalues), and so, applying \(A\):
\begin{equation}
Ac = \sum_{k}^{} a_{k} A v_{k} = \sum_{k}^{} \qty(a_{k} \lambda_{k}) v_{k}
\end{equation}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS205L JAN162025</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#hermitian-matrix>Hermitian Matrix</a></li><li><a href=#vector-deformation>vector deformation</a></li><li><a href=#spacial-deformation>spacial deformation</a></li><li><a href=#singular-value-decomposition--kbhsingular-value-decomposition-dot-md><a href=HAHAHUGOSHORTCODE1425s1HBHB>singular value decomposition</a></a><ul><li><a href=#notes-on-rectangular-diagonal-matrix>Notes on Rectangular Diagonal Matrix</a></li><li><a href=#rank-deficient>rank deficient</a></li><li><a href=#notes>notes</a></li><li><a href=#tip>tip</a></li></ul></li></ul></nav></aside><main><article><div><p>Random properties of eigenthings&mldr;</p><h2 id=hermitian-matrix>Hermitian Matrix</h2><p>A <a href=/posts/kbhmatricies/>matrix</a> such that: \(A^{*^{T}} = A\)</p><p>\begin{equation}
v^{*^{T}} A = \lambda^{*} v^{*^{T}}
\end{equation}</p><p>Meaning; \(Av = \lambda v \implies \qty(v^{*^{T}} A v = v^{*^{T}} \lambda v) \implies \lambda^{*} = \lambda\)</p><p>Symmetric matrices have this property.</p><h2 id=vector-deformation>vector deformation</h2><p>Suppose \(v\) are the eigenvectors of \(A\), some vector \(c = \sum_{k}^{} a_{k} v_{k}\) (as long as they are from different eigenvalues), and so, applying \(A\):</p><p>\begin{equation}
Ac = \sum_{k}^{} a_{k} A v_{k} = \sum_{k}^{} \qty(a_{k} \lambda_{k}) v_{k}
\end{equation}</p><p>&ldquo;you are rotating the vector towards the large eigenvalues, and away from the vectors corresponding to the small eigenvalues&rdquo;</p><h2 id=spacial-deformation>spacial deformation</h2><p>When one eigenvalue is very big, and the other is very small: you are squashing a unit sphere in the space into an &ldquo;ellipse&rdquo; which is dependent on one of the eigenvector&rsquo;s direction.</p><p>Hence, its possible for small perturbations in input to change the scales of outputs.</p><h2 id=singular-value-decomposition--kbhsingular-value-decomposition-dot-md><a href=/posts/kbhsingular_value_decomposition/>singular value decomposition</a></h2><p>see <a href=/posts/kbhsingular_value_decomposition/>singular value decomposition</a></p><p>Factorization:</p><p>\begin{equation}
A = U \Sigma V^{T}
\end{equation}</p><p>for \(A: m \times n\), \(\Sigma: m \times n\), \(V: n \times n\)</p><p>Notice&mdash;we &ldquo;amplify errors&rdquo; given by \(\Sigma\), but \(U\) and \(V\) are both orthonormal so errors don&rsquo;t propagate there.</p><p>Columns of \(V\) are eigenvectors of \(A^{T}A\), columns of \(U\) are eigenvectors of \(A A^{T}\). Singular values are the non-negative square roots of the</p><p>You can approximate a matrix by throwing away rows/columns which has zero/small singular values. In this case, we can throw away stuff that&rsquo;s close enough for \(0\).</p><h3 id=notes-on-rectangular-diagonal-matrix>Notes on Rectangular Diagonal Matrix</h3><p>\begin{equation}
\mqty(5 & 0 \\ 0 & 2 \\ 0 & 0) \mqty(c_1 \\ c_2) = \mqty(10 \\ -1 \\ \alpha)
\end{equation}</p><p>notice how this system has no solution at \(\alpha \neq 0\); when \(\alpha \approx 0\), we run into trouble numerically.</p><h3 id=rank-deficient>rank deficient</h3><p>If a matrix is rank-deficient, then there will be a \(0\) as a singular value.</p><h3 id=notes>notes</h3><figure><img src=/ox-hugo/2025-01-16_18-12-08_screenshot.png></figure><h3 id=tip>tip</h3><p>an orthonormal matrix has determinant \(\pm 1\), if \(v\) had an reflection, the determinant would be \(-1\), so just flipping it until its \(+1\) we&rsquo;d be fine.</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>