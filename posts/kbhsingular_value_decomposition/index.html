<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>singular value decomposition</title><meta name=description content="Singular value decomposition is a factorization of a matrix, which is a generalization of the eigendecomposition of normal matricies (i.e. where \(A = V^{-1} D V\) when \(A\) is diagonalizable, i.e. by the spectral theorem possible when matricies are normal).
Definitions Singular value decomposition Every \(m \times n\) matrix has a factorization of the form:
\begin{equation} M = U D^{\frac{1}{2}} V^{*} \end{equation}
where, \(U\) is an unitary matrix, \(D^{\frac{1}{2}}\) a diagonalish (i."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>singular value decomposition</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#definitions>Definitions</a></li><li><a href=#doing-it>Doing It</a></li><li><a href=#motivation-and-proof>Motivation and Proof</a></li><li><a href=#useful-corollaries>Useful corollaries</a><ul><li><a href=#if-lambda-is-an-en-eigenvalue-of-m-then-lambda-is-a-singular-value-of-m>If \(\lambda\) is an en eigenvalue of \(M\), then \(\lambda\) is a singular value of \(M\)</a></li></ul></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhsingular_value_decomposition/>Singular value decomposition</a> is a factorization of a <a href=/posts/kbhmatricies/>matrix</a>, which is a generalization of the <a href=/posts/kbhnus_math530_similar_to_diagonal/>eigendecomposition</a> of <a href=/posts/kbhaxler_7_a/#normal>normal</a> <a href=/posts/kbhmatricies/>matricies</a> (i.e. where \(A = V^{-1} D V\) when \(A\) is <a href=/posts/kbhdiagonal_matrix/#properties-of-diagonal-matrices>diagonalizable</a>, i.e. by the <a href=/posts/kbhaxler_7_a/#complex-spectral-theorem>spectral theorem</a> possible when matricies are <a href=/posts/kbhaxler_7_a/#normal>normal</a>).</p><h2 id=definitions>Definitions</h2><p><strong>Singular value decomposition</strong> Every \(m \times n\) matrix has a factorization of the form:</p><p>\begin{equation}
M = U D^{\frac{1}{2}} V^{*}
\end{equation}</p><p>where, \(U\) is an <a href=/posts/kbhaxler_7_a/#unitary>unitary</a> matrix, \(D^{\frac{1}{2}}\) a <a href=/posts/kbhdiagonal_matrix/>diagonal</a>ish (i.e. rectangular diagonal) matrix with non-negative numbers on its diagonal called <strong>singular values</strong>, which are the positive square roots of eigenvalues of \(M^{* }M\) &mdash; meaning the diagonal of \(D^{\frac{1}{2}}\) is non-negative (\(\geq 0\)). Finally, \(V\) is formed columns of orthonormal bases of eigenvectors of \(M^{*}M\).</p><p><a href=/posts/kbhsingular_value_decomposition/>SVD</a> is not technically unique, but we like to force a specific (convenient, see proof for why) ordering: where \(D^{\frac{1}{2}}\) (and the corresponding values in \(V^{*}\)) is sorted: such that the values.</p><h2 id=doing-it>Doing It</h2><p>Doing SVD is not actually super duper hard, but it takes some thinking on why it works, which we shall do below.</p><p>Recall that \(V^{* }\) is the conjugate transpose of the orthonormal eigenvectors of \(M^{*} M\). Then, we construct the square roots of the corresponding eigenvalues and arrange them into \(D^{\frac{1}{2}}\).</p><hr><p><strong>Tangent</strong>:</p><p>Why is it we can take square roots of these values (i.e. the eigenvalues are guaranteed positive or zero?) Recall the definition of adjoint:</p><p>\begin{equation}
\langle Tv, w \rangle = \langle v, T^{*}w \rangle
\end{equation}</p><p>Applying it here, we have</p><p>\begin{equation}
\langle M^{*}M v, v \rangle = \langle M v, M v \rangle
\end{equation}</p><p>And recall that, by definition of inner product, \(\langle Mv, Mv \rangle \geq 0\), and so \(\|Mv\|^{2} \geq 0\) and so \(\|Mv\| \geq 0\) so \(\| \lambda v \| \geq 0\).</p><hr><p>And so you can take the square roots of those singular values (i.e. square roots of eigenvalues of \(M^{*}M\)).</p><p>How do we get \(U\)? Well recall:</p><p>\begin{equation}
M = U D^{\frac{1}{2}} V^{*}
\end{equation}</p><p>And \(V\) is an operator lined with orthornomal eigenbases so it is unitary and so \(V = (V^{*})^{-1}\).</p><p>And therefore, we apply \(V\) on both sides:</p><p>\begin{equation}
MV = UD^{\frac{1}{2}}
\end{equation}</p><p>As \(D\) is diagonal, and we know the left side, we can then easily recover \(U\) by staring at it (and norming the vectors).</p><h2 id=motivation-and-proof>Motivation and Proof</h2><hr><p>We have a matrix \(M\), it may suck: it may not be <a href=/posts/kbhaxler_7_a/#normal>normal</a>, it may not even be an <a href=/posts/kbhoperator/>operator</a>.</p><p>So consider now:</p><p>\begin{equation}
M^{*} M
\end{equation}</p><p>you will note that this is now an <em>operator!</em> Not only that, \(M^{*}M\) is <a href=/posts/kbhaxler_7_a/#self-adjoint>self-adjoint</a> (\((M^{*}M)^{*} = M^{*}(M^{*})^{*} = M^{*}M\)). Of course <a href=/posts/kbhaxler_7_a/#self-adjoint>self-adjoint</a> matricies are <a href=/posts/kbhaxler_7_a/#normal>normal</a>, which is nice, so <a href=/posts/kbhaxler_7_a/#complex-spectral-theorem>spectral theorem</a> applies here (even the real version because <a href=/posts/kbhaxler_7_a/#self-adjoint>self-adjoint</a>!)</p><p>So, there are a basis of orthonormal <a href=/posts/kbheigenvalue/>eigenvector</a>s \(v_1, \dots v_{n}\) of \(M^{*}M\) such that:</p><p>Given:</p><p>\begin{equation}
V = (v_1 \dots v_{m})
\end{equation}</p><p>\begin{equation}
M^{*}M = V D V^{-1}
\end{equation}</p><p>(recall that <a href=/posts/kbhnus_math530_similar_to_diagonal/>T is diagonalizable IFF the matrix of T is similar to a diagonal matrix</a>). Recall, that, \(v_1, \dots v_{n}\) are <strong>orthonormal</strong>, so \(V\)&rsquo;s columns are <a href=/posts/kbhorthonormal/>orthonormal</a>. By definition, then, \(V\) is <a href=/posts/kbhaxler_7_a/#unitary>unitary</a>. This makes \(V^{-1} = V^{*}\) is <a href=/posts/kbhaxler_7_a/#unitary>unitary</a>. This makes \(V^{-1} = V^{*}\).</p><p>And therefore:</p><p>\begin{equation}
M^{*} M = V D V^{-1} = V D V^{*}
\end{equation}</p><hr><p>Tangent&mdash;</p><p>To make this better, we can order \(v_1, \dots v_{n}\) such that eigenvectors vectors corresponding to \(\lambda = 0\) comes last.</p><p>And so we make a \(V\):</p><p>\begin{equation}
V = (v_1 \dots v_{m-p} | v_{m-p+1} \dots v_{m})
\end{equation}</p><p>So we have two sub-matricies: an matrix \(V_1\) of shape \((m, m-p)\) which is filled by <a href=/posts/kbheigenvalue/>eigenvector</a>s corresponding to <a href=/posts/kbheigenvalue/>eigenvalue</a>s not \(=0\), and the other matrix \(V_2\) of shape \((m,p)\) which is made of <a href=/posts/kbheigenvalue/>eigenvector</a>s corresponding to zero <a href=/posts/kbheigenvalue/>eigenvalue</a>s.</p><hr><p>Ok, recall:</p><p>\begin{equation}
M^{*} M = V D V^{-1} = V D V^{*}
\end{equation}</p><p>Equivalently (by applying \(V\) and \(V^{*}\)) to both sides, we have:</p><p>\begin{equation}
V^{*} M^{*}M V = D
\end{equation}</p><p>Applying the breakup of \(V = (V_1\ V_2)\) above:</p><p>\begin{equation}
\mqty(V_1^{*} \\ V_2^{*}) M^{*} M \mqty(V_1\ V_2) = D
\end{equation}</p><p>The diagonal matrix, given that the columns in \(V_2\) correspond to zero <a href=/posts/kbheigenvalue/>eigenvalue</a>s, will look like these four <strong>not-necessarily same size</strong> quadrants:</p><p>\begin{equation}
\mqty(V_1^{*} \\ V_2^{*}) M^{*} M \mqty(V_1\ V_2) = \mqty(D&rsquo; & 0 \\ 0 & 0)
\end{equation}</p><p>The non-diagonals are zero by definition, the lower-right quadrant is \(0\) because the sub-part of \(V_2\) are eigenvectors corresponding to the zero eigenvalue.</p><p>Now, recall how matricies multiply:</p><p>\begin{align}
&\mqty(V_1^{*} \\ V_2^{*}) M^{*} M \mqty(V_1\ V_2) = \mqty(D&rsquo; & 0 \\ 0 & 0)\\
\Rightarrow\ &\mqty(V_1^{*} \\ V_2^{*}) \mqty(M^{*} M V_1\ M^{*} M V_2) = \mqty(D&rsquo; & 0 \\ 0 & 0) \\
\Rightarrow\ & \mqty(V_1^{*} M^{*} M V_1 & V_1^{*} M^{*} M V_2 \\ V_2^{*}M^{*} M V_1 & V_2^{*} M^{*} M V_2) = \mqty(D&rsquo; & 0 \\ 0 & 0)
\end{align}</p><hr><p>Tangent part 2&mdash;-</p><p>Ok:</p><p>\begin{equation}
V_1^{*} V_1 = I
\end{equation}</p><p>recall, because the rows/columns for \(V_1^{*}\) and \(V_1\) respectively are orthonormal.</p><p>In a similar vein:</p><p>\begin{equation}
V_2^{*} V_2 = I
\end{equation}</p><p>And now, consider:</p><p>\begin{equation}
V_1 V_1^{*}
\end{equation}</p><p>This is &ldquo;definitely singular&rdquo; <strong>why</strong>, consider? IDK.</p><p>Recall that:</p><p>\begin{equation}
V V^{*} = I, \mqty(V_1 & V_2) = V
\end{equation}</p><p>And so:</p><p>\begin{equation}
\mqty(V_1 & V_2) \mqty(V_1^{*} \\ V_2^{*}) = V V^{*} + I
\end{equation}</p><p>Finally:</p><p>\begin{align}
V_1V_1^{*} + V_2V_2^{*} &= \qty(\mqty(V_1 & 0) + \mqty(0 & V_2)) + \qty(\mqty(V_1^{*} \\ 0) + \mqty(0 \\ V_2^{*})) \\
&= \mqty(V_1 & V_2) \mqty(V_1^{*} \\ V_2^{*})\\
&= V V^{*} + I
\end{align}</p><p>Weirdly, we add two non-full rank matricies and end up to be the identity. So, again:</p><p>\begin{equation}
V_1 V_1^{*} + V_2V_2^{*} = I
\end{equation}</p><hr><p>FINALLY:</p><p><strong>DEFINE</strong> singular values: square roots of <a href=/posts/kbheigenvalue/>eigenvalue</a>s of \(M^{*} M\) are the <strong>singular values</strong> of \(M\).</p><p>Notice! Every <a href=/posts/kbheigenvalue/>eigenvalue</a> of \(M\) is a <strong>singular value</strong> of \(M\); but there maybe more singular values</p><p>Let \(U_1 = M V_1 D^{-\frac{1}{2}}\). (Where the square root of the matrix is the matrix \(D = D^{\frac{1}{2}} D^{\frac{1}{2}}\). Now, \(D^{-\frac{1}{2}} = (D^{\frac{1}{2}})^{-1}\). For diagonal matricies, this is particularly easy: the matrix \(D^{\frac{1}{2}}\) would just be the square roots of the diagonal.)</p><p>Recall that \(V_1 V_1^{*} + V_2V_2^{*} = I\), and that . Consider,</p><p>\begin{equation}
U_1 D^{\frac{1}{2}} V_1^{*} = (M V_1 D^{-\frac{1}{2}}) (D^{\frac{1}{2}} V_1^{*}) = M V_1 I V_1^{*} = M (I - V_2 V_2^{*}) = M - M V_2 V_2^{*} = M - 0 V_2^{*} = M
\end{equation}</p><p>You are now good at math.</p><p>So, we now know that:</p><p>\begin{equation}
M = U_1 D^{\frac{1}{2}} V_1^{*}
\end{equation}</p><p>\(U_1\) has shape \((m, m-p)\), \(D^{\frac{1}{2}}\) has shape \((m-p, m-p)\), and \(V_1^{*}\) has shape \((m-p,n)\). You can expand \(U_1\)&rsquo;s missing \(p\) column vectors into a basis of \(V\) to make thing things squared; and for the second part, you can add \(V_2\) back. Those get sent to \(0\) so it wouldn&rsquo;t matter. This makes \(D\) <a href=/posts/kbhdiagonal_matrix/#properties-of-diagonal-matrices>diagonalish</a>.</p><p>Will come and clean this up later today because uht no.</p><h2 id=useful-corollaries>Useful corollaries</h2><h3 id=if-lambda-is-an-en-eigenvalue-of-m-then-lambda-is-a-singular-value-of-m>If \(\lambda\) is an en eigenvalue of \(M\), then \(\lambda\) is a singular value of \(M\)</h3><p>So we have:</p><p>\begin{equation}
Mv = \lambda v
\end{equation}</p><p>We desire that \(\lambda^{2}\) is an eigenvalue of \(M^{*}M\).</p><p>Recall the definition of an adjoint:</p><p>\begin{equation}
\langle Mv, v \rangle = \langle v, M^{*}v \rangle
\end{equation}</p><p>And, given \(\lambda\) is an eigenvalue of \(M\):</p><p>\begin{equation}
\langle Mv,v \rangle = \lambda \langle v,v \rangle
\end{equation}</p><p>Consider, now:</p><p>\begin{equation}
M^{*}M v = \lambda v
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>