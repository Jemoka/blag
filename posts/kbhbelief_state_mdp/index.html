<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>belief-state MDP</title>
<meta name=description content="Our belief can be represented as vectors as the probability of us being in each state. If we have that, we can just use our belief vector as our state vector. Now use MDP any solving you&rsquo;d like, keeping in mind that the reward is just the expected reward:
\begin{equation} \mathbb{E}[R(b,a)] = \sum_{s} R(s,a) b(s) \end{equation}
we can estimate our transition between belief-states like so:
\begin{align} T(b&rsquo;|b,a) &= P(b&rsquo;|b,a) \\ &= \sum_{o}^{} P(b&rsquo;|b,a,o) P(o|b,a) \\ &= \sum_{o}^{} P(b&rsquo; = Update(b,a,o)) \sum_{s&rsquo;}^{}O(o|a,s&rsquo;) \sum_{s}^{}T(s&rsquo;|s,a)b(s) \end{align}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>belief-state MDP</h1><span class=tagbox></span></div><main><article><div><p>Our <a href=/posts/kbhbelief/>belief</a> can be represented as <a href=/posts/kbhvector/>vector</a>s as the probability of us being in each state. If we have that, we can just use our <a href=/posts/kbhbelief/>belief</a> vector as our state vector. Now use <a href=/posts/kbhmarkov_decision_process/>MDP</a> any solving you&rsquo;d like, keeping in mind that the reward is just the expected reward:</p><p>\begin{equation}
\mathbb{E}[R(b,a)] = \sum_{s} R(s,a) b(s)
\end{equation}</p><p>we can estimate our transition between belief-states like so:</p><p>\begin{align}
T(b&rsquo;|b,a) &= P(b&rsquo;|b,a) \\
&= \sum_{o}^{} P(b&rsquo;|b,a,o) P(o|b,a) \\
&= \sum_{o}^{} P(b&rsquo; = Update(b,a,o)) \sum_{s&rsquo;}^{}O(o|a,s&rsquo;) \sum_{s}^{}T(s&rsquo;|s,a)b(s)
\end{align}</p><p>&ldquo;the probability of the next belief being \(b&rsquo;\) is equal to how probable it is to get state b&rsquo; from conditions b,a,o, times the probability of getting that particular observation.&rdquo;.</p><p>However, this expression is quite unwheldy if your state-space is large. Hence, we turn to a technique like <a href=/posts/kbhconditional_plan/>conditional plan</a>s which foregos considering individual states altogether.</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>