<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>tokenization</title>
<meta name=description content="Every NLP task involve some kind of text normalization.

tokenizing words
normalizing word formats (lemmatize?)
sentence and paragraph segmentation

For Latin, Arabic, Cyrillic, Greek systems, spaces can usually be used for tokenization. Other writing systems can&rsquo;t do this. See morpheme
Subword Tokenization
Algorithms for breaking up tokens using corpus statistics which acts on lower-than-word level.

BPE
Unigram Language Modeling tokenization
WordPiece

They all work in 2 parst:

a token learner: takes training corpus and derives a vocabulary set
a token segmenter that tokenizes text according to the vocab

tr
For those languages, you can use these systems to perform tokenization."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>tokenization</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#subword-tokenization>Subword Tokenization</a></li><li><a href=#tr>tr</a></li><li><a href=#what-to-tokenize>What to Tokenize</a><ul><li><a href=#clitics>clitics</a></li></ul></li></ul></nav></aside><main><article><div><p>Every NLP task involve some kind of text normalization.</p><ol><li>tokenizing words</li><li>normalizing word formats (lemmatize?)</li><li>sentence and paragraph segmentation</li></ol><p>For Latin, Arabic, Cyrillic, Greek systems, spaces can usually be used for tokenization. Other writing systems can&rsquo;t do this. See <a href=/posts/kbhmorpheme/>morpheme</a></p><h2 id=subword-tokenization>Subword Tokenization</h2><p>Algorithms for breaking up tokens using <a href=/posts/kbhcorpus/>corpus</a> statistics which acts on lower-than-word level.</p><ul><li><a href=/posts/kbhbpe/>BPE</a></li><li>Unigram Language Modeling tokenization</li><li>WordPiece</li></ul><p>They all work in 2 parst:</p><ul><li>a token <strong>learner</strong>: takes training corpus and derives a vocabulary set</li><li>a token <strong>segmenter</strong> that tokenizes text according to the vocab</li></ul><h2 id=tr>tr</h2><p>For those languages, you can use these systems to perform tokenization.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tr -sc <span style=color:#d88200>&#34;A-Za-z&#34;</span> <span style=color:#d88200>&#34;\n&#34;</span> &lt; input.txt
</span></span></code></pre></div><p>this takes every form which is not text (<code>-c</code> is the complement operator) and replaces it with a newline. <code>-s</code> squeezes the text so that there are not multiple newlines.</p><p>This turns the text into one word per line.</p><p>Sorting it (because <code>uniq</code> requires it) and piping into <code>uniq</code> gives word count</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tr -sc <span style=color:#d88200>&#34;A-Za-z&#34;</span> <span style=color:#d88200>&#34;\n&#34;</span> &lt; input.txt <span style=color:#111>|</span> sort <span style=color:#111>|</span> uniq
</span></span></code></pre></div><p>We can then do a reverse numerical sort:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tr -sc <span style=color:#d88200>&#34;A-Za-z&#34;</span> <span style=color:#d88200>&#34;\n&#34;</span> &lt; input.txt <span style=color:#111>|</span> sort <span style=color:#111>|</span> uniq <span style=color:#111>|</span> sort -r -n
</span></span></code></pre></div><p>which gives a list of words per frequency.</p><p>This is a <strong>BAD RESULT</strong> most of the time: some words have punctuation with meaning that&rsquo;s not tokenizaiton: <code>m.p.h.</code>, or <code>AT&amp;T</code>, or <code>John's</code>, or <code>1/1/12</code>.</p><h2 id=what-to-tokenize>What to Tokenize</h2><p>&ldquo;I do uh main- mainly business data processing&rdquo;</p><ul><li><code>uh</code>: filled pause</li><li><code>main-</code>: fragments</li></ul><p>Consider:</p><p>&ldquo;Seuss&rsquo;s cat in the cat is different from other cats!&rdquo;</p><ul><li><code>cat</code> and <code>cats</code>: same <a href=/posts/kbhtokenization/>lemma</a> (i.e. stem + part of speech + word sense)</li><li><code>cat</code> and <code>cats</code>: different <a href=/posts/kbhtokenization/>wordform</a>s</li></ul><p>We usually consider a <a href=/posts/kbhtokenization/>token</a> as distinct <a href=/posts/kbhtokenization/>wordform</a>, counting duplicates; whereas, we usually consider <a href=/posts/kbhtokenization/>word type</a>s as unique, non-duplicated distinct <a href=/posts/kbhtokenization/>wordform</a>s.</p><h3 id=clitics>clitics</h3><p><code>John's</code>: word that doesn&rsquo;t stand on its own.</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>