<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>AML: Dipping into PyTorch</title><meta name=description content="Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.
Getting Started To get started, let&rsquo;s open a colab and import Torch!"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><div style="padding:20px 0 30px"><div id=title><h1>AML: Dipping into PyTorch</h1></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#getting-started>Getting Started</a></li><li><a href=#tensors-and-autograd>Tensors and AutoGrad</a><ul><li><a href=#your-first-tensors>Your First Tensors</a></li><li><a href=#autograd>Autograd</a></li><li><a href=#so-why-the-heck-are-we-doing-all-this>So why the heck are we doing all this</a></li></ul></li><li><a href=#y-mx-plus-b>y=mx+b</a></li><li><a href=#your-first-neural-network>Your First Neural Network</a></li></ul></nav></aside><main><article><div><p>Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.</p><h2 id=getting-started>Getting Started</h2><p>To get started, let&rsquo;s open a <a href=https://colab.research.google.com/>colab</a> and import Torch!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span></code></pre></div><p>The top line here import PyTorch generally, and the bottom line imports the Neural Network libraries. We will need both for today and into the future!</p><h2 id=tensors-and-autograd>Tensors and AutoGrad</h2><p>The most basic element we will be working with in Torch is something called a <strong>tensor</strong>. A tensor is a variable, which holds either a single number (<strong>scalar</strong>) or a list of numbers (<strong>vector</strong>), that <em>can change</em>. We will see what that means in a sec.</p><h3 id=your-first-tensors>Your First Tensors</h3><p>Let&rsquo;s create two scalar tensors!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>3.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>var_2</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>4.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3., requires_grad=True), tensor(4., requires_grad=True))
</span></span></code></pre></div><p>We initialized two numbers, <code>3</code>, which we named <code>var_1</code>, and <code>4</code>, which we named <code>var_2</code>.</p><p>The value <code>requires_grad</code> here tells PyTorch that these values can change, which we need it to do&mldr; very shortly!</p><p>First, though, let&rsquo;s create a <strong>latent</strong> variable. A &ldquo;latent&rdquo; value is a value that is the <em>result</em> of operations on other non-latent tensors. For instance, if I multiplied our two tensors together, we can create our very own latent tensor.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_latent_value</span> <span style=color:#f92672>=</span> <span style=color:#111>var_1</span><span style=color:#f92672>*</span><span style=color:#111>var_2</span>
</span></span><span style=display:flex><span><span style=color:#111>my_latent_value</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>Evidently, \(3 \cdot 4 = 12\).</p><h3 id=autograd>Autograd</h3><p>Now! The beauty of PyTorch is that we can tell it to set any particular latent variable to \(0\) (Why only \(0\), and \(0\) specifically? Calculus; turns out this limitation doesn&rsquo;t matter at all, as we will see), and it can update all of its constituent tensors with <code>required_grad</code> &ldquo;True&rdquo; such that the latent variable we told PyTorch to set to \(0\) indeed becomes \(0\)!</p><p>This process is called &ldquo;automatic gradient calculation&rdquo; and &ldquo;backpropagation.&rdquo; (Big asterisks throughout, but bear with us. Find Matt/Jack if you want more.)</p><p>To do this, we will leverage the help of a special optimization algorithm called <strong>stochastic gradient descent</strong>.</p><p>Let&rsquo;s get a box of this stuff first:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torch.optim</span> <span style=color:#f92672>import</span> <span style=color:#111>SGD</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>SGD</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&lt;class &#39;torch.optim.sgd.SGD&#39;&gt;
</span></span></code></pre></div><p>Excellent. By the way, from the <code>torch.optim</code> package, there&rsquo;s tonnes (like at least 20) different &ldquo;optimizer&rdquo; algorithms that all to the same thing (&ldquo;take this latent variable to \(0\) by updating its constituents please&rdquo;) but do them in important different ways. We will explore some of them through this semester, and others you can Google for yourself by looking up &ldquo;PyTorch optimizers&rdquo;.</p><p>Ok, to get this SGD thing up and spinning, we have to tell it every tensor it gets to play with in a list. For us, let&rsquo;s ask PyTorch SGD to update <code>var_1</code> and <code>var_2</code> such that <code>my_latent_value</code> (which, remember, is var1 times var2) becomes a new value.</p><hr><p>Aside: <strong>learning rate</strong></p><p>Now, if you recall the neural network simulation, our model does not reach the desired outcome immediately. It does so in <em>steps</em>. The size of these steps are called the <strong>learning rate</strong>; the LARGER these steps are, the quicker you will get <em>close</em> to your desired solution, but where you end up getting maybe farther away from the actual solution; and vise versa.</p><p>Think about the learning rate as a hoppy frog: a frog that can hop a yard at a time (&ldquo;high learning rate&rdquo;) can probably hit a target a mile away much quicker, but will have a hard time actually hitting the foot-wide target precisely; a frog that can hop an inch at a time (&ldquo;low learning rate&rdquo;) can probably hit a target a mile away&mldr;. years from now, but will definitely be precisely hitting the foot-wide target when it finally gets there.</p><p>So what does &ldquo;high&rdquo; and &ldquo;low&rdquo; mean? Usually, we adjust learning rate by considering the number of decimal places it has. \(1\) is considered a high learning rate, \(1 \times 10^{-3} = 0.001\) as medium-ish learning rate, and \(1 \times 10^{-5}=0.00001\) as a small one. There are, however, no hard and fast rules about this and it is subjcet to experimentation.</p><hr><p>So, choose also an appropriate <strong>learning rate</strong> for our optimizer. I would usually start with \(3 \times 10^{-3}\) and go from there. In Python, we write that as <code>3e-3</code>.</p><p>So, let&rsquo;s make a SGD, and give it <code>var_1</code> and <code>var_2</code> to play with, and set the learning rate to <code>3e-3</code>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_sgd</span> <span style=color:#f92672>=</span> <span style=color:#111>SGD</span><span style=color:#111>([</span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#111>],</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3e-3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>my_sgd</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>SGD (
</span></span><span style=display:flex><span>Parameter Group 0
</span></span><span style=display:flex><span>    dampening: 0
</span></span><span style=display:flex><span>    differentiable: False
</span></span><span style=display:flex><span>    foreach: None
</span></span><span style=display:flex><span>    lr: 0.003
</span></span><span style=display:flex><span>    maximize: False
</span></span><span style=display:flex><span>    momentum: 0
</span></span><span style=display:flex><span>    nesterov: False
</span></span><span style=display:flex><span>    weight_decay: 0
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Wonderful. Don&rsquo;t worry much about how many of these means for now; however, we will see it in action shortly.</p><p>Now! Recall that we allowed <code>my_sgd</code> to mess with <code>var_1</code> and <code>var_2</code> to change the value of <code>my_latent_value</code> (the product of <code>var_1</code> and <code>var_2</code>).</p><p>Current, <code>var_1</code> and <code>var_2</code> carries the values of:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3., requires_grad=True), tensor(4., requires_grad=True))
</span></span></code></pre></div><p>And, of course, their product <code>my_latent_value</code> carries the value of:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_latent_value</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>What if we want <code>my_latent_value</code> to be&mldr; \(15\)? That sounds like a good number. Let&rsquo;s ask our SGD algorithm to update <code>var_1</code> and <code>var_2</code> such that <code>my_latent_value</code> will be \(15\)!</p><p>Waaait. I mentioned that the optimizers can only take things to \(0\). How could it take <code>my_latent_value</code> to \(15\) then? Recall! I said SGD takes <em>a</em> latent variable to \(0\). So, we can just build another latent variable such that, when <code>my_latent_value</code> is \(15\), our new latent variable will be \(0\), and then ask SGD optimize on that!</p><p>What cloud that be&mldr; Well, the <em>difference</em> between \(15\) and <code>my_latent_value</code> is a good one. If <code>my_latent_value</code> is \(15\), the <em>difference</em> between it and \(15\) will be \(0\), as desired!</p><p>Turns out, the &ldquo;objective&rdquo; for SGD optimization, the thing that we ask SGD to take to \(0\) on our behalf by updating the parameters we allowed it to update (again, they are <code>var_1</code> and <code>var_2</code> in our case here), is called the <strong>loss</strong>. We used the &ldquo;subtract from 15&rdquo; operation here to compute the loss, so &ldquo;subtract from 15&rdquo; is our <strong>loss function</strong> for this toy problem. For a multitude of reasons, this loss function is a baaad idea when you are actually doing ML. But for demonstration purposes this is OK.</p><p>So let&rsquo;s do it! Let&rsquo;s create a tensor our loss:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>15</span><span style=color:#f92672>-</span><span style=color:#111>my_latent_value</span>
</span></span><span style=display:flex><span><span style=color:#111>loss</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(3., grad_fn=&lt;RsubBackward1&gt;)
</span></span></code></pre></div><p>Nice. So our loss is at \(3\) right now; when <code>my_latent_value</code> is correctly at \(15\), our loss will be at \(0\)! So, to get <code>my_latent_value</code> to \(15\), we will ask SGD to take</p><h3 id=so-why-the-heck-are-we-doing-all-this>So why the heck are we doing all this</h3><h2 id=y-mx-plus-b>y=mx+b</h2><h2 id=your-first-neural-network>Your First Neural Network</h2></div></article></main></div></div></body></html>