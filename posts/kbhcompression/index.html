<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-ENGR76 Unit 1 Index</title><meta name=description content="
Analog vs Digital Signal
bit (signal processing)

Source-Channel Separation Theorem



information

information source can be modeled as a random variable
surprise
information value and entropy

joint entropy

joint entropy of independent events





Source Coding

non-singular code
uniquely decodeable
Prefix-Free code, which is uniquely decodeable and instantaneously decodable

any prefix free code for a source has at least entropy length
prefix-free code has the same codeword lengths as any code


Average Codeword Length
generating Prefix-Free code with Huffman Coding

Huffman Coding is Bounded


Block Coding

Boundedness of Block Coding and why it saves space!


Diadic Source
Shannon&rsquo;s Source-Coding Theorem

Entropy Rate of the Source


Non-IID Sequence Can Have Smaller Entropy (which is why we use Entropy Rate of the Source as the measure of how good a code is)

signal

sinusoid and Fourier Series

Fourier Series as exactly a shifted sum of sinusoids
General Fourier Decomposition


L-periodic functions and triangle wave
Fourier Series components form a basis
Bandwidth

Finite-Bandwidth Signal


Discrete Fourier Transform
Lossless Sampling and nyquist sampling theorem

Finite-Bandwidth Signal
Baseband Signal and Passband Signal


Interpolation

Zero-Hold Interpolation
best:  Infinite-Degree Polynomial Interpolation (uses sinc function)

Shannon&rsquo;s Nyquist Theorem




"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-ENGR76 Unit 1 Index</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#information--kbhsu-engr76-apr042024-dot-md><a href=HAHAHUGOSHORTCODE253s3HBHB>information</a></a></li><li><a href=#source-coding--kbhsu-engr76-apr092024-dot-md><a href=HAHAHUGOSHORTCODE253s10HBHB>Source Coding</a></a></li><li><a href=#signal--kbhsu-engr76-apr162024-dot-md><a href=HAHAHUGOSHORTCODE253s29HBHB>signal</a></a></li></ul></nav></aside><main><article><div><ul><li><a href=/posts/kbhanalog_vs_digital_signal/>Analog vs Digital Signal</a></li><li><a href=/posts/kbhbit_signal_processing/>bit (signal processing)</a><ul><li><a href=/posts/kbhbit_signal_processing/#source-channel-separation-theorem>Source-Channel Separation Theorem</a></li></ul></li></ul><h2 id=information--kbhsu-engr76-apr042024-dot-md><a href=/posts/kbhsu_engr76_apr042024/#information>information</a></h2><ul><li><a href=/posts/kbhsu_engr76_apr042024/#information-source>information source</a> can be modeled as a random variable</li><li><a href=/posts/kbhsu_engr76_apr042024/#surprise>surprise</a></li><li><a href=/posts/kbhsu_engr76_apr042024/#information-value>information value</a> and <a href=/posts/kbhsu_engr76_apr042024/#information-value>entropy</a><ul><li><a href=/posts/kbhsu_engr76_apr092024/#joint-entropy>joint entropy</a><ul><li><a href=/posts/kbhsu_engr76_apr092024/#joint-entropy-of-independent-events>joint entropy of independent events</a></li></ul></li></ul></li></ul><h2 id=source-coding--kbhsu-engr76-apr092024-dot-md><a href=/posts/kbhsu_engr76_apr092024/#source-coding>Source Coding</a></h2><ul><li><a href=/posts/kbhsu_engr76_apr092024/#non-singular-code>non-singular code</a></li><li><a href=/posts/kbhsu_engr76_apr092024/#uniquely-decodeable>uniquely decodeable</a></li><li><a href=/posts/kbhsu_engr76_apr092024/#prefix-free>Prefix-Free</a> code, which is <a href=/posts/kbhsu_engr76_apr092024/#uniquely-decodeable>uniquely decodeable</a> and <a href=/posts/kbhsu_engr76_apr092024/#instantaneously-decodable>instantaneously decodable</a><ul><li><a href=/posts/kbhsu_engr76_apr092024/#any-prefix-free-code-for-a-source-has-at-least-entropy-length>any prefix free code for a source has at least entropy length</a></li><li><a href=/posts/kbhsu_engr76_apr092024/#prefix-free-code-has-the-same-codeword-lengths-as-any-code>prefix-free code has the same codeword lengths as any code</a></li></ul></li><li><a href=/posts/kbhsu_engr76_apr092024/#average-codeword-length>Average Codeword Length</a></li><li>generating <a href=/posts/kbhsu_engr76_apr092024/#prefix-free>Prefix-Free</a> code with <a href=/posts/kbhhuffman_coding/>Huffman Coding</a><ul><li><a href=/posts/kbhhuffman_coding/#huffman-coding-is-bounded>Huffman Coding is Bounded</a></li></ul></li><li><a href=/posts/kbhsu_engr76_apr092024/#block-coding>Block Coding</a><ul><li><a href=/posts/kbhsu_engr76_apr092024/#shannon-s-source-coding-theorem>Boundedness of Block Coding</a> and why it saves space!</li></ul></li><li><a href=/posts/kbhsu_engr76_apr112024/#diadic-source>Diadic Source</a></li><li><a href=/posts/kbhsu_engr76_apr092024/#shannon-s-source-coding-theorem>Shannon&rsquo;s Source-Coding Theorem</a><ul><li><a href=/posts/kbhsu_engr76_apr092024/#entropy-rate-of-the-source>Entropy Rate of the Source</a></li></ul></li><li><a href=/posts/kbhsu_engr76_apr162024/#non-iid-sequence-can-have-smaller-entropy>Non-IID Sequence Can Have Smaller Entropy</a> (which is why we use <a href=/posts/kbhsu_engr76_apr092024/#entropy-rate-of-the-source>Entropy Rate of the Source</a> as the measure of how good a code is)</li></ul><h2 id=signal--kbhsu-engr76-apr162024-dot-md><a href=/posts/kbhsu_engr76_apr162024/#signal>signal</a></h2><ul><li><a href=/posts/kbhsu_engr76_apr162024/#sinusoid>sinusoid</a> and <a href=/posts/kbhfourier_series/>Fourier Series</a><ul><li><a href=/posts/kbhsu_engr76_apr182024/#id-40a9278a-e67e-44da-ac09-69ba3daa24bc-fourier-series-as-exactly-a-shifted-sum-of-sinusoids>Fourier Series as exactly a shifted sum of sinusoids</a></li><li><a href=/posts/kbhfourier_series/#general-fourier-decomposition>General Fourier Decomposition</a></li></ul></li><li><a href=/posts/kbhsu_math53_feb252024/#l-periodicity>L-periodic</a> functions and <a href=/posts/kbhsu_engr76_apr162024/#triangle-wave>triangle wave</a></li><li><a href=/posts/kbhsu_engr76_apr232024/#id-40a9278a-e67e-44da-ac09-69ba3daa24bc-fourier-series-components-form-a-basis>Fourier Series components form a basis</a></li><li><a href=/posts/kbhsu_engr76_apr252024/#bandwidth>Bandwidth</a><ul><li><a href=/posts/kbhsu_engr76_apr252024/#finite-bandwidth-signal>Finite-Bandwidth Signal</a></li></ul></li><li><a href=/posts/kbhsu_engr76_apr252024/#discrete-fourier-transform>Discrete Fourier Transform</a></li><li><a href=/posts/kbhsu_engr76_apr302024/#lossless-sampling>Lossless Sampling</a> and <a href=/posts/kbhsu_engr76_may022024/#id-74c84568-2d0e-4b3c-b7ac-1c3605e08574-nyquist-sampling-theorem>nyquist sampling theorem</a><ul><li><a href=/posts/kbhsu_engr76_apr252024/#finite-bandwidth-signal>Finite-Bandwidth Signal</a></li><li><a href=/posts/kbhsu_engr76_may022024/#passband-signal>Baseband Signal</a> and <a href=/posts/kbhsu_engr76_may022024/#passband-signal>Passband Signal</a></li></ul></li><li><a href=/posts/kbhsu_engr76_may022024/#interpolation>Interpolation</a><ul><li><a href=/posts/kbhsu_engr76_may022024/#zero-hold-interpolation>Zero-Hold Interpolation</a></li><li>best: <a href=/posts/kbhsu_engr76_may022024/#infinite-degree-polynomial-interpolation>Infinite-Degree Polynomial Interpolation</a> (uses <a href>sinc function</a>)<ul><li><a href=/posts/kbhsu_engr76_may022024/#shannon-s-nyquist-theorem>Shannon&rsquo;s Nyquist Theorem</a></li></ul></li></ul></li></ul></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>