<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS109 DEC042023</title><meta name=description content="Diffusion Models
We can consider a model between random noise and trees.
For every step, we sample Gaussian noise and add it to the image. The original approach adds Gaussian to the pixels, and nowadays people replace the pixel.
Usually, there is a few thousand steps of noising.
Why is it that we can&rsquo;t have a one-step policy from noise to pictures? Because of a physics result that says the stability of diffusion becomes intractable at too large steps."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS109 DEC042023</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#diffusion-models>Diffusion Models</a><ul><li><a href=#loss-function>loss function</a></li><li><a href=#elbo>ELBO</a></li><li><a href=#lstms>LSTMs</a></li><li><a href=#cross-entropy>Cross Entropy</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=diffusion-models>Diffusion Models</h2><p>We can consider a model between random noise and trees.</p><p>For every step, we sample Gaussian noise and <strong>add</strong> it to the image. The original approach adds Gaussian to the pixels, and nowadays people replace the pixel.</p><p>Usually, there is a few thousand steps of noising.</p><p>Why is it that we can&rsquo;t have a one-step policy from noise to pictures? Because of a physics result that says the stability of diffusion becomes intractable at too large steps.</p><h3 id=loss-function>loss function</h3><p>One way we can model our objective is as a <a href=/posts/kbhmaximum_likelihood_parameter_learning/>MLE</a>. Because we are continuously adding noise, we can assume that</p><p>\begin{equation}
y \sim \mathcal{N}(\mu = \hat{y}(\theta), \sigma^{2}=k)
\end{equation}</p><p>If you compute MLE over the choice of \(\hat{y}(\theta)\), you get the squared error.</p><h3 id=elbo>ELBO</h3><p>A cool loss function that diffusion actually uses that leverages the fact above but considers the entire diffusion process.</p><h3 id=lstms>LSTMs</h3><p>Big text generation flaw with LSTMs: the latent state vector has to contain information about the ENTIRE sentence and have the information propagated through recursion. Information</p><h3 id=cross-entropy>Cross Entropy</h3><p>its MLE over a multinomials; the counts of everything that&rsquo;s not the one-hot thing just so happens to be 0.</p><p>We are essentially computing the derivative of:</p><p>\begin{equation}
\arg\max_{p_{correct}} p_{correct}
\end{equation}</p><p>which is trying to maximize the categorical of only the correct element.</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>