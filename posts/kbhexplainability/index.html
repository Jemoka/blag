<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>explainability</title>
<meta name=description content="
Explainability is the study of, when stuff breaks, understanding why it does.

Here are a set of explainability techniques!
policy visualization

Roll your system out and look at it

Some common strategies that people use to do this:

plot the policy: look at what the agent says to do at each state (if you have too many dimensions, just plot slices!)
slicing: one way to deal with history-dependent trajectories is to then just count the number of actions that your system takes at each step, and plot the argmax of it

feature importance
Our goal is still to understand the contribution of various features to the overall behavior of a system."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>explainability</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#policy-visualization>policy visualization</a></li><li><a href=#feature-importance>feature importance</a><ul><li><a href=#sensitivity-analysis>sensitivity analysis</a></li><li><a href=#shapley-values>shapley values</a></li></ul></li><li><a href=#surrogate-models>surrogate models</a></li></ul></nav></aside><main><article><div><div class=definition><span><p>Explainability is the study of, when stuff breaks, understanding why it does.</p><p></span></div></p><p>Here are a set of explainability techniques!</p><h2 id=policy-visualization>policy visualization</h2><div class=definition><span><p>Roll your system out and look at it</p><p></span></div></p><p>Some common strategies that people use to do this:</p><ul><li>plot the policy: look at what the agent says to do at each state (if you have too many dimensions, just plot slices!)</li><li><strong>slicing</strong>: one way to deal with history-dependent trajectories is to then just count the number of actions that your system takes at each step, and plot the argmax of it</li></ul><h2 id=feature-importance>feature importance</h2><p>Our goal is still to understand the contribution of various features to the overall behavior of a system.</p><h3 id=sensitivity-analysis>sensitivity analysis</h3><div class=definition><span><p><a href=#feature-importance>sensitivity analysis</a> allows us to understand how a particular output changes when a single feature is changed</p><ul><li>take a feature</li><li>screw with it</li><li>how does it contribute to the variance of the outcomes?</li></ul><p></span></div></p><div class=theorem><span><p>this is really slow</p><p></span></div></p><div class=proof><span><p>&mldr;.because preturbing each input sequentially is exponential in search space.</p><p></span></div></p><p>So instead, we could consider something like a</p><div class=definition><span><p>take the gradient of the output with respect to the input, and measure what they produce</p><p></span></div></p><p>this doesn&rsquo;t really handle gradients that are saturated (i.e. the changes were big, but once you get big enough the function stops changing). So instead, we could consider <a href=#feature-importance>integrated gradients</a>:</p><div class=definition><span><p>For function \(f\) under test, and feature perturbation \(x \in [x_0, x_1]\), we compute:</p><p>\begin{equation}
\frac{1}{x_1 - x_0} \int_{x_0}^{x_1} f\qty(x) \dd{x}
\end{equation}</p><p></span></div></p><h3 id=shapley-values>shapley values</h3><p>One problem with <a href=#feature-importance>sensitivity analysis</a> is that competing feature effects neutralizes: that is, if \(z = x \vee y\), preturbing \(x\) or \(y\) alone will not have any influence on the value of \(z\). <a href=#shapley-values>shapley values</a> helps us understand the subsets of features.</p><div class=definition><span><p>The Shapley Value is the expectation of the difference across all possible subsets of features.</p><ol><li>randomly fix a subset and randomly sample values in the subset</li><li>compute the target value</li><li>repeat 1-2 with the feature under test included in the randomly sampled subset</li><li>compute the difference between the case where you included and not included the target feature</li><li>compute the expectation of 4</li></ol><p></span></div></p><h2 id=surrogate-models>surrogate models</h2><p>see</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>