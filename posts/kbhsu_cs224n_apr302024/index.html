<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS224N APR302024</title>
<meta name=description content="Subword We use SUBWORD modeling modeling to deal with:
combinatorial morphology (resolving word form and infinitives) &mdash; &ldquo;a single word has a million forms in Finnish&rdquo; (&ldquo;transformify&rdquo;) misspelling extensions/emphasis (&ldquo;gooooood vibessssss&rdquo;) You mark each actual word ending with some of combine marker.
To fix this:
Byte-Pair Encoding &ldquo;find pieces of words that are common and treat them as a vocabulary&rdquo;
start with vocab containing only characters and EOS look at the corpus, and find the most common pair of adjacent characters replace all instances of the pair with the new subword repeat 2-3 until vecab size is big enough Writing Systems phonemic (directly translating sounds, see Spanish) fossilized phonemic (English, where sounds are whack) syllabic/moratic (each sound syllable written down) ideographic (syllabic, but no relation to sound instead have meaning) a combination of the above (Japanese) Whole-Model Pretraining all parameters are initalized via pretraining don&rsquo;t even bother training word vectors MLM and NTP are &ldquo;Universal Tasks&rdquo; Because in different circumstances, performing well MLM and NLP requires {local knowledge, scene representations, language, etc."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS224N APR302024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#subword>Subword</a><ul><li><a href=#byte-pair-encoding--kbhbpe-dot-md><a href=HAHAHUGOSHORTCODE1236s0HBHB>Byte-Pair Encoding</a></a></li></ul></li><li><a href=#writing-systems>Writing Systems</a></li><li><a href=#whole-model-pretraining>Whole-Model Pretraining</a></li><li><a href=#mlm-and-ntp-are-universal-tasks>MLM and NTP are &ldquo;Universal Tasks&rdquo;</a></li><li><a href=#why-pretraining>Why Pretraining</a></li><li><a href=#types-of-architecture>Types of Architecture</a><ul><li><a href=#encoders>Encoders</a></li><li><a href=#encoder-decoder>Encoder/Decoder</a></li><li><a href=#decoder>Decoder</a></li><li><a href=#in-context-learning>In-Context Learning</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=subword>Subword</h2><p>We use SUBWORD modeling modeling to deal with:</p><ol><li>combinatorial morphology (resolving word form and infinitives) &mdash; &ldquo;a single word has a million forms in Finnish&rdquo; (&ldquo;transformify&rdquo;)</li><li>misspelling</li><li>extensions/emphasis (&ldquo;gooooood vibessssss&rdquo;)</li></ol><p>You mark each actual word ending with some of combine marker.</p><p>To fix this:</p><h3 id=byte-pair-encoding--kbhbpe-dot-md><a href=/posts/kbhbpe/>Byte-Pair Encoding</a></h3><p>&ldquo;find pieces of words that are common and treat them as a vocabulary&rdquo;</p><ol><li>start with vocab containing only characters and EOS</li><li>look at the corpus, and find the most common pair of adjacent characters</li><li>replace all instances of the pair with the new subword</li><li>repeat 2-3 until vecab size is big enough</li></ol><h2 id=writing-systems>Writing Systems</h2><ul><li>phonemic (directly translating sounds, see Spanish)</li><li>fossilized phonemic (English, where sounds are whack)</li><li>syllabic/moratic (each sound syllable written down)</li><li>ideographic (syllabic, but no relation to sound instead have meaning)</li><li>a combination of the above (Japanese)</li></ul><h2 id=whole-model-pretraining>Whole-Model Pretraining</h2><ul><li>all parameters are initalized via pretraining</li><li>don&rsquo;t even bother training word vectors</li></ul><h2 id=mlm-and-ntp-are-universal-tasks>MLM and NTP are &ldquo;Universal Tasks&rdquo;</h2><p>Because in different circumstances, performing well MLM and NLP requires {local knowledge, scene representations, language, etc.}.</p><h2 id=why-pretraining>Why Pretraining</h2><ul><li>maybe local minima near pretraining weights generalize well</li><li>or maybe, because the outputs are sensible, gradients propagate nicely because they are modulated well</li></ul><h2 id=types-of-architecture>Types of Architecture</h2><h3 id=encoders>Encoders</h3><ul><li>bidirectional context</li><li>can condition on the future</li></ul><h4 id=bert>Bert</h4><ol><li>replace input word with [mask] 80% of time</li><li>replace <strong>input word</strong> with a <strong>RANDOM WORD</strong> 10% of the time</li><li>leaving the word unchanged 10% of the time</li></ol><p>i.e. BERT will then need to resolve a proper sentence representation from lots of noise</p><p>Original BERT <em>also</em> pretrained on top a next sentence prediction loss in addition to MLM, but that ended up being unnecessary.</p><ul><li><p>Bertish</p><ol><li>RoBERTa - train on longer context</li><li>SpanBert - mask a span</li></ol></li></ul><h3 id=encoder-decoder>Encoder/Decoder</h3><ul><li>do both</li><li>pretraining maybe hard</li></ul><h4 id=t5>T5</h4><p>Encoder/Decoder model. Pretraining task: blank inversion:</p><p>&ldquo;Thank you for inviting me to your party last week&rdquo;</p><p>&ldquo;Thank you &lt;x> to your &lt;y> last week&rdquo; =>
&ldquo;&lt;x> for inviting &lt;y> party &lt;z></p><p>This actually is BETTER than the LM training objective.</p><h3 id=decoder>Decoder</h3><ul><li>general LMs use this</li><li>nice to generate from + cannot condition no future words</li></ul><h3 id=in-context-learning>In-Context Learning</h3><ul><li>really only very capable at hundreds of billion parameters</li><li>uses no gradient steps&mdash;-repeat and attend to examples</li></ul></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>