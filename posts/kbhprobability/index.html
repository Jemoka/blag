<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>probability</title>
<meta name=description content="probability of an event is the proportion of times the event occurs in many repeated trials. It is &ldquo;our belief that an event \(E\) occurs&rdquo;.
Frequentist Definition of Probability That is, it is a number between \(0-1\). Whereby:
\begin{equation} P(E) = \lim_{n \to \infty} \frac{n(E)}{n} \end{equation}
&ldquo;frequentist definition of probability&rdquo;
probability is the ratio between the number of times \(E\) occurring \(n(E)\) divided by the number of times you did the thing \(n\)."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>probability</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#frequentist-definition-of-probability>Frequentist Definition of Probability</a></li><li><a href=#uncertainty--kbhuncertainty-dot-md--and-probability--kbhprobability-dot-md><a href=HAHAHUGOSHORTCODE926s2HBHB>uncertainty</a> and <a href=HAHAHUGOSHORTCODE926s3HBHB>probability</a></a></li><li><a href=#axiom-of-probability>axiom of probability</a></li><li><a href=#conditional-probability>conditional probability</a></li><li><a href=#law-of-total-probability>law of total probability</a></li><li><a href=#bayes-rule>Bayes rule</a></li><li><a href=#independence>independence</a></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhprobability/>probability</a> of an event is the proportion of times the event occurs in many repeated trials. It is &ldquo;our belief that an event \(E\) occurs&rdquo;.</p><h2 id=frequentist-definition-of-probability>Frequentist Definition of Probability</h2><p>That is, it is a number between \(0-1\). Whereby:</p><p>\begin{equation}
P(E) = \lim_{n \to \infty} \frac{n(E)}{n}
\end{equation}</p><p>&ldquo;frequentist definition of probability&rdquo;</p><p>probability is the ratio between the number of times \(E\) occurring \(n(E)\) divided by the number of times you did the thing \(n\). This system converge because of the <a href=/posts/kbhlaw_of_large_numbers/>law of large numbers</a>.</p><h2 id=uncertainty--kbhuncertainty-dot-md--and-probability--kbhprobability-dot-md><a href=/posts/kbhuncertainty/>uncertainty</a> and <a href=/posts/kbhprobability/>probability</a></h2><p>Say you are training some kind of model. When it says \(0.8\) for motorcycle, its not that there are \(80\%\) chance that there&rsquo;s a motorcycle there. Its that the model is \(80\%\) confident that there&rsquo;s a motorcycle.</p><p><strong><strong>Probability can not only represent the world, but our understanding of the world</strong></strong></p><h2 id=axiom-of-probability>axiom of probability</h2><ul><li>\(0 \leq P(E) \leq 1\)</li><li>\(P(S) = 1\), where \(S\) is the <a href=/posts/kbhsample_space/>sample space</a></li><li>if \(E\) and \(F\) are mutually exclusive, \(P(E) + P(F) = P(E \cup F)\)</li></ul><p>This last axiom can be chained</p><hr><p>This results in three correlaries:</p><ul><li>\(P(E^{C}) = 1- P(E)\)</li></ul><p>Proof:
We know that \(E^{C}, E\) are mutually exclusive.</p><p>\begin{equation}
P(E^{C} \cup E) = P(E) + P(E^{C})
\end{equation}</p><p>Now, recall the fact that something happening OR not happening is \(1\).</p><p>So we have:</p><ul><li>\(P(E \cup F) = P(E) + P(F) - P(E \cap F)\)</li><li>if \(E \subset F\), \(P(E) \leq P(F)\)</li></ul><h2 id=conditional-probability>conditional probability</h2><p>&ldquo;What is the new belief that something \(E\) happened, conditioned upon the fact that we know that \(F\) already happened.&rdquo;</p><p>Written as: \(P(E|F)\).</p><p>Furthermore, we have:</p><p>\begin{equation}
P (X, Y) = P(X\mid Y) \cdot P(Y)
\end{equation}</p><p>In this case, we call \(Y\) the &ldquo;evidence&rdquo;. this allows us to find &ldquo;what is the chance of \(x\) given \(y\)&rdquo;.</p><p>We can continue this to develop the <a href=#conditional-probability>probability chain rule</a>:</p><p>\begin{equation}
P(A_1, A_2 \dots, A_{n}) = P(A_{n} \mid A_1, A_2 \dots A_{n-1})P(A_1, A_2 \dots A_{n-1})
\end{equation}</p><p>and so:</p><p>\begin{equation}
P(E_1) \cdot P(E_2 | E_1) \cdot E(E_3 | E_1E_2) \cdot P(E_4 | E_1E_2E_3) \cdot \dots \cdot
\end{equation}</p><p>and so on.</p><p>If you are performing the chain rule on something that&rsquo;s already conditioned:</p><p>\begin{equation}
P(X,Y|A)
\end{equation}</p><p>you can break it up just remembering that \(A\) needs to be preserved as a condition, so:</p><p>\begin{equation}
P(X,Y|A) = P(X|Y,A) P(Y|A)
\end{equation}</p><p>Now:</p><p>\begin{equation}
\sum_{x}^{} p(x \mid y) = 1
\end{equation}</p><p>because this is <strong>still</strong> a probability over \(x\).</p><h2 id=law-of-total-probability>law of total probability</h2><p>say you have two variables \(x, y\).</p><p>&ldquo;what&rsquo;s the probablity of \(x\)&rdquo;</p><p>\begin{equation}
P(x) = \sum_{Y} P(x,y)
\end{equation}</p><p>a.k.a.:</p><p>\begin{equation}
p(x) = p(x|y_1)p(y_1) + \dots + p(x|y_{n})y_{n}
\end{equation}</p><p>by applying <a href=#conditional-probability>conditional probability</a> formula upon each term</p><p>This is because:</p><p>\begin{align}
p(x) &= p(x|y_1)p(y_1) + \dots + p(x|y_{n})y_{n} \\
&= p(x, y_1) + \dots + p(x, y_{n})
\end{align}</p><p>If its not conditional, it holds too:</p><p>\begin{equation}
p(AB^{C}) + p(AB)
\end{equation}</p><h2 id=bayes-rule>Bayes rule</h2><p>See: <a href=/posts/kbhbayes_theorem/>Bayes Theorem</a></p><h2 id=independence>independence</h2><p>If \(X\) and \(Y\) are independent (written as \(X \perp Y\)), we know that \(P(x,y) = P(x)P(y)\) for all \(x, y\).</p><p>Formally:</p><p>\begin{equation}
P(A) = P(A|B)
\end{equation}</p><p>if \(A\) and \(B\) is <a href=#independence>independent</a>. That is, \(P(AB) = P(A) \cdot P(B)\). You can check either of these statements (the latter is usually easier).</p><p><a href=#independence>Independence</a> is bidirectional. If \(A\) is independent of \(B\), then \(B\) is independent of \(A\). To show this, invoke the <a href=/posts/kbhbayes_theorem/>Bayes Theorem</a>.</p><p>This is generalized:</p><p>\begin{equation}
P(x_1, \dots, x_n) = P(x_1) \dots p(x_{n})
\end{equation}</p><p>and this tells us that subset of \(x_{j}\) is independent against each other.</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>