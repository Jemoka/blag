<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Language Model</title>
<meta name=description content="An LM predicts this distribution (&ldquo;what&rsquo;s the distribution of next word given the previous words):
\begin{equation}
W_{n} \sim P(\cdot | w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)})
\end{equation}
By applying the chain rule, we can also think of the language model as assigning a probability to a sequence of words:
\begin{align}
P(S) &= P(w^{(t)} | w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)}) \cdot P(w^{(t-1)} | w^{(t-2)}, \dots, w^{(1)}) \dots  \\
&= P(w^{(t)}, w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)})
\end{align}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Language Model</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#n-gram-lm>N-Gram LM</a><ul><li><a href=#problems>Problems</a></li></ul></li><li><a href=#neural-lm>Neural LM</a><ul><li><a href=#recurrent-neural-network>Recurrent Neural Network</a></li></ul></li></ul></nav></aside><main><article><div><p>An LM predicts this distribution (&ldquo;what&rsquo;s the distribution of next word given the previous words):</p><p>\begin{equation}
W_{n} \sim P(\cdot | w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)})
\end{equation}</p><p>By applying the chain rule, we can also think of the language model as assigning a probability to a sequence of words:</p><p>\begin{align}
P(S) &= P(w^{(t)} | w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)}) \cdot P(w^{(t-1)} | w^{(t-2)}, \dots, w^{(1)}) \dots \\
&= P(w^{(t)}, w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)})
\end{align}</p><h2 id=n-gram-lm>N-Gram LM</h2><p>We can use the Markov assumption: that a word only dependents on the last \(n-1\) terms.</p><p>By doing this, we can write:</p><p>\begin{align}
P(x_{t} | x_{t-1}, .. x_{0}) &\approx P(x_{t} | x_{t-1}, .. x_{t-n-1}) \\
&= \frac{P(x_{t}, x_{t-1}, .. x_{t-n-1})}{P(x_{t-1}, .. x_{t-n-1})}
\end{align}</p><p>and we can then just use the counts to make this prediction.</p><h3 id=problems>Problems</h3><p>Key idea: <strong>language is extremely sparse</strong></p><ul><li>any <a href=/posts/kbhn_grams/#oov-words>OOV Words</a> has the probability of zero, making it sad; we can kinda solve this by <a href=/posts/kbhbaysian_parameter_learning/#laplace-smoothing>Laplace Smoothing</a> but not really.</li><li>any given condition is hard to find; we can kinda solve this by <a href=/posts/kbhn_grams/#stupid-backoff>Stupid Backoff</a></li></ul><p>Conflicting demands: more context is better, but with more context had more sparseness problems.</p><h2 id=neural-lm>Neural LM</h2><p>Same task; we want a model for</p><p>\begin{equation}
W_{n} \sim P(\cdot | w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)})
\end{equation}</p><p>We use the same Markov assumption, and then use a fixed local window of text to try to predict the next word.</p><p>Naively learning from concatenated word vectors is bad, because that&rsquo;s weight are not length agnostic&mdash;that is, the <strong>position</strong> in the context in which a word occurs is very sensitive.</p><h3 id=recurrent-neural-network>Recurrent Neural Network</h3><p>Key <a href=#recurrent-neural-network>RNN</a> idea &mdash; apply the same weights \(W\) repeatedly. Each of our hidden state would be the sum of the previous hidden state by a matrix, and multiply the new word by a matrix. So each step:</p><p>\begin{equation}
h_{t} = \sigma \qty( W_{h}h_{t-1} + W_{e} e_{t} + b)
\end{equation}</p><p>Our LM input uses &ldquo;Teacher Forcing&rdquo;: i.e. we only generate a single input.</p><p>Usually we define our loss as a multilpying the loss with \(\frac{1}{T}\) where \(T\) is the token count per doc.</p><h4 id=problems>Problems</h4><ul><li>slow</li><li>hard to look into the past dramatically</li></ul><h4 id=gradient>Gradient</h4><p>&ldquo;gradient w.r.t. a repeated weight is the sum of the gradient for each time where it appears&rdquo;</p><p>Recall that gradient sum at outer branches, so calculate gradient at each step.</p><p>Sometimes you would give up after backdropping \(n\) steps (not all the way into history.</p><p>\begin{equation}
\pdv{J}{W_{h}} = \sum_{i=1}^{t} \pdv{J^{(t)}}{W_{h}}
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>