<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Zhu 2021</title>
<meta name=description content="DOI: 10.3389/fcomp.2021.624683
One-Liner
late fusion of multimodal signal on the CTP task using transformers, mobilnet, yamnet, and mockingjay
Novelty

Similar to  Martinc 2021 and Shah 2021 but actually used the the current Neural-Network state of the art
Used late fusion again after the base model training
Proposed that inconsistency in the diagnoses of MMSE scores could be a great contributing factor to multi-task learning performance hindrance

Notable Methods

Proposed base model for transfer learning from text based on MobileNet (image), YAMNet (audio), Mockingjay (speech) and BERT (text)
Data all sourced from recording/transcribing/recognizing CTP task

Key Figs
Figure 3 and 4



This figure tells us the late fusion architecture used"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Zhu 2021</h1><span class=tagbox><span class=tag onclick='window.location.href="/tags/ntj"'><span class=hash>#</span>
<span class=tagname>ntj</span></span></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#one-liner>One-Liner</a></li><li><a href=#novelty>Novelty</a></li><li><a href=#notable-methods>Notable Methods</a></li><li><a href=#key-figs>Key Figs</a><ul><li><a href=#figure-3-and-4>Figure 3 and 4</a></li><li><a href=#table-2>Table 2</a></li><li><a href=#table-3>Table 3</a></li></ul></li></ul></nav></aside><main><article><div><p>DOI: 10.3389/fcomp.2021.624683</p><h2 id=one-liner>One-Liner</h2><p><a href=/posts/kbhfusion/#late-fusion>late fusion</a> of multimodal signal on the <a href=/posts/kbhctp/>CTP</a> task using transformers, mobilnet, yamnet, and mockingjay</p><h2 id=novelty>Novelty</h2><ul><li>Similar to <a href=/posts/kbhmartinc_2021/>Martinc 2021</a> and <a href=/posts/kbhshah_2021/>Shah 2021</a> but actually used the the current Neural-Network state of the art</li><li>Used <a href=/posts/kbhfusion/#late-fusion>late fusion</a> again after the base model training</li><li>Proposed that inconsistency in the diagnoses of <a href=/posts/kbhmmse/>MMSE</a> scores could be a great contributing factor to multi-task learning performance hindrance</li></ul><h2 id=notable-methods>Notable Methods</h2><ul><li>Proposed base model for transfer learning from text based on MobileNet (image), YAMNet (audio), Mockingjay (speech) and BERT (text)</li><li>Data all sourced from recording/transcribing/recognizing <a href=/posts/kbhctp/>CTP</a> task</li></ul><h2 id=key-figs>Key Figs</h2><h3 id=figure-3-and-4>Figure 3 and 4</h3><figure><img src=/ox-hugo/2022-06-25_10-54-21_screenshot.png></figure><p>This figure tells us the <a href=/posts/kbhfusion/#late-fusion>late fusion</a> architecture used</p><h3 id=table-2>Table 2</h3><figure><img src=/ox-hugo/2022-06-25_10-55-53_screenshot.png></figure><p>Pre-training with an existing dataset had (not statistically quantified) improvement against a randomly seeded model.</p><h3 id=table-3>Table 3</h3><figure><img src=/ox-hugo/2022-06-25_10-56-22_screenshot.png></figure><p>Concat/Add fusion methods between audio and text provided even better results; confirms <a href=/posts/kbhmartinc_2021/>Martinc 2021</a> on newer data</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>