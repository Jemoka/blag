<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS224N APR112024</title><meta name=description content="Linguistic Structure
Humans somehow turn linear into complex meaning with bigger, non-linear units. We need to make explicit this structural complexity. Sometimes, this is even ambiguous.
We can use this to extract information from human languages.
Why is this hard?

coding: global clarity, local ambiguity (number of white spaces doesn&rsquo;t matter, but code always have one exact meaning)
speaking: global ambiguity, local clarity (words are always clearly said, but what they refer to maybe unclear)

Prepositional Ambiguity
Why? &mdash; Prepositional Phrase does not have clear attachment. The sequence of possible attachments grows exponentially."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS224N APR112024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#linguistic-structure>Linguistic Structure</a><ul><li><a href=#why-is-this-hard>Why is this hard?</a></li><li><a href=#two-representations>Two Representations</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=linguistic-structure>Linguistic Structure</h2><p>Humans somehow turn linear into complex meaning with bigger, non-linear units. We need to make explicit this structural complexity. Sometimes, this is even ambiguous.</p><p>We can use this to extract information from human languages.</p><h3 id=why-is-this-hard>Why is this hard?</h3><ul><li>coding: <strong>global clarity, local ambiguity</strong> (number of white spaces doesn&rsquo;t matter, but code always have one exact meaning)</li><li>speaking: <strong>global ambiguity, local clarity</strong> (words are always clearly said, but what they refer to maybe unclear)</li></ul><h4 id=prepositional-ambiguity>Prepositional Ambiguity</h4><p>Why? &mdash; Prepositional Phrase does not have clear attachment. The sequence of possible attachments grows exponentially.</p><h4 id=coordination-scope-ambiguity>Coordination Scope Ambiguity</h4><h3 id=two-representations>Two Representations</h3><h4 id=phrase-structure-grammar>Phrase-Structure Grammar</h4><p><a href=#phrase-structure-grammar>Phrase-Structure Grammar</a> uses <a href>Context-Free Grammar</a>s to represent language into broken phrases (nobody actually does this unless you want to go build a <a href=#phrase-structure-grammar>Constituency Parsing</a>.)</p><ol><li>starting with word units (perform <a href=/posts/kbhpos_tagging/>POS Tagging</a>)</li><li>combine words into phrase units (create NPs, VPs, etc.)</li><li>combine phrases into bigger phrases (build up sentences with phrase-level grammars)</li></ol><p>With this parsing information, you can force a <a href>Context-Free Grammar</a> based on what could be possible:</p><p>And you can use this CFG to parse our text (not very well&mldr;. because of ambiguity).</p><h4 id=dependency-grammar>Dependency Grammar</h4><p>Dependency structure uses a graph between words to represents relationship directly on the world level.</p><p>&ldquo;Which word is the head word, and what does it modify?&rdquo;</p><ol><li>find head word</li><li>which things modify the head word</li></ol><p>repeat</p><ul><li><p>Dependency Structure</p><p>We begin with the headword of a sentence (usually the verb); and then, we go through the text and connect the head to each dependent; this forms a tree with the head word on top, and each word POINTS to its dependent (i.e. headword on top).</p><p>Sometimes people point from dependent to head, but that&rsquo;s lame. So we point from HEAD to dependent (ROOT => Verb, and so on).</p></li></ul><ul><li><p>Sources of Dependency Information</p><ol><li>Bilexical affinities &mdash; is a particular pairwise dependency possible?</li><li>Dependency distance &mdash; most, but not all dependencies are between nearby words</li><li>Intervening material &mdash; dependencies rarely span intervening words or punctuations</li><li>valency &mdash;- how many dependencies on which side are usual for a head?</li></ol></li></ul><ul><li><p>Non-Projective Dependencies</p><p>Dependencies can sometimes cross each other</p></li></ul><ul><li><p>Parsing Mechanism</p><ul><li><p>Greedy Transition-Based Parsing</p><ul><li>take a stack, \(\sigma\), written with top tot the right</li><li>take a buffer, \(\beta\), written with to to the left</li><li>a set of dependency arcs \(A\), which we predict</li></ul><p>Three operations: push one word into the stack (&ldquo;shift&rdquo;), link last word on stack to second to last word (&ldquo;arc-left&rdquo;), link second to last word on stack to last word on stack (&ldquo;arc-right&rdquo;)</p></li></ul><ul><li><p>Graph Based Parsing</p><p>ask each word &ldquo;what am I a dependent of?&rdquo; So, it becomes a classification problem per word vs. other adjacent words. this is what Stanza uses</p></li></ul><ul><li><p>Evaluating Depparse</p><p>We assess whether or not we have a good parser by checking whether or not the parsed arcs are good.</p><ul><li><p>Unlabeled Dependency Score</p><p>We consider the P/R/A/F of tuples of (head, dep, word, word), considering only exact matches as positive.</p></li></ul><ul><li><p>Labeled Dependency Score</p><p>We consider the P/R/A/F of tuples of (head, dep, word, word, arc-label), considering only exact matches as positive.</p></li></ul></li></ul></li></ul></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>