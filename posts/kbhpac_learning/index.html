<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>PAC Learning</title>
<meta name=description content="probably approximately correct (PAC) learning is a DFA learning scheme. Suppose you have a concept \(c \in C\). We desire to find a hypothesis \(h \in H\) which gets as close to the boundary between concepts as possible.
We want to minimize false positives and false negatives.
constituents

Instance space \(X\)
Concept class \(C\) (functions over \(X\))
Hypothesis class \(H\) (functions over \(X\))

&ldquo;proper learning&rdquo; \(H=C\) &mdash;we are done


\(A\) PAC-learns \(C\) if

\(\forall c \in C, \forall D \sim X\), when \(A\) gets inputs sampled from \(D\) and outputs \(h \in H\), we want&mldr;



\begin{equation}
P_{A} [ P_{x \in D}[h(x) \neq c(x)] > \delta] < \epsilon
\end{equation}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>PAC Learning</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#constituents>constituents</a></li><li><a href=#occam-s-razor>Occam&rsquo;s Razor</a></li><li><a href=#pac-learn-a-dfa>PAC-learn a DFA</a></li></ul></nav></aside><main><article><div><p>probably approximately correct (PAC) learning is a <a href=/posts/kbhdeterministic_finite_automata/>DFA</a> learning scheme. Suppose you have a concept \(c \in C\). We desire to find a hypothesis \(h \in H\) which gets as close to the boundary between concepts as possible.</p><p>We want to minimize false positives and false negatives.</p><h2 id=constituents>constituents</h2><ul><li>Instance space \(X\)</li><li>Concept class \(C\) (functions over \(X\))</li><li>Hypothesis class \(H\) (functions over \(X\))<ul><li>&ldquo;proper learning&rdquo; \(H=C\) &mdash;we are done</li></ul></li><li>\(A\) PAC-learns \(C\) if<ul><li>\(\forall c \in C, \forall D \sim X\), when \(A\) gets inputs sampled from \(D\) and outputs \(h \in H\), we want&mldr;</li></ul></li></ul><p>\begin{equation}
P_{A} [ P_{x \in D}[h(x) \neq c(x)] > \delta] &lt; \epsilon
\end{equation}</p><p>our learning scheme wants the probability of an error beyond a super large \(\delta\) to be small \(&lt; \epsilon\).</p><h2 id=occam-s-razor>Occam&rsquo;s Razor</h2><p>any algorithm \(A\) that outputs a small hypothesis \(h\) that works on the input, then \(A\) has PAC-learned and its likely to generalize (you have to get very unlucky to have a simple explanation to explain a large bunch of input because its quite hard to overfit).</p><h2 id=pac-learn-a-dfa>PAC-learn a DFA</h2><p><a href=#occam-s-razor>Occam&rsquo;s Razor</a>, we should just keep building a <a href=/posts/kbhdeterministic_finite_automata/>DFA</a> until you get the right one starting from the smallest DFA.</p><p>butt&mldr; that&rsquo;s exponential. so:</p><p>let&rsquo;s define \(L^{?}\) such that either \(w \in L^{?}\), or \(w \not \in L^{?}\) , or we don&rsquo;t know. We want to say \(L^{?}\) distinguishes \(x\) and \(y\), then there exists some \(z\) such that \(x z \in L^{?}\) and \(y z \not \in L^{?}\) or vise versa.</p><p>this is not an equivalence relation, beause ther&rsquo;s is no transitivityy.</p><p>you probably can&rsquo;t though, learn DFAs actively; you can learn automata actively.</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>