<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>NLP Index</title>
<meta name=description content="Learning Goals Effective modern methods for deep NLP Word vectors, FFNN, recurrent networks, attention Transformers, encoder/decoder, pre-training, post-training (RLHF, SFT), adaptation, interoperability, agents Big picture in HUMAN LANGUAGES why are they hard why using computers to deal with them are doubly hard Making stuff (in PyTorch) word meaning dependency parsing machine translation QA Lectures SU-CS224N APR022024 SU-CS224N APR092024 SU-CS224N APR112024 SU-CS224N APR162024 SU-CS224N APR182024 SU-CS224N APR232024 SU-CS224N APR252024 SU-CS224N APR302024 SU-CS224N MAY022024 SU-CS224N MAY072024 "><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>NLP Index</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#learning-goals>Learning Goals</a></li><li><a href=#lectures>Lectures</a></li></ul></nav></aside><main><article><div><h2 id=learning-goals>Learning Goals</h2><ol><li>Effective modern methods for deep NLP<ul><li>Word vectors, FFNN, recurrent networks, attention</li><li>Transformers, encoder/decoder, pre-training, post-training (RLHF, SFT), adaptation, interoperability, agents</li></ul></li><li>Big picture in HUMAN LANGUAGES<ul><li>why are they hard</li><li>why using computers to deal with them are doubly hard</li></ul></li><li>Making stuff (in PyTorch)<ul><li>word meaning</li><li>dependency parsing</li><li>machine translation</li><li>QA</li></ul></li></ol><h2 id=lectures>Lectures</h2><ul><li><a href=/posts/kbhsu_cs224n_apr022024/>SU-CS224N APR022024</a></li><li><a href=/posts/kbhsu_cs224n_apr092024/>SU-CS224N APR092024</a></li><li><a href=/posts/kbhsu_cs224n_apr112024/>SU-CS224N APR112024</a></li><li><a href=/posts/kbhsu_cs224n_apr162024/>SU-CS224N APR162024</a></li><li><a href=/posts/kbhsu_cs224n_apr182024/>SU-CS224N APR182024</a></li><li><a href=/posts/kbhsu_cs224n_apr232024/>SU-CS224N APR232024</a></li><li><a href=/posts/kbhsu_cs224n_apr252024/>SU-CS224N APR252024</a></li><li><a href=/posts/kbhsu_cs224n_apr302024/>SU-CS224N APR302024</a></li><li><a href=/posts/kbhsu_cs224n_may022024/>SU-CS224N MAY022024</a></li><li><a href=/posts/kbhsu_cs224n_may072024/>SU-CS224N MAY072024</a></li></ul></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>