<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Transformer Speech Diarization</title><meta name=description content="Background Current deep-learning first approaches have shown promising results for the speech text diarization task. For ASR-independent diarization, specifically, two main methods appear as yielding fruitful conclusions:
Auditory feature extraction using deep learning to create a trained, fixed-size latent representation via Mel-frequency cepstral coefficients slices that came from any existing voice-activity detection (VAD) scheme ((Snyder et al. 2018)), where the features extracted with the neural network are later used with traditional clustering and Variational Bayes refinement ((Sell et al."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Transformer Speech Diarization</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#background>Background</a></li><li><a href=#motivation>Motivation</a></li></ul></nav></aside><main><article><div><h2 id=background>Background</h2><p>Current deep-learning first approaches have shown promising results for the speech text diarization task. For ASR-independent diarization, specifically, two main methods appear as yielding fruitful conclusions:</p><ol><li><p>Auditory feature extraction using deep learning to create a trained, fixed-size latent representation via Mel-frequency cepstral coefficients slices that came from any existing voice-activity detection (VAD) scheme ((<a href=#citeproc_bib_item_6>Snyder et al. 2018</a>)), where the features extracted with the neural network are later used with traditional clustering and Variational Bayes refinement ((<a href=#citeproc_bib_item_5>Sell et al. 2018</a>; <a href=#citeproc_bib_item_2>Landini et al. 2022</a>)) approaches to produce groups of diarized speakers</p></li><li><p>End-to-end neural approaches which takes temporally-dependent log-mel-frequency cepstrum and perform voice activity detection, speaker recognition, and diarization directly on the same neural network ((<a href=#citeproc_bib_item_1>Fujita et al. 2019</a>))</p><figure><img src=/ox-hugo/2023-04-09_23-26-04_screenshot.png></figure></li></ol><p>The latter, end-to-end approach, offers lower Diarization Error Rate (DER) than former clustering ((<a href=#citeproc_bib_item_1>Fujita et al. 2019</a>)), achiving 10.76 vs. 11.53 DER on the CALLHOME dataset respectively. However, it confers a few disadvantages: the end-to-end system produces a diarization result directly dependent on the time dimension of the input Log-Mel (i.e. it outputs probability per speaker per time slice), so its error could include <em>both</em> the error in voice activity detection and diarization; furthermore, the one-shot nature of this method allows no interpretation or manipulation of its actual outputs&mdash;such as specifying the number of speakers <em>after</em> diarization is completed (as is possible with clustering because one could simply choose the number of centroids to calculate) (<a href=#citeproc_bib_item_3>Park et al. 2021</a>).</p><p>We therefore desire here to combine the advantages of both methods discussed here in producing a diarization technique that both retains the flexible nature of vector-based approaches but also seeks to generate as complete and performant (in terms of DER) a pipeline as possible with deep learning.</p><h2 id=motivation>Motivation</h2><p>The discussion here is motivated by a few facts:</p><ol><li>Excellent ((<a href=#citeproc_bib_item_4>Radford et al. 2022</a>)) ASR models exist without being pre-trained on the diarization task, meaning they produce transcriptions without the speakers labels</li><li>The number of speakers is not exogenously known, yet could be specified after diarization</li></ol><style>.csl-entry{text-indent:-1.5em;margin-left:1.5em}</style><div class=csl-bib-body><div class=csl-entry><a id=citeproc_bib_item_1></a>Fujita, Yusuke, Naoyuki Kanda, Shota Horiguchi, Yawen Xue, Kenji Nagamatsu, and Shinji Watanabe. 2019. “End-to-End Neural Speaker Diarization with Self-Attention.” In <i>2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</i>, 296–303. IEEE.</div><div class=csl-entry><a id=citeproc_bib_item_2></a>Landini, Federico, Ján Profant, Mireia Diez, and Lukáš Burget. 2022. “Bayesian HMM Clustering of X-Vector Sequences (VBx) in Speaker Diarization: Theory, Implementation and Analysis on Standard Tasks.” <i>Computer Speech & Language</i> 71 (January): 101254. doi:<a href=https://doi.org/10.1016/j.csl.2021.101254>10.1016/j.csl.2021.101254</a>.</div><div class=csl-entry><a id=citeproc_bib_item_3></a>Park, Tae Jin, Naoyuki Kanda, Dimitrios Dimitriadis, Kyu J. Han, Shinji Watanabe, and Shrikanth Narayanan. 2021. “A Review of Speaker Diarization: Recent Advances with Deep Learning.” arXiv. <a href=http://arxiv.org/abs/2101.09624>http://arxiv.org/abs/2101.09624</a>.</div><div class=csl-entry><a id=citeproc_bib_item_4></a>Radford, Alec, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. “Robust Speech Recognition via Large-Scale Weak Supervision.” <i>arXiv Preprint arXiv:2212.04356</i>.</div><div class=csl-entry><a id=citeproc_bib_item_5></a>Sell, Gregory, David Snyder, Alan McCree, Daniel Garcia-Romero, Jesús Villalba, Matthew Maciejewski, Vimal Manohar, et al. 2018. “Diarization Is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge.” In <i>Interspeech 2018</i>, 2808–12. ISCA. doi:<a href=https://doi.org/10.21437/Interspeech.2018-1893>10.21437/Interspeech.2018-1893</a>.</div><div class=csl-entry><a id=citeproc_bib_item_6></a>Snyder, David, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. 2018. “X-Vectors: Robust DNN Embeddings for Speaker Recognition.” In <i>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 5329–33. Calgary, AB: IEEE. doi:<a href=https://doi.org/10.1109/ICASSP.2018.8461375>10.1109/ICASSP.2018.8461375</a>.</div></div></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>