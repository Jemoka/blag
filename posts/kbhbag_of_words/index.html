<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Bag of Words</title>
<meta name=description content="Bag of Words is an order-free representation of a corpus. Specifically, each word has a count which we assign to each word without any other information about ordering, etc.
With the Bayes Theorem, we have:
\begin{equation}
C_{MAP} = \arg\max_{c \in C} P(d|c)P( c)
\end{equation}
where \(d\) is the document, and \(c\) is the class.
So, given a document is a set of a bunch of words:
\begin{equation}
C_{MAP} = \arg\max_{c\in C} P(x_1, \dots, x_{n}|c)P( c)
\end{equation}"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Bag of Words</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#naive-bayes-for-text-classification>Naive Bayes for Text Classification</a><ul><li><a href=#p--c>\(P( c)\)</a></li><li><a href=#p--x-1-dot-dot-dot-x-n>\(P(x_1, &mldr;, x_{n})\)</a></li><li><a href=#parameter-estimation>Parameter Estimation</a></li><li><a href=#unknown-words>Unknown Words</a></li><li><a href=#binary-naive-bayes>Binary Naive Bayes</a></li><li><a href=#benefits>Benefits</a></li></ul></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhbag_of_words/>Bag of Words</a> is an order-free representation of a <a href=/posts/kbhcorpus/>corpus</a>. Specifically, each word has a count which we assign to each word without any other information about ordering, etc.</p><p>With the <a href=/posts/kbhbayes_theorem/>Bayes Theorem</a>, we have:</p><p>\begin{equation}
C_{MAP} = \arg\max_{c \in C} P(d|c)P( c)
\end{equation}</p><p>where \(d\) is the document, and \(c\) is the class.</p><p>So, given a document is a set of a bunch of words:</p><p>\begin{equation}
C_{MAP} = \arg\max_{c\in C} P(x_1, \dots, x_{n}|c)P( c)
\end{equation}</p><h2 id=naive-bayes-for-text-classification>Naive Bayes for Text Classification</h2><p>Assumptions of <a href=/posts/kbhbag_of_words/>Bag of Words</a> for <a href=/posts/kbhnaive_bayes/>Naive Bayes</a></p><h3 id=p--c>\(P( c)\)</h3><p>The right side is just relative frequencies (count of freq divided by count of class).</p><h3 id=p--x-1-dot-dot-dot-x-n>\(P(x_1, &mldr;, x_{n})\)</h3><ul><li><a href=/posts/kbhnaive_bayes/#id-6cdf74a2-2451-47d1-8a62-62aa6dff62c6-naive-bayes-assumption>Naive Bayes assumption</a> (each word&rsquo;s position doesn&rsquo;t matter)</li><li><a href=/posts/kbhbag_of_words/>Bag of Words</a> assumption (assume position doesn&rsquo;t matter)</li></ul><p>So we have:</p><p>\begin{equation}
C_{NB} = \arg\max_{c\in C} P(c_{j}) \prod_{x\in X} P(x|c)
\end{equation}</p><p>We eventually use logs to prevent underflow:</p><p>\begin{equation}
C_{NB} = \arg\max_{c\in C}\log P(c_{j}) +\sum_{x\in X} \log P(x|c)
\end{equation}</p><h3 id=parameter-estimation>Parameter Estimation</h3><p>Because we don&rsquo;t know new words completely decimating probability, we want to establish a <a href=/posts/kbhbaysian_parameter_learning/#beta-distribution>Beta Distribution</a> prior which smoothes the outcomes, meaning:</p><p>\begin{equation}
P(w_{k}|c_{j}) = \frac{n_{k} + \alpha }{n + \alpha |V|}
\end{equation}</p><p>where \(n_{k}\) is the number of occurrences of word \(k\) in class \(C\), and \(n\) is the number of words in total that occurs in class \(C\).</p><h3 id=unknown-words>Unknown Words</h3><p>We ignore them. Because knowing a class has lots of unknown words don&rsquo;t help.</p><h3 id=binary-naive-bayes>Binary Naive Bayes</h3><p>There is another version, which simply clip all the word count \(n_{k}\) to \(1\) for both train and test. You do this by de-duplicating the entire corpus by <strong>DOCUMENT</strong> (i.e. if a word appears twice in the same document, we count it only once).</p><h3 id=benefits>Benefits</h3><ul><li>Doesn&rsquo;t have significant <strong>fragmentation</strong> problems (i.e. many important features clotting up decision)</li><li>Robust to irrelevant features (which cancel each other out)</li></ul></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>