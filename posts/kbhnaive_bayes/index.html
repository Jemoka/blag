<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Naive Bayes</title><meta name=description content="Naive Bayes is a special class of Baysian Network inference problem which follows a specific structure used to solve classification problems.
The Naive Bayes classifier is a Baysian Network of the shape:
(Why is this backwards(ish)? Though we typically think about models as a function M(obs) = cls, the real world is almost kind of opposite; it kinda works like World(thing happening) = things we observe. Therefore, the observations are a RESULT of the class happening."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>Naive Bayes</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#inference--kbhinference-dot-md--with-naive-bayes--kbhnaive-bayes-dot-md><a href=HAHAHUGOSHORTCODE573s9HBHB>inference</a> with <a href=HAHAHUGOSHORTCODE573s10HBHB>Naive Bayes</a></a></li><li><a href=#motivation-bayes-rule--kbhbayes-theorem-dot-md>Motivation: <a href=HAHAHUGOSHORTCODE573s15HBHB>Bayes rule</a></a></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhnaive_bayes/>Naive Bayes</a> is a special class of <a href=/posts/kbhbaysian_network/>Baysian Network</a> <a href=/posts/kbhinference/>inference</a> problem which follows a specific structure used to solve classification problems.</p><p>The <a href=/posts/kbhnaive_bayes/>Naive Bayes</a> classifier is a <a href=/posts/kbhbaysian_network/>Baysian Network</a> of the shape:</p><figure><img src=/ox-hugo/2023-10-03_13-15-54_screenshot.png></figure><p>(Why is this backwards(ish)? Though we typically think about models as a function M(obs) = cls, the real world is almost kind of opposite; it kinda works like World(thing happening) = things we observe. Therefore, the observations are a RESULT of the class happening.)</p><p>We consider, <strong>naively</strong>, \(o_{1:n}\) are all <a href=/posts/kbhbaysian_network/#conditional-independence>conditionally independent</a> on \(c\). From this graph, we can therefore use the <a href=/posts/kbhprobability/#conditional-probability>probability chain rule</a> + <a href=/posts/kbhprobability/#conditional-probability>conditional probability</a> to write that:</p><p>\begin{equation}
P(c, o_{1:n}) = P( c) \prod_{i=1}^{n} P(o_{i} | c)
\end{equation}</p><h2 id=inference--kbhinference-dot-md--with-naive-bayes--kbhnaive-bayes-dot-md><a href=/posts/kbhinference/>inference</a> with <a href=/posts/kbhnaive_bayes/>Naive Bayes</a></h2><p>Recall the definition of <a href=/posts/kbhinference/>inference</a>, for our case here:</p><p>given observations \(o_{1:n}\), we desire to know what&rsquo;s the <a href=/posts/kbhprobability/>probability</a> of \(c\) happening. That is, from <a href=/posts/kbhprobability/#conditional-probability>conditional probability</a>:</p><p>\begin{equation}
P(c | o_{1:n}) = \frac{P(c, o_{1:n})}{P(o_{1:n})}
\end{equation}</p><p>Now, from above we have \(P(c, o_{1:n})\) already. To get the denominator, we invoke <a href=/posts/kbhprobability/#law-of-total-probability>law of total probability</a> to add up the probability of all observations occurring given all classes. That is:</p><p>\begin{equation}
P(o_{1:n}) = \sum_{c \in C} P(c, o_{1:n})
\end{equation}</p><p>You will note that this value \(P(o_{1:n})\) is actually constant as long as the network structure does not change. Therefore, we tend to write:</p><p>\begin{align}
P(c | o_{1:n}) &= \frac{P(c, o_{1:n})}{P(o_{1:n})} \\
&= \kappa P(c, o_{1:n})
\end{align}</p><p>or, that:</p><p>\begin{equation}
P(c|o_{1:n}) \propto P(c, o_{1:n})
\end{equation}</p><p>&ldquo;the probability of a class occurring given the inputs is proportional to the probability of that class occurring along with the inputs&rdquo;</p><h2 id=motivation-bayes-rule--kbhbayes-theorem-dot-md>Motivation: <a href=/posts/kbhbayes_theorem/>Bayes rule</a></h2><figure><img src=/ox-hugo/2023-10-05_09-14-17_screenshot.png></figure><p>This will give us:</p><figure><img src=/ox-hugo/2023-10-05_09-14-32_screenshot.png></figure><p>However, what if we don&rsquo;t want to use the <a href=/posts/kbhprobability/#law-of-total-probability>law of total probability</a> to add up \(P(FB&rsquo;)\)?</p><p>We can actually write a relation that essentially reminds us that the fact that \(P(FB&rsquo;)\) as not dependent on \(TSF\), so we can write:</p><p>\begin{equation}
P(TSF^{1}|FB^{1}) \porpto P(TSF^{1})P(FB^{1} | TSF^{1})
\end{equation}</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>