<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>machine learning</title>
<meta name=description content="CS229: instead of solving a problem, learn from data to find a model to solve the problem approximately.
CS109: machine learning is the act of using some input to come up with some prediction, where the model is parameterized via a bunch of parameters. Hence, parameter learning approaches is how machine learning works.

machine learning excels when we don&rsquo;t know how to program a computer to solve a particular problem; but, what we can do with effort is to collect input, output pairs that demonstrate what we want our programs to do."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>machine learning</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#pros-cons>pros/cons</a></li><li><a href=#applications>applications</a></li><li><a href=#types-of-ml>types of ML</a><ul><li><a href=#supervised-learning>supervised learning</a></li><li><a href=#unsupervised-learning>unsupervised learning</a></li><li><a href=#reinforcement-learning>reinforcement learning</a></li></ul></li><li><a href=#history-of-ml>history of ML</a></li><li><a href=#key-ideas-of-ml>Key Ideas of ML</a><ul><li><a href=#a-culture-shift>a culture shift</a></li></ul></li><li><a href=#imagenet>ImageNet</a><ul><li><a href=#motivation>motivation</a></li><li><a href=#how>how?</a></li><li><a href=#imagenet-large-scale-visual-recognition-challenge--ilsvrc>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</a></li></ul></li><li><a href=#language-model--kbhlanguage-model-dot-md--s><a href=HAHAHUGOSHORTCODE712s8HBHB>Language Model</a>s</a></li><li><a href=#risks-of-ml>Risks of ML</a></li></ul></nav></aside><main><article><div><p><strong>CS229</strong>: instead of solving a problem, learn from data to find a model to solve the problem approximately.</p><p><strong>CS109</strong>: <a href=/posts/kbhmachine_learning/>machine learning</a> is the act of using some input to come up with some prediction, where the model is parameterized via a bunch of <a href=/posts/kbhparameter/>parameter</a>s. Hence, <a href=/posts/kbhparameter_learning/>parameter learning</a> approaches is how machine learning works.</p><hr><p><a href=/posts/kbhmachine_learning/>machine learning</a> excels when we don&rsquo;t know how to program a computer to solve a particular problem; but, what we can do with effort is to collect input, output pairs that demonstrate what we want our programs to do.</p><p><a href=/posts/kbhmachine_learning/>machine learning</a> is a set of tools to learn programs from sets of data.</p><h2 id=pros-cons>pros/cons</h2><ul><li><strong>advantage</strong>: broadly applicable and can solve many programs</li><li><strong>disadvantage</strong>: need (potentially a bunch) of data, and learned results are approximations<ul><li>learned programs can fail in unexpected ways</li><li>approximate solutions maybe better than having no solution</li></ul></li></ul><h2 id=applications>applications</h2><ul><li>spam classification &mdash; in: email, out: spam/not span</li><li>sign detection (stop signs, etc.) &mdash; in: image, output: location of stop sign</li><li><a href=/posts/kbhhouse_price_prediction/>house price prediction</a> &mdash; in: house description, out: price</li></ul><h2 id=types-of-ml>types of ML</h2><p>our job is to do the things in the parentheses</p><h3 id=supervised-learning>supervised learning</h3><p>&ldquo;collect training data with both input and output examples, and make a prediction&rdquo;</p><h3 id=unsupervised-learning>unsupervised learning</h3><ul><li>applies with data with <strong>no labels</strong></li><li>allows us to find <strong>structure in our data</strong> (clustering)</li></ul><h3 id=reinforcement-learning>reinforcement learning</h3><ul><li>learn in an interactive environment (as opposed to static data)</li><li>controlling and games (chess, go)</li></ul><h2 id=history-of-ml>history of ML</h2><p>Samuel 1959: &ldquo;some studies in machine learning using the game of checkers&rdquo;</p><p>Rosenblatt 1958: perception &mdash; <strong>binary classification</strong> (prediction with two possible outputs); implemented on a sota computer; and trained to perform simple <strong>geometric pattern recognition</strong>. 20x20 grid of photocells; output: is a square in the left or right half of the image.</p><p>Noticed a trend of the 50s? Why? <a href=/posts/kbhibm704/>IBM704</a>!</p><p>Nothing then happened for many years. Then, ML started having impact again the last 15 years, especially the last 3 years: this is because we now have more <strong>compute</strong> and more <strong>data</strong>.</p><h2 id=key-ideas-of-ml>Key Ideas of ML</h2><ul><li>ML is largely guided by <strong>benchmarks</strong></li><li>several key datasets for each task (image calssification, detection, etc.)</li><li>algorithmic/model innovations justified by (usually mulitple) benchmarks</li><li>very little math</li><li>rapid progress over the decade</li></ul><h3 id=a-culture-shift>a culture shift</h3><ul><li>2000-2010: emperical progress goes with theoretical results; emphassis on theory, no specialized hardware</li><li>2010-now: appreciable progress comes without theory, emphasis on benchmarks, large-scale purely experimental work</li></ul><h2 id=imagenet>ImageNet</h2><p>Large image classification dataset. <strong>1.2 million train</strong>, <strong>1000</strong> classes.</p><h3 id=motivation>motivation</h3><p>WordNet was at Princeton; and so why don&rsquo;t we have the same thing for images?</p><ul><li>humans know thousands of visual categories</li><li>if we want human-like CV, we need correspondingly large datasets</li><li>goal: <strong>let&rsquo;s populate all of WordNet with 1000 images, per node</strong></li></ul><h3 id=how>how?</h3><p>&ldquo;get a really really good grad student&rdquo;</p><ul><li>&mldr;download all of the image in Flickr</li><li>&mldr;then label them with MTurk<ul><li>(lots of work in task design, annotation, etc.)</li></ul></li></ul><h3 id=imagenet-large-scale-visual-recognition-challenge--ilsvrc>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</h3><ul><li>1.2 million training images for 1,000 classes (roughly balanced)</li><li>50,000 images for 1,000 classes (exactly balanced)</li><li>150,000 images for 1,000 classes (exactly class-balanced, hidden labels)</li></ul><p>top-5 accuracy: 5 predictions per image</p><h4 id=alexnet>AlexNet</h4><p><a href=#alexnet>AlexNet</a> beat <a href=#imagenet>ImageNet</a> from 25 => 15% error</p><p>Its a large CNN. Invented&mldr;.</p><ol><li>ReLU</li><li>Local Response Normalization (not really used anymore)</li><li>Training on GPUs (GTX 580)</li><li>Overlapping pooling</li><li><a href=/posts/kbhsu_cs224n_apr162024/#dropout>Dropout</a></li><li>Data augmentation</li></ol><p>each of these is 0-2% improvements.</p><h4 id=networks-became-bigger>Networks Became Bigger</h4><ul><li>AleNet (8 layers)</li><li>VGG (17 layers)</li><li>ResNet (hundreds of layers)</li></ul><h2 id=language-model--kbhlanguage-model-dot-md--s><a href=/posts/kbhlanguage_model/>Language Model</a>s</h2><p>see <a href=/posts/kbhlanguage_model/>Language Model</a></p><h2 id=risks-of-ml>Risks of ML</h2><p>ML can be used for <strong>beneficial and harmful</strong></p><ul><li>surveillance</li><li>addictive social media</li><li>automated hacking</li></ul><p>Or malfunction:</p><ul><li>underperformance</li><li>biases</li></ul></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>