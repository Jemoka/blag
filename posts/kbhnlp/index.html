<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>NLP</title>
<meta name=description content="Complex System
Language Model A Language Model is a large neural network trained to predict the next token given some context.
&ldquo;Language models can discriminate behavior that they can&rsquo;t reliably generate.&rdquo;
Coherence Generative REVOLUTION
Why probability maximization sucks Its expensive!
Beam Search Take \(k\) candidates Expand \(k\) expansions for each of the \(k\) candidates Choose the highest probability \(k\) candidates \(k\) should be small: trying to maximizing
Branch and Bound See Branch and Bound"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>NLP</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#language-model>Language Model</a></li><li><a href=#coherence>Coherence</a><ul><li><a href=#why-probability-maximization-sucks>Why probability maximization sucks</a></li><li><a href=#beam-search>Beam Search</a></li><li><a href=#branch-and-bound--kbhbranch-and-bound-dot-md><a href=HAHAHUGOSHORTCODE716s1HBHB>Branch and Bound</a></a></li><li><a href=#challenges-of-direct-sampling>Challenges of Direct Sampling</a></li><li><a href=#top-k>Top-K</a></li><li><a href=#nucleaus-sampling>Nucleaus Sampling</a></li></ul></li><li><a href=#correctness>Correctness</a><ul><li><a href=#surface-form-competition>Surface Form Competition</a></li></ul></li><li><a href=#coverage>Coverage</a><ul><li><a href=#hallucination>Hallucination</a></li><li><a href=#in-context-learning>In-Context Learning</a></li></ul></li><li><a href=#future-work>Future Work</a><ul><li><a href=#so>So:</a></li></ul></li><li><a href=#questions>Questions</a></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhcomplex_system/>Complex System</a></p><h2 id=language-model>Language Model</h2><p>A <a href=#language-model>Language Model</a> is a large neural network trained to predict the <strong>next token</strong> given some context.</p><p>&ldquo;Language models can discriminate behavior that they can&rsquo;t reliably generate.&rdquo;</p><h2 id=coherence>Coherence</h2><p><strong>Generative REVOLUTION</strong></p><h3 id=why-probability-maximization-sucks>Why probability maximization sucks</h3><p>Its expensive!</p><h3 id=beam-search>Beam Search</h3><ol><li>Take \(k\) candidates</li><li>Expand \(k\) expansions for each of the \(k\) candidates</li><li>Choose the highest probability \(k\) candidates</li></ol><p>\(k\) should be small: trying to maximizing</p><h3 id=branch-and-bound--kbhbranch-and-bound-dot-md><a href=/posts/kbhbranch_and_bound/>Branch and Bound</a></h3><p>See <a href=/posts/kbhbranch_and_bound/>Branch and Bound</a></p><h3 id=challenges-of-direct-sampling>Challenges of Direct Sampling</h3><p><a href=/posts/kbhdirect_sampling/>Direct Sampling</a> sucks. Its sucks. It sucks. Just sampling from the distribution sucks. This has to do with the fact that assigning slightly lower scores &ldquo;being less confident&rdquo; is exponentially worse.</p><p>The model has to therefore be VERY conservative about giving low confidences; so, it is over confident about worst tokens.</p><h3 id=top-k>Top-K</h3><p>Top-k is too broad, and top</p><h3 id=nucleaus-sampling>Nucleaus Sampling</h3><p>Find the smallest set of tokens that make up to \(p\) probability.</p><h2 id=correctness>Correctness</h2><ul><li>The highest probability answer isn&rsquo;t always right</li><li>Generative models consider every answer, so we want another model to compute the correct answer</li></ul><h3 id=surface-form-competition>Surface Form Competition</h3><p>The <a href=#surface-form-competition>Surface Form Competition</a> problem results when top probabity token &ldquo;steals&rdquo; probability from the other tokens.</p><p>The predicted frequency of a possible string is a main comfounder. And so we can use models to decompose their own predictions:</p><p>Turns out:</p><p>\(P(answer|question) \approx P(answer\ is\ valid)P(answer|domain)\)</p><p>So&mldr;</p><p>\begin{equation}
P(answer\ is\ valid) = \frac{P(answer|question)}{P(answer|domain)}
\end{equation}</p><p>This is better :point_up:. Futher reading: (<a href=#citeproc_bib_item_1>Holtzman et al. 2021</a>)</p><h4 id=domain>Domain</h4><p><a href=#domain>Domain</a> is the context in which that the text may occur.</p><h2 id=coverage>Coverage</h2><p>Why aren&rsquo;t models controllable</p><h3 id=hallucination>Hallucination</h3><ul><li>Language models predict what&rsquo;s most likely</li><li>We hope to control them with natural-language semantics</li></ul><h3 id=in-context-learning>In-Context Learning</h3><p>If we show the model some context which has example input output pairs, it can output. (<a href=#language-model>Language Model</a> model are few shot learners)</p><h4 id=correct-scoring>Correct Scoring</h4><p>We can reverse the output to predict the input to prevent model from loosing information, and use that to rerank the info. Of course, if the model can&rsquo;t generate the desired input, the output is probably missing information.</p><p>Smaller models can be made better because of info reranking.</p><p>Th Degenerative Discriminative Gap.</p><h2 id=future-work>Future Work</h2><p>The fact that the single comma shift the input. What we need is a language to control language behavior.</p><p><strong><strong>The Ability to Control a Model are the Goal of Understand the Model</strong></strong></p><p>We should only claim to understand a model when we can make a theory map about it: &ldquo;when X is fed into the model, we get Y&rdquo;</p><h3 id=so>So:</h3><ul><li>we should look at what the model is biased about (<a href=#surface-form-competition>Surface Form Competition</a>, for instance)</li><li>we would be closer to prime behaviors such that they mimic the human behavior (in pieces, not just &ldquo;complete these tokens&rdquo;) in completion</li><li>We see success as the actual evaluation metrics; we can use machines vs. other machines as the the results</li></ul><h2 id=questions>Questions</h2><p><a href=mailto:ahai@uw.edu>ahai@uw.edu</a></p><p>Marcel Just</p><p>anthropic ai papers</p><p><strong>percy liang</strong></p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>