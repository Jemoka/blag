<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>runtime monitoring</title>
<meta name=description content="Goal: flags situations that might be hazardous when they occur to trigger some fallback mechanism.
Three steps:

check if you are even in the design: operational design domain monitoring
even if you are, check how certain you are: uncertainty quantification
once you know what your model is going to do: failure monitoring

operational design domain monitoring
&ldquo;Out of distribution detection.&rdquo; Insight: if we operate outside of the ODD, our validation results may no longer be valid, so we should monitor whether or not we are in the operational design monitor whether or not we are in the operational design domain."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://bsky.app/profile/jemoka.com class=header-social id=header-twitter><i class="ic fa-brands fa-bluesky"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>runtime monitoring</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#operational-design-domain-monitoring>operational design domain monitoring</a><ul><li><a href=#step-1-represent-the-design-domain>Step 1: represent the design domain</a></li></ul></li><li><a href=#uncertainty-quantification>uncertainty quantification</a><ul><li><a href=#bayesian-model-averaging>Bayesian Model Averaging</a></li></ul></li><li><a href=#failure-monitoring>failure monitoring</a></li></ul></nav></aside><main><article><div><p><strong>Goal</strong>: flags situations that might be hazardous when they occur to trigger some fallback mechanism.</p><p>Three steps:</p><ol><li>check if you are even in the design: <a href=#operational-design-domain-monitoring>operational design domain monitoring</a></li><li>even if you are, check how certain you are: <a href=#uncertainty-quantification>uncertainty quantification</a></li><li>once you know what your model is going to do: <a href=#failure-monitoring>failure monitoring</a></li></ol><h2 id=operational-design-domain-monitoring>operational design domain monitoring</h2><p>&ldquo;Out of distribution detection.&rdquo; Insight: if we operate outside of the ODD, our validation results may no longer be valid, so we should monitor whether or not we are in the operational design monitor whether or not we are in the operational design domain.</p><h3 id=step-1-represent-the-design-domain>Step 1: represent the design domain</h3><ul><li>requires domain knowledge: hand-design some constraints</li><li>requires data: take a data driven approach to interpolate one</li></ul><h4 id=data-driven-approach>data driven approach</h4><p>Take all of the states we saw during validation / in data, and call that &ldquo;in domain.&rdquo;</p><p>We can do some extra interpolation: the set of states that are <em>near</em> our collected states is also in domain&mdash;set ODD as the set of the point <em>near</em> in the data set within a certain distance.</p><ul><li><p>nearest neighbor approach</p><p>Two parameters:</p><ul><li>threshold&mdash;how close does a region has to be to a point to be within domain?</li><li>k&mdash;we have to be near how many neighbors to be in domain?</li></ul></li></ul><ul><li><p>bounding set</p><p>We can take a convex hull of the data, but it may be way too conservative. So, we can our data into a few bits (or perhaps cluster them), and then take a convex hull of each bit, and then union them all together.</p></li></ul><ul><li><p>superlevel set</p><p>we partition our dataset into pieces, and the fit a function for whether or not things are in and out of the operational domain.</p><p>drawback: we need to have data for out of the domain, which is hard. also, we will have to fit distributions, meaning distances needs to be good&mdash;often we need to run a PCA to reduce distances being wonky in large dimensions.</p><ul><li><p>feature collapse</p><p>You maybe tempted to use some kind of a projection, but then may get <a href=#feature-collapse>feature collapse</a> whereby stuff that are out of the domain is squished down to something in domain.</p></li></ul></li></ul><h2 id=uncertainty-quantification>uncertainty quantification</h2><ul><li>output uncertainty: <strong>single input, multiple different possible output</strong> (aleatoric uncertainty / irreducable uncertainty)</li><li>model uncertainty: <strong>model used to predict system behavior is bad</strong> (epistemic uncertainty / educable uncertainty)</li></ul><p>We can cast model uncertainty into output uncertainty by making hte model predict its own variance:</p><h3 id=bayesian-model-averaging>Bayesian Model Averaging</h3><p>Take a Baysian approach averaging over all parameters</p><p>\begin{equation}
p\qty(y|x, D) = \int p\qty(y | x, \theta) p\qty(\theta | D) \dd{\theta}
\end{equation}</p><p>Say you have a discrete set of models:</p><p>\begin{equation}
p \qty(y | x, D) \approx \frac{1}{|\mathcal{M}|} \sum_{\theta \in \mathcal{M}}^{} p\qty(y | x, \theta)
\end{equation}</p><h2 id=failure-monitoring>failure monitoring</h2><p>just run a bunch of calculations; or perhaps just compute states where we may have a higher probability of failure.</p></div></article></main><footer><p id=footer>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>