<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css><link rel=alternate type=application/rss+xml href=/posts/index.xml title=jemoka.com></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><main><div id=title><h1><span class=listhash>#</span>Posts</h1></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmatricies/"'><span class=top><h1><a class=noannot>matricies</a></h1><span class="modbox right">Last edited: <span class=moddate>December 12, 2022</span></span></span>
<span class=summary>matricies are like buckets of numbers. ok, ok, seriously:
matricies are a way of encoding the basis of domain proof: that if Linear Maps are determined uniquely by where they map the basis anyways, why don&rsquo;t we just make a mathematical object that represents that to encode the linear maps.
definition Let \(n\), \(m\) be positive integer. An \(m\) by \(n\) matrix \(A\) is a rectangular array of elements of \(\mathbb{F}\) with \(m\) rows and \(n\) columns:</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmatrix_multiplication/"'><span class=top><h1><a class=noannot>matrix multiplication</a></h1><span class="modbox right">Last edited: <span class=moddate>November 11, 2022</span></span></span>
<span class=summary>matrix multiplication is defined such that the expression \(\mathcal{M}(ST) = \mathcal{M}(S)\mathcal{M}(T)\) holds:
\begin{equation} (AC)_{j,k} = \sum_{r=1}^{n}A_{j,r}C_{r,k} \end{equation}
While matrix multiplication is distributive and associative, it is NOT!!!!!!!!!!! commutative. I hope you can see that \(ST\neq TS\).
memorization its always row-by-column, move down rows first then columns multiply element-wise and add (row times column and add) other ways of thinking about matrix multiplication it is &ldquo;row times column&rdquo;: \((AC)_{j,k} = A_{j, .</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmartingale_model/"'><span class=top><h1><a class=noannot>Martingale Model</a></h1><span class="modbox right">Last edited: <span class=moddate>September 9, 2022</span></span></span>
<span class=summary>The Martingale Model states: if we observed the closing price of the market yesterday, we expect that the market is going to open at the close price yesterday.
Formally:
\begin{equation} E\qty [X_{k}|X_{k-1}, X_{k-2},\ldots] = X_{k-1} \end{equation}
&ldquo;irrespective of what you know, no matter how long the history, the best expectation of today&rsquo;s price is yesterday&rsquo;s price.&rdquo;
This is not a for sure! modeling statement: this is simply the expected value!! That means, after \(\infty\) times of re-running the universe starting &ldquo;yesterday&rdquo;, the new opening price will converge to the last closing price.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmfa_disfluency_measurement/"'><span class=top><h1><a class=noannot>MFA Disfluency Measurement</a></h1><span class="modbox right">Last edited: <span class=moddate>July 7, 2022</span></span></span>
<span class=summary>Applying the MFA aligner upon the Pitt (cookie only) data and performing statistics upon the calculated disfluency information. The ultimate goal is to replicate Wang 2019.
The code is available here.
The (unvalidated, draft) results are reported below:
Mean value reported, standard deviation in parens. For our data, \(N=422\), cases balanced.
Variable AD (Pitt, ours) MCI (Wang) Control (ours) Control (Wang) Silence Duration 28.10 (21.28) 13.55 (5.53) 18.06 (12.52) 7.71 (5.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmeghanani_2021/"'><span class=top><h1><a class=noannot>Meghanani 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.3389/fcomp.2021.624558
One-Liner analyzed spontaneous speech transcripts (only!) from TD and AD patients with fastText and CNN; best was \(83.33\%\) acc.
Novelty threw the NLP kitchen sink to transcripts fastText CNN (with vary n-gram kernel 2,3,4,5 sizes) Notable Methods embeddings seaded by GloVe fastText are much faster, but CNN won out Key Figs the qual results PAR (participant), INV (investigator)
Notes Hey look a review of the field:</span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/65/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/64/ aria-label="Page 64" class=page-link role=button>64</a></li><li class=page-item><a href=/posts/page/65/ aria-label="Page 65" class=page-link role=button>65</a></li><li class="page-item active"><a aria-current=page aria-label="Page 66" class=page-link role=button>66</a></li><li class=page-item><a href=/posts/page/67/ aria-label="Page 67" class=page-link role=button>67</a></li><li class=page-item><a href=/posts/page/68/ aria-label="Page 68" class=page-link role=button>68</a></li><li class=page-item><a href=/posts/page/67/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/113/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>