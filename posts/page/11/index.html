<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css><link rel=alternate type=application/rss+xml href=/posts/index.xml title=jemoka.com></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><main><div id=title><h1><span class=listhash>#</span>Posts</h1></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsum_of_vector_and_subspace/"'><span class=top><h1><a class=noannot>sum of vector and subspace</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>Suppose \(v \in V\), and \(U \subset V\). Then, \(v+U\) is the subset (not a subspace, obviously):
\begin{equation} v + U = \{v+u : u \in U\} \end{equation}</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhaml_dipping_into_pytorch/"'><span class=top><h1><a class=noannot>AML: Dipping into PyTorch</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.
Getting Started To get started, let&rsquo;s open a colab and import Torch!</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhaxler_3_d/"'><span class=top><h1><a class=noannot>Axler 3.D</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>isomorphisms. Somebody&rsquo;s new favourite word since last year.
Key Sequence we showed that a linear map&rsquo;s inverse is unique, and so named the inverse \(T^{-1}\) we then showed an important result, that injectivity and surjectivity implies invertability this property allowed us to use invertable maps to define isomorphic spaces, naming the invertable map between them as the isomorphism we see that having the same dimension is enough to show invertability (IFF), because we can use basis of domain to map the basis of one space to another we then use that property to establish that matricies and linear maps have an isomorphism between them: namely, the matrixify operator \(\mathcal{M}\).</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhisomorphism/"'><span class=top><h1><a class=noannot>isomorphism</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>An isomorphism is an invertable Linear Map. Two vector spaces are called isomorphic if there is an isomorphism from one to another.
&ldquo;A linear map that maintain the correct structure of the structure.&rdquo;
This makes the vector spaces that are isomorphic &ldquo;equivalent&rdquo;, because the isomorphism is the equivalence relationship. Of course, they are still not equal.
Generally, isomorphisms can only be built between vector spaces over the same field.
additional information matrices We know we can represent Linear Maps as matricies.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhinvertability/"'><span class=top><h1><a class=noannot>invertability</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>A Linear Map is invertable if it can be undone. It is called a nonsingular matrix
constituents A linear map \(T \in \mathcal{L}(V,W)\)
requirements A Linear Map \(T \in \mathcal{L}(V,W)\) is called invertable if \(\exists T^{-1} \in \mathcal{L}(W,V): T^{-1}T=I \in \mathcal{L}(V), TT^{-1} = I \in \mathcal{L}(W)\).
&ldquo;a map is invertable if there is an inverse&rdquo;: that combining the commutable inverse and itself will result in the identity map.
additional information matrix invertability Matrices whose determinants are not \(0\) (i.</span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/10/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/9/ aria-label="Page 9" class=page-link role=button>9</a></li><li class=page-item><a href=/posts/page/10/ aria-label="Page 10" class=page-link role=button>10</a></li><li class="page-item active"><a aria-current=page aria-label="Page 11" class=page-link role=button>11</a></li><li class=page-item><a href=/posts/page/12/ aria-label="Page 12" class=page-link role=button>12</a></li><li class=page-item><a href=/posts/page/13/ aria-label="Page 13" class=page-link role=button>13</a></li><li class=page-item><a href=/posts/page/12/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/126/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>