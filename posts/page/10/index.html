<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css><link rel=alternate type=application/rss+xml href=/posts/index.xml title=jemoka.com></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><main><div id=title><h1><span class=listhash>#</span>Posts</h1></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhproduct_of_vector_spaces/"'><span class=top><h1><a class=noannot>Product of Vector Space</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>A product of vector spaces is a vector space formed by putting an element from each space into an element of the vector.
constituents Suppose \(V_1 \dots V_{m}\) are vector spaces over the same field \(\mathbb{F}\)
requirements Product between \(V_1 \dots V_{m}\) is defined:
\begin{equation} V_1 \times \dots \times V_{m} = \{(v_1, \dots, v_{m}): v_1 \in V_1 \dots v_{m} \in V_{m}\} \end{equation}
&ldquo;chain an element from each space into another vector&rdquo;</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhproduct_summation_map/"'><span class=top><h1><a class=noannot>product summation map</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>Let \(U_1, \dots, U_{m}\) be subspaces of \(V\); we define a linear
We define \(\Gamma\) to be a map \(U_1 \times \dots U_{m} \to U_1 + \dots + U_{m}\) such that:
\begin{equation} \Gamma (u_1, \dots, u_{m}) = u_1 + \dots + u_{m} \end{equation}
Essentially, \(\Gamma\) is the sum operation of the elements of the tuple made by the Product of Vector Spaces.
\(U_1 + \dots + U_{m}\) is a direct sum IFF \(\Gamma\) is injective Proof:</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhquotient_map/"'><span class=top><h1><a class=noannot>quotient map</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>The quotient map \(\pi\) is the Linear Map \(V \to V / U\) such that:
\begin{equation} \pi(v) = v+U \end{equation}
for \(v \in V\).
I.e.: the quotient map is affine subsetification map given a vector.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsum_of_vector_and_subspace/"'><span class=top><h1><a class=noannot>sum of vector and subspace</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>Suppose \(v \in V\), and \(U \subset V\). Then, \(v+U\) is the subset (not a subspace, obviously):
\begin{equation} v + U = \{v+u : u \in U\} \end{equation}</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhaml_dipping_into_pytorch/"'><span class=top><h1><a class=noannot>AML: Dipping into PyTorch</a></h1><span class="modbox right">Last edited: <span class=moddate>January 1, 2023</span></span></span>
<span class=summary>Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.
Getting Started To get started, let&rsquo;s open a colab and import Torch!</span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/9/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/8/ aria-label="Page 8" class=page-link role=button>8</a></li><li class=page-item><a href=/posts/page/9/ aria-label="Page 9" class=page-link role=button>9</a></li><li class="page-item active"><a aria-current=page aria-label="Page 10" class=page-link role=button>10</a></li><li class=page-item><a href=/posts/page/11/ aria-label="Page 11" class=page-link role=button>11</a></li><li class=page-item><a href=/posts/page/12/ aria-label="Page 12" class=page-link role=button>12</a></li><li class=page-item><a href=/posts/page/11/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/126/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>