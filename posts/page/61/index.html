<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css><link rel=alternate type=application/rss+xml href=/posts/index.xml title=jemoka.com></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><main><div id=title><h1><span class=listhash>#</span>Posts</h1></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmatricies/"'><span class=top><h1><a class=noannot>matricies</a></h1><span class="modbox right">Last edited: <span class=moddate>September 9, 2022</span></span></span>
<span class=summary>we never do them anyways
additional information multiplying matricies its always row-by-column, move down rows first then columns multiply element-wise and add (row times column and add) invertability Matrices whose determinants are not \(0\) (i.e. it is invertable) is called &ldquo;nonsingular matrix&rdquo;. If it doesn&rsquo;t have an inverse, it is called a singular matrix.
Gaussian elimination The point of Gaussian elimination is to solve/identiy-ify a linear equation. Take, if you have a matrix expression:</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmartingale_model/"'><span class=top><h1><a class=noannot>Martingale Model</a></h1><span class="modbox right">Last edited: <span class=moddate>September 9, 2022</span></span></span>
<span class=summary>The Martingale Model states: if we observed the closing price of the market yesterday, we expect that the market is going to open at the close price yesterday.
Formally:
\begin{equation} E\qty [X_{k}|X_{k-1}, X_{k-2},\ldots] = X_{k-1} \end{equation}
&ldquo;irrespective of what you know, no matter how long the history, the best expectation of today&rsquo;s price is yesterday&rsquo;s price.&rdquo;
This is not a for sure! modeling statement: this is simply the expected value!! That means, after \(\infty\) times of re-running the universe starting &ldquo;yesterday&rdquo;, the new opening price will converge to the last closing price.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmfa_disfluency_measurement/"'><span class=top><h1><a class=noannot>MFA Disfluency Measurement</a></h1><span class="modbox right">Last edited: <span class=moddate>July 7, 2022</span></span></span>
<span class=summary>Applying the MFA aligner upon the Pitt (cookie only) data and performing statistics upon the calculated disfluency information. The ultimate goal is to replicate Wang 2019.
The code is available here.
The (unvalidated, draft) results are reported below:
Mean value reported, standard deviation in parens. For our data, \(N=422\), cases balanced.
Variable AD (Pitt, ours) MCI (Wang) Control (ours) Control (Wang) Silence Duration 28.10 (21.28) 13.55 (5.53) 18.06 (12.52) 7.71 (5.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmartinc_2021/"'><span class=top><h1><a class=noannot>Martinc 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.3389/fnagi.2021.642647
One-Liner Combined bag-of-words on transcript + ADR on audio to various classifiers for AD; ablated BERT&rsquo;s decesion space for attention to make more easy models in the future.
Novelty Pre-processed each of the two modalities before fusing it (late fusion) Archieved \(93.75\%\) accuracy on AD detection The data being forced-aligned and fed with late fusion allows one to see what sounds/words the BERT model was focusing on by just focusing on the attention on the words Notable Methods Used classic cookie theft data bag of words to do ADR but for words multimodality but late fusion with one (hot-swappable) classifier Key Figs How they did it This is how the combined the forced aligned (:tada:) audio and transcript together.</span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhmeghanani_2021/"'><span class=top><h1><a class=noannot>Meghanani 2021</a></h1><span class="modbox right">Last edited: <span class=moddate>June 6, 2022</span></span></span>
<span class=summary>DOI: 10.3389/fcomp.2021.624558
One-Liner analyzed spontaneous speech transcripts (only!) from TD and AD patients with fastText and CNN; best was \(83.33\%\) acc.
Novelty threw the NLP kitchen sink to transcripts fastText CNN (with vary n-gram kernel 2,3,4,5 sizes) Notable Methods embeddings seaded by GloVe fastText are much faster, but CNN won out Key Figs the qual results PAR (participant), INV (investigator)
Notes Hey look a review of the field:</span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/60/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/59/ aria-label="Page 59" class=page-link role=button>59</a></li><li class=page-item><a href=/posts/page/60/ aria-label="Page 60" class=page-link role=button>60</a></li><li class="page-item active"><a aria-current=page aria-label="Page 61" class=page-link role=button>61</a></li><li class=page-item><a href=/posts/page/62/ aria-label="Page 62" class=page-link role=button>62</a></li><li class=page-item><a href=/posts/page/63/ aria-label="Page 63" class=page-link role=button>63</a></li><li class=page-item><a href=/posts/page/62/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/101/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer><p id=footer>&copy; 2019-2022 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>