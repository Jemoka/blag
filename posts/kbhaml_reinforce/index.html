<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>AML: REINFORCE(ment learning)</title><meta name=description content="Woof. As I begin to write this I should add that this unit is going to be conceptually dense. Though we are teaching one particular algorithm (incidentally, named, REINFORCE), the world of reinforcement learning is build by one, if not many, college level classes.
So if anything, I would focus on getting the conceptual flavor of how these problems are formulated and discuses. If you can be along for the mathematical and algorithmic journey, then even better &mdash; but by no means required or expected&mldr; There&rsquo;s still lots for all of us to learn together."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://twitter.com/jemokajack class=header-social id=header-twitter><i class="ic fa-brands fa-twitter"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>AML: REINFORCE(ment learning)</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li><li><a href=#rl-in-brief>RL, in Brief</a><ul><li><a href=#the-environment-state-action-and-policy>The Environment: State, Action, and Policy</a></li><li><a href=#reward>Reward</a></li><li><a href=#cumulative-discounted-reward>Cumulative Discounted Reward</a></li><li><a href=#policy-gradient-theorem>Policy Gradient Theorem</a></li><li><a href=#monte-carlo-and-reinforce>Monte-Carlo and REINFORCE</a></li></ul></li></ul></nav></aside><main><article><div><p>Woof. As I begin to write this I should add that <strong>this unit is going to be conceptually dense</strong>. Though we are teaching one particular algorithm (incidentally, named, REINFORCE), the world of reinforcement learning is build by one, if not many, college level classes.</p><p>So if anything, I would focus on getting the conceptual flavor of how these problems are formulated and discuses. If you can be along for the mathematical and algorithmic journey, then even better &mdash; but by no means required or expected&mldr; There&rsquo;s still lots for all of us to learn together.</p><p>Speaking of college level classes, I <em>loved</em> the detailed and accessible overview of Reinforcement Learning methods by Professors Charles Isbell and Michael Littlman from Georgia Tech CoC. If you find yourself gravitating towards the topic of this unit, go check them out: <a href=https://omscs.gatech.edu/cs-7642-reinforcement-learning-course-videos>https://omscs.gatech.edu/cs-7642-reinforcement-learning-course-videos</a></p><p>Ok. Let&rsquo;s dive in.</p><h2 id=motivation>Motivation</h2><p>We are used to a clear, <strong>differentiable</strong> loss function. One particular exercise in class we do a lot is to shout out a problem, and think about its loss function:</p><ul><li>&ldquo;classifying Pokemon!&rdquo; &mldr; &ldquo;cross entropy!&rdquo;</li><li>&ldquo;generating stock price!&rdquo; &mldr; &ldquo;MSE!&rdquo;</li><li>&ldquo;making pictures of rice!&rdquo; &mldr; &ldquo;GAN non-saturating loss!&rdquo;</li></ul><p>and so on. Regardless of the classification/regression difference, you will note that these functions are all of the shape:</p><p>\begin{align}
&amp;f(\hat{y}, y) = \text{single float value}
\end{align}</p><p>Meaning, it takes <strong>two vectors</strong>&mdash;the <em>output</em> (&ldquo;prediction&rdquo;, \(\hat{y}\)) of the network, and the <em>desired output</em> (&ldquo;target&rdquo;, \(y\)) in your training data, and produces (sometimes with much mathematical gymnastics) a single scalar value representing which we try to optimize to be lower.</p><p>Note that, regardless of <strong>supervised learning</strong> (like Pokemon classification; we have input, desired targets, and actual output) or <strong>unsupervised learning</strong> (like GAN rice generation; we have only the desired targets and actual output), we <em>have the desired targets</em> in hand. We <em>know</em> what the model is supposed to do (i.e. have many examples of correct behavior), and are just teaching the model to do so one way or other.</p><p>But what if&mldr;. we <em>don&rsquo;t</em> know the correct behavior of the model? Can you brainstorm some tasks that would very well might want to automate using ML, but can&rsquo;t provide precise labels for the desired action?</p><p>&mldr;</p><p>Take, for instance, the task of <a href=https://gymnasium.farama.org/environments/mujoco/humanoid_standup/>teaching this poor stick figure how to stand up</a>:</p><figure><img src=/ox-hugo/2023-04-30_12-37-54_screenshot.png alt="Figure 1: aw so sad"><figcaption><p><span class=figure-number>Figure 1: </span>aw so sad</p></figcaption></figure><p>you are given a list of forces currently hitting the figure, and you are to produce a list of forces the figure&rsquo;s limbs should produce.</p><p>Of course you can&rsquo;t know precisely the labels at every given moment: there are no &ldquo;best&rdquo; or, arguably, even a &ldquo;correct&rdquo; strategy for standing the figure up. There&rsquo;s no labels which you can use to even begin to approach this task!</p><p>What to do?</p><p><strong>In come Reinforcement Learning (RL)</strong></p><h2 id=rl-in-brief>RL, in Brief</h2><p>Ok, this is where the math will begin. I encourage you to take a piece of paper and start writing down each symbol we define together, and refer to that piece of paper copiously to understand the expressions.</p><p>If you want to learn this more, the conceptual basis we are working with is called <strong>policy gradient</strong>, specifically the <strong>REINFORCE</strong> algorithm. This is <em>not even close</em> to being the only way to approach the Reinforcement Learning task; but its one fairly interesting and successful approach.</p><h3 id=the-environment-state-action-and-policy>The Environment: State, Action, and Policy</h3><p>Three variables underlie the basics of <strong>Reinforcement Learning</strong>:</p><ul><li><strong>state</strong> \(s_{t}\): the &ldquo;situation&rdquo; of the <strong>environment</strong>, what can be &ldquo;observed&rdquo;; for our example above, this looks like the forces on each limb of our humanoid.</li><li><strong>action</strong> \(a\): a certain perturbation one can do to the <strong>environment</strong>; for our example, this looks like moving (&ldquo;translating&rdquo;/&ldquo;applying force on&rdquo;) one or many limbs.</li><li><strong>policy</strong> \(\pi\): the <strong>policy</strong> is a function which takes the <strong>state</strong> as input, and produces a probability distribution (think &ldquo;softmax&rdquo;) over all the <strong>actions</strong> one could choose. We will talk extensively about this shortly.</li></ul><hr><p>IMPORTANT NOTE: policy is as function \(\pi(s_{t})\), literally a <em>function named pi</em>. It has nothing to do with the ratio between the radius and circumference of a circle. Its <em>just called pi&mldr;</em> Unfortunately, we are working to stick to the language used by current literature, but sometimes their symbol choice is rather deranged.</p><hr><h3 id=reward>Reward</h3><p>In lieu of a loss function, <strong>Reinforcement Learning</strong> is a class of models that learn from a numerical signal called <strong>reward</strong>. The reward function typically looks like this:</p><p>\begin{equation}
r_{t}(s_{t},a_{t}) = \text{single float value}
\end{equation}</p><p>Instead of calculating the difference between the desired and actual output of the model, the <strong>reward</strong> signal scores <em>how good taking a certain action is</em> in an environment. It takes two vectors as input: the <strong>state</strong> and an <strong>action</strong> on the state, to produce a certain score.</p><p>Unlike what we are used to with the loss, this <strong>reward</strong> value is <em>not</em> differentiable w.r.t. the parameters of the network! The action is a <em>sample</em> from the distribution; so this score can be generated however you&rsquo;d like. Furthermore, unlike what we are used to with <strong>loss</strong>, a <strong>higher</strong> <strong>reward</strong> value means a better action.</p><p>For our example, for instance, this could mean if we raised the agent&rsquo;s head higher at that step.</p><h3 id=cumulative-discounted-reward>Cumulative Discounted Reward</h3><h3 id=policy-gradient-theorem>Policy Gradient Theorem</h3><h3 id=monte-carlo-and-reinforce>Monte-Carlo and REINFORCE</h3></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>