<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>SU-CS224N APR232024</title>
<meta name=description content="Evaluating Machine Translation
BLEU
Compare machine vs. multiple-human reference translations. Uses N-Gram geometric mean&mdash;the actual n gram size isn&rsquo;t super special.
Original idea to have multiple reference translations&mdash;but maybe people to do this only one reference translation&mdash;good score in expectation.
Limitations

good translation can get a bad BLEU because it has low n gram overlap
penalty to too-short system translations (i.e. translating only easy sentences isn&rsquo;t a good metric)
you really can&rsquo;t get to 100 in BLEU because of variations in text

attention
Given a vector of values, a vector query, attention is a technique to compute a weighted sum of the values depending on the query."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>SU-CS224N APR232024</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#evaluating-machine-translation>Evaluating Machine Translation</a><ul><li><a href=#bleu>BLEU</a></li></ul></li><li><a href=#attention>attention</a><ul><li><a href=#motivation>motivation</a></li><li><a href=#implementation>implementation</a></li></ul></li></ul></nav></aside><main><article><div><h2 id=evaluating-machine-translation>Evaluating Machine Translation</h2><h3 id=bleu>BLEU</h3><p>Compare machine vs. multiple-human reference translations. Uses <a href=/posts/kbhn_grams/>N-Gram</a> geometric mean&mdash;the actual n gram size isn&rsquo;t super special.</p><p>Original idea to have <strong>multiple reference translations</strong>&mdash;but maybe people to do this only one reference translation&mdash;good score <strong>in expectation</strong>.</p><h4 id=limitations>Limitations</h4><ul><li>good translation can get a bad BLEU because it has low n gram overlap</li><li>penalty to too-short system translations (i.e. translating only easy sentences isn&rsquo;t a good metric)</li><li>you really can&rsquo;t get to 100 in BLEU because of variations in text</li></ul><h2 id=attention>attention</h2><p>Given a vector of <strong>values</strong>, a vector <strong>query</strong>, attention is a technique to compute a weighted sum of the values depending on the query.</p><h3 id=motivation>motivation</h3><p>machine translation problem&mdash;naive <a href=/posts/kbhsu_cs224n_apr182024/#lstm>LSTM</a> implementation has to stuff the entire information about a sentence into a single ending vector.</p><ul><li>improves performance</li><li>more human like model for the MT</li><li>solves the bottleneck problem</li><li>helps solving <a href=/posts/kbhsu_cs224n_apr182024/#vanishing-gradients>Vanishing Gradients</a></li><li>interoperability &mdash; provides soft phrase-level alignments, and know what is being translated</li></ul><h3 id=implementation>implementation</h3><p><strong>each step of the decoder, we will insert direct connections to the encoder to look at particular parts of the input source sequence</strong></p><p>dot every output state against every input state, softmax and add against the source sequence input.</p><p>with encoder \(h_{j}\) and decoder \(s_{k}\):</p><h4 id=dot-product-attention>dot product attention</h4><p>\begin{align}
e_{i} = s^{T} h_{i}
\end{align}</p><p><strong>limitation</strong>: LSTM latent layers are a little bit too busy&mdash;some of the information is not as useful as others&mdash;also forces everything to have dimension-to-dimension match</p><h4 id=multiplicative-attention>multiplicative attention</h4><p>&ldquo;learn a map from encoder vectors to decoder vectors&mdash;working out the right place to pay attention by learning it&rdquo;</p><p>\begin{equation}
e_{i} = s^{T} W h_{i}
\end{equation}</p><p><strong>limitation</strong>: lots of parameters to learn in \(W\) for no good reason</p><h4 id=reduced-rank-multiplicative-attention>reduced-rank multiplicative attention</h4><p>\begin{equation}
e_{i} = s^{T} Q R h_{i} = (Q s^{T})^{T} (R h_{i})
\end{equation}</p><p>essentially, why don&rsquo;t we project \(s\) and \(h\) down to smaller dimensions before the dot product is taken?</p><p>this motivates also transformers</p><h4 id=additive-attention>additive attention</h4><p>\begin{equation}
e_{i} = v^{T} \text{tanh} \qty(W_1 h_{i} + W_{2} s)
\end{equation}</p><p>where \(v\) and \(W_{j}\) are learned.</p></div></article></main><footer><p id=footer>&copy; 2019-2024 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>