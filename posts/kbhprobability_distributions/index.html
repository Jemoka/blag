<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Commissioner:wght@100;300;400;500;700&family=IBM+Plex+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,100;0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>probability distribution</title><meta name=description content="probability distributions &ldquo;assigns probability to outcomes&rdquo;
\(X\) follows distribution \(D\). \(X\) is a &ldquo;\(D\) random variable&rdquo;, where \(D\) is some distribution (normal, gaussian, etc.)
syntax: \(X \sim D\).
Each distribution has three properties:
variables (what is being modeled) values (what values can they take on) parameters (how many degrees of freedom do we have) Methods of Compressing the Parameters of a Distribution So, for instance, for a binary distribution with \(n\) variables which we know nothing about, we have:"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/global.css><link rel=stylesheet href=/css/syntax.css></head><body><div class=center-clearfix><header><span id=header-name onclick='window.location.href="/"' style=cursor:pointer>Houjun Liu</span><div id=socialpanel><a href=https://www.jemoka.com/search/ class=header-social id=header-search><i class="ic fa-solid fa-magnifying-glass"></i></i></a>
<a href=https://github.com/Jemoka/ class=header-social id=header-github><i class="ic fa-brands fa-github"></i></a>
<a href=https://maly.io/@jemoka class=header-social id=header-twitter><i class="ic fa-brands fa-mastodon"></i></a>
<a href=https://www.reddit.com/user/Jemoka/ class=header-social id=header-reddit><i class="ic fa-brands fa-reddit"></i></a></div></header><div id=title><h1>probability distribution</h1><span class=tagbox></span></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#methods-of-compressing-the-parameters-of-a-distribution>Methods of Compressing the Parameters of a Distribution</a><ul><li><a href=#assuming-independence>assuming independence</a></li><li><a href=#decision-tree>decision tree</a></li><li><a href=#baysian-networks>Baysian networks</a></li></ul></li><li><a href=#types-of-probability-distributions>types of probability distributions</a></li><li><a href=#distribution-of-note>distribution of note</a><ul><li><a href=#uniform-distribution>uniform distribution</a></li><li><a href=#gaussian-things>Gaussian Things</a></li></ul></li><li><a href=#three-ways-of-analysis>three ways of analysis</a><ul><li><a href=#probability-mass-function>probability mass function</a></li><li><a href=#probability-density-functions>probability density functions</a></li><li><a href=#cumulative-distribution-function>cumulative distribution function</a></li><li><a href=#quantile-function>quantile function</a></li></ul></li></ul></nav></aside><main><article><div><p><a href=/posts/kbhprobability_distributions/>probability distributions</a> &ldquo;assigns probability to outcomes&rdquo;</p><p>\(X\) follows distribution \(D\). \(X\) is a &ldquo;\(D\) random variable&rdquo;, where \(D\) is some distribution (<a href=/posts/kbhaxler_7_a/#normal>normal</a>, gaussian, etc.)</p><p>syntax: \(X \sim D\).</p><p>Each distribution has three properties:</p><ul><li>variables (what is being modeled)</li><li>values (what values can they take on)</li><li>parameters (how many degrees of freedom do we have)</li></ul><h2 id=methods-of-compressing-the-parameters-of-a-distribution>Methods of Compressing the Parameters of a Distribution</h2><p>So, for instance, for a binary distribution with \(n\) variables which we know nothing about, we have:</p><p>\begin{equation}
2^{n} - 1
\end{equation}</p><p>parameters (\(2^{n}\) different possibilities of combinations, and \(1\) non-free variables to ensure that the distribution add up)</p><h3 id=assuming-independence>assuming independence</h3><p>HOWEVER, if the variables were <a href=/posts/kbhprobability/#independence>independent</a>, this becomes much easier. Because the variables are independent, we can claim that:</p><p>\begin{equation}
p(x_{1\dots n}) = \prod_{i}^{} p(x_{i)}
\end{equation}</p><h3 id=decision-tree>decision tree</h3><p>For instance, you can have a decision tree which you selectively ignore some combinations.</p><figure><img src=/ox-hugo/2023-09-28_10-13-07_screenshot.png></figure><p>In this case, we ignored \(z\) if both \(x\) and \(y\) are \(0\).</p><h3 id=baysian-networks>Baysian networks</h3><p>see <a href=/posts/kbhbaysian_network/>Baysian Network</a></p><h2 id=types-of-probability-distributions>types of probability distributions</h2><ul><li><a href=/posts/kbhdiscrete_distribution/>discrete distribution</a></li><li><a href=/posts/kbhcontinuous_distribution/>continuous distribution</a></li><li><a href=/posts/kbhjoint_probability_distribution/>joint probability distribution</a></li></ul><h2 id=distribution-of-note>distribution of note</h2><ul><li><a href=#uniform-distribution>uniform distribution</a></li><li>gaussian distributions<ul><li><a href=#gaussian-distribution>Gaussian distribution</a></li><li><a href=#truncated-gaussian-distribution>Truncated Gaussian distribution</a></li><li><a href=#gaussian-mixture-model>Gaussian mixture model</a></li></ul></li></ul><h3 id=uniform-distribution>uniform distribution</h3><p>\begin{equation}
X \sim Uni(\alpha, \beta)
\end{equation}</p><p>\begin{equation}
f(x) = \begin{cases}
\frac{1}{\beta -\alpha }, 0\leq x \leq 10 \\0
\end{cases}
\end{equation}</p><p>\begin{equation}
E[x] = \frac{1}{2}(\alpha +\beta)
\end{equation}</p><p>\begin{equation}
Var(X) = \frac{1}{12}(\beta -\alpha )^{2}
\end{equation}</p><h3 id=gaussian-things>Gaussian Things</h3><h4 id=standard-normal-density-function>standard normal density function</h4><p>This is a function used to model many Gaussian distributions.</p><p>\begin{equation}
\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^{2}}{2}}
\end{equation}</p><h4 id=gaussian-distribution>Gaussian distribution</h4><p>\begin{equation}
X \sim N(\mu, \sigma^{2})
\end{equation}</p><p>Its <a href=#probability-density-functions>PDF</a> is:</p><p>\begin{equation}
\mathcal{N}(x \mid \mu, \sigma^{2}) = \frac{1}{\sigma} \phi \qty(\frac{x-\mu}{\sigma})
\end{equation}</p><p>where, \(\phi\) is the <a href=#standard-normal-density-function>standard normal density function</a></p><p>And its expectations:</p><p>\(E(X) = \mu\)</p><p>\(Var(X) = \sigma^{2}\)</p><h4 id=truncated-gaussian-distribution>Truncated Gaussian distribution</h4><p>Sometimes, we don&rsquo;t want to use a <a href=#gaussian-distribution>Gaussian distribution</a> for values above or below a threshold (say if they are physically impossible). In those cases, we have some:</p><p>\begin{equation}
X \sim N(\mu, \sigma^{2}, a, b)
\end{equation}</p><p>bounded within the interval of \((a,b)\). The <a href=#probability-density-functions>PDF</a> of this function is given by:</p><p>\begin{equation}
N(\mu, \sigma^{2}, a, b) = \frac{\frac{1}{\sigma} \phi \qty(\frac{x-\mu }{\sigma })}{\Phi \qty(\frac{b-\mu }{\sigma }) - \Phi \qty(\frac{a-\mu}{\sigma})}
\end{equation}</p><p>where:</p><p>\begin{equation}
\Phi = \int_{-\infty}^{x} \phi (x&rsquo;) \dd{x&rsquo;}
\end{equation}</p><p>and where \(\phi\) is the <a href=#standard-normal-density-function>standard normal density function</a>.</p><h4 id=gaussian-mixture-model>Gaussian mixture model</h4><p>Gaussian models are typically <a href=/posts/kbhunimodal/>unimodal</a>, meaning they have one peak (things decrease to the left of that peak, increases to the right of it).</p><p>Therefore, in order to model something more complex with multiple peaks, we just weighted average multiple gaussian models</p><p>\begin{equation}
p(x | \dots ) = \sum_{i-1}^{n}p_i \mathcal{N}(x | u_{i}, {\sigma_{i}}^{2})
\end{equation}</p><p>whereby,</p><h2 id=three-ways-of-analysis>three ways of analysis</h2><h3 id=probability-mass-function>probability mass function</h3><p><a href=#probability-mass-function>PMF</a> is a function that maps possible outcomes of a discrete random variables to the corresponding probability.</p><p>\(P(event) = value\)</p><h3 id=probability-density-functions>probability density functions</h3><p><a href=#probability-density-functions>PDF</a>s is a function that maps continuous random variables to the corresponding probability.</p><p>and \(\int P\ dE = 1\) because of <a href=/posts/kbhprobability/#axiom-of-probability>axiom of probability</a></p><h3 id=cumulative-distribution-function>cumulative distribution function</h3><p>What is the probability that a <a href=/posts/kbhrandom_variables/>random variable</a> takes on value less tha</p><p>\begin{equation}
cdf_{x}(x) = P(X&lt;x) = \int_{-\infty}^{x} p(x&rsquo;) dx'
\end{equation}</p><h3 id=quantile-function>quantile function</h3><p>\begin{equation}
\text{quantile}_{X}(\alpha)
\end{equation}</p><p>is the value \(x\) such that:</p><p>\begin{equation}
P(X \leq x) = \alpha
\end{equation}</p><p>That is, the <a href=#quantile-function>quantile function</a> returns the minimum value of \(x\) at which point a certain <a href=#cumulative-distribution-function>cumulative distribution</a> value desired is achieved.</p></div></article></main><footer><p id=footer>&copy; 2019-2023 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div></body></html>