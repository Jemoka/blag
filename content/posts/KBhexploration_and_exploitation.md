+++
title = "Exploration and Exploitation"
author = ["Houjun Liu"]
draft = false
+++

You are the president, and you are trying to choose the secretary of state. You can only interview people in sequence, and you have to hire on the spot. There are a known number of candidates. We want to maximize the probability of selecting the best candidate. You are given no priors.

How do we know which candidates we explore, and which candidates we exploit. Roughly 37%

---


## Binary Bandit {#binary-bandit}

We are playing with \\(n\\) binary slot machines.

1.  arm \\(j\\) pays off \\(1\\) with probability \\(\theta\_{j}\\), and pays of \\(0\\) otherwise. we do not know $&theta;<sub>j</sub>$s exogenously and have to learn it
2.  we only have \\(h\\) pulls in total across all \\(n\\) slot machines

As we perform \\(k\\) pulls, we can keep track of a separate [Beta Distribution]({{< relref "KBhbaysian_parameter_learning.md#beta-distribution" >}}) representing the probability of success for each of the slot machines.

If our distribution for a given machine is super flat, perhaps its good to try it even if the mean reward is lower if we have


### Aside: Computing the Distribution {#aside-computing-the-distribution}

We want to compute \\(\rho\_{a}\\):

\begin{equation}
\rho\_{a} = P(win\_{a} | w\_{a}, l\_{a}) = \int\_{0}^{1} \theta \times Beta(\theta | w\_{a}+1, l\_{a}+1) \dd{\theta}
\end{equation}

where, \\(w\_{a}\\) is the number of wins observed for slot machine \\(a\\), and \\(w\_{b}\\) is the number observed for slot machine \\(b\\).

This is exactly the \\(\mathbb{E}[Beta(w\_{a}+1, l\_{a}+1)] = \frac{w\_{a}+1}{(w\_{a}+1)+(l\_{a}+1)}\\)


### Approximate Exploration Strategies {#approximate-exploration-strategies}

-   [Undirected Exploration]({{< relref "KBhundirected_exploration.md#undirected-exploration" >}})
-   [Directed Exploration]({{< relref "KBhdirected_exploration.md#directed-exploration" >}})


### Optimal Exploration {#optimal-exploration}

Suppose we have offline statistic regarding wins and losses as our state:

\begin{equation}
w\_1, l\_{1}, \dots, w\_{n},  l\_{n}
\end{equation}

We construct a [value function]({{< relref "KBhaction_value_function.md#id-0b1509e0-4d88-44d1-b6fa-fe8e86d200bb-value-function" >}}):

\begin{equation}
U^{\*}([w\_1, l\_{1}, \dots, w\_{n},  l\_{n}]) = \max\_{a} Q^{\*}([w\_1, l\_{1}, \dots, w\_{n}, l\_{n}], a)
\end{equation}

our policy is the [action-value]({{< relref "KBhaction_value_function.md" >}}) policy:

\begin{equation}
U^{\*}([w\_1, l\_{1}, \dots, w\_{n},  l\_{n}]) = \arg\max\_{a} Q^{\*}([w\_1, l\_{1}, \dots, w\_{n}, l\_{n}], a)
\end{equation}

Now, how do we go about calculating the [action-value]({{< relref "KBhaction_value_function.md" >}}):

\begin{align}
Q ([w\_1, l\_{1}, \dots, w\_{n}, l\_{n}], a) =\ & \frac{w\_{a}+1}{w\_{a}+l\_{a}+2} (1 + U^{\*}(\dots, w\_{a}+1, l\_{a}, \dots)) \\\&+ \qty(1-\frac{w\_{a}+1}{w\_{a}+l\_{a}+2})(1 + U^{\*}(\dots, w\_{a}, l\_{a}+1, \dots))
\end{align}

"the probability of you win" (expectation of [Beta Distribution]({{< relref "KBhbaysian_parameter_learning.md#beta-distribution" >}})), times the instantaneous reward you win + the utility of you doing that.

To solve this, note that at time \\(t=k\\), your \\(U^{\*}\\) is \\(0\\) because you have nothing to pull anymore. Then, you can back up slowly to get each previous state.
